2023-01-07 08:56:49,849 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:56:49,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:49,887 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:49,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:49,887 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:56:49,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,760 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,760 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,760 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:56:50,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,762 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,762 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,762 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:56:50,762 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,763 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,763 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,763 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:56:50,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,764 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,764 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,764 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:56:50,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,802 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,802 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,802 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:56:50,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,803 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,803 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,804 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:56:50,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,805 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,805 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,805 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:56:50,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,806 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,806 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,806 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:56:50,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,807 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,807 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,807 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:56:50,807 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,808 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,808 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,808 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:56:50,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,809 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,809 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,809 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:56:50,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,810 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,810 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,811 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:56:50,811 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,811 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,812 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,812 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:56:50,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,813 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,813 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,813 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:56:50,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,814 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,814 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,814 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:56:50,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,815 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,815 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,815 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:56:50,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,816 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,816 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,816 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:56:50,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,817 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,817 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,817 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:56:50,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,818 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,818 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,819 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:56:50,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,820 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,820 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,820 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:56:50,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,821 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,821 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,821 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:56:50,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,822 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,822 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,822 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:56:50,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,823 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,823 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,823 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:56:50,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,825 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,825 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,825 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:56:50,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,826 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,826 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,826 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:56:50,826 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,827 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,827 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,827 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:56:50,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,828 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,828 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,828 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:56:50,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,829 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,829 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,829 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:56:50,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,831 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,831 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,831 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:56:50,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,832 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,832 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,832 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:56:50,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,833 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,833 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,833 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:56:50,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,834 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,834 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,834 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:56:50,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,835 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,836 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,836 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:56:50,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,837 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,837 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,837 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:56:50,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,838 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,838 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,838 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:56:50,838 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,839 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,839 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,839 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:56:50,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,840 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,840 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,840 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:56:50,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,841 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,841 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,841 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:56:50,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,842 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,842 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,843 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:56:50,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,844 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,844 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,844 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:56:50,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,845 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,845 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,845 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:56:50,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,846 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,846 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,846 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:56:50,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,847 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,847 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,847 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:56:50,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,848 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,848 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,848 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:56:50,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,849 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,849 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,849 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:56:50,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,851 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,851 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,851 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:56:50,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,852 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,852 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,852 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:56:50,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,853 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,853 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,853 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:56:50,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,854 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,854 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,854 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:56:50,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,855 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,855 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,855 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:56:50,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,857 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,857 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,857 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:56:50,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,858 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,858 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,858 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:56:50,858 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,859 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,859 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,859 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:56:50,859 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,860 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,860 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,860 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:56:50,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,861 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,861 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,861 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:56:50,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,862 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,862 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,862 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:56:50,863 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,863 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,863 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,864 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:56:50,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,865 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,865 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,865 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:56:50,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,866 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,866 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,866 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:56:50,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,867 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,867 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,867 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:56:50,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,868 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,868 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,868 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:56:50,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,869 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,869 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,870 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:56:50,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,870 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,871 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,871 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:56:50,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,872 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,872 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,872 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:56:50,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,873 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,873 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,873 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:56:50,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,874 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,874 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,874 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:56:50,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,875 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,875 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,875 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:56:50,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,876 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,876 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,876 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:56:50,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,877 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,877 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,877 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:56:50,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,879 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,879 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,879 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:56:50,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,880 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,880 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,880 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:56:50,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,881 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,881 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,881 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:56:50,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,882 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,882 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,882 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:56:50,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,883 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,883 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,883 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:56:50,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,884 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,884 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:56:50,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,885 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,885 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,885 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:56:50,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,886 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,886 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,886 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:56:50,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,887 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,888 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,888 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:56:50,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,889 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,889 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:56:50,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,890 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,890 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,890 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:56:50,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,891 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,891 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,891 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:56:50,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,892 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,892 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:56:50,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,893 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,893 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,893 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:56:50,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,894 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,894 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,894 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:56:50,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,895 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,895 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,895 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:56:50,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,897 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,897 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,897 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:56:50,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,898 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,898 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,898 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:56:50,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,899 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,899 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,899 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:56:50,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,900 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,900 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,900 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:56:50,900 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,901 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,901 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,901 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:56:50,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,902 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,902 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,902 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:56:50,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,903 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,903 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,903 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:56:50,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,904 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,904 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,904 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:56:50,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,906 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,906 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,906 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:56:50,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,907 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,907 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,907 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:56:50,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,908 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,908 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,908 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:56:50,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,909 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,909 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,909 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:56:50,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,910 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,910 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,910 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:56:50,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,911 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,911 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,912 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:56:50,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,913 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,913 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,913 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:56:50,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,914 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,914 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,914 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:56:50,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,915 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,915 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,915 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:56:50,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,916 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,916 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,916 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:56:50,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,917 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,917 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:56:50,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,918 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,918 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,918 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:56:50,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:56:50,920 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:50,920 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:56:50,920 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:56:50,921 > [DEBUG] 0 :: 7.23439359664917
2023-01-07 08:56:50,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,926 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,926 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.016448974609375
2023-01-07 08:56:50,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.55511474609375
2023-01-07 08:56:50,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,929 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,929 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2965625524520874
2023-01-07 08:56:50,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,930 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -185.166015625
2023-01-07 08:56:50,930 > [DEBUG] 0 :: before allreduce fusion buffer :: -356.4950866699219
2023-01-07 08:56:50,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,940 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,941 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.825270652770996
2023-01-07 08:56:50,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06330159306526184
2023-01-07 08:56:50,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,942 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,942 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.033313196152448654
2023-01-07 08:56:50,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,942 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -139.42921447753906
2023-01-07 08:56:50,942 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9985790252685547
2023-01-07 08:56:50,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,944 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,945 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -7.968842506408691
2023-01-07 08:56:50,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40418440103530884
2023-01-07 08:56:50,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,946 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,946 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.04591246694326401
2023-01-07 08:56:50,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,946 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -339.6507263183594
2023-01-07 08:56:50,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2628828287124634
2023-01-07 08:56:50,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,948 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,948 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 5.92556619644165
2023-01-07 08:56:50,948 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.015132823958992958
2023-01-07 08:56:50,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,949 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,949 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.32063576579093933
2023-01-07 08:56:50,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -371.9351806640625
2023-01-07 08:56:50,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6090248823165894
2023-01-07 08:56:50,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,951 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,951 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.518089294433594
2023-01-07 08:56:50,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06940719485282898
2023-01-07 08:56:50,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,952 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,952 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0060212817043066025
2023-01-07 08:56:50,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -175.75192260742188
2023-01-07 08:56:50,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07782311737537384
2023-01-07 08:56:50,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,954 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,954 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -0.24960064888000488
2023-01-07 08:56:50,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4187145531177521
2023-01-07 08:56:50,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,956 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,956 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.001498628407716751
2023-01-07 08:56:50,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,956 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -345.29461669921875
2023-01-07 08:56:50,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24538373947143555
2023-01-07 08:56:50,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,957 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,958 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -10.494057655334473
2023-01-07 08:56:50,958 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.015709519386291504
2023-01-07 08:56:50,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,959 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,959 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.1452222466468811
2023-01-07 08:56:50,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,959 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -482.690185546875
2023-01-07 08:56:50,959 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7115391492843628
2023-01-07 08:56:50,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,961 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,961 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 15.837343215942383
2023-01-07 08:56:50,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3500029146671295
2023-01-07 08:56:50,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,962 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,962 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.013640478253364563
2023-01-07 08:56:50,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,963 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -468.1350402832031
2023-01-07 08:56:50,963 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5626925826072693
2023-01-07 08:56:50,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,964 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,964 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.9486122131347656
2023-01-07 08:56:50,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36701980233192444
2023-01-07 08:56:50,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,965 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,965 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.045505162328481674
2023-01-07 08:56:50,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,966 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -247.40467834472656
2023-01-07 08:56:50,966 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.905393123626709
2023-01-07 08:56:50,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,968 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,968 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 32.84893035888672
2023-01-07 08:56:50,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.704709768295288
2023-01-07 08:56:50,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,969 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,969 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.019260190427303314
2023-01-07 08:56:50,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,970 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -319.40887451171875
2023-01-07 08:56:50,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.226896286010742
2023-01-07 08:56:50,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,971 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,971 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.9575657844543457
2023-01-07 08:56:50,972 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03261880576610565
2023-01-07 08:56:50,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,973 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,973 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.048648539930582047
2023-01-07 08:56:50,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,973 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -350.83428955078125
2023-01-07 08:56:50,973 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1734910011291504
2023-01-07 08:56:50,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,975 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,975 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.989673376083374
2023-01-07 08:56:50,975 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3015965223312378
2023-01-07 08:56:50,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,976 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.01562526822090149
2023-01-07 08:56:50,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -247.41436767578125
2023-01-07 08:56:50,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.572736144065857
2023-01-07 08:56:50,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,978 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,979 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 11.970529556274414
2023-01-07 08:56:50,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09364721179008484
2023-01-07 08:56:50,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,980 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,980 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.09402556717395782
2023-01-07 08:56:50,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,980 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -228.98202514648438
2023-01-07 08:56:50,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0845372676849365
2023-01-07 08:56:50,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,982 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,982 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -3.3671746253967285
2023-01-07 08:56:50,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7046506404876709
2023-01-07 08:56:50,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,983 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,983 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.02987167239189148
2023-01-07 08:56:50,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,984 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -341.5923767089844
2023-01-07 08:56:50,984 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27840298414230347
2023-01-07 08:56:50,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,985 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.114727973937988
2023-01-07 08:56:50,985 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014661729335784912
2023-01-07 08:56:50,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,986 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.049171559512615204
2023-01-07 08:56:50,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -236.82012939453125
2023-01-07 08:56:50,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6908617615699768
2023-01-07 08:56:50,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,988 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 21.509174346923828
2023-01-07 08:56:50,989 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4900211691856384
2023-01-07 08:56:50,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,990 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.05649957060813904
2023-01-07 08:56:50,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -211.192138671875
2023-01-07 08:56:50,990 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.325175940990448
2023-01-07 08:56:50,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,991 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -17.36310577392578
2023-01-07 08:56:50,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24807459115982056
2023-01-07 08:56:50,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,993 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.28707098960876465
2023-01-07 08:56:50,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -347.77777099609375
2023-01-07 08:56:50,993 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5828924775123596
2023-01-07 08:56:50,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,995 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.936253547668457
2023-01-07 08:56:50,995 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6261626482009888
2023-01-07 08:56:50,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,996 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,996 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.09117401391267776
2023-01-07 08:56:50,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:50,996 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -226.6207733154297
2023-01-07 08:56:50,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1656816005706787
2023-01-07 08:56:50,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,998 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,998 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -7.013481616973877
2023-01-07 08:56:50,998 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22845882177352905
2023-01-07 08:56:50,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:50,999 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:50,999 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.23992158472537994
2023-01-07 08:56:51,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,000 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -238.87612915039062
2023-01-07 08:56:51,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.384352207183838
2023-01-07 08:56:51,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,001 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,001 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 4.893420219421387
2023-01-07 08:56:51,002 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.046345725655555725
2023-01-07 08:56:51,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,002 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,003 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3795604407787323
2023-01-07 08:56:51,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,003 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -329.3013610839844
2023-01-07 08:56:51,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4946975708007812
2023-01-07 08:56:51,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,004 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 20.086158752441406
2023-01-07 08:56:51,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5060505270957947
2023-01-07 08:56:51,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,006 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.07050500810146332
2023-01-07 08:56:51,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -199.3629608154297
2023-01-07 08:56:51,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.00360107421875
2023-01-07 08:56:51,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,008 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,008 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -34.258052825927734
2023-01-07 08:56:51,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0572595596313477
2023-01-07 08:56:51,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,009 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,009 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.27385857701301575
2023-01-07 08:56:51,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,010 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -242.5295867919922
2023-01-07 08:56:51,010 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7312314510345459
2023-01-07 08:56:51,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,011 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,011 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -2.3009910583496094
2023-01-07 08:56:51,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4857895374298096
2023-01-07 08:56:51,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,013 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,013 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.9245577454566956
2023-01-07 08:56:51,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,013 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -308.6922912597656
2023-01-07 08:56:51,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9897265434265137
2023-01-07 08:56:51,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,015 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,015 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.0618233680725098
2023-01-07 08:56:51,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6691234111785889
2023-01-07 08:56:51,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,016 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,016 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.07207763195037842
2023-01-07 08:56:51,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,016 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -175.2939910888672
2023-01-07 08:56:51,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6972622871398926
2023-01-07 08:56:51,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,018 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,018 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 20.735761642456055
2023-01-07 08:56:51,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4412960410118103
2023-01-07 08:56:51,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,019 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -156.03016662597656
2023-01-07 08:56:51,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.437131881713867
2023-01-07 08:56:51,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,021 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,021 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 23.352792739868164
2023-01-07 08:56:51,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.739103376865387
2023-01-07 08:56:51,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,022 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,022 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.5704174041748047
2023-01-07 08:56:51,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,023 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -257.9768371582031
2023-01-07 08:56:51,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9161887168884277
2023-01-07 08:56:51,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,025 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,026 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 18.33499526977539
2023-01-07 08:56:51,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7541030645370483
2023-01-07 08:56:51,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,027 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,027 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.16031025350093842
2023-01-07 08:56:51,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,027 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -90.11390686035156
2023-01-07 08:56:51,028 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1101871132850647
2023-01-07 08:56:51,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,029 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,029 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -3.830230712890625
2023-01-07 08:56:51,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9473726749420166
2023-01-07 08:56:51,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,030 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,030 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.24383330345153809
2023-01-07 08:56:51,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,031 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -139.15513610839844
2023-01-07 08:56:51,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.81412935256958
2023-01-07 08:56:51,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,033 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,033 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -75.23050689697266
2023-01-07 08:56:51,033 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.545440435409546
2023-01-07 08:56:51,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,034 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -208.11489868164062
2023-01-07 08:56:51,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.223346710205078
2023-01-07 08:56:51,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,036 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,036 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 11.373271942138672
2023-01-07 08:56:51,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8950096368789673
2023-01-07 08:56:51,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,038 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -169.25595092773438
2023-01-07 08:56:51,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4582853317260742
2023-01-07 08:56:51,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 22.922605514526367
2023-01-07 08:56:51,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2625769376754761
2023-01-07 08:56:51,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,041 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -99.86083984375
2023-01-07 08:56:51,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7496976852416992
2023-01-07 08:56:51,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,042 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,042 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -10.394611358642578
2023-01-07 08:56:51,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.960262298583984
2023-01-07 08:56:51,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -130.75807189941406
2023-01-07 08:56:51,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8883955478668213
2023-01-07 08:56:51,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,046 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,046 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -30.983978271484375
2023-01-07 08:56:51,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.303857803344727
2023-01-07 08:56:51,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,047 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -221.1050567626953
2023-01-07 08:56:51,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.169065952301025
2023-01-07 08:56:51,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,048 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,049 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -17.930862426757812
2023-01-07 08:56:51,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3532465100288391
2023-01-07 08:56:51,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,050 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -149.64784240722656
2023-01-07 08:56:51,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.095361709594727
2023-01-07 08:56:51,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,052 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,052 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 145.77175903320312
2023-01-07 08:56:51,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7237017154693604
2023-01-07 08:56:51,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 12.743431091308594
2023-01-07 08:56:51,053 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.116405487060547
2023-01-07 08:56:51,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,054 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,055 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -7.3799591064453125
2023-01-07 08:56:51,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.249244689941406
2023-01-07 08:56:51,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,056 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.5615662336349487
2023-01-07 08:56:51,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -194.21102905273438
2023-01-07 08:56:51,057 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.645504474639893
2023-01-07 08:56:51,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.6769938468933105
2023-01-07 08:56:51,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.676447868347168
2023-01-07 08:56:51,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,059 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -137.86007690429688
2023-01-07 08:56:51,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31018900871276855
2023-01-07 08:56:51,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,061 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,061 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 58.040855407714844
2023-01-07 08:56:51,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.03101634979248
2023-01-07 08:56:51,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -71.10108947753906
2023-01-07 08:56:51,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.734153747558594
2023-01-07 08:56:51,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,064 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,064 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 17.078338623046875
2023-01-07 08:56:51,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1688476800918579
2023-01-07 08:56:51,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,065 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.924264669418335
2023-01-07 08:56:51,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -138.37557983398438
2023-01-07 08:56:51,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.687355041503906
2023-01-07 08:56:51,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,067 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 7.038160800933838
2023-01-07 08:56:51,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43314898014068604
2023-01-07 08:56:51,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -45.72783660888672
2023-01-07 08:56:51,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.814070224761963
2023-01-07 08:56:51,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,070 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,070 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -73.70452117919922
2023-01-07 08:56:51,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.723878383636475
2023-01-07 08:56:51,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,072 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -189.72323608398438
2023-01-07 08:56:51,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.12379264831543
2023-01-07 08:56:51,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,073 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,074 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -3.1434669494628906
2023-01-07 08:56:51,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.50989532470703
2023-01-07 08:56:51,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,075 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -2.3046069145202637
2023-01-07 08:56:51,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -125.82180786132812
2023-01-07 08:56:51,076 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.74333953857422
2023-01-07 08:56:51,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,077 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -721.310791015625
2023-01-07 08:56:51,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.043039321899414
2023-01-07 08:56:51,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,078 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,079 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.10129976272583
2023-01-07 08:56:51,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,079 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,079 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 46.61156463623047
2023-01-07 08:56:51,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,079 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -967.83544921875
2023-01-07 08:56:51,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.63721466064453
2023-01-07 08:56:51,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,081 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1069.38525390625
2023-01-07 08:56:51,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.444724082946777
2023-01-07 08:56:51,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,083 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -606.8973388671875
2023-01-07 08:56:51,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9512529373168945
2023-01-07 08:56:51,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,085 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,085 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.29200744628906
2023-01-07 08:56:51,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.094303131103516
2023-01-07 08:56:51,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,086 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,086 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.31064140796661377
2023-01-07 08:56:51,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,087 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -128.48995971679688
2023-01-07 08:56:51,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.09490203857422
2023-01-07 08:56:51,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,088 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -553.893310546875
2023-01-07 08:56:51,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.433589935302734
2023-01-07 08:56:51,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,090 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,090 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.9759674072265625
2023-01-07 08:56:51,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,090 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,090 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -52.3306770324707
2023-01-07 08:56:51,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.788185119628906
2023-01-07 08:56:51,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,093 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -731.6365966796875
2023-01-07 08:56:51,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.494121551513672
2023-01-07 08:56:51,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,094 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,094 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 8.075907707214355
2023-01-07 08:56:51,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,095 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1258.449462890625
2023-01-07 08:56:51,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.543033599853516
2023-01-07 08:56:51,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,096 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -813.806396484375
2023-01-07 08:56:51,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.80179786682129
2023-01-07 08:56:51,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,098 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,098 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -81.55531311035156
2023-01-07 08:56:51,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.65326690673828
2023-01-07 08:56:51,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,099 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -891.3416748046875
2023-01-07 08:56:51,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.47832489013672
2023-01-07 08:56:51,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,101 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 5.607870578765869
2023-01-07 08:56:51,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,101 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -28.711544036865234
2023-01-07 08:56:51,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,101 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1014.5731811523438
2023-01-07 08:56:51,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,102 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1434.6611328125
2023-01-07 08:56:51,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.705936431884766
2023-01-07 08:56:51,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,103 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1144.6583251953125
2023-01-07 08:56:51,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.518230438232422
2023-01-07 08:56:51,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,105 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,105 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -175.37765502929688
2023-01-07 08:56:51,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.63339233398438
2023-01-07 08:56:51,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,106 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1202.062255859375
2023-01-07 08:56:51,107 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.256855010986328
2023-01-07 08:56:51,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,108 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -884.0214233398438
2023-01-07 08:56:51,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,108 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1438.053466796875
2023-01-07 08:56:51,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.17061614990234
2023-01-07 08:56:51,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,110 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -598.8653564453125
2023-01-07 08:56:51,110 > [DEBUG] 0 :: before allreduce fusion buffer :: -207.69644165039062
2023-01-07 08:56:51,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,111 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,111 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -1.0347893238067627
2023-01-07 08:56:51,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,111 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -621.10400390625
2023-01-07 08:56:51,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -168.13079833984375
2023-01-07 08:56:51,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,113 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1483.3984375
2023-01-07 08:56:51,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.54863739013672
2023-01-07 08:56:51,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,115 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2195.29248046875
2023-01-07 08:56:51,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.61852264404297
2023-01-07 08:56:51,118 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:56:51,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:51,119 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -305.0423583984375
2023-01-07 08:56:51,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,120 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2786.55322265625
2023-01-07 08:56:51,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,122 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1452.364013671875
2023-01-07 08:56:51,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,123 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -795.1138916015625
2023-01-07 08:56:51,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,124 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -505.6295471191406
2023-01-07 08:56:51,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,125 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -661.795166015625
2023-01-07 08:56:51,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,126 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -757.506591796875
2023-01-07 08:56:51,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,127 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -718.9137573242188
2023-01-07 08:56:51,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,128 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -607.5698852539062
2023-01-07 08:56:51,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,128 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -663.7969970703125
2023-01-07 08:56:51,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,129 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -712.7194213867188
2023-01-07 08:56:51,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,129 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -718.1235961914062
2023-01-07 08:56:51,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,130 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -800.3562622070312
2023-01-07 08:56:51,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,130 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -644.0794067382812
2023-01-07 08:56:51,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -640.3845825195312
2023-01-07 08:56:51,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -894.3411865234375
2023-01-07 08:56:51,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -994.733642578125
2023-01-07 08:56:51,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -659.7946166992188
2023-01-07 08:56:51,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -966.3883666992188
2023-01-07 08:56:51,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -826.1878051757812
2023-01-07 08:56:51,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -961.4240112304688
2023-01-07 08:56:51,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -928.3584594726562
2023-01-07 08:56:51,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -913.50927734375
2023-01-07 08:56:51,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -992.4766845703125
2023-01-07 08:56:51,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -967.5482177734375
2023-01-07 08:56:51,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,136 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -953.4818725585938
2023-01-07 08:56:51,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -325.9353332519531
2023-01-07 08:56:51,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1027.120849609375
2023-01-07 08:56:51,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -989.5474243164062
2023-01-07 08:56:51,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -927.262451171875
2023-01-07 08:56:51,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1036.7384033203125
2023-01-07 08:56:51,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -984.7753295898438
2023-01-07 08:56:51,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -952.0402221679688
2023-01-07 08:56:51,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1052.254150390625
2023-01-07 08:56:51,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,140 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -994.5549926757812
2023-01-07 08:56:51,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,141 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1056.16162109375
2023-01-07 08:56:51,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.888099670410156
2023-01-07 08:56:51,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,142 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -793.9678344726562
2023-01-07 08:56:51,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,142 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -984.517578125
2023-01-07 08:56:51,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,143 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -999.5691528320312
2023-01-07 08:56:51,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,143 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1051.9808349609375
2023-01-07 08:56:51,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.361799240112305
2023-01-07 08:56:51,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,144 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -727.4337158203125
2023-01-07 08:56:51,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,144 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -889.2838134765625
2023-01-07 08:56:51,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,145 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -986.970703125
2023-01-07 08:56:51,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.779691219329834
2023-01-07 08:56:51,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,146 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -948.088623046875
2023-01-07 08:56:51,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:51,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:51,146 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -345.5634765625
2023-01-07 08:56:51,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 717.345703125
2023-01-07 08:56:52,001 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -1262.209716796875
2023-01-07 08:56:52,001 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,001 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,001 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:52,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,001 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,001 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -4.942687511444092
2023-01-07 08:56:52,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,002 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2703.780517578125
2023-01-07 08:56:52,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 97.60336303710938
2023-01-07 08:56:52,004 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -5.241582870483398
2023-01-07 08:56:52,004 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,004 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,004 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -12.131708145141602
2023-01-07 08:56:52,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,004 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,004 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 98.1849136352539
2023-01-07 08:56:52,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,004 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2656.478515625
2023-01-07 08:56:52,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 84.44438171386719
2023-01-07 08:56:52,006 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -25.974990844726562
2023-01-07 08:56:52,006 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,006 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,006 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:52,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,006 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2653.90625
2023-01-07 08:56:52,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 119.01406860351562
2023-01-07 08:56:52,007 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -15.59414291381836
2023-01-07 08:56:52,008 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,008 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,008 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:52,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,008 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2656.868408203125
2023-01-07 08:56:52,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -294.83392333984375
2023-01-07 08:56:52,009 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -2629.7265625
2023-01-07 08:56:52,009 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,009 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,009 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:52,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,010 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,010 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 4.378416061401367
2023-01-07 08:56:52,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,010 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,010 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 25.433210372924805
2023-01-07 08:56:52,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,010 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1266.0706787109375
2023-01-07 08:56:52,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.333473205566406
2023-01-07 08:56:52,012 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 3.146897315979004
2023-01-07 08:56:52,012 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,012 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,012 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:52,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,012 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1310.844482421875
2023-01-07 08:56:52,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.314133644104004
2023-01-07 08:56:52,013 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 107.45128631591797
2023-01-07 08:56:52,013 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,014 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,014 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:52,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,014 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,014 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 1.0085680484771729
2023-01-07 08:56:52,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,014 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -559.644775390625
2023-01-07 08:56:52,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.46388053894043
2023-01-07 08:56:52,015 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 5.06158447265625
2023-01-07 08:56:52,015 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,016 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,016 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:52,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,016 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -435.98651123046875
2023-01-07 08:56:52,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 95.55652618408203
2023-01-07 08:56:52,017 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1647.5556640625
2023-01-07 08:56:52,017 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,017 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:52,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,017 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -409.1173400878906
2023-01-07 08:56:52,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -65.65056610107422
2023-01-07 08:56:52,019 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -35.389923095703125
2023-01-07 08:56:52,019 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:52,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,019 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -492.8421325683594
2023-01-07 08:56:52,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.79029846191406
2023-01-07 08:56:52,020 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -600.1544189453125
2023-01-07 08:56:52,020 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,020 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:52,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,021 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -2.444876194000244
2023-01-07 08:56:52,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -530.8284912109375
2023-01-07 08:56:52,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -118.0298843383789
2023-01-07 08:56:52,023 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -2.369013786315918
2023-01-07 08:56:52,023 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,023 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,023 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:52,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,023 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -556.7867431640625
2023-01-07 08:56:52,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,023 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1480.5477294921875
2023-01-07 08:56:52,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.771448135375977
2023-01-07 08:56:52,025 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -579.591064453125
2023-01-07 08:56:52,025 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,025 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,025 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:52,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,025 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.9364986419677734
2023-01-07 08:56:52,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -500.0247802734375
2023-01-07 08:56:52,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.043655395507812
2023-01-07 08:56:52,027 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.5367605686187744
2023-01-07 08:56:52,027 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,027 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,027 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:52,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1294.367431640625
2023-01-07 08:56:52,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.439178466796875
2023-01-07 08:56:52,028 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -514.402099609375
2023-01-07 08:56:52,029 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,029 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:52,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -687.5302734375
2023-01-07 08:56:52,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.96240520477295
2023-01-07 08:56:52,030 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -58.30308532714844
2023-01-07 08:56:52,030 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,030 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:52,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -697.2646484375
2023-01-07 08:56:52,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1339.2476806640625
2023-01-07 08:56:52,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.709516525268555
2023-01-07 08:56:52,032 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -705.6134033203125
2023-01-07 08:56:52,032 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,033 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:52,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,033 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1335.2730712890625
2023-01-07 08:56:52,033 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.69063949584961
2023-01-07 08:56:52,034 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -15.866663932800293
2023-01-07 08:56:52,034 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,034 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,034 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:52,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1359.0928955078125
2023-01-07 08:56:52,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.352495193481445
2023-01-07 08:56:52,036 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -1372.033935546875
2023-01-07 08:56:52,036 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,036 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,036 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:52,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,036 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1.380769968032837
2023-01-07 08:56:52,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,036 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 28.66809844970703
2023-01-07 08:56:52,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1346.5960693359375
2023-01-07 08:56:52,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.26038360595703
2023-01-07 08:56:52,038 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.529064416885376
2023-01-07 08:56:52,038 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,038 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:52,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1492.8778076171875
2023-01-07 08:56:52,039 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.125267028808594
2023-01-07 08:56:52,040 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 57.326847076416016
2023-01-07 08:56:52,040 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,040 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,040 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.465963363647461
2023-01-07 08:56:52,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,040 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1331.3857421875
2023-01-07 08:56:52,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.863293170928955
2023-01-07 08:56:52,042 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -95.89569091796875
2023-01-07 08:56:52,042 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,042 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,042 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:52,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,042 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1520.5205078125
2023-01-07 08:56:52,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0837087631225586
2023-01-07 08:56:52,043 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1308.7257080078125
2023-01-07 08:56:52,043 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,043 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,044 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:52,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1575.992431640625
2023-01-07 08:56:52,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.02749252319336
2023-01-07 08:56:52,045 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -44.19133758544922
2023-01-07 08:56:52,045 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,045 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,045 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:52,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,045 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1585.628173828125
2023-01-07 08:56:52,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.89479064941406
2023-01-07 08:56:52,047 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -1618.8558349609375
2023-01-07 08:56:52,047 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,047 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,047 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,047 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,047 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.6389771699905396
2023-01-07 08:56:52,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,047 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -711.5433349609375
2023-01-07 08:56:52,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.95516586303711
2023-01-07 08:56:52,049 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 2.110405683517456
2023-01-07 08:56:52,049 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,049 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,049 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.756298542022705
2023-01-07 08:56:52,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,049 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -729.434814453125
2023-01-07 08:56:52,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.505870819091797
2023-01-07 08:56:52,050 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -731.2986450195312
2023-01-07 08:56:52,050 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,051 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:52,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,051 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,051 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.9921870231628418
2023-01-07 08:56:52,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,051 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -508.1566467285156
2023-01-07 08:56:52,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.847604751586914
2023-01-07 08:56:52,052 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.9990231990814209
2023-01-07 08:56:52,052 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,052 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,053 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -14.141868591308594
2023-01-07 08:56:52,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -520.0401000976562
2023-01-07 08:56:52,053 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.257970809936523
2023-01-07 08:56:52,054 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -523.9080810546875
2023-01-07 08:56:52,054 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,054 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,054 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:52,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,054 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -670.2149658203125
2023-01-07 08:56:52,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.815608024597168
2023-01-07 08:56:52,056 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -81.57178497314453
2023-01-07 08:56:52,056 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,056 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:52,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -662.5484008789062
2023-01-07 08:56:52,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.442389488220215
2023-01-07 08:56:52,057 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -657.7576904296875
2023-01-07 08:56:52,057 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,057 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,058 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.20563173294067383
2023-01-07 08:56:52,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -744.1266479492188
2023-01-07 08:56:52,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.200101137161255
2023-01-07 08:56:52,059 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.3537764549255371
2023-01-07 08:56:52,059 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,059 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,059 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.551721572875977
2023-01-07 08:56:52,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,060 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -701.4896850585938
2023-01-07 08:56:52,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8824176788330078
2023-01-07 08:56:52,061 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -745.9493408203125
2023-01-07 08:56:52,061 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,061 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,061 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,061 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.4754534959793091
2023-01-07 08:56:52,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -728.4963989257812
2023-01-07 08:56:52,062 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.98868179321289
2023-01-07 08:56:52,063 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.9837609529495239
2023-01-07 08:56:52,063 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,063 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,063 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.180285930633545
2023-01-07 08:56:52,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -730.6580200195312
2023-01-07 08:56:52,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.707572937011719
2023-01-07 08:56:52,065 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -735.9921875
2023-01-07 08:56:52,065 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,065 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:52,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -606.3055419921875
2023-01-07 08:56:52,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.673807144165039
2023-01-07 08:56:52,066 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -102.590087890625
2023-01-07 08:56:52,066 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,066 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:52,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -601.2178955078125
2023-01-07 08:56:52,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1145436763763428
2023-01-07 08:56:52,068 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -608.275390625
2023-01-07 08:56:52,068 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,068 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,069 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.7120836973190308
2023-01-07 08:56:52,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -662.70166015625
2023-01-07 08:56:52,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.346038818359375
2023-01-07 08:56:52,070 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.8361510634422302
2023-01-07 08:56:52,070 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,071 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 8.06689453125
2023-01-07 08:56:52,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -718.4432983398438
2023-01-07 08:56:52,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.608116149902344
2023-01-07 08:56:52,072 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -722.0858764648438
2023-01-07 08:56:52,072 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,072 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,072 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,073 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.09156662225723267
2023-01-07 08:56:52,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,073 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -710.5970458984375
2023-01-07 08:56:52,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.087620735168457
2023-01-07 08:56:52,074 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.40854722261428833
2023-01-07 08:56:52,074 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 30.583946228027344
2023-01-07 08:56:52,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -692.6849365234375
2023-01-07 08:56:52,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.765182971954346
2023-01-07 08:56:52,076 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -693.5675048828125
2023-01-07 08:56:52,076 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,076 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,076 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:52,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,076 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,076 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4553048014640808
2023-01-07 08:56:52,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,076 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -717.4472045898438
2023-01-07 08:56:52,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9410696029663086
2023-01-07 08:56:52,078 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.8843249082565308
2023-01-07 08:56:52,078 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,078 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -70.87334442138672
2023-01-07 08:56:52,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -710.642333984375
2023-01-07 08:56:52,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.794919013977051
2023-01-07 08:56:52,079 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -717.14794921875
2023-01-07 08:56:52,079 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.17241734266281128
2023-01-07 08:56:52,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -799.4588012695312
2023-01-07 08:56:52,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.488720893859863
2023-01-07 08:56:52,081 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.12777501344680786
2023-01-07 08:56:52,081 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,081 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.382711410522461
2023-01-07 08:56:52,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -830.4824829101562
2023-01-07 08:56:52,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6889978647232056
2023-01-07 08:56:52,083 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -837.9087524414062
2023-01-07 08:56:52,083 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,083 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:52,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,083 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.17283368110656738
2023-01-07 08:56:52,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -643.6015014648438
2023-01-07 08:56:52,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.312479019165039
2023-01-07 08:56:52,085 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.11865049600601196
2023-01-07 08:56:52,085 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,085 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -10.846620559692383
2023-01-07 08:56:52,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,085 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -639.0030517578125
2023-01-07 08:56:52,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8172826766967773
2023-01-07 08:56:52,087 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -642.518310546875
2023-01-07 08:56:52,087 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,087 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,087 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:52,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,087 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,087 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.20870447158813477
2023-01-07 08:56:52,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -645.3250732421875
2023-01-07 08:56:52,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.794841289520264
2023-01-07 08:56:52,089 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.09151127934455872
2023-01-07 08:56:52,089 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,089 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,089 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -31.834835052490234
2023-01-07 08:56:52,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,089 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -651.996826171875
2023-01-07 08:56:52,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2159812450408936
2023-01-07 08:56:52,090 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -648.887939453125
2023-01-07 08:56:52,091 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,091 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,091 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:52,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.8004765510559082
2023-01-07 08:56:52,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1165.306640625
2023-01-07 08:56:52,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.270478248596191
2023-01-07 08:56:52,093 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.9847728610038757
2023-01-07 08:56:52,093 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,093 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,093 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -29.614931106567383
2023-01-07 08:56:52,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1188.580078125
2023-01-07 08:56:52,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.940931797027588
2023-01-07 08:56:52,094 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1190.031005859375
2023-01-07 08:56:52,094 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,095 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:52,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,095 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -877.36279296875
2023-01-07 08:56:52,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.836223840713501
2023-01-07 08:56:52,096 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -47.657413482666016
2023-01-07 08:56:52,096 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:52,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -870.5481567382812
2023-01-07 08:56:52,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8248958587646484
2023-01-07 08:56:52,098 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -862.227783203125
2023-01-07 08:56:52,098 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,098 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,098 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:52,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -224.18177795410156
2023-01-07 08:56:52,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.651219606399536
2023-01-07 08:56:52,099 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -189.94493103027344
2023-01-07 08:56:52,100 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,100 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,100 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:52,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -217.29696655273438
2023-01-07 08:56:52,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8045481443405151
2023-01-07 08:56:52,101 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -211.39613342285156
2023-01-07 08:56:52,101 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:52,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -764.8314819335938
2023-01-07 08:56:52,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.070847034454346
2023-01-07 08:56:52,103 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -188.3511505126953
2023-01-07 08:56:52,103 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:52,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -763.4830322265625
2023-01-07 08:56:52,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.364678382873535
2023-01-07 08:56:52,105 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -766.203125
2023-01-07 08:56:52,105 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,105 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:52,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,105 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:56:52,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.06403639912605286
2023-01-07 08:56:52,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -276.75732421875
2023-01-07 08:56:52,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.554990530014038
2023-01-07 08:56:52,107 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.2873504161834717
2023-01-07 08:56:52,107 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.49056625366211
2023-01-07 08:56:52,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,107 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -281.1678466796875
2023-01-07 08:56:52,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5477629899978638
2023-01-07 08:56:52,108 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -285.07696533203125
2023-01-07 08:56:52,108 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,108 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:52,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -833.6627197265625
2023-01-07 08:56:52,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.715028703212738
2023-01-07 08:56:52,110 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -43.03620147705078
2023-01-07 08:56:52,110 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,110 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,110 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:52,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,110 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -832.5372314453125
2023-01-07 08:56:52,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4908342659473419
2023-01-07 08:56:52,112 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -833.6162109375
2023-01-07 08:56:52,113 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,113 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,113 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:52,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -927.0838012695312
2023-01-07 08:56:52,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4298455715179443
2023-01-07 08:56:52,114 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -181.84564208984375
2023-01-07 08:56:52,114 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:52,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -926.031494140625
2023-01-07 08:56:52,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.179114818572998
2023-01-07 08:56:52,116 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -922.7307739257812
2023-01-07 08:56:52,116 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:52,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -879.7385864257812
2023-01-07 08:56:52,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.412585735321045
2023-01-07 08:56:52,118 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -42.911808013916016
2023-01-07 08:56:52,118 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:52,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -882.1633911132812
2023-01-07 08:56:52,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6842671632766724
2023-01-07 08:56:52,119 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -884.352294921875
2023-01-07 08:56:52,120 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,120 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,120 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:52,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1011.2777099609375
2023-01-07 08:56:52,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4897196292877197
2023-01-07 08:56:52,121 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -42.34941101074219
2023-01-07 08:56:52,121 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:52,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,122 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1007.0169067382812
2023-01-07 08:56:52,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.526638984680176
2023-01-07 08:56:52,123 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1006.9293212890625
2023-01-07 08:56:52,123 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:52,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -967.3173828125
2023-01-07 08:56:52,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4436023235321045
2023-01-07 08:56:52,125 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -177.98199462890625
2023-01-07 08:56:52,125 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:52,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -965.8065795898438
2023-01-07 08:56:52,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1999115943908691
2023-01-07 08:56:52,126 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -964.05419921875
2023-01-07 08:56:52,127 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:52,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3625.062744140625
2023-01-07 08:56:52,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.319723606109619
2023-01-07 08:56:52,128 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -43.31586837768555
2023-01-07 08:56:52,128 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:52,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3628.11083984375
2023-01-07 08:56:52,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7221927642822266
2023-01-07 08:56:52,130 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -3625.640625
2023-01-07 08:56:52,130 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:52,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,130 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1030.0660400390625
2023-01-07 08:56:52,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19140732288360596
2023-01-07 08:56:52,131 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -43.24785614013672
2023-01-07 08:56:52,132 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:52,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1029.0718994140625
2023-01-07 08:56:52,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9934139251708984
2023-01-07 08:56:52,133 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1027.9912109375
2023-01-07 08:56:52,133 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:52,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -919.9612426757812
2023-01-07 08:56:52,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3585751950740814
2023-01-07 08:56:52,135 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -175.69435119628906
2023-01-07 08:56:52,135 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:52,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -920.970458984375
2023-01-07 08:56:52,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.28557032346725464
2023-01-07 08:56:52,137 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -921.6127319335938
2023-01-07 08:56:52,137 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:52,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -560.3465576171875
2023-01-07 08:56:52,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9639562964439392
2023-01-07 08:56:52,138 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -44.1695556640625
2023-01-07 08:56:52,138 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,138 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:52,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -559.4268798828125
2023-01-07 08:56:52,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6152068376541138
2023-01-07 08:56:52,140 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -558.2005615234375
2023-01-07 08:56:52,140 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:52,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2247.96044921875
2023-01-07 08:56:52,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2044259309768677
2023-01-07 08:56:52,142 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -43.94623947143555
2023-01-07 08:56:52,142 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,142 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,142 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:52,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2246.2890625
2023-01-07 08:56:52,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2444450855255127
2023-01-07 08:56:52,144 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2246.11279296875
2023-01-07 08:56:52,144 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:52,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -494.71710205078125
2023-01-07 08:56:52,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.662564992904663
2023-01-07 08:56:52,145 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -175.0475616455078
2023-01-07 08:56:52,145 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,145 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,145 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:52,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,146 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -496.1624755859375
2023-01-07 08:56:52,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10665787756443024
2023-01-07 08:56:52,147 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -496.7479248046875
2023-01-07 08:56:52,147 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,147 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:52,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -335.81341552734375
2023-01-07 08:56:52,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6504197120666504
2023-01-07 08:56:52,149 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -44.24275588989258
2023-01-07 08:56:52,149 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:52,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,149 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -334.35894775390625
2023-01-07 08:56:52,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19178126752376556
2023-01-07 08:56:52,150 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -337.1873779296875
2023-01-07 08:56:52,150 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:52,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -506.87799072265625
2023-01-07 08:56:52,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2530531883239746
2023-01-07 08:56:52,152 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -44.043113708496094
2023-01-07 08:56:52,152 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:52,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,153 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -504.94635009765625
2023-01-07 08:56:52,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7210261821746826
2023-01-07 08:56:52,154 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -504.10418701171875
2023-01-07 08:56:52,154 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:52,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,154 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -7.12860107421875
2023-01-07 08:56:52,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2484323978424072
2023-01-07 08:56:52,155 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -175.11911010742188
2023-01-07 08:56:52,156 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:52,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,156 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.762359619140625
2023-01-07 08:56:52,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1161298751831055
2023-01-07 08:56:52,157 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -3.7620849609375
2023-01-07 08:56:52,157 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:52,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,158 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -5528.47314453125
2023-01-07 08:56:52,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1621341705322266
2023-01-07 08:56:52,159 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -87.53468322753906
2023-01-07 08:56:52,159 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,159 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:52,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -5527.6826171875
2023-01-07 08:56:52,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.276744544506073
2023-01-07 08:56:52,161 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -5528.43994140625
2023-01-07 08:56:52,161 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:52,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,161 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -43.0146484375
2023-01-07 08:56:52,161 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5121772289276123
2023-01-07 08:56:52,162 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.4799918830394745
2023-01-07 08:56:52,162 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,162 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:52,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,163 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -43.2423095703125
2023-01-07 08:56:52,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4355470836162567
2023-01-07 08:56:52,164 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -44.0806884765625
2023-01-07 08:56:52,164 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,164 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,164 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:52,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,165 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2573.65087890625
2023-01-07 08:56:52,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9096537828445435
2023-01-07 08:56:52,166 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -377.12237548828125
2023-01-07 08:56:52,166 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:52,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,166 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2571.315673828125
2023-01-07 08:56:52,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08750294148921967
2023-01-07 08:56:52,167 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2571.38720703125
2023-01-07 08:56:52,167 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,167 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:52,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,168 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 33.937469482421875
2023-01-07 08:56:52,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19008155167102814
2023-01-07 08:56:52,169 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -378.00628662109375
2023-01-07 08:56:52,169 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,169 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:52,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,169 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 34.246368408203125
2023-01-07 08:56:52,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06838718056678772
2023-01-07 08:56:52,171 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 33.517852783203125
2023-01-07 08:56:52,171 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,171 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:52,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,171 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -10000.5458984375
2023-01-07 08:56:52,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8036043643951416
2023-01-07 08:56:52,172 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -86.85343933105469
2023-01-07 08:56:52,172 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,173 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:52,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,173 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -10001.7685546875
2023-01-07 08:56:52,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1804533004760742
2023-01-07 08:56:52,174 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -10002.4326171875
2023-01-07 08:56:52,174 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:52,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,175 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1028.766357421875
2023-01-07 08:56:52,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39727988839149475
2023-01-07 08:56:52,176 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.1668308675289154
2023-01-07 08:56:52,176 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:52,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,176 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1029.263427734375
2023-01-07 08:56:52,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7326623797416687
2023-01-07 08:56:52,177 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1028.64208984375
2023-01-07 08:56:52,178 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:52,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,178 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7487.18798828125
2023-01-07 08:56:52,178 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39556005597114563
2023-01-07 08:56:52,179 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -377.86138916015625
2023-01-07 08:56:52,179 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:52,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,180 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7487.1220703125
2023-01-07 08:56:52,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4429718255996704
2023-01-07 08:56:52,181 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 7486.83837890625
2023-01-07 08:56:52,181 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:52,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,181 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -21391.884765625
2023-01-07 08:56:52,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.640774130821228
2023-01-07 08:56:52,182 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -87.2657241821289
2023-01-07 08:56:52,183 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:52,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,183 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -21392.341796875
2023-01-07 08:56:52,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03315715491771698
2023-01-07 08:56:52,184 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -21392.1953125
2023-01-07 08:56:52,184 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:52,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,185 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2523.623046875
2023-01-07 08:56:52,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45489442348480225
2023-01-07 08:56:52,186 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -87.10054779052734
2023-01-07 08:56:52,186 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:52,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,186 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2523.20458984375
2023-01-07 08:56:52,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.38433459401130676
2023-01-07 08:56:52,188 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 2523.41455078125
2023-01-07 08:56:52,188 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:52,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,188 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17262.91796875
2023-01-07 08:56:52,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9860767126083374
2023-01-07 08:56:52,189 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -377.2224426269531
2023-01-07 08:56:52,189 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:52,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,190 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17266.015625
2023-01-07 08:56:52,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.130359411239624
2023-01-07 08:56:52,191 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 17267.326171875
2023-01-07 08:56:52,191 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:52,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:52,192 > [DEBUG] 0 :: 7.318024635314941
2023-01-07 08:56:52,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,196 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.034393310546875
2023-01-07 08:56:52,196 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,196 > [DEBUG] 0 :: before allreduce fusion buffer :: -280.59173583984375
2023-01-07 08:56:52,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,199 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3489060699939728
2023-01-07 08:56:52,199 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,200 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -288.563720703125
2023-01-07 08:56:52,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -297.6935729980469
2023-01-07 08:56:52,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,204 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 0.5993870496749878
2023-01-07 08:56:52,204 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,204 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012553386390209198
2023-01-07 08:56:52,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,207 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.012524444609880447
2023-01-07 08:56:52,207 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,207 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -59.0656852722168
2023-01-07 08:56:52,207 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3689749240875244
2023-01-07 08:56:52,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,209 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -12.551277160644531
2023-01-07 08:56:52,209 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03711560368537903
2023-01-07 08:56:52,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,210 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.012343496084213257
2023-01-07 08:56:52,210 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,210 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -12.362892150878906
2023-01-07 08:56:52,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30990248918533325
2023-01-07 08:56:52,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,212 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -4.017026901245117
2023-01-07 08:56:52,212 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29175201058387756
2023-01-07 08:56:52,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,213 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2888917922973633
2023-01-07 08:56:52,213 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,214 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -4.324126243591309
2023-01-07 08:56:52,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2890009880065918
2023-01-07 08:56:52,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,215 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.4027838706970215
2023-01-07 08:56:52,215 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06490756571292877
2023-01-07 08:56:52,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0042115189135074615
2023-01-07 08:56:52,217 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,217 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.730027198791504
2023-01-07 08:56:52,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3798420429229736
2023-01-07 08:56:52,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.344059944152832
2023-01-07 08:56:52,218 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9457879662513733
2023-01-07 08:56:52,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,220 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.00041499175131320953
2023-01-07 08:56:52,220 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,220 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.723936080932617
2023-01-07 08:56:52,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24550072848796844
2023-01-07 08:56:52,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -7.955151557922363
2023-01-07 08:56:52,222 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0633258819580078
2023-01-07 08:56:52,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,223 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.38653168082237244
2023-01-07 08:56:52,223 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,223 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -7.404109954833984
2023-01-07 08:56:52,223 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7689100503921509
2023-01-07 08:56:52,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,225 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -24.930206298828125
2023-01-07 08:56:52,225 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06800735741853714
2023-01-07 08:56:52,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,226 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.2733457386493683
2023-01-07 08:56:52,226 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,226 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.985424041748047
2023-01-07 08:56:52,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4095866084098816
2023-01-07 08:56:52,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,228 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -3.430818557739258
2023-01-07 08:56:52,228 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,228 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5480622053146362
2023-01-07 08:56:52,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,229 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.06738027185201645
2023-01-07 08:56:52,229 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,229 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2.123241901397705
2023-01-07 08:56:52,230 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5682807564735413
2023-01-07 08:56:52,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,231 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 6.040398597717285
2023-01-07 08:56:52,231 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9841721653938293
2023-01-07 08:56:52,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,232 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.023431159555912018
2023-01-07 08:56:52,232 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,233 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 6.091274261474609
2023-01-07 08:56:52,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.177526593208313
2023-01-07 08:56:52,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,234 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 19.47510528564453
2023-01-07 08:56:52,234 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6452646851539612
2023-01-07 08:56:52,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,236 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.02512955665588379
2023-01-07 08:56:52,236 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 17.729320526123047
2023-01-07 08:56:52,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8269798159599304
2023-01-07 08:56:52,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.864194869995117
2023-01-07 08:56:52,238 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40263357758522034
2023-01-07 08:56:52,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,239 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.10018548369407654
2023-01-07 08:56:52,239 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.503298282623291
2023-01-07 08:56:52,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4981316328048706
2023-01-07 08:56:52,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 9.203859329223633
2023-01-07 08:56:52,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6476598381996155
2023-01-07 08:56:52,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.13822203874588013
2023-01-07 08:56:52,242 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 9.771251678466797
2023-01-07 08:56:52,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.855506420135498
2023-01-07 08:56:52,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 14.326595306396484
2023-01-07 08:56:52,244 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7084413766860962
2023-01-07 08:56:52,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.08457055687904358
2023-01-07 08:56:52,246 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 14.382049560546875
2023-01-07 08:56:52,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.396695077419281
2023-01-07 08:56:52,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.7903633117675781
2023-01-07 08:56:52,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5458119511604309
2023-01-07 08:56:52,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.18771222233772278
2023-01-07 08:56:52,249 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 2.369361639022827
2023-01-07 08:56:52,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7612948417663574
2023-01-07 08:56:52,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 15.885971069335938
2023-01-07 08:56:52,251 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7188634872436523
2023-01-07 08:56:52,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.050661228597164154
2023-01-07 08:56:52,252 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 15.137418746948242
2023-01-07 08:56:52,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5129996538162231
2023-01-07 08:56:52,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,254 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -9.761515617370605
2023-01-07 08:56:52,254 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5856748223304749
2023-01-07 08:56:52,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.05135834217071533
2023-01-07 08:56:52,255 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -8.141725540161133
2023-01-07 08:56:52,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1077075004577637
2023-01-07 08:56:52,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.404061794281006
2023-01-07 08:56:52,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7612210512161255
2023-01-07 08:56:52,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.028297126293182373
2023-01-07 08:56:52,258 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -6.9791364669799805
2023-01-07 08:56:52,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4651575088500977
2023-01-07 08:56:52,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -57.78248596191406
2023-01-07 08:56:52,260 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16427750885486603
2023-01-07 08:56:52,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.15712226927280426
2023-01-07 08:56:52,261 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -57.6036376953125
2023-01-07 08:56:52,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.168459415435791
2023-01-07 08:56:52,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 5.155121326446533
2023-01-07 08:56:52,263 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2100446224212646
2023-01-07 08:56:52,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.06103735789656639
2023-01-07 08:56:52,264 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,265 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 5.9945878982543945
2023-01-07 08:56:52,265 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42239630222320557
2023-01-07 08:56:52,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -24.845718383789062
2023-01-07 08:56:52,266 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2634873390197754
2023-01-07 08:56:52,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,268 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.18748006224632263
2023-01-07 08:56:52,268 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,268 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -24.152809143066406
2023-01-07 08:56:52,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.988919734954834
2023-01-07 08:56:52,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,269 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 11.947105407714844
2023-01-07 08:56:52,270 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2605252265930176
2023-01-07 08:56:52,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,271 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.21904803812503815
2023-01-07 08:56:52,271 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,271 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 12.931316375732422
2023-01-07 08:56:52,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.492016315460205
2023-01-07 08:56:52,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.4631104469299316
2023-01-07 08:56:52,273 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7543089389801025
2023-01-07 08:56:52,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,274 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.662529706954956
2023-01-07 08:56:52,274 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,274 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -7.114428997039795
2023-01-07 08:56:52,274 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.742767333984375
2023-01-07 08:56:52,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,276 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 5.921548843383789
2023-01-07 08:56:52,276 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.327336311340332
2023-01-07 08:56:52,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,277 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.15452060103416443
2023-01-07 08:56:52,277 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 7.699852466583252
2023-01-07 08:56:52,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19514167308807373
2023-01-07 08:56:52,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,279 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -13.26500129699707
2023-01-07 08:56:52,279 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.480149269104004
2023-01-07 08:56:52,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,280 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -2.957103729248047
2023-01-07 08:56:52,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.435230255126953
2023-01-07 08:56:52,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,282 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.581993103027344
2023-01-07 08:56:52,282 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5914580821990967
2023-01-07 08:56:52,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,283 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.1202121376991272
2023-01-07 08:56:52,283 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 6.067665100097656
2023-01-07 08:56:52,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7595620155334473
2023-01-07 08:56:52,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,285 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -45.27328872680664
2023-01-07 08:56:52,285 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26874226331710815
2023-01-07 08:56:52,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,286 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.042129456996917725
2023-01-07 08:56:52,286 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,287 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -35.136878967285156
2023-01-07 08:56:52,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.934917449951172
2023-01-07 08:56:52,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,288 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -12.717583656311035
2023-01-07 08:56:52,288 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8441498279571533
2023-01-07 08:56:52,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,289 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.39699625968933105
2023-01-07 08:56:52,290 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,290 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -0.16960430145263672
2023-01-07 08:56:52,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.190923690795898
2023-01-07 08:56:52,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,291 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 4.390220642089844
2023-01-07 08:56:52,292 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.745870113372803
2023-01-07 08:56:52,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,293 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.4020309448242188
2023-01-07 08:56:52,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.228848457336426
2023-01-07 08:56:52,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,294 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 0.3133506774902344
2023-01-07 08:56:52,294 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6540775299072266
2023-01-07 08:56:52,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,296 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -3.9580955505371094
2023-01-07 08:56:52,296 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.506523609161377
2023-01-07 08:56:52,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 11.548980712890625
2023-01-07 08:56:52,297 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.033275842666626
2023-01-07 08:56:52,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 25.163875579833984
2023-01-07 08:56:52,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.420473098754883
2023-01-07 08:56:52,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 18.395376205444336
2023-01-07 08:56:52,300 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.263678550720215
2023-01-07 08:56:52,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 21.606111526489258
2023-01-07 08:56:52,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.303796768188477
2023-01-07 08:56:52,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,302 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.028837203979492
2023-01-07 08:56:52,303 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0110907554626465
2023-01-07 08:56:52,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.298511505126953
2023-01-07 08:56:52,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.463074207305908
2023-01-07 08:56:52,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 7.923624038696289
2023-01-07 08:56:52,305 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6749346256256104
2023-01-07 08:56:52,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 14.131370544433594
2023-01-07 08:56:52,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.319871425628662
2023-01-07 08:56:52,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -43.441444396972656
2023-01-07 08:56:52,308 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.110038757324219
2023-01-07 08:56:52,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.6548957824707
2023-01-07 08:56:52,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.827404975891113
2023-01-07 08:56:52,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -58.12214660644531
2023-01-07 08:56:52,311 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.164055824279785
2023-01-07 08:56:52,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,312 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.027177542448043823
2023-01-07 08:56:52,312 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,312 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -54.421875
2023-01-07 08:56:52,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8834214210510254
2023-01-07 08:56:52,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -20.473880767822266
2023-01-07 08:56:52,314 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4924206733703613
2023-01-07 08:56:52,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 3.031325340270996
2023-01-07 08:56:52,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.864481925964355
2023-01-07 08:56:52,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 235.43174743652344
2023-01-07 08:56:52,317 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.798339366912842
2023-01-07 08:56:52,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 228.17237854003906
2023-01-07 08:56:52,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.80373764038086
2023-01-07 08:56:52,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,320 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -21.305904388427734
2023-01-07 08:56:52,320 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.895026683807373
2023-01-07 08:56:52,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,321 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -2.974299192428589
2023-01-07 08:56:52,321 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,321 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -15.914788246154785
2023-01-07 08:56:52,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7076781988143921
2023-01-07 08:56:52,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,323 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 24.37027359008789
2023-01-07 08:56:52,324 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1446085274219513
2023-01-07 08:56:52,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -118.03986358642578
2023-01-07 08:56:52,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.10064172744751
2023-01-07 08:56:52,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -2.573697566986084
2023-01-07 08:56:52,327 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,327 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9710060358047485
2023-01-07 08:56:52,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 1306.5196533203125
2023-01-07 08:56:52,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6649293899536133
2023-01-07 08:56:52,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,329 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 113.83494567871094
2023-01-07 08:56:52,329 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.034526824951172
2023-01-07 08:56:52,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,331 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.5638679265975952
2023-01-07 08:56:52,331 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,331 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 320.95635986328125
2023-01-07 08:56:52,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.535590171813965
2023-01-07 08:56:52,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,332 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 551.4736328125
2023-01-07 08:56:52,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.46980357170105
2023-01-07 08:56:52,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,334 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.0378797054290771
2023-01-07 08:56:52,334 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -25.71936798095703
2023-01-07 08:56:52,334 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 792.1783447265625
2023-01-07 08:56:52,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.879457473754883
2023-01-07 08:56:52,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 791.9036865234375
2023-01-07 08:56:52,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.019387245178223
2023-01-07 08:56:52,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 237.78408813476562
2023-01-07 08:56:52,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.32778549194336
2023-01-07 08:56:52,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,339 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -110.31195831298828
2023-01-07 08:56:52,339 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.61085510253906
2023-01-07 08:56:52,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.40065354108810425
2023-01-07 08:56:52,340 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 93.35963439941406
2023-01-07 08:56:52,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.61554718017578
2023-01-07 08:56:52,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,342 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 266.8982238769531
2023-01-07 08:56:52,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.16458511352539
2023-01-07 08:56:52,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,343 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.8017218112945557
2023-01-07 08:56:52,343 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,343 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 48.471336364746094
2023-01-07 08:56:52,343 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.26038932800293
2023-01-07 08:56:52,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 443.1684265136719
2023-01-07 08:56:52,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.851858139038086
2023-01-07 08:56:52,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 4.179340362548828
2023-01-07 08:56:52,346 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,346 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1014.5230712890625
2023-01-07 08:56:52,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.883087158203125
2023-01-07 08:56:52,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,348 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 656.75732421875
2023-01-07 08:56:52,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.678227424621582
2023-01-07 08:56:52,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,349 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 287.6646728515625
2023-01-07 08:56:52,349 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.197953224182129
2023-01-07 08:56:52,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,351 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 838.738037109375
2023-01-07 08:56:52,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.500803232192993
2023-01-07 08:56:52,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,352 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.437305450439453
2023-01-07 08:56:52,352 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,352 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -0.27518653869628906
2023-01-07 08:56:52,353 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,353 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1029.820556640625
2023-01-07 08:56:52,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,353 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1031.8758544921875
2023-01-07 08:56:52,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.60289001464844
2023-01-07 08:56:52,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,355 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1078.50927734375
2023-01-07 08:56:52,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.59490203857422
2023-01-07 08:56:52,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,356 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 160.84515380859375
2023-01-07 08:56:52,356 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 159.36532592773438
2023-01-07 08:56:52,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1308.4476318359375
2023-01-07 08:56:52,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5809553861618042
2023-01-07 08:56:52,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,359 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 456.25421142578125
2023-01-07 08:56:52,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,359 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1017.2928466796875
2023-01-07 08:56:52,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.911296844482422
2023-01-07 08:56:52,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 330.1343994140625
2023-01-07 08:56:52,360 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9925785064697266
2023-01-07 08:56:52,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,362 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 1.7712090015411377
2023-01-07 08:56:52,362 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,362 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 560.5313110351562
2023-01-07 08:56:52,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 224.77015686035156
2023-01-07 08:56:52,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 923.0340576171875
2023-01-07 08:56:52,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.45520782470703
2023-01-07 08:56:52,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,365 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 923.0340576171875
2023-01-07 08:56:52,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -127.41849517822266
2023-01-07 08:56:52,369 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:56:52,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,370 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 5920.26416015625
2023-01-07 08:56:52,370 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:52,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,371 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1674.590576171875
2023-01-07 08:56:52,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,372 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1033.949462890625
2023-01-07 08:56:52,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,372 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4713.57177734375
2023-01-07 08:56:52,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,373 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2130.921142578125
2023-01-07 08:56:52,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,374 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -118.59416198730469
2023-01-07 08:56:52,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 58.31907653808594
2023-01-07 08:56:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -24.397640228271484
2023-01-07 08:56:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -46.7860107421875
2023-01-07 08:56:52,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -144.47418212890625
2023-01-07 08:56:52,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 44.47386169433594
2023-01-07 08:56:52,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -40.52426528930664
2023-01-07 08:56:52,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -21.43395233154297
2023-01-07 08:56:52,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 23.323659896850586
2023-01-07 08:56:52,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -35.84950256347656
2023-01-07 08:56:52,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 18.039772033691406
2023-01-07 08:56:52,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 9.544447898864746
2023-01-07 08:56:52,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -38.24993133544922
2023-01-07 08:56:52,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 27.141563415527344
2023-01-07 08:56:52,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 22.5775146484375
2023-01-07 08:56:52,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -4.3050456047058105
2023-01-07 08:56:52,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -4.471988201141357
2023-01-07 08:56:52,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 18.978620529174805
2023-01-07 08:56:52,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.8539037704467773
2023-01-07 08:56:52,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 5.707426071166992
2023-01-07 08:56:52,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -50.88428497314453
2023-01-07 08:56:52,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 5949.70556640625
2023-01-07 08:56:52,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -12.435639381408691
2023-01-07 08:56:52,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -5.46608829498291
2023-01-07 08:56:52,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 18.828489303588867
2023-01-07 08:56:52,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 5.443352699279785
2023-01-07 08:56:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 14.879363059997559
2023-01-07 08:56:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 9.490835189819336
2023-01-07 08:56:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 8.59981918334961
2023-01-07 08:56:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 17.47734260559082
2023-01-07 08:56:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 4.983367919921875
2023-01-07 08:56:52,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.29566955566406
2023-01-07 08:56:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.447160243988037
2023-01-07 08:56:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -24.456214904785156
2023-01-07 08:56:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -4.776134490966797
2023-01-07 08:56:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,389 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.822188377380371
2023-01-07 08:56:52,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.743160247802734
2023-01-07 08:56:52,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.0659061670303345
2023-01-07 08:56:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.486024856567383
2023-01-07 08:56:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -12.398612022399902
2023-01-07 08:56:52,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.59317398071289
2023-01-07 08:56:52,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,391 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -59.227230072021484
2023-01-07 08:56:52,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:52,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:52,391 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -639.8241577148438
2023-01-07 08:56:52,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 1248.7607421875
2023-01-07 08:56:53,232 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 672.28515625
2023-01-07 08:56:53,232 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,232 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:53,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,232 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 0.6118674278259277
2023-01-07 08:56:53,233 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,233 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1751.527099609375
2023-01-07 08:56:53,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 174.81988525390625
2023-01-07 08:56:53,234 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.08170318603515625
2023-01-07 08:56:53,234 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -12.131708145141602
2023-01-07 08:56:53,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -50.517417907714844
2023-01-07 08:56:53,235 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1762.7838134765625
2023-01-07 08:56:53,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.582740783691406
2023-01-07 08:56:53,236 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -135.2014617919922
2023-01-07 08:56:53,237 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:53,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,237 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1765.378173828125
2023-01-07 08:56:53,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.532489776611328
2023-01-07 08:56:53,238 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 13.095867156982422
2023-01-07 08:56:53,238 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,238 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:53,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,239 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1782.98876953125
2023-01-07 08:56:53,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.875133514404297
2023-01-07 08:56:53,240 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 1818.0655517578125
2023-01-07 08:56:53,240 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:53,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,240 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -7.226799964904785
2023-01-07 08:56:53,240 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,241 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -27.508575439453125
2023-01-07 08:56:53,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,241 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1566.80810546875
2023-01-07 08:56:53,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.93685531616211
2023-01-07 08:56:53,242 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -10.559122085571289
2023-01-07 08:56:53,242 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:53,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,243 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1538.5054931640625
2023-01-07 08:56:53,243 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.89460754394531
2023-01-07 08:56:53,244 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 36.464927673339844
2023-01-07 08:56:53,244 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:53,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,245 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -6.158586502075195
2023-01-07 08:56:53,245 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,245 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 228.88497924804688
2023-01-07 08:56:53,245 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.6058349609375
2023-01-07 08:56:53,246 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -2.148587703704834
2023-01-07 08:56:53,246 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:53,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,247 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 498.40802001953125
2023-01-07 08:56:53,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -109.16358947753906
2023-01-07 08:56:53,248 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 876.7357788085938
2023-01-07 08:56:53,248 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:53,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,248 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 431.96160888671875
2023-01-07 08:56:53,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.56464195251465
2023-01-07 08:56:53,249 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -43.04926300048828
2023-01-07 08:56:53,249 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:53,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,250 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 345.178955078125
2023-01-07 08:56:53,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.795772552490234
2023-01-07 08:56:53,251 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 228.02894592285156
2023-01-07 08:56:53,251 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:53,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,252 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -0.6652990579605103
2023-01-07 08:56:53,252 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,252 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 382.2817077636719
2023-01-07 08:56:53,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 324.359130859375
2023-01-07 08:56:53,253 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -2.670441150665283
2023-01-07 08:56:53,254 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:53,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,254 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 425.6058654785156
2023-01-07 08:56:53,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,254 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 844.5078735351562
2023-01-07 08:56:53,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.03435516357422
2023-01-07 08:56:53,256 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 447.66741943359375
2023-01-07 08:56:53,256 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:53,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,256 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.08800554275512695
2023-01-07 08:56:53,256 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,256 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 209.13595581054688
2023-01-07 08:56:53,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.327117919921875
2023-01-07 08:56:53,258 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.4437904357910156
2023-01-07 08:56:53,258 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:53,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,258 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1505.2628173828125
2023-01-07 08:56:53,258 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.533058166503906
2023-01-07 08:56:53,259 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 200.6431884765625
2023-01-07 08:56:53,259 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:53,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 341.92279052734375
2023-01-07 08:56:53,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.64995765686035
2023-01-07 08:56:53,261 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -82.18244934082031
2023-01-07 08:56:53,261 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:53,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,261 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 349.78314208984375
2023-01-07 08:56:53,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,262 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1479.243896484375
2023-01-07 08:56:53,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.144427299499512
2023-01-07 08:56:53,263 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 346.1536865234375
2023-01-07 08:56:53,263 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:53,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,264 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1554.003662109375
2023-01-07 08:56:53,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.463884353637695
2023-01-07 08:56:53,265 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -11.098278999328613
2023-01-07 08:56:53,265 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:53,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1584.164306640625
2023-01-07 08:56:53,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.02151107788086
2023-01-07 08:56:53,266 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1603.06884765625
2023-01-07 08:56:53,267 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:53,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,267 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.18832719326019287
2023-01-07 08:56:53,267 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,267 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -29.732112884521484
2023-01-07 08:56:53,267 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,268 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 453.0262451171875
2023-01-07 08:56:53,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.37055969238281
2023-01-07 08:56:53,269 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.5918539762496948
2023-01-07 08:56:53,269 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:53,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,270 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 874.5302124023438
2023-01-07 08:56:53,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.05585479736328
2023-01-07 08:56:53,271 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -48.08641052246094
2023-01-07 08:56:53,271 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.465963363647461
2023-01-07 08:56:53,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 447.7637634277344
2023-01-07 08:56:53,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.621763229370117
2023-01-07 08:56:53,273 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -22.93844985961914
2023-01-07 08:56:53,273 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,273 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:53,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,273 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 886.02978515625
2023-01-07 08:56:53,273 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.572181701660156
2023-01-07 08:56:53,274 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 409.735107421875
2023-01-07 08:56:53,274 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:53,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 901.5703735351562
2023-01-07 08:56:53,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.244122505187988
2023-01-07 08:56:53,276 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 3.1332571506500244
2023-01-07 08:56:53,276 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:53,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 855.3720703125
2023-01-07 08:56:53,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.712743759155273
2023-01-07 08:56:53,278 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 816.1270141601562
2023-01-07 08:56:53,278 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.8482694029808044
2023-01-07 08:56:53,278 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4733.23486328125
2023-01-07 08:56:53,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.556041717529297
2023-01-07 08:56:53,280 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 2.8360629081726074
2023-01-07 08:56:53,280 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,280 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.756298542022705
2023-01-07 08:56:53,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4732.8232421875
2023-01-07 08:56:53,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9521985054016113
2023-01-07 08:56:53,281 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 4736.5830078125
2023-01-07 08:56:53,282 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,282 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,282 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:53,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 2.7343034744262695
2023-01-07 08:56:53,282 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2108.421142578125
2023-01-07 08:56:53,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.180039167404175
2023-01-07 08:56:53,283 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 3.685544729232788
2023-01-07 08:56:53,284 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -14.141868591308594
2023-01-07 08:56:53,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,284 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2086.716796875
2023-01-07 08:56:53,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.712442874908447
2023-01-07 08:56:53,285 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2095.948974609375
2023-01-07 08:56:53,285 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:53,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -82.94963073730469
2023-01-07 08:56:53,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.507823944091797
2023-01-07 08:56:53,287 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -10.530197143554688
2023-01-07 08:56:53,287 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:53,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,287 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -96.37506866455078
2023-01-07 08:56:53,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9132722616195679
2023-01-07 08:56:53,288 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -88.48123931884766
2023-01-07 08:56:53,288 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.9017235040664673
2023-01-07 08:56:53,289 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -125.67007446289062
2023-01-07 08:56:53,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.513748168945312
2023-01-07 08:56:53,290 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.30460965633392334
2023-01-07 08:56:53,290 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.551721572875977
2023-01-07 08:56:53,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,291 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -97.6201171875
2023-01-07 08:56:53,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7609431743621826
2023-01-07 08:56:53,292 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -100.45642852783203
2023-01-07 08:56:53,292 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.9432191848754883
2023-01-07 08:56:53,293 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -45.021575927734375
2023-01-07 08:56:53,293 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.031852722167969
2023-01-07 08:56:53,294 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.9703824520111084
2023-01-07 08:56:53,294 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.180285930633545
2023-01-07 08:56:53,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -32.095741271972656
2023-01-07 08:56:53,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.800416469573975
2023-01-07 08:56:53,296 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -29.703880310058594
2023-01-07 08:56:53,296 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:53,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -46.53555679321289
2023-01-07 08:56:53,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.901456832885742
2023-01-07 08:56:53,297 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 2.7384161949157715
2023-01-07 08:56:53,298 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:53,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -36.775726318359375
2023-01-07 08:56:53,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.9657697677612305
2023-01-07 08:56:53,299 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -41.00751495361328
2023-01-07 08:56:53,299 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5503941774368286
2023-01-07 08:56:53,300 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -141.73822021484375
2023-01-07 08:56:53,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.79608154296875
2023-01-07 08:56:53,301 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.6887650489807129
2023-01-07 08:56:53,301 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 8.06689453125
2023-01-07 08:56:53,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,302 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -152.49253845214844
2023-01-07 08:56:53,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.155853271484375
2023-01-07 08:56:53,303 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -140.72207641601562
2023-01-07 08:56:53,303 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,303 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.30645787715911865
2023-01-07 08:56:53,304 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 45.251094818115234
2023-01-07 08:56:53,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5404038429260254
2023-01-07 08:56:53,305 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.5727338790893555
2023-01-07 08:56:53,305 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 30.583946228027344
2023-01-07 08:56:53,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 39.35926818847656
2023-01-07 08:56:53,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.435121536254883
2023-01-07 08:56:53,307 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40.13117218017578
2023-01-07 08:56:53,307 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:53,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4368807375431061
2023-01-07 08:56:53,307 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -41.62066650390625
2023-01-07 08:56:53,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4687696695327759
2023-01-07 08:56:53,309 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.3608768582344055
2023-01-07 08:56:53,309 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -70.87334442138672
2023-01-07 08:56:53,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -36.967918395996094
2023-01-07 08:56:53,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.821382522583008
2023-01-07 08:56:53,310 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -45.317108154296875
2023-01-07 08:56:53,310 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.5593892335891724
2023-01-07 08:56:53,311 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -23.732330322265625
2023-01-07 08:56:53,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.926347732543945
2023-01-07 08:56:53,313 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.7053357362747192
2023-01-07 08:56:53,313 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.382711410522461
2023-01-07 08:56:53,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -31.634140014648438
2023-01-07 08:56:53,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.035826683044434
2023-01-07 08:56:53,315 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -36.748626708984375
2023-01-07 08:56:53,315 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,315 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:53,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.09683078527450562
2023-01-07 08:56:53,315 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 22.88404083251953
2023-01-07 08:56:53,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.268707275390625
2023-01-07 08:56:53,317 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.24398449063301086
2023-01-07 08:56:53,317 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,317 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -10.846620559692383
2023-01-07 08:56:53,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 28.23294448852539
2023-01-07 08:56:53,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.414234161376953
2023-01-07 08:56:53,318 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 30.92533302307129
2023-01-07 08:56:53,318 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:53,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.5340415239334106
2023-01-07 08:56:53,319 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,319 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -57.98290252685547
2023-01-07 08:56:53,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24084365367889404
2023-01-07 08:56:53,320 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.6380495429039001
2023-01-07 08:56:53,320 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -31.834835052490234
2023-01-07 08:56:53,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,321 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -56.48504638671875
2023-01-07 08:56:53,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.075842380523682
2023-01-07 08:56:53,322 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -56.95956802368164
2023-01-07 08:56:53,322 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:53,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,323 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.013422369956970215
2023-01-07 08:56:53,323 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,323 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -520.6732177734375
2023-01-07 08:56:53,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.602176666259766
2023-01-07 08:56:53,324 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.20240187644958496
2023-01-07 08:56:53,324 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -29.614931106567383
2023-01-07 08:56:53,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,325 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -542.9841918945312
2023-01-07 08:56:53,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.609831809997559
2023-01-07 08:56:53,326 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -551.20068359375
2023-01-07 08:56:53,326 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:53,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,326 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 126.72583770751953
2023-01-07 08:56:53,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.042889595031738
2023-01-07 08:56:53,328 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -1.8752717971801758
2023-01-07 08:56:53,328 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,328 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,328 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:53,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 123.17019653320312
2023-01-07 08:56:53,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2308574914932251
2023-01-07 08:56:53,329 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 118.66728973388672
2023-01-07 08:56:53,329 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,330 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:53,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 475.95843505859375
2023-01-07 08:56:53,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40187138319015503
2023-01-07 08:56:53,331 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 18.74437713623047
2023-01-07 08:56:53,331 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:53,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 478.21502685546875
2023-01-07 08:56:53,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6296749114990234
2023-01-07 08:56:53,333 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 473.5004577636719
2023-01-07 08:56:53,333 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:53,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 266.5610656738281
2023-01-07 08:56:53,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.006683349609375
2023-01-07 08:56:53,334 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9.729874610900879
2023-01-07 08:56:53,334 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,334 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:53,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,335 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 268.53509521484375
2023-01-07 08:56:53,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.619100332260132
2023-01-07 08:56:53,336 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 267.8714904785156
2023-01-07 08:56:53,336 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:53,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.3324684500694275
2023-01-07 08:56:53,337 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:56:53,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 624.102783203125
2023-01-07 08:56:53,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4564096927642822
2023-01-07 08:56:53,338 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.2906177043914795
2023-01-07 08:56:53,338 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.49056625366211
2023-01-07 08:56:53,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,339 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 629.1519775390625
2023-01-07 08:56:53,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0194146633148193
2023-01-07 08:56:53,340 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 632.5821533203125
2023-01-07 08:56:53,340 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:53,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,340 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 109.18815612792969
2023-01-07 08:56:53,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0068864822387695
2023-01-07 08:56:53,341 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.08969050645828247
2023-01-07 08:56:53,342 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:53,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,342 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 109.68702697753906
2023-01-07 08:56:53,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3307876586914062
2023-01-07 08:56:53,343 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 109.14591979980469
2023-01-07 08:56:53,343 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:53,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,344 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -14.94264030456543
2023-01-07 08:56:53,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.774059295654297
2023-01-07 08:56:53,345 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 3.493333578109741
2023-01-07 08:56:53,345 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,345 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:53,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.243096351623535
2023-01-07 08:56:53,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19625946879386902
2023-01-07 08:56:53,347 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -14.640670776367188
2023-01-07 08:56:53,347 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:53,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 12.294761657714844
2023-01-07 08:56:53,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.32881498336792
2023-01-07 08:56:53,348 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.9199651479721069
2023-01-07 08:56:53,348 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:53,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 7.525427341461182
2023-01-07 08:56:53,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9654154777526855
2023-01-07 08:56:53,350 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 3.921766757965088
2023-01-07 08:56:53,350 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:53,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 16.14761734008789
2023-01-07 08:56:53,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.629963755607605
2023-01-07 08:56:53,352 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.34211766719818115
2023-01-07 08:56:53,352 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:53,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.436758995056152
2023-01-07 08:56:53,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3226984739303589
2023-01-07 08:56:53,353 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 13.488962173461914
2023-01-07 08:56:53,353 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 0.8278944492340088
2023-01-07 08:56:53,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11412817239761353
2023-01-07 08:56:53,355 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.10009104013442993
2023-01-07 08:56:53,355 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:53,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1.3857216835021973
2023-01-07 08:56:53,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.284027099609375
2023-01-07 08:56:53,357 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -0.5676066875457764
2023-01-07 08:56:53,357 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:53,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1351.2178955078125
2023-01-07 08:56:53,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.250868320465088
2023-01-07 08:56:53,358 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.07382568717002869
2023-01-07 08:56:53,358 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:53,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,359 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1349.38671875
2023-01-07 08:56:53,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7653635144233704
2023-01-07 08:56:53,360 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 1349.4783935546875
2023-01-07 08:56:53,360 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:53,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -17.431549072265625
2023-01-07 08:56:53,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9239373207092285
2023-01-07 08:56:53,362 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.03344005346298218
2023-01-07 08:56:53,362 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:53,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -20.64665412902832
2023-01-07 08:56:53,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4547767639160156
2023-01-07 08:56:53,363 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -21.16842269897461
2023-01-07 08:56:53,363 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:53,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -40.49122619628906
2023-01-07 08:56:53,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9116916656494141
2023-01-07 08:56:53,365 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.3878743350505829
2023-01-07 08:56:53,365 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:53,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,366 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -39.71113586425781
2023-01-07 08:56:53,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4866505265235901
2023-01-07 08:56:53,367 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -40.93069839477539
2023-01-07 08:56:53,367 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:53,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -178.1703643798828
2023-01-07 08:56:53,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5959013104438782
2023-01-07 08:56:53,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.3117832541465759
2023-01-07 08:56:53,369 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:53,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -182.00885009765625
2023-01-07 08:56:53,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.335805058479309
2023-01-07 08:56:53,370 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -182.36924743652344
2023-01-07 08:56:53,370 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:53,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1604.4620361328125
2023-01-07 08:56:53,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7128777503967285
2023-01-07 08:56:53,372 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.45878931879997253
2023-01-07 08:56:53,372 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:53,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1604.7369384765625
2023-01-07 08:56:53,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11596136540174484
2023-01-07 08:56:53,374 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1604.9205322265625
2023-01-07 08:56:53,374 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:53,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,374 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 488.981201171875
2023-01-07 08:56:53,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21235214173793793
2023-01-07 08:56:53,375 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.5874431133270264
2023-01-07 08:56:53,375 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:53,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 489.11187744140625
2023-01-07 08:56:53,376 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7489122748374939
2023-01-07 08:56:53,377 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 488.46636962890625
2023-01-07 08:56:53,377 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:53,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 626.4791259765625
2023-01-07 08:56:53,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7805928587913513
2023-01-07 08:56:53,379 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.2819763422012329
2023-01-07 08:56:53,380 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:53,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 625.9017333984375
2023-01-07 08:56:53,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6848164200782776
2023-01-07 08:56:53,381 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 627.8369140625
2023-01-07 08:56:53,381 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:53,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 550.5623779296875
2023-01-07 08:56:53,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47909319400787354
2023-01-07 08:56:53,383 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.04844701290130615
2023-01-07 08:56:53,383 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:53,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 549.611328125
2023-01-07 08:56:53,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3040546476840973
2023-01-07 08:56:53,385 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 549.8839111328125
2023-01-07 08:56:53,385 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:53,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 987.5957641601562
2023-01-07 08:56:53,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1805027425289154
2023-01-07 08:56:53,386 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.10128992795944214
2023-01-07 08:56:53,386 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:53,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,387 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 988.4959106445312
2023-01-07 08:56:53,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6166918277740479
2023-01-07 08:56:53,388 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 987.5897827148438
2023-01-07 08:56:53,388 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:53,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,389 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3523.26708984375
2023-01-07 08:56:53,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7103785276412964
2023-01-07 08:56:53,390 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.5291915535926819
2023-01-07 08:56:53,390 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:53,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3525.83154296875
2023-01-07 08:56:53,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.047208309173584
2023-01-07 08:56:53,392 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 3525.70751953125
2023-01-07 08:56:53,392 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:53,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,392 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -473.2720031738281
2023-01-07 08:56:53,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23991894721984863
2023-01-07 08:56:53,393 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.25356948375701904
2023-01-07 08:56:53,393 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:53,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -473.2660827636719
2023-01-07 08:56:53,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1603204011917114
2023-01-07 08:56:53,395 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -472.4140930175781
2023-01-07 08:56:53,395 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:53,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2763.827392578125
2023-01-07 08:56:53,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2716332972049713
2023-01-07 08:56:53,397 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 1.592608094215393
2023-01-07 08:56:53,397 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:53,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,397 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2763.81884765625
2023-01-07 08:56:53,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.231426477432251
2023-01-07 08:56:53,398 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2764.196533203125
2023-01-07 08:56:53,398 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:53,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,399 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1019.8787231445312
2023-01-07 08:56:53,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.48836028575897217
2023-01-07 08:56:53,400 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.7256437540054321
2023-01-07 08:56:53,400 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:53,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,401 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1020.2559814453125
2023-01-07 08:56:53,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10424677282571793
2023-01-07 08:56:53,402 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1019.5352783203125
2023-01-07 08:56:53,402 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:53,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,402 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4457.69873046875
2023-01-07 08:56:53,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3462496995925903
2023-01-07 08:56:53,403 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.3150312304496765
2023-01-07 08:56:53,404 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:53,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,404 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4457.83154296875
2023-01-07 08:56:53,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2698054313659668
2023-01-07 08:56:53,405 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 4458.03125
2023-01-07 08:56:53,405 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:53,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,406 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1899.711669921875
2023-01-07 08:56:53,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16851380467414856
2023-01-07 08:56:53,407 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.3192203640937805
2023-01-07 08:56:53,407 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:53,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,407 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1900.050537109375
2023-01-07 08:56:53,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4236258566379547
2023-01-07 08:56:53,409 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1899.428955078125
2023-01-07 08:56:53,409 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,409 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:53,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,409 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8456.236328125
2023-01-07 08:56:53,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11123490333557129
2023-01-07 08:56:53,410 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -0.44432029128074646
2023-01-07 08:56:53,410 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:53,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,411 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8456.375
2023-01-07 08:56:53,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3299487829208374
2023-01-07 08:56:53,412 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8455.537109375
2023-01-07 08:56:53,412 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:53,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,412 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 11678.0791015625
2023-01-07 08:56:53,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4670240879058838
2023-01-07 08:56:53,414 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.024625226855278015
2023-01-07 08:56:53,414 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,414 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:53,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,414 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 11677.447265625
2023-01-07 08:56:53,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43852564692497253
2023-01-07 08:56:53,415 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 11677.2646484375
2023-01-07 08:56:53,415 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:53,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,416 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1897.2601318359375
2023-01-07 08:56:53,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4049493968486786
2023-01-07 08:56:53,417 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.013156378641724586
2023-01-07 08:56:53,417 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:53,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,417 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1897.5633544921875
2023-01-07 08:56:53,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31113746762275696
2023-01-07 08:56:53,419 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -1897.3714599609375
2023-01-07 08:56:53,419 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,419 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,419 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:53,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,419 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13987.81640625
2023-01-07 08:56:53,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.931765079498291
2023-01-07 08:56:53,420 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -429.2581787109375
2023-01-07 08:56:53,420 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,421 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:53,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,421 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 13990.7421875
2023-01-07 08:56:53,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1453180313110352
2023-01-07 08:56:53,422 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 13991.970703125
2023-01-07 08:56:53,422 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:53,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:53,423 > [DEBUG] 0 :: 7.346672534942627
2023-01-07 08:56:53,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,427 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.01837158203125
2023-01-07 08:56:53,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -317.8331298828125
2023-01-07 08:56:53,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,429 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3573053777217865
2023-01-07 08:56:53,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,430 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -244.1998291015625
2023-01-07 08:56:53,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -377.09051513671875
2023-01-07 08:56:53,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.551902770996094
2023-01-07 08:56:53,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19220766425132751
2023-01-07 08:56:53,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.03740411996841431
2023-01-07 08:56:53,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -110.17507934570312
2023-01-07 08:56:53,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3747498691082001
2023-01-07 08:56:53,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,438 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -10.863367080688477
2023-01-07 08:56:53,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18829965591430664
2023-01-07 08:56:53,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.02528725191950798
2023-01-07 08:56:53,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -75.89193725585938
2023-01-07 08:56:53,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4435839354991913
2023-01-07 08:56:53,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,441 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -8.932303428649902
2023-01-07 08:56:53,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13504891097545624
2023-01-07 08:56:53,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.22935539484024048
2023-01-07 08:56:53,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -74.41878509521484
2023-01-07 08:56:53,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5136755704879761
2023-01-07 08:56:53,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.269137144088745
2023-01-07 08:56:53,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.49522238969802856
2023-01-07 08:56:53,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,446 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.041502002626657486
2023-01-07 08:56:53,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,446 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -62.473487854003906
2023-01-07 08:56:53,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8276203870773315
2023-01-07 08:56:53,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,448 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.287328243255615
2023-01-07 08:56:53,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7100592851638794
2023-01-07 08:56:53,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.06083023548126221
2023-01-07 08:56:53,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -57.5031623840332
2023-01-07 08:56:53,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06195736676454544
2023-01-07 08:56:53,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.5553908348083496
2023-01-07 08:56:53,451 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.089356929063797
2023-01-07 08:56:53,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,452 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.258257120847702
2023-01-07 08:56:53,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,452 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 3.6682777404785156
2023-01-07 08:56:53,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7698849439620972
2023-01-07 08:56:53,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,454 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -39.7999382019043
2023-01-07 08:56:53,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.040580034255981445
2023-01-07 08:56:53,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,455 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.2804950475692749
2023-01-07 08:56:53,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -37.22877502441406
2023-01-07 08:56:53,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1031086444854736
2023-01-07 08:56:53,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,457 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -17.094669342041016
2023-01-07 08:56:53,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10590796172618866
2023-01-07 08:56:53,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.09752359986305237
2023-01-07 08:56:53,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -15.070877075195312
2023-01-07 08:56:53,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4028172492980957
2023-01-07 08:56:53,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,460 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 38.522125244140625
2023-01-07 08:56:53,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0074270963668823
2023-01-07 08:56:53,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.030289694666862488
2023-01-07 08:56:53,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,462 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 39.108421325683594
2023-01-07 08:56:53,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.691582202911377
2023-01-07 08:56:53,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,463 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 14.105108261108398
2023-01-07 08:56:53,464 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4868347644805908
2023-01-07 08:56:53,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,465 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.044486161321401596
2023-01-07 08:56:53,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,465 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 16.730438232421875
2023-01-07 08:56:53,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6237869262695312
2023-01-07 08:56:53,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.4215507507324219
2023-01-07 08:56:53,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8416057825088501
2023-01-07 08:56:53,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.09930581599473953
2023-01-07 08:56:53,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.4311389923095703
2023-01-07 08:56:53,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9677835702896118
2023-01-07 08:56:53,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,470 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 28.18010711669922
2023-01-07 08:56:53,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9486609101295471
2023-01-07 08:56:53,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.1547086238861084
2023-01-07 08:56:53,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 28.86180305480957
2023-01-07 08:56:53,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6834158897399902
2023-01-07 08:56:53,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,473 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.023270606994629
2023-01-07 08:56:53,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2029333114624023
2023-01-07 08:56:53,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.3067101240158081
2023-01-07 08:56:53,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.240139961242676
2023-01-07 08:56:53,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.344208002090454
2023-01-07 08:56:53,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1.2535617351531982
2023-01-07 08:56:53,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18646392226219177
2023-01-07 08:56:53,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.012325655668973923
2023-01-07 08:56:53,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.8372480869293213
2023-01-07 08:56:53,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.461322546005249
2023-01-07 08:56:53,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 29.218769073486328
2023-01-07 08:56:53,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9221494197845459
2023-01-07 08:56:53,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.025004595518112183
2023-01-07 08:56:53,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 29.637611389160156
2023-01-07 08:56:53,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9212877750396729
2023-01-07 08:56:53,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,482 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 16.749252319335938
2023-01-07 08:56:53,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.362962245941162
2023-01-07 08:56:53,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,483 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.021441087126731873
2023-01-07 08:56:53,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 18.53192901611328
2023-01-07 08:56:53,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.363511323928833
2023-01-07 08:56:53,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,485 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -11.88479232788086
2023-01-07 08:56:53,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23592448234558105
2023-01-07 08:56:53,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.011646758764982224
2023-01-07 08:56:53,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,487 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.58350658416748
2023-01-07 08:56:53,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.243628740310669
2023-01-07 08:56:53,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 13.10364055633545
2023-01-07 08:56:53,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3461391925811768
2023-01-07 08:56:53,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.08758524060249329
2023-01-07 08:56:53,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 14.199339866638184
2023-01-07 08:56:53,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.037151575088501
2023-01-07 08:56:53,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -43.572486877441406
2023-01-07 08:56:53,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8689286708831787
2023-01-07 08:56:53,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.01158124953508377
2023-01-07 08:56:53,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -37.50784683227539
2023-01-07 08:56:53,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2710564136505127
2023-01-07 08:56:53,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.857511520385742
2023-01-07 08:56:53,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8676278591156006
2023-01-07 08:56:53,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.04680517315864563
2023-01-07 08:56:53,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -2.200900077819824
2023-01-07 08:56:53,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6233854293823242
2023-01-07 08:56:53,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 1.6649665832519531
2023-01-07 08:56:53,498 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4470903277397156
2023-01-07 08:56:53,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.22469650208950043
2023-01-07 08:56:53,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.833982467651367
2023-01-07 08:56:53,499 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8711676597595215
2023-01-07 08:56:53,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 4.7345476150512695
2023-01-07 08:56:53,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2881462574005127
2023-01-07 08:56:53,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.6138730049133301
2023-01-07 08:56:53,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.929171562194824
2023-01-07 08:56:53,502 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.905184268951416
2023-01-07 08:56:53,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -14.39091968536377
2023-01-07 08:56:53,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2003394365310669
2023-01-07 08:56:53,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.07984484732151031
2023-01-07 08:56:53,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -9.940850257873535
2023-01-07 08:56:53,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6352423429489136
2023-01-07 08:56:53,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,507 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -10.877082824707031
2023-01-07 08:56:53,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3535988330841064
2023-01-07 08:56:53,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -5.046678066253662
2023-01-07 08:56:53,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.843738555908203
2023-01-07 08:56:53,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,510 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 18.745548248291016
2023-01-07 08:56:53,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.747123718261719
2023-01-07 08:56:53,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.08007726818323135
2023-01-07 08:56:53,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.675731658935547
2023-01-07 08:56:53,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.39146900177002
2023-01-07 08:56:53,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -69.5852279663086
2023-01-07 08:56:53,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0898735523223877
2023-01-07 08:56:53,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.8723934888839722
2023-01-07 08:56:53,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -68.99911499023438
2023-01-07 08:56:53,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.493134081363678
2023-01-07 08:56:53,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,516 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -49.40555191040039
2023-01-07 08:56:53,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5667784214019775
2023-01-07 08:56:53,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.34326252341270447
2023-01-07 08:56:53,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -49.89620590209961
2023-01-07 08:56:53,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.68692684173584
2023-01-07 08:56:53,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,519 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -114.45578002929688
2023-01-07 08:56:53,519 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.284601211547852
2023-01-07 08:56:53,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -114.99888610839844
2023-01-07 08:56:53,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.104215621948242
2023-01-07 08:56:53,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,522 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 0.9140357971191406
2023-01-07 08:56:53,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.0892839431762695
2023-01-07 08:56:53,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -6.89836311340332
2023-01-07 08:56:53,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8970283269882202
2023-01-07 08:56:53,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -45.130706787109375
2023-01-07 08:56:53,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6447024345397949
2023-01-07 08:56:53,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -51.174102783203125
2023-01-07 08:56:53,526 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.800500869750977
2023-01-07 08:56:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,527 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -21.86506462097168
2023-01-07 08:56:53,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.944499969482422
2023-01-07 08:56:53,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,528 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -25.318941116333008
2023-01-07 08:56:53,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.988296508789062
2023-01-07 08:56:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -0.29551219940185547
2023-01-07 08:56:53,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.747452735900879
2023-01-07 08:56:53,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,531 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -36.06519317626953
2023-01-07 08:56:53,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.688213348388672
2023-01-07 08:56:53,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -11.20453929901123
2023-01-07 08:56:53,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4657384753227234
2023-01-07 08:56:53,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -34.08448028564453
2023-01-07 08:56:53,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.353278398513794
2023-01-07 08:56:53,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -84.67135620117188
2023-01-07 08:56:53,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15979748964309692
2023-01-07 08:56:53,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,536 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -107.21514892578125
2023-01-07 08:56:53,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5565550327301025
2023-01-07 08:56:53,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,538 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 51.22263717651367
2023-01-07 08:56:53,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2423582077026367
2023-01-07 08:56:53,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.27947258949279785
2023-01-07 08:56:53,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 24.119552612304688
2023-01-07 08:56:53,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.744087219238281
2023-01-07 08:56:53,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -8.201835632324219
2023-01-07 08:56:53,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.857670783996582
2023-01-07 08:56:53,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,542 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -44.20564651489258
2023-01-07 08:56:53,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.84984827041626
2023-01-07 08:56:53,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -173.66885375976562
2023-01-07 08:56:53,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.781487941741943
2023-01-07 08:56:53,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,545 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -181.59930419921875
2023-01-07 08:56:53,545 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.2425537109375
2023-01-07 08:56:53,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,547 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -34.935951232910156
2023-01-07 08:56:53,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.60230827331543
2023-01-07 08:56:53,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.2921385765075684
2023-01-07 08:56:53,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -57.27778244018555
2023-01-07 08:56:53,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.439523696899414
2023-01-07 08:56:53,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -105.34479522705078
2023-01-07 08:56:53,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9934585094451904
2023-01-07 08:56:53,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -110.37179565429688
2023-01-07 08:56:53,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.993734836578369
2023-01-07 08:56:53,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -83.13613891601562
2023-01-07 08:56:53,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5618017911911011
2023-01-07 08:56:53,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -121.89371490478516
2023-01-07 08:56:53,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.948160648345947
2023-01-07 08:56:53,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -162.23976135253906
2023-01-07 08:56:53,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.712732315063477
2023-01-07 08:56:53,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.8605053424835205
2023-01-07 08:56:53,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,557 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -194.6908416748047
2023-01-07 08:56:53,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.684200286865234
2023-01-07 08:56:53,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2187.23046875
2023-01-07 08:56:53,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10577774047851562
2023-01-07 08:56:53,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,559 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.539100170135498
2023-01-07 08:56:53,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -97.14569091796875
2023-01-07 08:56:53,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2184.150634765625
2023-01-07 08:56:53,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.24585723876953
2023-01-07 08:56:53,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3039.408203125
2023-01-07 08:56:53,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.842117309570312
2023-01-07 08:56:53,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 3426.40576171875
2023-01-07 08:56:53,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.4876708984375
2023-01-07 08:56:53,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,564 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -132.46176147460938
2023-01-07 08:56:53,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.191532135009766
2023-01-07 08:56:53,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,566 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.7377691268920898
2023-01-07 08:56:53,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,566 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -117.01607513427734
2023-01-07 08:56:53,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.264240264892578
2023-01-07 08:56:53,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,567 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1456.5970458984375
2023-01-07 08:56:53,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.51955032348633
2023-01-07 08:56:53,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,569 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -2.951353073120117
2023-01-07 08:56:53,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,569 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 35.98401641845703
2023-01-07 08:56:53,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.11418151855469
2023-01-07 08:56:53,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,570 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1491.033935546875
2023-01-07 08:56:53,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.652801513671875
2023-01-07 08:56:53,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,572 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -67.01406860351562
2023-01-07 08:56:53,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3033.213134765625
2023-01-07 08:56:53,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.224517822265625
2023-01-07 08:56:53,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,573 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1530.78369140625
2023-01-07 08:56:53,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.173420906066895
2023-01-07 08:56:53,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,575 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 32.2750244140625
2023-01-07 08:56:53,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -70.0752944946289
2023-01-07 08:56:53,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,576 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1528.744384765625
2023-01-07 08:56:53,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.70034408569336
2023-01-07 08:56:53,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,578 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.626872539520264
2023-01-07 08:56:53,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,578 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 23.895050048828125
2023-01-07 08:56:53,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,579 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1413.3934326171875
2023-01-07 08:56:53,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,579 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3025.740478515625
2023-01-07 08:56:53,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.7611083984375
2023-01-07 08:56:53,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,580 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1374.129638671875
2023-01-07 08:56:53,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.700008392333984
2023-01-07 08:56:53,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,582 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -151.162109375
2023-01-07 08:56:53,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -207.38638305664062
2023-01-07 08:56:53,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,583 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1352.91845703125
2023-01-07 08:56:53,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.881072998046875
2023-01-07 08:56:53,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,584 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -258.017333984375
2023-01-07 08:56:53,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3038.229248046875
2023-01-07 08:56:53,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.87578201293945
2023-01-07 08:56:53,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,587 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 495.1684265136719
2023-01-07 08:56:53,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.37374496459961
2023-01-07 08:56:53,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,588 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.693044662475586
2023-01-07 08:56:53,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 408.0201721191406
2023-01-07 08:56:53,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.22740936279297
2023-01-07 08:56:53,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,590 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 140.57846069335938
2023-01-07 08:56:53,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 177.42306518554688
2023-01-07 08:56:53,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,591 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -187.724365234375
2023-01-07 08:56:53,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.42257308959961
2023-01-07 08:56:53,593 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:56:53,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,594 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -4107.5654296875
2023-01-07 08:56:53,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,594 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -417.55572509765625
2023-01-07 08:56:53,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2995.198486328125
2023-01-07 08:56:53,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2263.839111328125
2023-01-07 08:56:53,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,596 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -155.91360473632812
2023-01-07 08:56:53,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,596 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -35.122745513916016
2023-01-07 08:56:53,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,597 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -121.37451171875
2023-01-07 08:56:53,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,597 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -127.24851989746094
2023-01-07 08:56:53,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,597 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 31.450321197509766
2023-01-07 08:56:53,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -126.11016845703125
2023-01-07 08:56:53,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -68.53775787353516
2023-01-07 08:56:53,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,599 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -36.47139358520508
2023-01-07 08:56:53,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,599 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -91.03768157958984
2023-01-07 08:56:53,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,600 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -63.893524169921875
2023-01-07 08:56:53,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -11.224870681762695
2023-01-07 08:56:53,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -143.66763305664062
2023-01-07 08:56:53,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -75.36573791503906
2023-01-07 08:56:53,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -73.55776977539062
2023-01-07 08:56:53,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 14.989724159240723
2023-01-07 08:56:53,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -7.5459818840026855
2023-01-07 08:56:53,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -17.729759216308594
2023-01-07 08:56:53,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -6.9525651931762695
2023-01-07 08:56:53,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -5.208625793457031
2023-01-07 08:56:53,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.0559806823730469
2023-01-07 08:56:53,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -32.933597564697266
2023-01-07 08:56:53,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,606 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 22.36821746826172
2023-01-07 08:56:53,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -4755.009765625
2023-01-07 08:56:53,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.979852676391602
2023-01-07 08:56:53,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 16.949222564697266
2023-01-07 08:56:53,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 27.503719329833984
2023-01-07 08:56:53,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.0065181255340576
2023-01-07 08:56:53,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.99544620513916
2023-01-07 08:56:53,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 30.592918395996094
2023-01-07 08:56:53,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,610 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.6153225898742676
2023-01-07 08:56:53,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 16.07267189025879
2023-01-07 08:56:53,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 39.03551483154297
2023-01-07 08:56:53,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 117.97916412353516
2023-01-07 08:56:53,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -12.438965797424316
2023-01-07 08:56:53,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -36.754608154296875
2023-01-07 08:56:53,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.789607048034668
2023-01-07 08:56:53,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -55.57516098022461
2023-01-07 08:56:53,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.27650451660156
2023-01-07 08:56:53,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -60.760902404785156
2023-01-07 08:56:53,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,614 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -73.70252227783203
2023-01-07 08:56:53,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,614 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -75.23577117919922
2023-01-07 08:56:53,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.73360824584961
2023-01-07 08:56:53,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -106.74685668945312
2023-01-07 08:56:53,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:53,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:53,615 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -694.679443359375
2023-01-07 08:56:53,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 694.1887817382812
2023-01-07 08:56:54,456 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -125.79468536376953
2023-01-07 08:56:54,457 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:54,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,457 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 4.734649181365967
2023-01-07 08:56:54,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,457 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -523.68115234375
2023-01-07 08:56:54,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 146.32522583007812
2023-01-07 08:56:54,459 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 9.55955696105957
2023-01-07 08:56:54,459 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,459 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,459 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -12.131708145141602
2023-01-07 08:56:54,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,459 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -15.410652160644531
2023-01-07 08:56:54,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,459 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -633.9754638671875
2023-01-07 08:56:54,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1200027465820312
2023-01-07 08:56:54,461 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 124.48802185058594
2023-01-07 08:56:54,461 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:54,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,461 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -638.3158569335938
2023-01-07 08:56:54,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.411170959472656
2023-01-07 08:56:54,462 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 22.389007568359375
2023-01-07 08:56:54,463 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:54,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,463 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -417.4435729980469
2023-01-07 08:56:54,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.814823150634766
2023-01-07 08:56:54,464 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -473.9105224609375
2023-01-07 08:56:54,464 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:54,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,465 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 1.0951300859451294
2023-01-07 08:56:54,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,465 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -16.04266357421875
2023-01-07 08:56:54,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,465 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1228.7559814453125
2023-01-07 08:56:54,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.55887222290039
2023-01-07 08:56:54,467 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 1.7017390727996826
2023-01-07 08:56:54,467 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:54,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,467 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1212.2020263671875
2023-01-07 08:56:54,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6371116638183594
2023-01-07 08:56:54,468 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -4.049459457397461
2023-01-07 08:56:54,469 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:54,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,469 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.11460351943969727
2023-01-07 08:56:54,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,469 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -77.17337799072266
2023-01-07 08:56:54,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.183347702026367
2023-01-07 08:56:54,470 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -0.33375048637390137
2023-01-07 08:56:54,470 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:54,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,471 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 13.459217071533203
2023-01-07 08:56:54,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5094356536865234
2023-01-07 08:56:54,472 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -513.8818359375
2023-01-07 08:56:54,472 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:54,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,472 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 43.773887634277344
2023-01-07 08:56:54,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.17585754394531
2023-01-07 08:56:54,473 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 16.76740264892578
2023-01-07 08:56:54,474 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:54,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,474 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.9913787841796875
2023-01-07 08:56:54,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.733306884765625
2023-01-07 08:56:54,475 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -74.35726928710938
2023-01-07 08:56:54,475 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,475 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:54,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,476 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.1741094589233398
2023-01-07 08:56:54,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,476 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -82.35969543457031
2023-01-07 08:56:54,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.59290885925293
2023-01-07 08:56:54,478 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.7169945240020752
2023-01-07 08:56:54,478 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:54,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,478 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -206.24996948242188
2023-01-07 08:56:54,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,478 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3126.15869140625
2023-01-07 08:56:54,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.411731719970703
2023-01-07 08:56:54,480 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -241.28460693359375
2023-01-07 08:56:54,480 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:54,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,480 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.5687661170959473
2023-01-07 08:56:54,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,480 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -33.973453521728516
2023-01-07 08:56:54,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.557043075561523
2023-01-07 08:56:54,482 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.6149606704711914
2023-01-07 08:56:54,482 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,482 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,482 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:54,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,482 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1266.705322265625
2023-01-07 08:56:54,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -74.57230377197266
2023-01-07 08:56:54,483 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -18.917362213134766
2023-01-07 08:56:54,483 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:54,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,484 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 6.496185302734375
2023-01-07 08:56:54,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.03502655029297
2023-01-07 08:56:54,485 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 27.708927154541016
2023-01-07 08:56:54,485 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,485 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,485 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:54,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,486 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 22.50749397277832
2023-01-07 08:56:54,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,486 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1164.7613525390625
2023-01-07 08:56:54,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.720500946044922
2023-01-07 08:56:54,487 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 28.84983253479004
2023-01-07 08:56:54,487 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:54,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,488 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1207.990234375
2023-01-07 08:56:54,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.509284973144531
2023-01-07 08:56:54,489 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 10.671151161193848
2023-01-07 08:56:54,489 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:54,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,489 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1264.030517578125
2023-01-07 08:56:54,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.178187370300293
2023-01-07 08:56:54,491 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1209.861328125
2023-01-07 08:56:54,491 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:54,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,491 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.4930967092514038
2023-01-07 08:56:54,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,491 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 1.7501220703125
2023-01-07 08:56:54,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,492 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 4993.41552734375
2023-01-07 08:56:54,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.45586395263672
2023-01-07 08:56:54,493 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.8949716091156006
2023-01-07 08:56:54,493 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:54,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,494 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3059.854248046875
2023-01-07 08:56:54,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.232172012329102
2023-01-07 08:56:54,495 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 1.002166748046875
2023-01-07 08:56:54,495 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,495 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.465963363647461
2023-01-07 08:56:54,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,495 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 4983.8046875
2023-01-07 08:56:54,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.208263397216797
2023-01-07 08:56:54,497 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 13.859360694885254
2023-01-07 08:56:54,497 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:54,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,497 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3020.912353515625
2023-01-07 08:56:54,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.578052520751953
2023-01-07 08:56:54,498 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 5017.505859375
2023-01-07 08:56:54,499 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:54,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,499 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2993.51318359375
2023-01-07 08:56:54,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13019180297851562
2023-01-07 08:56:54,500 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 3.254640817642212
2023-01-07 08:56:54,500 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,500 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,500 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:54,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,501 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3042.089599609375
2023-01-07 08:56:54,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.988903045654297
2023-01-07 08:56:54,502 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 3036.1669921875
2023-01-07 08:56:54,502 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,502 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,502 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 1.1865530014038086
2023-01-07 08:56:54,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,503 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2324.44482421875
2023-01-07 08:56:54,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.264848709106445
2023-01-07 08:56:54,504 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 1.9268418550491333
2023-01-07 08:56:54,504 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,504 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.756298542022705
2023-01-07 08:56:54,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,504 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2346.707763671875
2023-01-07 08:56:54,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4085731506347656
2023-01-07 08:56:54,505 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 2346.435791015625
2023-01-07 08:56:54,506 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,506 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:54,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,506 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.3145259618759155
2023-01-07 08:56:54,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,506 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -69.03897094726562
2023-01-07 08:56:54,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.177939414978027
2023-01-07 08:56:54,507 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.43904435634613037
2023-01-07 08:56:54,508 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,508 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,508 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -14.141868591308594
2023-01-07 08:56:54,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,508 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.21735382080078
2023-01-07 08:56:54,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.31813383102417
2023-01-07 08:56:54,509 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -61.25163269042969
2023-01-07 08:56:54,509 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:54,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,509 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 24.089942932128906
2023-01-07 08:56:54,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1135926246643066
2023-01-07 08:56:54,511 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.0585776567459106
2023-01-07 08:56:54,511 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:54,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,511 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 17.29822540283203
2023-01-07 08:56:54,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.13275146484375
2023-01-07 08:56:54,512 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 28.71195411682129
2023-01-07 08:56:54,512 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,513 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.14942002296447754
2023-01-07 08:56:54,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,513 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 10.782722473144531
2023-01-07 08:56:54,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.500340938568115
2023-01-07 08:56:54,514 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.3942070007324219
2023-01-07 08:56:54,514 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.551721572875977
2023-01-07 08:56:54,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,515 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 12.543991088867188
2023-01-07 08:56:54,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.5
2023-01-07 08:56:54,516 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 14.332881927490234
2023-01-07 08:56:54,516 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,517 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.5266521573066711
2023-01-07 08:56:54,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,517 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -119.02168273925781
2023-01-07 08:56:54,517 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.261169910430908
2023-01-07 08:56:54,518 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.039528846740722656
2023-01-07 08:56:54,518 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.180285930633545
2023-01-07 08:56:54,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,519 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -115.6410903930664
2023-01-07 08:56:54,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.647845268249512
2023-01-07 08:56:54,520 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -114.11064147949219
2023-01-07 08:56:54,520 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:54,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,520 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 29.34020233154297
2023-01-07 08:56:54,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9756104946136475
2023-01-07 08:56:54,521 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -30.018932342529297
2023-01-07 08:56:54,521 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:54,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,522 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 28.66175079345703
2023-01-07 08:56:54,522 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.54326057434082
2023-01-07 08:56:54,523 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 27.548473358154297
2023-01-07 08:56:54,523 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.41767334938049316
2023-01-07 08:56:54,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -129.5221405029297
2023-01-07 08:56:54,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.63893127441406
2023-01-07 08:56:54,525 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.7327625155448914
2023-01-07 08:56:54,525 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,525 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,525 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 8.06689453125
2023-01-07 08:56:54,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -155.15203857421875
2023-01-07 08:56:54,526 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.311092376708984
2023-01-07 08:56:54,527 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -138.79580688476562
2023-01-07 08:56:54,527 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,527 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.4855765998363495
2023-01-07 08:56:54,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,528 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -66.12157440185547
2023-01-07 08:56:54,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40564537048339844
2023-01-07 08:56:54,529 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8037394285202026
2023-01-07 08:56:54,529 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,529 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 30.583946228027344
2023-01-07 08:56:54,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.88386154174805
2023-01-07 08:56:54,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.603640556335449
2023-01-07 08:56:54,531 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -60.69243240356445
2023-01-07 08:56:54,531 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,531 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:54,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,531 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.22362342476844788
2023-01-07 08:56:54,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -34.61447525024414
2023-01-07 08:56:54,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.934823036193848
2023-01-07 08:56:54,533 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.884635329246521
2023-01-07 08:56:54,533 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -70.87334442138672
2023-01-07 08:56:54,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,533 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -41.59623718261719
2023-01-07 08:56:54,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.313098907470703
2023-01-07 08:56:54,535 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -40.397621154785156
2023-01-07 08:56:54,535 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.2132982760667801
2023-01-07 08:56:54,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -86.33917999267578
2023-01-07 08:56:54,536 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6815706491470337
2023-01-07 08:56:54,537 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.08572149276733398
2023-01-07 08:56:54,537 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.382711410522461
2023-01-07 08:56:54,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,537 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -85.86865997314453
2023-01-07 08:56:54,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.79397201538086
2023-01-07 08:56:54,538 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -82.19271850585938
2023-01-07 08:56:54,538 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:54,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.23511114716529846
2023-01-07 08:56:54,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -64.93960571289062
2023-01-07 08:56:54,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8986294269561768
2023-01-07 08:56:54,541 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.23394346237182617
2023-01-07 08:56:54,541 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -10.846620559692383
2023-01-07 08:56:54,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -54.65534973144531
2023-01-07 08:56:54,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19554734230041504
2023-01-07 08:56:54,542 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -55.929595947265625
2023-01-07 08:56:54,542 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:54,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,543 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.17759451270103455
2023-01-07 08:56:54,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,543 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 7.314479827880859
2023-01-07 08:56:54,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.318682670593262
2023-01-07 08:56:54,544 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.32087475061416626
2023-01-07 08:56:54,544 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,544 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -31.834835052490234
2023-01-07 08:56:54,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,545 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 14.020631790161133
2023-01-07 08:56:54,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35163992643356323
2023-01-07 08:56:54,546 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 14.062051773071289
2023-01-07 08:56:54,546 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:54,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,546 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.28264081478118896
2023-01-07 08:56:54,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -403.2692565917969
2023-01-07 08:56:54,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.49969482421875
2023-01-07 08:56:54,548 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.06945371627807617
2023-01-07 08:56:54,548 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -29.614931106567383
2023-01-07 08:56:54,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -401.4272155761719
2023-01-07 08:56:54,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.101760864257812
2023-01-07 08:56:54,550 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -398.2148132324219
2023-01-07 08:56:54,550 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:54,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 43.628997802734375
2023-01-07 08:56:54,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.848077774047852
2023-01-07 08:56:54,551 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.41349706053733826
2023-01-07 08:56:54,552 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,552 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:54,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 46.95655822753906
2023-01-07 08:56:54,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7459626197814941
2023-01-07 08:56:54,553 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 49.881324768066406
2023-01-07 08:56:54,553 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:54,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,554 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 379.59149169921875
2023-01-07 08:56:54,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2764889597892761
2023-01-07 08:56:54,555 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 2.2309584617614746
2023-01-07 08:56:54,555 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,555 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,555 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:54,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 388.9464111328125
2023-01-07 08:56:54,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.558544635772705
2023-01-07 08:56:54,556 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 390.7493591308594
2023-01-07 08:56:54,557 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:54,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,557 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 228.18020629882812
2023-01-07 08:56:54,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.493627667427063
2023-01-07 08:56:54,558 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 14.311203002929688
2023-01-07 08:56:54,558 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:54,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,559 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 228.63861083984375
2023-01-07 08:56:54,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04691562056541443
2023-01-07 08:56:54,560 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 225.22100830078125
2023-01-07 08:56:54,560 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:54,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,560 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.08503252267837524
2023-01-07 08:56:54,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,561 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 551.726806640625
2023-01-07 08:56:54,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.808126449584961
2023-01-07 08:56:54,562 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.2975432574748993
2023-01-07 08:56:54,562 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.49056625366211
2023-01-07 08:56:54,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 543.9824829101562
2023-01-07 08:56:54,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.037117004394531
2023-01-07 08:56:54,564 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 547.291748046875
2023-01-07 08:56:54,564 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:54,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,564 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 123.97384643554688
2023-01-07 08:56:54,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.577793836593628
2023-01-07 08:56:54,565 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.7799354195594788
2023-01-07 08:56:54,565 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,566 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:54,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,566 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 124.33403015136719
2023-01-07 08:56:54,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9256521463394165
2023-01-07 08:56:54,567 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 124.18391418457031
2023-01-07 08:56:54,567 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,567 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:54,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -19.592805862426758
2023-01-07 08:56:54,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4225850105285645
2023-01-07 08:56:54,569 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 6.912039756774902
2023-01-07 08:56:54,569 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -19.068220138549805
2023-01-07 08:56:54,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2696025371551514
2023-01-07 08:56:54,570 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -21.56968879699707
2023-01-07 08:56:54,571 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:54,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -2.2907466888427734
2023-01-07 08:56:54,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.503559112548828
2023-01-07 08:56:54,572 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.3853389322757721
2023-01-07 08:56:54,572 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:54,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -7.579076766967773
2023-01-07 08:56:54,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38443097472190857
2023-01-07 08:56:54,574 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -8.341371536254883
2023-01-07 08:56:54,574 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,574 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,574 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:54,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 12.096277236938477
2023-01-07 08:56:54,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29842326045036316
2023-01-07 08:56:54,575 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.4004150629043579
2023-01-07 08:56:54,576 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,576 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,576 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:54,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 11.122177124023438
2023-01-07 08:56:54,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43842723965644836
2023-01-07 08:56:54,577 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 10.917181015014648
2023-01-07 08:56:54,577 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:54,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 13.503029823303223
2023-01-07 08:56:54,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4280909597873688
2023-01-07 08:56:54,579 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 2.1081502437591553
2023-01-07 08:56:54,579 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,579 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,579 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:54,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,579 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 10.764881134033203
2023-01-07 08:56:54,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1696908473968506
2023-01-07 08:56:54,581 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 5.954935073852539
2023-01-07 08:56:54,581 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,581 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:54,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -237.45138549804688
2023-01-07 08:56:54,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04907439649105072
2023-01-07 08:56:54,582 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.12973427772521973
2023-01-07 08:56:54,582 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:54,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,583 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -238.58990478515625
2023-01-07 08:56:54,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.155834197998047
2023-01-07 08:56:54,584 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -237.57070922851562
2023-01-07 08:56:54,584 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,584 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:54,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,584 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -12.171226501464844
2023-01-07 08:56:54,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7419161796569824
2023-01-07 08:56:54,586 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.1349397897720337
2023-01-07 08:56:54,586 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,586 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:54,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,586 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -13.279891014099121
2023-01-07 08:56:54,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32620733976364136
2023-01-07 08:56:54,587 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -13.62425422668457
2023-01-07 08:56:54,587 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:54,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,588 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 38.12965393066406
2023-01-07 08:56:54,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.575373888015747
2023-01-07 08:56:54,589 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 3.0464489459991455
2023-01-07 08:56:54,589 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,589 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,589 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 39.25457000732422
2023-01-07 08:56:54,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5138724446296692
2023-01-07 08:56:54,591 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 37.777626037597656
2023-01-07 08:56:54,591 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,591 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:54,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,591 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -23.77056884765625
2023-01-07 08:56:54,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2481611967086792
2023-01-07 08:56:54,592 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.564671516418457
2023-01-07 08:56:54,592 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:54,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -20.413604736328125
2023-01-07 08:56:54,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.936765193939209
2023-01-07 08:56:54,594 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -21.434524536132812
2023-01-07 08:56:54,594 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,594 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,594 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:54,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1437.10791015625
2023-01-07 08:56:54,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3618520498275757
2023-01-07 08:56:54,596 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.12256191670894623
2023-01-07 08:56:54,596 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:54,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1438.002197265625
2023-01-07 08:56:54,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.953121304512024
2023-01-07 08:56:54,597 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1438.638671875
2023-01-07 08:56:54,598 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,598 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,598 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:54,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,598 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 521.3387451171875
2023-01-07 08:56:54,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6714437007904053
2023-01-07 08:56:54,599 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.2481633871793747
2023-01-07 08:56:54,599 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:54,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,600 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 520.4330444335938
2023-01-07 08:56:54,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.059735655784607
2023-01-07 08:56:54,601 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 520.9664306640625
2023-01-07 08:56:54,601 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:54,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 586.2838745117188
2023-01-07 08:56:54,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6959651112556458
2023-01-07 08:56:54,602 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.9925374984741211
2023-01-07 08:56:54,603 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:54,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 583.7159423828125
2023-01-07 08:56:54,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06944039463996887
2023-01-07 08:56:54,604 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 586.2575073242188
2023-01-07 08:56:54,604 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,604 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:54,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 523.611328125
2023-01-07 08:56:54,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3573604226112366
2023-01-07 08:56:54,606 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.8034893274307251
2023-01-07 08:56:54,606 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,606 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:54,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,607 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 522.0826416015625
2023-01-07 08:56:54,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4802587628364563
2023-01-07 08:56:54,608 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 522.4383544921875
2023-01-07 08:56:54,608 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,608 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:54,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,608 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 965.909912109375
2023-01-07 08:56:54,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8453409671783447
2023-01-07 08:56:54,609 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.6467228531837463
2023-01-07 08:56:54,609 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,610 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,610 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:54,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 966.673828125
2023-01-07 08:56:54,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4175330698490143
2023-01-07 08:56:54,611 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 965.8018798828125
2023-01-07 08:56:54,611 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:54,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 363.4468994140625
2023-01-07 08:56:54,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3726909160614014
2023-01-07 08:56:54,613 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 1.084478735923767
2023-01-07 08:56:54,613 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,613 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:54,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 363.15850830078125
2023-01-07 08:56:54,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03502856567502022
2023-01-07 08:56:54,614 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 363.0234375
2023-01-07 08:56:54,615 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:54,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -7.773345947265625
2023-01-07 08:56:54,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7039226293563843
2023-01-07 08:56:54,616 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.32841289043426514
2023-01-07 08:56:54,616 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,617 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.31817626953125
2023-01-07 08:56:54,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2506670355796814
2023-01-07 08:56:54,618 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -5.6820526123046875
2023-01-07 08:56:54,618 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,619 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2985.92578125
2023-01-07 08:56:54,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6452080011367798
2023-01-07 08:56:54,620 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 2.1735892295837402
2023-01-07 08:56:54,620 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,620 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,620 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:54,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2987.7275390625
2023-01-07 08:56:54,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.189258098602295
2023-01-07 08:56:54,621 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2987.98486328125
2023-01-07 08:56:54,621 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1033.986083984375
2023-01-07 08:56:54,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.325152039527893
2023-01-07 08:56:54,623 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.7248449325561523
2023-01-07 08:56:54,623 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:54,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1033.6968994140625
2023-01-07 08:56:54,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5747197866439819
2023-01-07 08:56:54,625 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1033.7811279296875
2023-01-07 08:56:54,625 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:54,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,625 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1010.4984130859375
2023-01-07 08:56:54,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0147380828857422
2023-01-07 08:56:54,626 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -65.52740478515625
2023-01-07 08:56:54,626 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:54,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,627 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1009.8560180664062
2023-01-07 08:56:54,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19266098737716675
2023-01-07 08:56:54,628 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -1010.0244140625
2023-01-07 08:56:54,628 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:54,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,629 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1328.4736328125
2023-01-07 08:56:54,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2670338749885559
2023-01-07 08:56:54,630 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.1376708447933197
2023-01-07 08:56:54,630 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:54,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,630 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1328.06494140625
2023-01-07 08:56:54,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4980154037475586
2023-01-07 08:56:54,632 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1328.27294921875
2023-01-07 08:56:54,632 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,632 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:54,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,632 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8181.87255859375
2023-01-07 08:56:54,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2484760284423828
2023-01-07 08:56:54,633 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -65.44656372070312
2023-01-07 08:56:54,633 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:54,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,634 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8181.5810546875
2023-01-07 08:56:54,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.050826482474803925
2023-01-07 08:56:54,635 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8181.53955078125
2023-01-07 08:56:54,635 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,635 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:54,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,635 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1096.37060546875
2023-01-07 08:56:54,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14343377947807312
2023-01-07 08:56:54,637 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -65.83489227294922
2023-01-07 08:56:54,637 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:54,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1096.562744140625
2023-01-07 08:56:54,637 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03843647614121437
2023-01-07 08:56:54,638 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -1096.509765625
2023-01-07 08:56:54,638 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,639 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:54,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,640 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 88.76165771484375
2023-01-07 08:56:54,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2790636420249939
2023-01-07 08:56:54,641 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -65.876708984375
2023-01-07 08:56:54,641 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:54,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,641 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 89.39019775390625
2023-01-07 08:56:54,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24680639803409576
2023-01-07 08:56:54,643 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 89.47442626953125
2023-01-07 08:56:54,643 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:54,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,643 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 14529.3974609375
2023-01-07 08:56:54,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.018808364868164
2023-01-07 08:56:54,644 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -433.5032958984375
2023-01-07 08:56:54,644 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:54,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,645 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 14532.97265625
2023-01-07 08:56:54,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.273678183555603
2023-01-07 08:56:54,646 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 14534.259765625
2023-01-07 08:56:54,646 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:54,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:54,647 > [DEBUG] 0 :: 7.571673393249512
2023-01-07 08:56:54,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,651 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00347900390625
2023-01-07 08:56:54,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -318.62298583984375
2023-01-07 08:56:54,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,653 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.41602790355682373
2023-01-07 08:56:54,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,654 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -176.14321899414062
2023-01-07 08:56:54,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -206.18020629882812
2023-01-07 08:56:54,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,657 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2.9679651260375977
2023-01-07 08:56:54,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01181054301559925
2023-01-07 08:56:54,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,659 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.007859660312533379
2023-01-07 08:56:54,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,660 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -63.63511657714844
2023-01-07 08:56:54,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13176721334457397
2023-01-07 08:56:54,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,663 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -14.325349807739258
2023-01-07 08:56:54,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10756036639213562
2023-01-07 08:56:54,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,664 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.027330754324793816
2023-01-07 08:56:54,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,665 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -13.754996299743652
2023-01-07 08:56:54,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08881296217441559
2023-01-07 08:56:54,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -10.200518608093262
2023-01-07 08:56:54,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03979310020804405
2023-01-07 08:56:54,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.40050679445266724
2023-01-07 08:56:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -9.647188186645508
2023-01-07 08:56:54,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5895863771438599
2023-01-07 08:56:54,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 10.476669311523438
2023-01-07 08:56:54,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8689476251602173
2023-01-07 08:56:54,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.03325698524713516
2023-01-07 08:56:54,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 10.950618743896484
2023-01-07 08:56:54,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3892096281051636
2023-01-07 08:56:54,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,673 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -23.476428985595703
2023-01-07 08:56:54,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4484095573425293
2023-01-07 08:56:54,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.0009242212399840355
2023-01-07 08:56:54,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -22.592979431152344
2023-01-07 08:56:54,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7986499071121216
2023-01-07 08:56:54,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -31.631614685058594
2023-01-07 08:56:54,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15329639613628387
2023-01-07 08:56:54,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,677 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.256550669670105
2023-01-07 08:56:54,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,677 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -31.43262481689453
2023-01-07 08:56:54,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9730867147445679
2023-01-07 08:56:54,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,679 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.075131416320801
2023-01-07 08:56:54,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06612439453601837
2023-01-07 08:56:54,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.3112425208091736
2023-01-07 08:56:54,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.9748172760009766
2023-01-07 08:56:54,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5652170181274414
2023-01-07 08:56:54,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,682 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -8.964038848876953
2023-01-07 08:56:54,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32272040843963623
2023-01-07 08:56:54,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,683 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.07287412881851196
2023-01-07 08:56:54,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,684 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -8.692071914672852
2023-01-07 08:56:54,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5391391515731812
2023-01-07 08:56:54,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,685 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -56.369529724121094
2023-01-07 08:56:54,685 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7698597311973572
2023-01-07 08:56:54,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,686 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.06654059141874313
2023-01-07 08:56:54,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,687 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -54.74029541015625
2023-01-07 08:56:54,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4340696334838867
2023-01-07 08:56:54,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,688 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -8.1098051071167
2023-01-07 08:56:54,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3933989405632019
2023-01-07 08:56:54,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,689 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.09206779301166534
2023-01-07 08:56:54,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,690 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -8.844740867614746
2023-01-07 08:56:54,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37829336524009705
2023-01-07 08:56:54,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,691 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -8.45930290222168
2023-01-07 08:56:54,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4258460998535156
2023-01-07 08:56:54,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,693 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.09080596268177032
2023-01-07 08:56:54,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,693 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -7.738030433654785
2023-01-07 08:56:54,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14293017983436584
2023-01-07 08:56:54,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,695 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.7972373962402344
2023-01-07 08:56:54,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4008326530456543
2023-01-07 08:56:54,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,696 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.007346697151660919
2023-01-07 08:56:54,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,696 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.3796019554138184
2023-01-07 08:56:54,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44000640511512756
2023-01-07 08:56:54,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,698 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1.390249252319336
2023-01-07 08:56:54,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19144119322299957
2023-01-07 08:56:54,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.004729863256216049
2023-01-07 08:56:54,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.10441780090332031
2023-01-07 08:56:54,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.455864667892456
2023-01-07 08:56:54,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,701 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -10.103616714477539
2023-01-07 08:56:54,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16873028874397278
2023-01-07 08:56:54,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.12241160869598389
2023-01-07 08:56:54,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -10.07115364074707
2023-01-07 08:56:54,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.363792061805725
2023-01-07 08:56:54,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,704 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.726484298706055
2023-01-07 08:56:54,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19778525829315186
2023-01-07 08:56:54,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,705 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.16169676184654236
2023-01-07 08:56:54,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,706 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 10.750755310058594
2023-01-07 08:56:54,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0061631202697754
2023-01-07 08:56:54,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,707 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -11.661958694458008
2023-01-07 08:56:54,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1950411796569824
2023-01-07 08:56:54,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.14583256840705872
2023-01-07 08:56:54,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,709 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -10.574943542480469
2023-01-07 08:56:54,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4513776302337646
2023-01-07 08:56:54,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,710 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -4.183917045593262
2023-01-07 08:56:54,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7022674083709717
2023-01-07 08:56:54,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,711 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.01268274337053299
2023-01-07 08:56:54,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -2.3159027099609375
2023-01-07 08:56:54,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.435551404953003
2023-01-07 08:56:54,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,713 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 3.6402549743652344
2023-01-07 08:56:54,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9586526155471802
2023-01-07 08:56:54,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,714 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.0405331552028656
2023-01-07 08:56:54,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 5.518451690673828
2023-01-07 08:56:54,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0686116218566895
2023-01-07 08:56:54,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,716 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -15.01129150390625
2023-01-07 08:56:54,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7989134788513184
2023-01-07 08:56:54,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.05149640142917633
2023-01-07 08:56:54,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,718 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.01882266998291
2023-01-07 08:56:54,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.832817792892456
2023-01-07 08:56:54,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -19.451332092285156
2023-01-07 08:56:54,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38984793424606323
2023-01-07 08:56:54,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,721 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.1346524953842163
2023-01-07 08:56:54,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,721 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -17.214998245239258
2023-01-07 08:56:54,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5604909658432007
2023-01-07 08:56:54,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -72.48663330078125
2023-01-07 08:56:54,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.639341354370117
2023-01-07 08:56:54,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.1374257355928421
2023-01-07 08:56:54,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -70.82728576660156
2023-01-07 08:56:54,724 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7717540860176086
2023-01-07 08:56:54,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 29.595752716064453
2023-01-07 08:56:54,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.387104511260986
2023-01-07 08:56:54,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.17959283292293549
2023-01-07 08:56:54,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 30.831382751464844
2023-01-07 08:56:54,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1170966625213623
2023-01-07 08:56:54,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,729 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 11.278116226196289
2023-01-07 08:56:54,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1602476835250854
2023-01-07 08:56:54,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.46318307518959045
2023-01-07 08:56:54,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 23.235946655273438
2023-01-07 08:56:54,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12425824999809265
2023-01-07 08:56:54,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -49.71772003173828
2023-01-07 08:56:54,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.571887493133545
2023-01-07 08:56:54,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -44.62268829345703
2023-01-07 08:56:54,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.097907066345215
2023-01-07 08:56:54,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 34.955318450927734
2023-01-07 08:56:54,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34738826751708984
2023-01-07 08:56:54,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.711661696434021
2023-01-07 08:56:54,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 27.399066925048828
2023-01-07 08:56:54,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.921420574188232
2023-01-07 08:56:54,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 12.106525421142578
2023-01-07 08:56:54,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.991725206375122
2023-01-07 08:56:54,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.2802910804748535
2023-01-07 08:56:54,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.506668090820312
2023-01-07 08:56:54,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.161149024963379
2023-01-07 08:56:54,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,741 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 23.745838165283203
2023-01-07 08:56:54,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1165390014648438
2023-01-07 08:56:54,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.17122456431388855
2023-01-07 08:56:54,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 22.408138275146484
2023-01-07 08:56:54,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9954721927642822
2023-01-07 08:56:54,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,744 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 224.812255859375
2023-01-07 08:56:54,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.353651762008667
2023-01-07 08:56:54,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 224.4540252685547
2023-01-07 08:56:54,745 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.18281364440918
2023-01-07 08:56:54,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -26.008296966552734
2023-01-07 08:56:54,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.509742736816406
2023-01-07 08:56:54,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -23.091510772705078
2023-01-07 08:56:54,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.124502182006836
2023-01-07 08:56:54,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,749 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -1.2813771963119507
2023-01-07 08:56:54,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.813969373703003
2023-01-07 08:56:54,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,751 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 7.954414367675781
2023-01-07 08:56:54,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.960387945175171
2023-01-07 08:56:54,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,752 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 8.272964477539062
2023-01-07 08:56:54,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.040017127990723
2023-01-07 08:56:54,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,753 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 30.782028198242188
2023-01-07 08:56:54,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.408981323242188
2023-01-07 08:56:54,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,755 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -30.987709045410156
2023-01-07 08:56:54,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.289562702178955
2023-01-07 08:56:54,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,756 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -17.19397735595703
2023-01-07 08:56:54,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.556661605834961
2023-01-07 08:56:54,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -16.18441390991211
2023-01-07 08:56:54,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.612071990966797
2023-01-07 08:56:54,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,759 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.40085840225219727
2023-01-07 08:56:54,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.584841728210449
2023-01-07 08:56:54,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,760 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -257.5644836425781
2023-01-07 08:56:54,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.61693000793457
2023-01-07 08:56:54,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,762 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -235.3197479248047
2023-01-07 08:56:54,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.556741714477539
2023-01-07 08:56:54,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,763 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -21.836437225341797
2023-01-07 08:56:54,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.307825565338135
2023-01-07 08:56:54,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,764 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.8711371421813965
2023-01-07 08:56:54,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,765 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -1.8514175415039062
2023-01-07 08:56:54,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6070219278335571
2023-01-07 08:56:54,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,767 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -4.660647869110107
2023-01-07 08:56:54,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8859033584594727
2023-01-07 08:56:54,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,768 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 34.698299407958984
2023-01-07 08:56:54,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.441360473632812
2023-01-07 08:56:54,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,769 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -108.47772216796875
2023-01-07 08:56:54,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.035853147506714
2023-01-07 08:56:54,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,770 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -37.021018981933594
2023-01-07 08:56:54,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.8974609375
2023-01-07 08:56:54,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,772 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 12.189332962036133
2023-01-07 08:56:54,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.208569049835205
2023-01-07 08:56:54,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,773 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.3864619731903076
2023-01-07 08:56:54,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,773 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 35.82472610473633
2023-01-07 08:56:54,774 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.972902297973633
2023-01-07 08:56:54,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,775 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -73.21171569824219
2023-01-07 08:56:54,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3315417766571045
2023-01-07 08:56:54,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,776 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -32.49925231933594
2023-01-07 08:56:54,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.476413726806641
2023-01-07 08:56:54,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,778 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -62.33378601074219
2023-01-07 08:56:54,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.3584136962890625
2023-01-07 08:56:54,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,779 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 1073.046142578125
2023-01-07 08:56:54,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4117724895477295
2023-01-07 08:56:54,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,780 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -325.3236389160156
2023-01-07 08:56:54,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.98330307006836
2023-01-07 08:56:54,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.6715400218963623
2023-01-07 08:56:54,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 745.314697265625
2023-01-07 08:56:54,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.843759536743164
2023-01-07 08:56:54,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2595.76708984375
2023-01-07 08:56:54,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.271764755249023
2023-01-07 08:56:54,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,785 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.30430305004119873
2023-01-07 08:56:54,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 15.608715057373047
2023-01-07 08:56:54,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3786.045166015625
2023-01-07 08:56:54,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.789138793945312
2023-01-07 08:56:54,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,787 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3958.026611328125
2023-01-07 08:56:54,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.690975189208984
2023-01-07 08:56:54,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1801.5654296875
2023-01-07 08:56:54,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.00391960144043
2023-01-07 08:56:54,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,790 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -61.94974899291992
2023-01-07 08:56:54,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.59986877441406
2023-01-07 08:56:54,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,791 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.8869445323944092
2023-01-07 08:56:54,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,791 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 971.83447265625
2023-01-07 08:56:54,791 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.926204681396484
2023-01-07 08:56:54,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,793 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1002.2884521484375
2023-01-07 08:56:54,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.17554473876953
2023-01-07 08:56:54,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,794 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.3954664468765259
2023-01-07 08:56:54,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,794 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 64.13316345214844
2023-01-07 08:56:54,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.078413009643555
2023-01-07 08:56:54,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,796 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1006.0869750976562
2023-01-07 08:56:54,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.963759422302246
2023-01-07 08:56:54,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,797 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 26.284812927246094
2023-01-07 08:56:54,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,797 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5047.7021484375
2023-01-07 08:56:54,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.786418914794922
2023-01-07 08:56:54,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,799 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 973.3616943359375
2023-01-07 08:56:54,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.519916534423828
2023-01-07 08:56:54,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,800 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 128.4425048828125
2023-01-07 08:56:54,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.612545013427734
2023-01-07 08:56:54,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,801 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 956.6681518554688
2023-01-07 08:56:54,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.208626747131348
2023-01-07 08:56:54,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,803 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -2.0479767322540283
2023-01-07 08:56:54,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,803 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 26.128868103027344
2023-01-07 08:56:54,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,804 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 927.0623168945312
2023-01-07 08:56:54,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,804 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5074.83251953125
2023-01-07 08:56:54,804 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4211113452911377
2023-01-07 08:56:54,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,805 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 910.6507568359375
2023-01-07 08:56:54,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.964591026306152
2023-01-07 08:56:54,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,807 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 66.31626892089844
2023-01-07 08:56:54,807 > [DEBUG] 0 :: before allreduce fusion buffer :: 89.23174285888672
2023-01-07 08:56:54,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,808 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 905.8956909179688
2023-01-07 08:56:54,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.833845138549805
2023-01-07 08:56:54,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,809 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 50.948978424072266
2023-01-07 08:56:54,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,810 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5077.74853515625
2023-01-07 08:56:54,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.058013916015625
2023-01-07 08:56:54,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,811 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -79.58301544189453
2023-01-07 08:56:54,811 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.61161804199219
2023-01-07 08:56:54,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,812 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.1480469703674316
2023-01-07 08:56:54,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,813 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 44.977439880371094
2023-01-07 08:56:54,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 283.47119140625
2023-01-07 08:56:54,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,814 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 82.91497039794922
2023-01-07 08:56:54,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.04064178466797
2023-01-07 08:56:54,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,816 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 82.91497039794922
2023-01-07 08:56:54,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -107.2243423461914
2023-01-07 08:56:54,820 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:56:54,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,821 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1257.2681884765625
2023-01-07 08:56:54,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,821 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 758.59228515625
2023-01-07 08:56:54,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,822 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5098.5390625
2023-01-07 08:56:54,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,823 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2940.5244140625
2023-01-07 08:56:54,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,824 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -10.533483505249023
2023-01-07 08:56:54,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,824 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -59.275413513183594
2023-01-07 08:56:54,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,825 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -12.18093490600586
2023-01-07 08:56:54,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,826 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 94.91134643554688
2023-01-07 08:56:54,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,827 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -46.22389221191406
2023-01-07 08:56:54,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,827 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -143.82614135742188
2023-01-07 08:56:54,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.28699779510498
2023-01-07 08:56:54,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -23.837221145629883
2023-01-07 08:56:54,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 74.00494384765625
2023-01-07 08:56:54,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,829 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 14.485647201538086
2023-01-07 08:56:54,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -8.614858627319336
2023-01-07 08:56:54,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 207.83419799804688
2023-01-07 08:56:54,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 7.752633571624756
2023-01-07 08:56:54,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,830 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.712066650390625
2023-01-07 08:56:54,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,830 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 33.82963562011719
2023-01-07 08:56:54,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,830 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -44.898773193359375
2023-01-07 08:56:54,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 35.54658889770508
2023-01-07 08:56:54,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 33.944976806640625
2023-01-07 08:56:54,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -65.25596618652344
2023-01-07 08:56:54,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.495573043823242
2023-01-07 08:56:54,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.888833045959473
2023-01-07 08:56:54,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 8.794647216796875
2023-01-07 08:56:54,833 > [DEBUG] 0 :: before allreduce fusion buffer :: -1942.97998046875
2023-01-07 08:56:54,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,834 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.905672550201416
2023-01-07 08:56:54,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,835 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -11.290462493896484
2023-01-07 08:56:54,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,835 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 10.842620849609375
2023-01-07 08:56:54,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,835 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -12.728931427001953
2023-01-07 08:56:54,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1.5375232696533203
2023-01-07 08:56:54,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.3566207885742188
2023-01-07 08:56:54,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -8.81715202331543
2023-01-07 08:56:54,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,837 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -9.284789085388184
2023-01-07 08:56:54,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,837 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -54.29202651977539
2023-01-07 08:56:54,837 > [DEBUG] 0 :: before allreduce fusion buffer :: -102.64521026611328
2023-01-07 08:56:54,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -7.819841384887695
2023-01-07 08:56:54,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.536797523498535
2023-01-07 08:56:54,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -31.94124984741211
2023-01-07 08:56:54,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -23.465930938720703
2023-01-07 08:56:54,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 255.94754028320312
2023-01-07 08:56:54,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,840 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 12.066889762878418
2023-01-07 08:56:54,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -9.222557067871094
2023-01-07 08:56:54,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -12.817924499511719
2023-01-07 08:56:54,841 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.42612075805664
2023-01-07 08:56:54,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,842 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -62.1671142578125
2023-01-07 08:56:54,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:54,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:54,842 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -634.2711181640625
2023-01-07 08:56:54,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 519.7117919921875
2023-01-07 08:56:55,682 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -0.4369374215602875
2023-01-07 08:56:55,683 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,683 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:55,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,683 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 3.039031505584717
2023-01-07 08:56:55,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,683 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 943.1312866210938
2023-01-07 08:56:55,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.95528793334961
2023-01-07 08:56:55,685 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 2.240077018737793
2023-01-07 08:56:55,685 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,686 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -12.131708145141602
2023-01-07 08:56:55,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,686 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -34.45370864868164
2023-01-07 08:56:55,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,686 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 944.7190551757812
2023-01-07 08:56:55,686 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.300582885742188
2023-01-07 08:56:55,688 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 11.21209716796875
2023-01-07 08:56:55,688 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,688 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:55,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,688 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 946.5057373046875
2023-01-07 08:56:55,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.48655319213867
2023-01-07 08:56:55,689 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 70.66934967041016
2023-01-07 08:56:55,689 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.5403618812561035
2023-01-07 08:56:55,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,690 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 975.0439453125
2023-01-07 08:56:55,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.43367767333984
2023-01-07 08:56:55,691 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 1065.034912109375
2023-01-07 08:56:55,691 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:55,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,691 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 4.641838550567627
2023-01-07 08:56:55,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,692 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 76.69379425048828
2023-01-07 08:56:55,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,692 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 967.2999877929688
2023-01-07 08:56:55,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 80.93937683105469
2023-01-07 08:56:55,693 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5.356754302978516
2023-01-07 08:56:55,694 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,694 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:55,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,694 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 988.3936767578125
2023-01-07 08:56:55,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.384273529052734
2023-01-07 08:56:55,695 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 140.869384765625
2023-01-07 08:56:55,695 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:55,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,696 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 2.29522967338562
2023-01-07 08:56:55,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,696 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 10.018928527832031
2023-01-07 08:56:55,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.23965835571289
2023-01-07 08:56:55,697 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 1.0461153984069824
2023-01-07 08:56:55,698 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,698 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:55,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,698 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 94.03363037109375
2023-01-07 08:56:55,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.221232414245605
2023-01-07 08:56:55,700 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 156.00823974609375
2023-01-07 08:56:55,700 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,700 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:55,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,700 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 163.47474670410156
2023-01-07 08:56:55,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.483570098876953
2023-01-07 08:56:55,701 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 10.055008888244629
2023-01-07 08:56:55,701 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:55,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,702 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 223.81005859375
2023-01-07 08:56:55,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.615352630615234
2023-01-07 08:56:55,703 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 9.106979370117188
2023-01-07 08:56:55,703 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,703 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:55,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,703 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.04076576232910156
2023-01-07 08:56:55,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,704 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 191.29400634765625
2023-01-07 08:56:55,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 167.12710571289062
2023-01-07 08:56:55,705 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.6926226615905762
2023-01-07 08:56:55,705 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -13.727958679199219
2023-01-07 08:56:55,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 85.69841003417969
2023-01-07 08:56:55,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,706 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5372.67041015625
2023-01-07 08:56:55,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.687423706054688
2023-01-07 08:56:55,707 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 56.224647521972656
2023-01-07 08:56:55,707 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:55,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,708 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.3054852485656738
2023-01-07 08:56:55,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,708 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -23.398378372192383
2023-01-07 08:56:55,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.17680358886719
2023-01-07 08:56:55,709 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.8541326522827148
2023-01-07 08:56:55,709 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:55,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 997.5247802734375
2023-01-07 08:56:55,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.983734130859375
2023-01-07 08:56:55,711 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 21.89617156982422
2023-01-07 08:56:55,711 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:55,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,711 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 19.164031982421875
2023-01-07 08:56:55,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 74.10381317138672
2023-01-07 08:56:55,712 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 19.61026954650879
2023-01-07 08:56:55,713 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.518760681152344
2023-01-07 08:56:55,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 62.21910858154297
2023-01-07 08:56:55,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1100.168212890625
2023-01-07 08:56:55,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.280254364013672
2023-01-07 08:56:55,714 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 64.4716796875
2023-01-07 08:56:55,715 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:55,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,715 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1034.2115478515625
2023-01-07 08:56:55,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -108.75978088378906
2023-01-07 08:56:55,716 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 3.0278241634368896
2023-01-07 08:56:55,716 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,716 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.507719993591309
2023-01-07 08:56:55,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,717 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1027.707275390625
2023-01-07 08:56:55,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.167276382446289
2023-01-07 08:56:55,718 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1082.376953125
2023-01-07 08:56:55,718 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:56:55,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,718 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.6697944402694702
2023-01-07 08:56:55,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,719 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -1.0598058700561523
2023-01-07 08:56:55,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,719 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 4218.41015625
2023-01-07 08:56:55,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9432601928710938
2023-01-07 08:56:55,720 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.8709512948989868
2023-01-07 08:56:55,721 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,721 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:55,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5397.220703125
2023-01-07 08:56:55,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.02976131439209
2023-01-07 08:56:55,722 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 15.438045501708984
2023-01-07 08:56:55,722 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.465963363647461
2023-01-07 08:56:55,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 4228.0009765625
2023-01-07 08:56:55,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.86550235748291
2023-01-07 08:56:55,724 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 54.43330764770508
2023-01-07 08:56:55,724 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:55,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,724 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5366.7353515625
2023-01-07 08:56:55,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.95993709564209
2023-01-07 08:56:55,726 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 4196.49951171875
2023-01-07 08:56:55,726 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:55,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,726 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5343.7783203125
2023-01-07 08:56:55,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.852303981781006
2023-01-07 08:56:55,727 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 18.984832763671875
2023-01-07 08:56:55,727 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.05637168884277344
2023-01-07 08:56:55,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,728 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 5357.59765625
2023-01-07 08:56:55,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.349014759063721
2023-01-07 08:56:55,729 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 5371.35302734375
2023-01-07 08:56:55,729 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,729 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 1.6653642654418945
2023-01-07 08:56:55,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,730 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2981.83837890625
2023-01-07 08:56:55,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9698975086212158
2023-01-07 08:56:55,731 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 2.3157238960266113
2023-01-07 08:56:55,731 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,731 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.756298542022705
2023-01-07 08:56:55,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2964.5126953125
2023-01-07 08:56:55,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.921525001525879
2023-01-07 08:56:55,733 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 2972.861328125
2023-01-07 08:56:55,733 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:55,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.38836532831192017
2023-01-07 08:56:55,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 51.92430877685547
2023-01-07 08:56:55,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.98759651184082
2023-01-07 08:56:55,734 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.47636544704437256
2023-01-07 08:56:55,735 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,735 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -14.141868591308594
2023-01-07 08:56:55,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 65.1576919555664
2023-01-07 08:56:55,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.228471755981445
2023-01-07 08:56:55,736 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 70.85717010498047
2023-01-07 08:56:55,736 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:55,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -36.377655029296875
2023-01-07 08:56:55,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.033189296722412
2023-01-07 08:56:55,738 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 68.74151611328125
2023-01-07 08:56:55,738 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 41.528770446777344
2023-01-07 08:56:55,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,738 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -20.06083869934082
2023-01-07 08:56:55,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.680549621582031
2023-01-07 08:56:55,739 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -16.336475372314453
2023-01-07 08:56:55,740 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.3406723141670227
2023-01-07 08:56:55,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 12.549360275268555
2023-01-07 08:56:55,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.39677619934082
2023-01-07 08:56:55,741 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.1664592027664185
2023-01-07 08:56:55,741 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -9.551721572875977
2023-01-07 08:56:55,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -18.982419967651367
2023-01-07 08:56:55,742 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.511083602905273
2023-01-07 08:56:55,743 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -24.245365142822266
2023-01-07 08:56:55,743 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,743 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.3988800048828125
2023-01-07 08:56:55,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 104.50018310546875
2023-01-07 08:56:55,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.603317260742188
2023-01-07 08:56:55,745 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.17504894733428955
2023-01-07 08:56:55,745 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.180285930633545
2023-01-07 08:56:55,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 99.17172241210938
2023-01-07 08:56:55,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.682907104492188
2023-01-07 08:56:55,747 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 85.40296936035156
2023-01-07 08:56:55,747 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:55,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,747 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -45.49364471435547
2023-01-07 08:56:55,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.040937900543213
2023-01-07 08:56:55,748 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35.79541778564453
2023-01-07 08:56:55,748 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.35566329956055
2023-01-07 08:56:55,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,749 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -45.81318283081055
2023-01-07 08:56:55,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.354365587234497
2023-01-07 08:56:55,750 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -45.7454719543457
2023-01-07 08:56:55,750 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,750 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,750 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,751 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 1.2451609373092651
2023-01-07 08:56:55,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,751 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -143.16763305664062
2023-01-07 08:56:55,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.16445541381836
2023-01-07 08:56:55,752 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 1.3115503787994385
2023-01-07 08:56:55,752 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,752 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,752 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 8.06689453125
2023-01-07 08:56:55,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,753 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -120.43516540527344
2023-01-07 08:56:55,753 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0402231216430664
2023-01-07 08:56:55,754 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -157.6197509765625
2023-01-07 08:56:55,754 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,754 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.16318678855895996
2023-01-07 08:56:55,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,755 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 10.559348106384277
2023-01-07 08:56:55,755 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.914041519165039
2023-01-07 08:56:55,756 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8418076038360596
2023-01-07 08:56:55,756 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 30.583946228027344
2023-01-07 08:56:55,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,756 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 18.047649383544922
2023-01-07 08:56:55,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.528684139251709
2023-01-07 08:56:55,758 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 16.74059295654297
2023-01-07 08:56:55,758 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:55,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.9945900440216064
2023-01-07 08:56:55,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -23.750782012939453
2023-01-07 08:56:55,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39464521408081055
2023-01-07 08:56:55,760 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 1.8730430603027344
2023-01-07 08:56:55,760 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,760 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -70.87334442138672
2023-01-07 08:56:55,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,760 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -17.496219635009766
2023-01-07 08:56:55,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20200377702713013
2023-01-07 08:56:55,761 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -4.598730087280273
2023-01-07 08:56:55,761 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,762 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.5181280374526978
2023-01-07 08:56:55,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,762 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 71.16755676269531
2023-01-07 08:56:55,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.623674392700195
2023-01-07 08:56:55,763 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.3412840962409973
2023-01-07 08:56:55,763 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,763 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.382711410522461
2023-01-07 08:56:55,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,764 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 93.13668823242188
2023-01-07 08:56:55,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.5781192779541
2023-01-07 08:56:55,765 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 124.40283966064453
2023-01-07 08:56:55,765 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,765 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:56:55,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,766 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.15523642301559448
2023-01-07 08:56:55,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,766 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 14.564872741699219
2023-01-07 08:56:55,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.58609390258789
2023-01-07 08:56:55,767 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.040981292724609375
2023-01-07 08:56:55,767 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -10.846620559692383
2023-01-07 08:56:55,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,768 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 14.661308288574219
2023-01-07 08:56:55,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.548401832580566
2023-01-07 08:56:55,769 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 13.826818466186523
2023-01-07 08:56:55,769 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,769 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,769 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:56:55,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,770 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.4695768356323242
2023-01-07 08:56:55,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -5.4355573654174805
2023-01-07 08:56:55,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.329442024230957
2023-01-07 08:56:55,771 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -1.202153205871582
2023-01-07 08:56:55,771 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -31.834835052490234
2023-01-07 08:56:55,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,772 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -4.860617637634277
2023-01-07 08:56:55,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.244057655334473
2023-01-07 08:56:55,773 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -5.570639610290527
2023-01-07 08:56:55,773 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:55,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.5268663167953491
2023-01-07 08:56:55,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,774 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -573.2930908203125
2023-01-07 08:56:55,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.76322078704834
2023-01-07 08:56:55,775 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5628293752670288
2023-01-07 08:56:55,775 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,775 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,775 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -29.614931106567383
2023-01-07 08:56:55,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,775 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -565.960205078125
2023-01-07 08:56:55,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0748977661132812
2023-01-07 08:56:55,777 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -554.804443359375
2023-01-07 08:56:55,777 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,777 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:55,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,777 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 125.75701141357422
2023-01-07 08:56:55,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8754157423973083
2023-01-07 08:56:55,778 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 3.172974109649658
2023-01-07 08:56:55,778 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.931751251220703
2023-01-07 08:56:55,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,779 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 126.27958679199219
2023-01-07 08:56:55,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0118775367736816
2023-01-07 08:56:55,780 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 121.1081771850586
2023-01-07 08:56:55,780 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,780 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,780 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:55,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,780 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 457.59393310546875
2023-01-07 08:56:55,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5923635959625244
2023-01-07 08:56:55,782 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 2.9220266342163086
2023-01-07 08:56:55,782 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,782 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -7.219727516174316
2023-01-07 08:56:55,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,782 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 455.946044921875
2023-01-07 08:56:55,782 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.943789005279541
2023-01-07 08:56:55,783 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 453.92333984375
2023-01-07 08:56:55,783 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:55,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,784 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 220.26895141601562
2023-01-07 08:56:55,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.298548698425293
2023-01-07 08:56:55,785 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -1.0144639015197754
2023-01-07 08:56:55,785 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.001495361328125
2023-01-07 08:56:55,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 223.9217987060547
2023-01-07 08:56:55,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0421876907348633
2023-01-07 08:56:55,787 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 226.41290283203125
2023-01-07 08:56:55,787 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,787 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,787 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:56:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.323272705078125
2023-01-07 08:56:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 558.2052001953125
2023-01-07 08:56:55,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.356892585754395
2023-01-07 08:56:55,789 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.5367209315299988
2023-01-07 08:56:55,789 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.49056625366211
2023-01-07 08:56:55,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 563.5000610351562
2023-01-07 08:56:55,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4037448167800903
2023-01-07 08:56:55,790 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 559.3287353515625
2023-01-07 08:56:55,791 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:55,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 148.76602172851562
2023-01-07 08:56:55,791 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.136780023574829
2023-01-07 08:56:55,792 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 1.4640172719955444
2023-01-07 08:56:55,792 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,792 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 47.099639892578125
2023-01-07 08:56:55,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 148.36380004882812
2023-01-07 08:56:55,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.424736022949219
2023-01-07 08:56:55,794 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 147.62364196777344
2023-01-07 08:56:55,794 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:55,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.86197566986084
2023-01-07 08:56:55,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4367140531539917
2023-01-07 08:56:55,795 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 5.783738136291504
2023-01-07 08:56:55,796 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,796 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,796 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 47.64364242553711
2023-01-07 08:56:55,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,796 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 9.329203605651855
2023-01-07 08:56:55,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3031109571456909
2023-01-07 08:56:55,797 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 4.287365436553955
2023-01-07 08:56:55,797 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:55,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 18.743120193481445
2023-01-07 08:56:55,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.165456771850586
2023-01-07 08:56:55,799 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.6508046388626099
2023-01-07 08:56:55,799 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.318273544311523
2023-01-07 08:56:55,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,799 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 20.736238479614258
2023-01-07 08:56:55,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1578750610351562
2023-01-07 08:56:55,801 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 26.19538688659668
2023-01-07 08:56:55,801 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,801 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:55,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,801 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 9.58840560913086
2023-01-07 08:56:55,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.594409704208374
2023-01-07 08:56:55,802 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.6424479484558105
2023-01-07 08:56:55,802 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 29.640993118286133
2023-01-07 08:56:55,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,803 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 8.783204078674316
2023-01-07 08:56:55,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1009812355041504
2023-01-07 08:56:55,804 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 9.034271240234375
2023-01-07 08:56:55,804 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:55,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,804 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -2.222470760345459
2023-01-07 08:56:55,805 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7265331745147705
2023-01-07 08:56:55,806 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 1.4179335832595825
2023-01-07 08:56:55,806 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -15.361900329589844
2023-01-07 08:56:55,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,806 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -7.061731815338135
2023-01-07 08:56:55,806 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.090835094451904
2023-01-07 08:56:55,807 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -9.640283584594727
2023-01-07 08:56:55,807 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,808 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:55,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,808 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 285.6387939453125
2023-01-07 08:56:55,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.272721767425537
2023-01-07 08:56:55,809 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.09651237726211548
2023-01-07 08:56:55,809 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -12.083669662475586
2023-01-07 08:56:55,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,809 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 283.34027099609375
2023-01-07 08:56:55,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.032945394515991
2023-01-07 08:56:55,811 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 284.5938720703125
2023-01-07 08:56:55,811 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,811 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,811 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:55,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,811 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -2.3982908725738525
2023-01-07 08:56:55,811 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0584943294525146
2023-01-07 08:56:55,812 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.13775543868541718
2023-01-07 08:56:55,812 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,812 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,813 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 35.94120407104492
2023-01-07 08:56:55,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,813 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.9713072776794434
2023-01-07 08:56:55,813 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.094146966934204
2023-01-07 08:56:55,814 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -2.0370187759399414
2023-01-07 08:56:55,814 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,814 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,814 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:55,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,815 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -44.87995147705078
2023-01-07 08:56:55,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1316043138504028
2023-01-07 08:56:55,816 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 1.1104915142059326
2023-01-07 08:56:55,816 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,816 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,816 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 41.35844039916992
2023-01-07 08:56:55,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,816 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -40.99660110473633
2023-01-07 08:56:55,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1977038383483887
2023-01-07 08:56:55,817 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -38.36635208129883
2023-01-07 08:56:55,817 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,818 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,818 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:55,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,818 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -471.7554626464844
2023-01-07 08:56:55,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3069387674331665
2023-01-07 08:56:55,819 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.17671936750411987
2023-01-07 08:56:55,819 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,819 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,819 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -29.883983612060547
2023-01-07 08:56:55,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,820 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -472.2813415527344
2023-01-07 08:56:55,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.796677589416504
2023-01-07 08:56:55,821 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -471.4709777832031
2023-01-07 08:56:55,821 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,821 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,821 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:55,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,821 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1666.158447265625
2023-01-07 08:56:55,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3825573921203613
2023-01-07 08:56:55,822 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.00838974118232727
2023-01-07 08:56:55,822 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,822 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,823 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -0.40043163299560547
2023-01-07 08:56:55,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,823 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1666.960693359375
2023-01-07 08:56:55,823 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.113291621208191
2023-01-07 08:56:55,824 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1665.5726318359375
2023-01-07 08:56:55,824 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,824 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,824 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:55,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,825 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 517.942138671875
2023-01-07 08:56:55,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.50999116897583
2023-01-07 08:56:55,826 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.9425340890884399
2023-01-07 08:56:55,826 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,826 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,826 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -58.59798812866211
2023-01-07 08:56:55,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,826 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 518.163818359375
2023-01-07 08:56:55,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.485048294067383
2023-01-07 08:56:55,827 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 517.77978515625
2023-01-07 08:56:55,828 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,828 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,828 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:55,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,828 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 670.5128784179688
2023-01-07 08:56:55,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26929065585136414
2023-01-07 08:56:55,829 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.7407199144363403
2023-01-07 08:56:55,829 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,829 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 14.548171997070312
2023-01-07 08:56:55,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,830 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 672.266357421875
2023-01-07 08:56:55,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.64213228225708
2023-01-07 08:56:55,831 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 672.4370727539062
2023-01-07 08:56:55,831 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,831 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,831 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:55,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 546.734130859375
2023-01-07 08:56:55,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9624757170677185
2023-01-07 08:56:55,833 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.4411367177963257
2023-01-07 08:56:55,833 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,833 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,833 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.680994033813477
2023-01-07 08:56:55,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,833 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 547.1397094726562
2023-01-07 08:56:55,833 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2499283105134964
2023-01-07 08:56:55,834 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 547.5723876953125
2023-01-07 08:56:55,834 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,834 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,835 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:55,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 982.322265625
2023-01-07 08:56:55,835 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13961364328861237
2023-01-07 08:56:55,836 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 1.5555139780044556
2023-01-07 08:56:55,836 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,836 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,836 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7034673690795898
2023-01-07 08:56:55,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,836 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 981.5503540039062
2023-01-07 08:56:55,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.113764762878418
2023-01-07 08:56:55,838 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 983.4508666992188
2023-01-07 08:56:55,838 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,838 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,838 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:55,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1440.407958984375
2023-01-07 08:56:55,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9347667694091797
2023-01-07 08:56:55,839 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.6068192720413208
2023-01-07 08:56:55,839 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,839 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,840 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -43.33367919921875
2023-01-07 08:56:55,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,840 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1439.5565185546875
2023-01-07 08:56:55,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1685057878494263
2023-01-07 08:56:55,841 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 1438.322265625
2023-01-07 08:56:55,841 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,841 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,841 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:55,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1029.0001220703125
2023-01-07 08:56:55,842 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7398762702941895
2023-01-07 08:56:55,843 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.34919291734695435
2023-01-07 08:56:55,843 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,843 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,843 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 45.49131774902344
2023-01-07 08:56:55,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1028.6116943359375
2023-01-07 08:56:55,843 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.524442195892334
2023-01-07 08:56:55,844 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -1028.641357421875
2023-01-07 08:56:55,845 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,845 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,845 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:55,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,845 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2723.700927734375
2023-01-07 08:56:55,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8892644047737122
2023-01-07 08:56:55,846 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.14207375049591064
2023-01-07 08:56:55,846 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,846 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 9.79226303100586
2023-01-07 08:56:55,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2724.1806640625
2023-01-07 08:56:55,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1733333021402359
2023-01-07 08:56:55,848 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2724.110595703125
2023-01-07 08:56:55,848 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,848 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,848 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:55,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1065.84228515625
2023-01-07 08:56:55,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4180924892425537
2023-01-07 08:56:55,849 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.6363813877105713
2023-01-07 08:56:55,849 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,850 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,850 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -16.016372680664062
2023-01-07 08:56:55,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1065.8338623046875
2023-01-07 08:56:55,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9011437892913818
2023-01-07 08:56:55,851 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1065.925537109375
2023-01-07 08:56:55,851 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,851 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,851 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:55,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1008.0878295898438
2023-01-07 08:56:55,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1310015767812729
2023-01-07 08:56:55,853 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.46928733587265015
2023-01-07 08:56:55,853 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,853 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,853 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 79.97810363769531
2023-01-07 08:56:55,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,853 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1008.5794677734375
2023-01-07 08:56:55,854 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25682592391967773
2023-01-07 08:56:55,855 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -1009.099853515625
2023-01-07 08:56:55,855 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,855 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,855 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:55,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,855 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1804.173095703125
2023-01-07 08:56:55,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3374599814414978
2023-01-07 08:56:55,856 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.17323824763298035
2023-01-07 08:56:55,856 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,856 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 4.30811071395874
2023-01-07 08:56:55,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,857 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1803.63037109375
2023-01-07 08:56:55,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29259851574897766
2023-01-07 08:56:55,858 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1803.826171875
2023-01-07 08:56:55,858 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,858 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,858 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:55,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,858 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8416.08984375
2023-01-07 08:56:55,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9474617838859558
2023-01-07 08:56:55,860 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.9144924879074097
2023-01-07 08:56:55,860 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,860 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,860 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.950218200683594
2023-01-07 08:56:55,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8416.12109375
2023-01-07 08:56:55,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.158512681722641
2023-01-07 08:56:55,861 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8416.8095703125
2023-01-07 08:56:55,861 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,862 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:55,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,862 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3283.361083984375
2023-01-07 08:56:55,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1039528027176857
2023-01-07 08:56:55,863 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.12249649316072464
2023-01-07 08:56:55,863 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 34.03148651123047
2023-01-07 08:56:55,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,863 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3283.015869140625
2023-01-07 08:56:55,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5937210917472839
2023-01-07 08:56:55,865 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 3282.5234375
2023-01-07 08:56:55,865 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:55,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,865 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4244.49853515625
2023-01-07 08:56:55,865 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4725501537322998
2023-01-07 08:56:55,866 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -66.58938598632812
2023-01-07 08:56:55,866 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,866 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,867 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 8.640400886535645
2023-01-07 08:56:55,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,867 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4244.40625
2023-01-07 08:56:55,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6285402774810791
2023-01-07 08:56:55,868 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -4244.646484375
2023-01-07 08:56:55,868 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,868 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,868 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:55,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,869 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 14096.4423828125
2023-01-07 08:56:55,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.081547737121582
2023-01-07 08:56:55,870 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -414.36773681640625
2023-01-07 08:56:55,870 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,870 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,870 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -7.182888031005859
2023-01-07 08:56:55,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,870 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 14099.4052734375
2023-01-07 08:56:55,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.203661322593689
2023-01-07 08:56:55,872 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 14100.6611328125
2023-01-07 08:56:55,872 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:56:55,872 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:56:55,873 > [DEBUG] 0 :: 7.41771936416626
2023-01-07 08:56:55,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,876 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.060577392578125
2023-01-07 08:56:55,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -321.81365966796875
2023-01-07 08:56:55,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,880 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2928016483783722
2023-01-07 08:56:55,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,880 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -247.65975952148438
2023-01-07 08:56:55,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -329.33233642578125
2023-01-07 08:56:55,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,884 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.5628582239151
2023-01-07 08:56:55,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08626408874988556
2023-01-07 08:56:55,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,887 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.02638350985944271
2023-01-07 08:56:55,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,888 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -103.5631103515625
2023-01-07 08:56:55,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2897506356239319
2023-01-07 08:56:55,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,889 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2.919724225997925
2023-01-07 08:56:55,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09182616323232651
2023-01-07 08:56:55,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,890 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.01584184169769287
2023-01-07 08:56:55,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,891 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -99.10924530029297
2023-01-07 08:56:55,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02607084810733795
2023-01-07 08:56:55,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,892 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.25577449798584
2023-01-07 08:56:55,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03767355531454086
2023-01-07 08:56:55,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,893 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2509598135948181
2023-01-07 08:56:55,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,894 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -99.6133041381836
2023-01-07 08:56:55,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3875274956226349
2023-01-07 08:56:55,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,895 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2.1297388076782227
2023-01-07 08:56:55,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.051847875118255615
2023-01-07 08:56:55,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,897 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.04990728199481964
2023-01-07 08:56:55,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,897 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -99.7982177734375
2023-01-07 08:56:55,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6717509627342224
2023-01-07 08:56:55,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,898 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 8.975915908813477
2023-01-07 08:56:55,899 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2293235957622528
2023-01-07 08:56:55,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,900 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.03375037759542465
2023-01-07 08:56:55,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,900 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -92.73456573486328
2023-01-07 08:56:55,900 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8195785284042358
2023-01-07 08:56:55,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,902 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.4679219722747803
2023-01-07 08:56:55,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10395009070634842
2023-01-07 08:56:55,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,903 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.35560813546180725
2023-01-07 08:56:55,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,903 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.7649998664855957
2023-01-07 08:56:55,903 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25293681025505066
2023-01-07 08:56:55,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,905 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.9807446002960205
2023-01-07 08:56:55,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9057570099830627
2023-01-07 08:56:55,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,906 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.33219069242477417
2023-01-07 08:56:55,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,906 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.1584742069244385
2023-01-07 08:56:55,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3161250352859497
2023-01-07 08:56:55,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.8833656311035156
2023-01-07 08:56:55,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33438941836357117
2023-01-07 08:56:55,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.10499949753284454
2023-01-07 08:56:55,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.8066082000732422
2023-01-07 08:56:55,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.334270477294922
2023-01-07 08:56:55,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,912 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.6005725860595703
2023-01-07 08:56:55,912 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24931973218917847
2023-01-07 08:56:55,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,913 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.007972538471221924
2023-01-07 08:56:55,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,913 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2.744241714477539
2023-01-07 08:56:55,914 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2271697521209717
2023-01-07 08:56:55,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,915 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -2.8205642700195312
2023-01-07 08:56:55,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3146686255931854
2023-01-07 08:56:55,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,916 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.020636703819036484
2023-01-07 08:56:55,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,917 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -2.980792999267578
2023-01-07 08:56:55,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7892817258834839
2023-01-07 08:56:55,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,918 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 9.956512451171875
2023-01-07 08:56:55,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.332999587059021
2023-01-07 08:56:55,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,920 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.08349164575338364
2023-01-07 08:56:55,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,920 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 10.213850021362305
2023-01-07 08:56:55,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0487468242645264
2023-01-07 08:56:55,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,922 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -1.9650850296020508
2023-01-07 08:56:55,922 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1447513997554779
2023-01-07 08:56:55,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,923 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.13945481181144714
2023-01-07 08:56:55,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,923 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -2.448823928833008
2023-01-07 08:56:55,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5922704935073853
2023-01-07 08:56:55,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,925 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.300107479095459
2023-01-07 08:56:55,925 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14017397165298462
2023-01-07 08:56:55,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.16463984549045563
2023-01-07 08:56:55,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.475508213043213
2023-01-07 08:56:55,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.425926685333252
2023-01-07 08:56:55,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,928 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 10.447168350219727
2023-01-07 08:56:55,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0810836553573608
2023-01-07 08:56:55,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,929 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.09077366441488266
2023-01-07 08:56:55,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,929 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 9.710090637207031
2023-01-07 08:56:55,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35843515396118164
2023-01-07 08:56:55,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,931 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 2.3787193298339844
2023-01-07 08:56:55,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6982443928718567
2023-01-07 08:56:55,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,932 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.03389090299606323
2023-01-07 08:56:55,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,933 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 2.246763229370117
2023-01-07 08:56:55,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.036815132945775986
2023-01-07 08:56:55,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,934 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -16.953310012817383
2023-01-07 08:56:55,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.38708168268203735
2023-01-07 08:56:55,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,935 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.2478000968694687
2023-01-07 08:56:55,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,936 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -16.763437271118164
2023-01-07 08:56:55,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1595354080200195
2023-01-07 08:56:55,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,937 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.737672328948975
2023-01-07 08:56:55,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3857574462890625
2023-01-07 08:56:55,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.06911270320415497
2023-01-07 08:56:55,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,939 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 7.856348514556885
2023-01-07 08:56:55,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6989915370941162
2023-01-07 08:56:55,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,940 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -23.95014190673828
2023-01-07 08:56:55,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.343340277671814
2023-01-07 08:56:55,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,942 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.005338162183761597
2023-01-07 08:56:55,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,942 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -22.702852249145508
2023-01-07 08:56:55,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5689654350280762
2023-01-07 08:56:55,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,943 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -9.7666015625
2023-01-07 08:56:55,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4300273656845093
2023-01-07 08:56:55,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.29923397302627563
2023-01-07 08:56:55,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -8.09335708618164
2023-01-07 08:56:55,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.550384283065796
2023-01-07 08:56:55,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 11.006247520446777
2023-01-07 08:56:55,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6416226625442505
2023-01-07 08:56:55,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.009202718734741211
2023-01-07 08:56:55,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.76739501953125
2023-01-07 08:56:55,948 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06941306591033936
2023-01-07 08:56:55,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,950 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 42.64723587036133
2023-01-07 08:56:55,950 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3111613988876343
2023-01-07 08:56:55,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,951 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.04686659574508667
2023-01-07 08:56:55,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,951 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 43.296043395996094
2023-01-07 08:56:55,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2789225578308105
2023-01-07 08:56:55,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,953 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -15.615860939025879
2023-01-07 08:56:55,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21507123112678528
2023-01-07 08:56:55,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,954 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.05676440894603729
2023-01-07 08:56:55,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,954 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -17.09378433227539
2023-01-07 08:56:55,954 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2744436264038086
2023-01-07 08:56:55,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,956 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.6443405151367188
2023-01-07 08:56:55,956 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6255806684494019
2023-01-07 08:56:55,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,957 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.219117671251297
2023-01-07 08:56:55,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,958 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -0.6045699119567871
2023-01-07 08:56:55,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9973351955413818
2023-01-07 08:56:55,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,959 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 4.800480842590332
2023-01-07 08:56:55,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26080745458602905
2023-01-07 08:56:55,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,960 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 3.3486900329589844
2023-01-07 08:56:55,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1870959997177124
2023-01-07 08:56:55,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,962 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 4.000402927398682
2023-01-07 08:56:55,962 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8816235065460205
2023-01-07 08:56:55,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,963 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.3286595940589905
2023-01-07 08:56:55,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,963 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5.473532199859619
2023-01-07 08:56:55,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.16003704071045
2023-01-07 08:56:55,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,965 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -12.655920028686523
2023-01-07 08:56:55,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0218052864074707
2023-01-07 08:56:55,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,966 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.45363107323646545
2023-01-07 08:56:55,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,967 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -13.307222366333008
2023-01-07 08:56:55,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.667536735534668
2023-01-07 08:56:55,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 8.95016098022461
2023-01-07 08:56:55,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9331207275390625
2023-01-07 08:56:55,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,969 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.13161364197731018
2023-01-07 08:56:55,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,970 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.0601634979248047
2023-01-07 08:56:55,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.278599262237549
2023-01-07 08:56:55,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,971 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 93.4568099975586
2023-01-07 08:56:55,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5096395015716553
2023-01-07 08:56:55,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 96.05265045166016
2023-01-07 08:56:55,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.821273803710938
2023-01-07 08:56:55,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,974 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 45.303646087646484
2023-01-07 08:56:55,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35104262828826904
2023-01-07 08:56:55,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,975 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 52.00017166137695
2023-01-07 08:56:55,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.726118087768555
2023-01-07 08:56:55,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,977 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 26.904327392578125
2023-01-07 08:56:55,977 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.268386960029602
2023-01-07 08:56:55,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,978 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 49.20932388305664
2023-01-07 08:56:55,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.882499098777771
2023-01-07 08:56:55,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,979 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -46.849884033203125
2023-01-07 08:56:55,980 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.414798736572266
2023-01-07 08:56:55,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,981 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -39.59550476074219
2023-01-07 08:56:55,981 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.069955348968506
2023-01-07 08:56:55,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -13.711345672607422
2023-01-07 08:56:55,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6818599700927734
2023-01-07 08:56:55,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -2.7036960124969482
2023-01-07 08:56:55,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.258432388305664
2023-01-07 08:56:55,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,985 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 13.646356582641602
2023-01-07 08:56:55,985 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9625053405761719
2023-01-07 08:56:55,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 21.410268783569336
2023-01-07 08:56:55,986 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4463446140289307
2023-01-07 08:56:55,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,988 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -10.817028045654297
2023-01-07 08:56:55,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.834855079650879
2023-01-07 08:56:55,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,989 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 12.046695709228516
2023-01-07 08:56:55,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.364523887634277
2023-01-07 08:56:55,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,990 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 24.294496536254883
2023-01-07 08:56:55,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1306328773498535
2023-01-07 08:56:55,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,992 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.3188410997390747
2023-01-07 08:56:55,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,992 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 37.901485443115234
2023-01-07 08:56:55,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.489588499069214
2023-01-07 08:56:55,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,994 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -13.781752586364746
2023-01-07 08:56:55,994 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.381258964538574
2023-01-07 08:56:55,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,995 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -50.371978759765625
2023-01-07 08:56:55,995 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.835731506347656
2023-01-07 08:56:55,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 92.16736602783203
2023-01-07 08:56:55,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.937833309173584
2023-01-07 08:56:55,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,998 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 111.35966491699219
2023-01-07 08:56:55,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.657155990600586
2023-01-07 08:56:55,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:55,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:55,999 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 11.384538650512695
2023-01-07 08:56:55,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9755182862281799
2023-01-07 08:56:56,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,000 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.6318306922912598
2023-01-07 08:56:56,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 33.79655456542969
2023-01-07 08:56:56,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.911675453186035
2023-01-07 08:56:56,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.098068237304688
2023-01-07 08:56:56,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7745234966278076
2023-01-07 08:56:56,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 25.49492645263672
2023-01-07 08:56:56,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.440431594848633
2023-01-07 08:56:56,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,005 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 61.95452880859375
2023-01-07 08:56:56,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.978299856185913
2023-01-07 08:56:56,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,006 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 105.76908874511719
2023-01-07 08:56:56,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.983901977539062
2023-01-07 08:56:56,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,008 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 233.3399658203125
2023-01-07 08:56:56,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.794228553771973
2023-01-07 08:56:56,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.8520419597625732
2023-01-07 08:56:56,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 279.5782470703125
2023-01-07 08:56:56,009 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.107692718505859
2023-01-07 08:56:56,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,011 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 302.8301696777344
2023-01-07 08:56:56,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.332582473754883
2023-01-07 08:56:56,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,012 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.0144686698913574
2023-01-07 08:56:56,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,012 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -126.81268310546875
2023-01-07 08:56:56,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,013 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 343.666015625
2023-01-07 08:56:56,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.854547500610352
2023-01-07 08:56:56,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,014 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 320.92218017578125
2023-01-07 08:56:56,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.329116821289062
2023-01-07 08:56:56,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,015 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -78.0953598022461
2023-01-07 08:56:56,016 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.19688415527344
2023-01-07 08:56:56,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,017 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -269.838134765625
2023-01-07 08:56:56,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.257816314697266
2023-01-07 08:56:56,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,018 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 2.3035151958465576
2023-01-07 08:56:56,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,018 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -180.11886596679688
2023-01-07 08:56:56,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.883174896240234
2023-01-07 08:56:56,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,020 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -185.60308837890625
2023-01-07 08:56:56,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.427780628204346
2023-01-07 08:56:56,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.06031477451324463
2023-01-07 08:56:56,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -47.17085647583008
2023-01-07 08:56:56,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.6148681640625
2023-01-07 08:56:56,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,023 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -195.4846649169922
2023-01-07 08:56:56,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.128746032714844
2023-01-07 08:56:56,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,024 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -30.59068489074707
2023-01-07 08:56:56,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,025 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 398.21112060546875
2023-01-07 08:56:56,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.51437759399414
2023-01-07 08:56:56,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,026 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -171.98818969726562
2023-01-07 08:56:56,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.42501449584961
2023-01-07 08:56:56,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 271.1377868652344
2023-01-07 08:56:56,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.146852493286133
2023-01-07 08:56:56,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -223.22499084472656
2023-01-07 08:56:56,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.316816329956055
2023-01-07 08:56:56,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,030 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.708667755126953
2023-01-07 08:56:56,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 104.56434631347656
2023-01-07 08:56:56,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -330.05963134765625
2023-01-07 08:56:56,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,031 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 428.10223388671875
2023-01-07 08:56:56,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.162437438964844
2023-01-07 08:56:56,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,033 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -309.25616455078125
2023-01-07 08:56:56,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.51249885559082
2023-01-07 08:56:56,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -67.91607666015625
2023-01-07 08:56:56,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.67789840698242
2023-01-07 08:56:56,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -348.2386474609375
2023-01-07 08:56:56,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.128711700439453
2023-01-07 08:56:56,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,037 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -177.8472442626953
2023-01-07 08:56:56,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 431.6116943359375
2023-01-07 08:56:56,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.047961235046387
2023-01-07 08:56:56,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 578.4078369140625
2023-01-07 08:56:56,039 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.584266662597656
2023-01-07 08:56:56,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,040 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 3.322567939758301
2023-01-07 08:56:56,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,040 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 329.32452392578125
2023-01-07 08:56:56,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.12684631347656
2023-01-07 08:56:56,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,042 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 129.70004272460938
2023-01-07 08:56:56,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.525495529174805
2023-01-07 08:56:56,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:56:56,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:56:56,043 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -69.20225524902344
2023-01-07 08:56:56,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.322998046875
