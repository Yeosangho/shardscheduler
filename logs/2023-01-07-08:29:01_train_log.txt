2023-01-07 08:29:07,921 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:29:07,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:07,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:07,961 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:29:07,961 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:07,961 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:07,961 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:29:07,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,838 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,838 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,838 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,838 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:29:08,838 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,840 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:29:08,840 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,840 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,840 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:29:08,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,842 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,842 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,842 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,842 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:29:08,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,843 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:29:08,844 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,844 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,844 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:29:08,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,880 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,881 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,881 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,881 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:29:08,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,882 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,882 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,882 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,883 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:29:08,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,884 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,884 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,884 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:29:08,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,885 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,885 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,886 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,886 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:29:08,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,887 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,887 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,887 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:29:08,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,888 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,889 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,889 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:29:08,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,890 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,890 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,890 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,890 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:29:08,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,892 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:29:08,892 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,892 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:29:08,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,893 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,893 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,893 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,893 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:29:08,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,895 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,895 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,895 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,895 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:29:08,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,896 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,896 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,896 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,896 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:29:08,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,898 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,898 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,898 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,898 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:29:08,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,899 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,899 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,899 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,899 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:29:08,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,900 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,901 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:29:08,901 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,901 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,901 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:29:08,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,902 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:29:08,902 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,902 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,902 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:29:08,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,904 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:29:08,904 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,904 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,904 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:29:08,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,905 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,905 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,905 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,905 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:29:08,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,907 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:29:08,907 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,907 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,907 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:29:08,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,908 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,908 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,909 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,909 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:29:08,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,910 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:29:08,910 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,910 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,910 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:29:08,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,912 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,912 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,912 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,912 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:29:08,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,913 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,913 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,914 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,914 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:29:08,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,915 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:08,915 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,915 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,915 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:29:08,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,916 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:29:08,917 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,917 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:29:08,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,918 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:08,918 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,918 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,918 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:29:08,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,920 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,920 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,920 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,920 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:29:08,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,921 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,921 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,921 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,921 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:29:08,922 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,923 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:29:08,923 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,923 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,923 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:29:08,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,925 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,925 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,925 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,925 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:29:08,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,926 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,926 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,926 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,926 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:29:08,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,927 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:08,928 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,928 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,928 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:29:08,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,929 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,929 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,929 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,929 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:29:08,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,931 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,931 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,931 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,931 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:29:08,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,932 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:29:08,932 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,932 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:29:08,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,934 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,934 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,934 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,934 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:29:08,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,935 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,935 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,935 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,936 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:29:08,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,937 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:08,937 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,937 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,937 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:29:08,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,938 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,939 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,939 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,939 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:29:08,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,940 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,940 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,940 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,940 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:29:08,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,942 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:29:08,942 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,942 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,942 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:29:08,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,943 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:29:08,943 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,943 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,943 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:29:08,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,945 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:29:08,945 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,945 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,945 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:29:08,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,946 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:08,946 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,946 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,946 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:29:08,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,948 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:29:08,948 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,948 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,948 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:29:08,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,949 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,949 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,949 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,949 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:29:08,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,951 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:08,951 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,951 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,951 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:29:08,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,952 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,953 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,953 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,953 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:29:08,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,954 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,954 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,954 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:29:08,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,956 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,956 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,956 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:29:08,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,957 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:29:08,957 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,957 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:29:08,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,958 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,959 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,959 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,959 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:29:08,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,960 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,960 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,960 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,960 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:29:08,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,962 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,962 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,962 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:29:08,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,963 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:08,963 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,963 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,963 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:29:08,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,965 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,965 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,965 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:29:08,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,966 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,966 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,967 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:29:08,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,968 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,968 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,968 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,968 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:29:08,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,970 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,970 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,970 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:29:08,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,971 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,971 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,971 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:29:08,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,973 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:08,973 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,973 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:29:08,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,974 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,974 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,974 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,974 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:29:08,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,975 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,976 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,976 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:29:08,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,977 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,977 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,977 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,977 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:29:08,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,978 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,979 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,979 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,979 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:29:08,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,980 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,980 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,980 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,980 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:29:08,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,982 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:08,982 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,982 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,982 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:29:08,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,983 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,983 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,983 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,983 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:29:08,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,985 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,985 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,985 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,985 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:29:08,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,986 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,986 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,986 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,986 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:29:08,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,988 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,988 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,988 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,988 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:29:08,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,989 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,989 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,989 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,989 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:29:08,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,991 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:08,991 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,991 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,991 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:29:08,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,992 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,992 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,992 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,992 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:29:08,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,994 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,994 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,994 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,994 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:29:08,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,995 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:08,995 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,995 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,995 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:29:08,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,997 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:08,997 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,997 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,997 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:29:08,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,998 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:08,998 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:08,998 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:08,998 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:29:08,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:08,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,000 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:29:09,000 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,000 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,000 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:29:09,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,001 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:29:09,001 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,001 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,001 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:29:09,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,003 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:29:09,003 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,003 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,003 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:29:09,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,004 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:29:09,004 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,004 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,004 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:29:09,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,006 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:29:09,006 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,006 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,006 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:29:09,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,007 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,007 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,007 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,007 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:29:09,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,009 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:29:09,009 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,009 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,009 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:29:09,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,010 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,010 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,010 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,010 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:29:09,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,012 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:29:09,012 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,012 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,012 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:29:09,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,013 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:29:09,013 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,013 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,013 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:29:09,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,015 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:29:09,015 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,015 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,015 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:29:09,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,016 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:29:09,016 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,016 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,016 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:29:09,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,018 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:29:09,018 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,018 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,018 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:29:09,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,019 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,019 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,019 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,019 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:29:09,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,021 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:29:09,021 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,021 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,021 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:29:09,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,022 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,022 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,023 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,023 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:29:09,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,024 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:29:09,024 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,024 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:29:09,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,025 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:29:09,026 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,026 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,026 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:29:09,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,027 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:29:09,027 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,027 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,027 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:29:09,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,028 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,028 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,029 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:29:09,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,030 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:29:09,030 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,030 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,030 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:29:09,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,031 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:29:09,031 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,032 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:29:09,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,033 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:29:09,033 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,033 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,033 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:29:09,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,034 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:29:09,034 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,034 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,034 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:29:09,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:29:09,036 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:29:09,036 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,036 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:29:09,036 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:29:09,038 > [DEBUG] 0 :: 7.302767753601074
2023-01-07 08:29:09,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,043 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,043 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.049713134765625
2023-01-07 08:29:09,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -368.82635498046875
2023-01-07 08:29:09,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,048 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,048 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.265518456697464
2023-01-07 08:29:09,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,049 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -201.35894775390625
2023-01-07 08:29:09,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -356.293212890625
2023-01-07 08:29:09,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,059 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,059 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.642795443534851
2023-01-07 08:29:09,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.060626864433288574
2023-01-07 08:29:09,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,060 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,060 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.041937511414289474
2023-01-07 08:29:09,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,060 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -78.34053802490234
2023-01-07 08:29:09,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38712525367736816
2023-01-07 08:29:09,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,063 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,063 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 9.277803421020508
2023-01-07 08:29:09,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4905492067337036
2023-01-07 08:29:09,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,064 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,064 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.0384179912507534
2023-01-07 08:29:09,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,064 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 7.687543869018555
2023-01-07 08:29:09,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28545239567756653
2023-01-07 08:29:09,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,066 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,066 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -4.193355083465576
2023-01-07 08:29:09,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5779551863670349
2023-01-07 08:29:09,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,067 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,067 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.26043543219566345
2023-01-07 08:29:09,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,068 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -15.560861587524414
2023-01-07 08:29:09,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012325320392847061
2023-01-07 08:29:09,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,069 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,069 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.680215835571289
2023-01-07 08:29:09,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16305653750896454
2023-01-07 08:29:09,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,070 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,070 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.037181902676820755
2023-01-07 08:29:09,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,071 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.20631217956543
2023-01-07 08:29:09,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4787421226501465
2023-01-07 08:29:09,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,072 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,073 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -13.874947547912598
2023-01-07 08:29:09,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.811577558517456
2023-01-07 08:29:09,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,074 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,074 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.08991412073373795
2023-01-07 08:29:09,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,074 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -24.737590789794922
2023-01-07 08:29:09,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2594771981239319
2023-01-07 08:29:09,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,076 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,076 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 9.527100563049316
2023-01-07 08:29:09,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6927099823951721
2023-01-07 08:29:09,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,077 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.18714827299118042
2023-01-07 08:29:09,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.6571872234344482
2023-01-07 08:29:09,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6033916473388672
2023-01-07 08:29:09,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,079 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,079 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 38.83283996582031
2023-01-07 08:29:09,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.271432101726532
2023-01-07 08:29:09,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,080 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.11728549748659134
2023-01-07 08:29:09,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,081 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 28.983055114746094
2023-01-07 08:29:09,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33381974697113037
2023-01-07 08:29:09,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,082 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 16.115198135375977
2023-01-07 08:29:09,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09127752482891083
2023-01-07 08:29:09,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,083 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,083 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.05924049764871597
2023-01-07 08:29:09,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,084 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 13.60676097869873
2023-01-07 08:29:09,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014071539044380188
2023-01-07 08:29:09,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,087 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,087 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 33.44371032714844
2023-01-07 08:29:09,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06961427628993988
2023-01-07 08:29:09,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,088 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,088 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.03591079264879227
2023-01-07 08:29:09,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,088 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 28.941499710083008
2023-01-07 08:29:09,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3224838078022003
2023-01-07 08:29:09,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,090 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,090 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.52066993713379
2023-01-07 08:29:09,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1378422975540161
2023-01-07 08:29:09,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.10459600389003754
2023-01-07 08:29:09,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,092 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 9.91771125793457
2023-01-07 08:29:09,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0762858390808105
2023-01-07 08:29:09,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,093 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -5.858820915222168
2023-01-07 08:29:09,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9189102649688721
2023-01-07 08:29:09,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,094 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,095 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.025780178606510162
2023-01-07 08:29:09,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,095 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -11.230426788330078
2023-01-07 08:29:09,095 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4030357599258423
2023-01-07 08:29:09,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,097 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 26.80654525756836
2023-01-07 08:29:09,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22597548365592957
2023-01-07 08:29:09,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,098 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.0549885518848896
2023-01-07 08:29:09,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,099 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 24.65542221069336
2023-01-07 08:29:09,099 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2824301719665527
2023-01-07 08:29:09,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,100 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -2.6060009002685547
2023-01-07 08:29:09,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.284606456756592
2023-01-07 08:29:09,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.020268604159355164
2023-01-07 08:29:09,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.811352729797363
2023-01-07 08:29:09,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4987367391586304
2023-01-07 08:29:09,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,103 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.902227401733398
2023-01-07 08:29:09,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.48171842098236084
2023-01-07 08:29:09,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,104 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.06340116262435913
2023-01-07 08:29:09,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -27.93312644958496
2023-01-07 08:29:09,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.030817985534668
2023-01-07 08:29:09,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,106 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.731409072875977
2023-01-07 08:29:09,107 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2834668159484863
2023-01-07 08:29:09,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,108 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.12774011492729187
2023-01-07 08:29:09,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -6.002092361450195
2023-01-07 08:29:09,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2912274897098541
2023-01-07 08:29:09,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 17.669750213623047
2023-01-07 08:29:09,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16327731311321259
2023-01-07 08:29:09,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,110 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.152439147233963
2023-01-07 08:29:09,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 52.02342224121094
2023-01-07 08:29:09,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7557197213172913
2023-01-07 08:29:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -7.317270278930664
2023-01-07 08:29:09,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.018727213144302368
2023-01-07 08:29:09,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,114 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.13466067612171173
2023-01-07 08:29:09,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.369930267333984
2023-01-07 08:29:09,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.499783992767334
2023-01-07 08:29:09,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,115 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3.1086244583129883
2023-01-07 08:29:09,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.007018089294434
2023-01-07 08:29:09,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,117 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.08601179718971252
2023-01-07 08:29:09,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 7.061324119567871
2023-01-07 08:29:09,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09080100059509277
2023-01-07 08:29:09,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.35670280456543
2023-01-07 08:29:09,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5208661556243896
2023-01-07 08:29:09,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,120 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.04882056266069412
2023-01-07 08:29:09,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -684.98876953125
2023-01-07 08:29:09,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2085535526275635
2023-01-07 08:29:09,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,121 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -2.6990489959716797
2023-01-07 08:29:09,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14810296893119812
2023-01-07 08:29:09,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,123 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.027922704815864563
2023-01-07 08:29:09,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 5.827240943908691
2023-01-07 08:29:09,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5578970909118652
2023-01-07 08:29:09,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,124 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -43.71783447265625
2023-01-07 08:29:09,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.711883306503296
2023-01-07 08:29:09,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,126 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.13536402583122253
2023-01-07 08:29:09,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -43.31536102294922
2023-01-07 08:29:09,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.693199157714844
2023-01-07 08:29:09,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,128 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.8133058547973633
2023-01-07 08:29:09,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6390852928161621
2023-01-07 08:29:09,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,129 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.13962939381599426
2023-01-07 08:29:09,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -24.753755569458008
2023-01-07 08:29:09,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3214184045791626
2023-01-07 08:29:09,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,131 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -6.0770583152771
2023-01-07 08:29:09,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5652259588241577
2023-01-07 08:29:09,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.10449478030204773
2023-01-07 08:29:09,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.733761787414551
2023-01-07 08:29:09,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8885998725891113
2023-01-07 08:29:09,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,134 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 42.54599380493164
2023-01-07 08:29:09,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7903440594673157
2023-01-07 08:29:09,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 18.700862884521484
2023-01-07 08:29:09,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.729931831359863
2023-01-07 08:29:09,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,136 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 15.129230499267578
2023-01-07 08:29:09,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9156882762908936
2023-01-07 08:29:09,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,138 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.3330804705619812
2023-01-07 08:29:09,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.911421775817871
2023-01-07 08:29:09,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5969984531402588
2023-01-07 08:29:09,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,140 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 32.38343811035156
2023-01-07 08:29:09,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9948484301567078
2023-01-07 08:29:09,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,141 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.14040690660476685
2023-01-07 08:29:09,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.21176528930664
2023-01-07 08:29:09,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.290802001953125
2023-01-07 08:29:09,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 9.024266242980957
2023-01-07 08:29:09,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3517474830150604
2023-01-07 08:29:09,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,144 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.525035560131073
2023-01-07 08:29:09,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,145 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 14.531989097595215
2023-01-07 08:29:09,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3184120655059814
2023-01-07 08:29:09,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,146 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 100.00015258789062
2023-01-07 08:29:09,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.409320831298828
2023-01-07 08:29:09,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 91.26673126220703
2023-01-07 08:29:09,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.48748779296875
2023-01-07 08:29:09,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,150 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -4.091707229614258
2023-01-07 08:29:09,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6066879034042358
2023-01-07 08:29:09,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 6.923357009887695
2023-01-07 08:29:09,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7950756549835205
2023-01-07 08:29:09,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,152 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,152 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.5892609357833862
2023-01-07 08:29:09,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.696014881134033
2023-01-07 08:29:09,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,154 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.888106346130371
2023-01-07 08:29:09,154 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9784733057022095
2023-01-07 08:29:09,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,155 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,155 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 29.576080322265625
2023-01-07 08:29:09,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.209633827209473
2023-01-07 08:29:09,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,157 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 22.250415802001953
2023-01-07 08:29:09,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.911529541015625
2023-01-07 08:29:09,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,158 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,158 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 19.176738739013672
2023-01-07 08:29:09,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7661664485931396
2023-01-07 08:29:09,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,159 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 1.1613441705703735
2023-01-07 08:29:09,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.602569580078125
2023-01-07 08:29:09,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,161 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,161 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -29.280441284179688
2023-01-07 08:29:09,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.927703380584717
2023-01-07 08:29:09,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,162 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -47.19715118408203
2023-01-07 08:29:09,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5372471809387207
2023-01-07 08:29:09,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,164 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,164 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 67.99576568603516
2023-01-07 08:29:09,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.54710578918457
2023-01-07 08:29:09,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,165 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 64.39297485351562
2023-01-07 08:29:09,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.768802642822266
2023-01-07 08:29:09,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,166 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,166 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -67.75271606445312
2023-01-07 08:29:09,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.131873369216919
2023-01-07 08:29:09,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,168 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,168 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.38995450735092163
2023-01-07 08:29:09,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,168 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -68.46981811523438
2023-01-07 08:29:09,168 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.061038970947266
2023-01-07 08:29:09,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,169 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,169 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 13.081697463989258
2023-01-07 08:29:09,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.831867218017578
2023-01-07 08:29:09,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,171 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 5.791853904724121
2023-01-07 08:29:09,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2344491481781006
2023-01-07 08:29:09,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,172 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,172 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 33.77540588378906
2023-01-07 08:29:09,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.068104267120361
2023-01-07 08:29:09,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 23.404556274414062
2023-01-07 08:29:09,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.44226837158203
2023-01-07 08:29:09,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,175 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.2823219299316406
2023-01-07 08:29:09,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5255385637283325
2023-01-07 08:29:09,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,176 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,176 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.8554689288139343
2023-01-07 08:29:09,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,176 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -24.702404022216797
2023-01-07 08:29:09,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.440204620361328
2023-01-07 08:29:09,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,178 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,178 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 60.339176177978516
2023-01-07 08:29:09,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.192824363708496
2023-01-07 08:29:09,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,179 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 50.071563720703125
2023-01-07 08:29:09,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.796889781951904
2023-01-07 08:29:09,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,181 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,181 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 39.764373779296875
2023-01-07 08:29:09,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.048861026763916
2023-01-07 08:29:09,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,182 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 20.245744705200195
2023-01-07 08:29:09,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.50006103515625
2023-01-07 08:29:09,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,184 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,184 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -183.60302734375
2023-01-07 08:29:09,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.465843200683594
2023-01-07 08:29:09,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,185 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,185 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.8743631839752197
2023-01-07 08:29:09,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,185 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -202.1077880859375
2023-01-07 08:29:09,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.378119468688965
2023-01-07 08:29:09,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,187 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -191.65301513671875
2023-01-07 08:29:09,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.372119188308716
2023-01-07 08:29:09,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,188 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.6370041370391846
2023-01-07 08:29:09,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,188 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -44.566673278808594
2023-01-07 08:29:09,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,189 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -192.60781860351562
2023-01-07 08:29:09,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.90502166748047
2023-01-07 08:29:09,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,191 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -197.44171142578125
2023-01-07 08:29:09,191 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1529839038848877
2023-01-07 08:29:09,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,192 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -23.41718101501465
2023-01-07 08:29:09,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.55634307861328
2023-01-07 08:29:09,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,194 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,194 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -17.678861618041992
2023-01-07 08:29:09,194 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.225374221801758
2023-01-07 08:29:09,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,195 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,195 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5140042304992676
2023-01-07 08:29:09,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,195 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -50.496498107910156
2023-01-07 08:29:09,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.57673454284668
2023-01-07 08:29:09,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,197 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -106.19027709960938
2023-01-07 08:29:09,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.654735565185547
2023-01-07 08:29:09,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,198 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,198 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 2.709836006164551
2023-01-07 08:29:09,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,198 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,199 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 13.259321212768555
2023-01-07 08:29:09,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.73982620239258
2023-01-07 08:29:09,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,201 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -148.89434814453125
2023-01-07 08:29:09,201 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0564470291137695
2023-01-07 08:29:09,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,202 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -68.00389099121094
2023-01-07 08:29:09,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,203 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -213.97637939453125
2023-01-07 08:29:09,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -83.90403747558594
2023-01-07 08:29:09,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,204 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -165.35733032226562
2023-01-07 08:29:09,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.08503723144531
2023-01-07 08:29:09,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,205 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -243.43228149414062
2023-01-07 08:29:09,206 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.2083740234375
2023-01-07 08:29:09,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,207 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -189.49594116210938
2023-01-07 08:29:09,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.71134567260742
2023-01-07 08:29:09,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,208 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -11.130607604980469
2023-01-07 08:29:09,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,208 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 25.533100128173828
2023-01-07 08:29:09,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -214.36038208007812
2023-01-07 08:29:09,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -212.67495727539062
2023-01-07 08:29:09,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.673095703125
2023-01-07 08:29:09,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,210 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -208.37005615234375
2023-01-07 08:29:09,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.822696685791016
2023-01-07 08:29:09,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,212 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 167.3870849609375
2023-01-07 08:29:09,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.304832458496094
2023-01-07 08:29:09,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,213 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -213.85348510742188
2023-01-07 08:29:09,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.055070877075195
2023-01-07 08:29:09,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,214 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 169.29934692382812
2023-01-07 08:29:09,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,215 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -209.87728881835938
2023-01-07 08:29:09,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 188.75537109375
2023-01-07 08:29:09,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,216 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,216 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 315.90673828125
2023-01-07 08:29:09,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 91.75839233398438
2023-01-07 08:29:09,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,217 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,217 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -0.7984088659286499
2023-01-07 08:29:09,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,218 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 359.29583740234375
2023-01-07 08:29:09,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.809492111206055
2023-01-07 08:29:09,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,219 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 486.17681884765625
2023-01-07 08:29:09,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 130.19412231445312
2023-01-07 08:29:09,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,221 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 589.0445556640625
2023-01-07 08:29:09,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.2874870300293
2023-01-07 08:29:09,223 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:29:09,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,223 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,223 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1899.0380859375
2023-01-07 08:29:09,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,223 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 785.4033203125
2023-01-07 08:29:09,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,224 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,224 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -6.336111068725586
2023-01-07 08:29:09,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,224 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.92764663696289
2023-01-07 08:29:09,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -17.83173942565918
2023-01-07 08:29:09,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 20.237319946289062
2023-01-07 08:29:09,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 5.68595027923584
2023-01-07 08:29:09,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -55.979583740234375
2023-01-07 08:29:09,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,226 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 30.512283325195312
2023-01-07 08:29:09,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,226 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -34.993797302246094
2023-01-07 08:29:09,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,226 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.0079402923584
2023-01-07 08:29:09,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,227 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -16.18767547607422
2023-01-07 08:29:09,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,227 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 3.445359230041504
2023-01-07 08:29:09,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,227 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 18.778724670410156
2023-01-07 08:29:09,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,227 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 74.3249282836914
2023-01-07 08:29:09,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -14.698129653930664
2023-01-07 08:29:09,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.089360237121582
2023-01-07 08:29:09,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -55.05862808227539
2023-01-07 08:29:09,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -57.0361328125
2023-01-07 08:29:09,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 107.60653686523438
2023-01-07 08:29:09,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 118.5108871459961
2023-01-07 08:29:09,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -41.17881774902344
2023-01-07 08:29:09,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 57.9107666015625
2023-01-07 08:29:09,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -674.3261108398438
2023-01-07 08:29:09,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1378.9188232421875
2023-01-07 08:29:09,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -2173.23193359375
2023-01-07 08:29:09,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,232 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1957.2694091796875
2023-01-07 08:29:09,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 132.3009490966797
2023-01-07 08:29:09,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 119.67164611816406
2023-01-07 08:29:09,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 20.177635192871094
2023-01-07 08:29:09,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.90947151184082
2023-01-07 08:29:09,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 19.67485809326172
2023-01-07 08:29:09,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -26.537639617919922
2023-01-07 08:29:09,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,235 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 27.70879554748535
2023-01-07 08:29:09,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,235 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 77.98258209228516
2023-01-07 08:29:09,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 78.5039291381836
2023-01-07 08:29:09,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 14.899505615234375
2023-01-07 08:29:09,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.404861450195312
2023-01-07 08:29:09,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.3719282150268555
2023-01-07 08:29:09,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -41.50065231323242
2023-01-07 08:29:09,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.04589080810547
2023-01-07 08:29:09,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,238 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 19.89347267150879
2023-01-07 08:29:09,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,238 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -10.745357513427734
2023-01-07 08:29:09,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,239 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.7579278945922852
2023-01-07 08:29:09,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.932928085327148
2023-01-07 08:29:09,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,240 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -57.727027893066406
2023-01-07 08:29:09,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,240 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -198.70721435546875
2023-01-07 08:29:09,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 721.3172607421875
2023-01-07 08:29:09,241 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,241 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -1899.0380859375
2023-01-07 08:29:09,241 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,241 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,241 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -0.7466068267822266
2023-01-07 08:29:09,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,242 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 785.4033203125
2023-01-07 08:29:09,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.34282112121582
2023-01-07 08:29:09,243 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,243 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -0.7466068267822266
2023-01-07 08:29:09,243 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.171111106872559
2023-01-07 08:29:09,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,243 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,243 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -8.333395004272461
2023-01-07 08:29:09,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,244 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 785.4033203125
2023-01-07 08:29:09,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.960601806640625
2023-01-07 08:29:09,245 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,245 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -8.333395004272461
2023-01-07 08:29:09,245 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:09,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,246 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 785.4033203125
2023-01-07 08:29:09,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 164.14425659179688
2023-01-07 08:29:09,247 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,247 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 9.040674209594727
2023-01-07 08:29:09,247 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:09,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,247 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 785.4033203125
2023-01-07 08:29:09,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.47429656982422
2023-01-07 08:29:09,248 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,248 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 785.4033203125
2023-01-07 08:29:09,249 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,249 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,249 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 2.0797157287597656
2023-01-07 08:29:09,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,249 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,249 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 44.94397735595703
2023-01-07 08:29:09,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,250 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.689945220947266
2023-01-07 08:29:09,251 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,251 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 2.0797157287597656
2023-01-07 08:29:09,251 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,251 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,251 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.320369720458984
2023-01-07 08:29:09,252 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,252 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 44.94397735595703
2023-01-07 08:29:09,253 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,253 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,253 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -2.9586033821105957
2023-01-07 08:29:09,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,253 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 44.19218444824219
2023-01-07 08:29:09,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.376480102539062
2023-01-07 08:29:09,254 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,254 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -2.9586033821105957
2023-01-07 08:29:09,254 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,255 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -231.65655517578125
2023-01-07 08:29:09,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.98286437988281
2023-01-07 08:29:09,256 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,256 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 233.8115692138672
2023-01-07 08:29:09,256 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,256 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -231.65655517578125
2023-01-07 08:29:09,257 > [DEBUG] 0 :: before allreduce fusion buffer :: -64.2809066772461
2023-01-07 08:29:09,257 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,258 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -10.390886306762695
2023-01-07 08:29:09,258 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,258 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -231.65655517578125
2023-01-07 08:29:09,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.54393768310547
2023-01-07 08:29:09,259 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,259 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 44.19218444824219
2023-01-07 08:29:09,259 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,260 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 1.594423532485962
2023-01-07 08:29:09,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -231.65655517578125
2023-01-07 08:29:09,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.886016845703125
2023-01-07 08:29:09,261 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,261 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 1.594423532485962
2023-01-07 08:29:09,261 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,262 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -231.65655517578125
2023-01-07 08:29:09,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,262 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.27700424194336
2023-01-07 08:29:09,263 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,263 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -231.65655517578125
2023-01-07 08:29:09,263 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,264 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,264 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.4544522762298584
2023-01-07 08:29:09,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,264 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -87.17450714111328
2023-01-07 08:29:09,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.77483367919922
2023-01-07 08:29:09,265 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,265 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.4544522762298584
2023-01-07 08:29:09,265 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,266 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.800628662109375
2023-01-07 08:29:09,267 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,267 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -87.17450714111328
2023-01-07 08:29:09,267 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:09,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,267 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 8.407597541809082
2023-01-07 08:29:09,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.35236930847168
2023-01-07 08:29:09,268 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,268 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -18.055870056152344
2023-01-07 08:29:09,269 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:09,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,269 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 8.407597541809082
2023-01-07 08:29:09,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,269 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.87602424621582
2023-01-07 08:29:09,270 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,271 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 8.407597541809082
2023-01-07 08:29:09,271 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,271 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.437557220458984
2023-01-07 08:29:09,272 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,272 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -0.7751283645629883
2023-01-07 08:29:09,272 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,273 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.04901123046875
2023-01-07 08:29:09,273 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.99434471130371
2023-01-07 08:29:09,274 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,274 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -167.04901123046875
2023-01-07 08:29:09,274 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,274 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,274 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.34671682119369507
2023-01-07 08:29:09,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,274 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,275 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 2.789769172668457
2023-01-07 08:29:09,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -27.93755531311035
2023-01-07 08:29:09,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.786996841430664
2023-01-07 08:29:09,276 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,276 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.34671682119369507
2023-01-07 08:29:09,276 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.069580078125
2023-01-07 08:29:09,278 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,278 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 2.789769172668457
2023-01-07 08:29:09,278 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.23777961730957
2023-01-07 08:29:09,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -27.93755531311035
2023-01-07 08:29:09,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.645702838897705
2023-01-07 08:29:09,279 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,279 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -10.767580032348633
2023-01-07 08:29:09,279 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.919826865196228
2023-01-07 08:29:09,281 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,281 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -27.93755531311035
2023-01-07 08:29:09,281 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7201545238494873
2023-01-07 08:29:09,282 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,283 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -4.60162353515625
2023-01-07 08:29:09,283 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -210.31021118164062
2023-01-07 08:29:09,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.893896102905273
2023-01-07 08:29:09,284 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,284 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -210.31021118164062
2023-01-07 08:29:09,284 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,285 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.5725835561752319
2023-01-07 08:29:09,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -6.336111068725586
2023-01-07 08:29:09,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.859333038330078
2023-01-07 08:29:09,286 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,286 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.5725835561752319
2023-01-07 08:29:09,286 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.787897109985352
2023-01-07 08:29:09,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,287 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -6.336111068725586
2023-01-07 08:29:09,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.297100067138672
2023-01-07 08:29:09,288 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,288 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -6.336111068725586
2023-01-07 08:29:09,288 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,288 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.7376952767372131
2023-01-07 08:29:09,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.92764663696289
2023-01-07 08:29:09,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.351455688476562
2023-01-07 08:29:09,290 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,290 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.7376952767372131
2023-01-07 08:29:09,290 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -20.516536712646484
2023-01-07 08:29:09,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,290 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.92764663696289
2023-01-07 08:29:09,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.807960033416748
2023-01-07 08:29:09,291 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,291 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 17.92764663696289
2023-01-07 08:29:09,292 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:09,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -17.83173942565918
2023-01-07 08:29:09,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.957878112792969
2023-01-07 08:29:09,293 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,293 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -31.29385757446289
2023-01-07 08:29:09,293 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:09,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -17.83173942565918
2023-01-07 08:29:09,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4564110040664673
2023-01-07 08:29:09,295 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,295 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -17.83173942565918
2023-01-07 08:29:09,295 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,295 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,295 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 1.0445305109024048
2023-01-07 08:29:09,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 20.237319946289062
2023-01-07 08:29:09,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.33705139160156
2023-01-07 08:29:09,297 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,297 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 1.0445305109024048
2023-01-07 08:29:09,297 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.0566999912261963
2023-01-07 08:29:09,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 20.237319946289062
2023-01-07 08:29:09,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.475013732910156
2023-01-07 08:29:09,299 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,299 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 20.237319946289062
2023-01-07 08:29:09,299 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,300 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.016020119190216064
2023-01-07 08:29:09,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 5.68595027923584
2023-01-07 08:29:09,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.167610168457031
2023-01-07 08:29:09,301 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,301 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.016020119190216064
2023-01-07 08:29:09,301 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 15.858409881591797
2023-01-07 08:29:09,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,302 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 5.68595027923584
2023-01-07 08:29:09,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.298808097839355
2023-01-07 08:29:09,303 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,303 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 5.68595027923584
2023-01-07 08:29:09,303 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,303 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:09,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -55.979583740234375
2023-01-07 08:29:09,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.626047134399414
2023-01-07 08:29:09,304 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,304 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -8.322285652160645
2023-01-07 08:29:09,305 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:09,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -55.979583740234375
2023-01-07 08:29:09,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2066872119903564
2023-01-07 08:29:09,306 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,307 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -55.979583740234375
2023-01-07 08:29:09,307 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,307 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.527002215385437
2023-01-07 08:29:09,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 30.512283325195312
2023-01-07 08:29:09,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.791574478149414
2023-01-07 08:29:09,309 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,309 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.527002215385437
2023-01-07 08:29:09,309 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 12.577384948730469
2023-01-07 08:29:09,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 30.512283325195312
2023-01-07 08:29:09,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.18921661376953
2023-01-07 08:29:09,310 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,310 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 30.512283325195312
2023-01-07 08:29:09,310 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,311 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.1504390835762024
2023-01-07 08:29:09,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -34.993797302246094
2023-01-07 08:29:09,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.022796630859375
2023-01-07 08:29:09,312 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,312 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.1504390835762024
2023-01-07 08:29:09,312 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.623693943023682
2023-01-07 08:29:09,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -34.993797302246094
2023-01-07 08:29:09,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.584805488586426
2023-01-07 08:29:09,314 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,314 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -34.993797302246094
2023-01-07 08:29:09,314 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,314 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.36639025807380676
2023-01-07 08:29:09,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.0079402923584
2023-01-07 08:29:09,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20770108699798584
2023-01-07 08:29:09,316 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,316 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.36639025807380676
2023-01-07 08:29:09,316 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -25.3018798828125
2023-01-07 08:29:09,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,316 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.0079402923584
2023-01-07 08:29:09,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.863095760345459
2023-01-07 08:29:09,317 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,318 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -20.0079402923584
2023-01-07 08:29:09,318 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,318 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.21860098838806152
2023-01-07 08:29:09,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -16.18767547607422
2023-01-07 08:29:09,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.108112335205078
2023-01-07 08:29:09,319 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,320 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.21860098838806152
2023-01-07 08:29:09,320 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.52092742919922
2023-01-07 08:29:09,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,320 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -16.18767547607422
2023-01-07 08:29:09,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8659698963165283
2023-01-07 08:29:09,321 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,321 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -16.18767547607422
2023-01-07 08:29:09,321 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,322 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.3141065239906311
2023-01-07 08:29:09,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 3.445359230041504
2023-01-07 08:29:09,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9316930770874023
2023-01-07 08:29:09,323 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,323 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.3141065239906311
2023-01-07 08:29:09,323 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.565258026123047
2023-01-07 08:29:09,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 3.445359230041504
2023-01-07 08:29:09,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.127645015716553
2023-01-07 08:29:09,325 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,325 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 3.445359230041504
2023-01-07 08:29:09,325 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,325 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,325 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,325 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.07904483377933502
2023-01-07 08:29:09,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,326 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 18.778724670410156
2023-01-07 08:29:09,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.87108039855957
2023-01-07 08:29:09,327 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,327 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.07904483377933502
2023-01-07 08:29:09,327 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.985057830810547
2023-01-07 08:29:09,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,327 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 18.778724670410156
2023-01-07 08:29:09,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8942469358444214
2023-01-07 08:29:09,329 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,329 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 18.778724670410156
2023-01-07 08:29:09,329 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,329 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,329 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.3430616855621338
2023-01-07 08:29:09,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 74.3249282836914
2023-01-07 08:29:09,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.13949203491211
2023-01-07 08:29:09,331 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,331 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.3430616855621338
2023-01-07 08:29:09,331 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.22715377807617188
2023-01-07 08:29:09,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,332 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 74.3249282836914
2023-01-07 08:29:09,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.009140491485596
2023-01-07 08:29:09,333 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,333 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 74.3249282836914
2023-01-07 08:29:09,333 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:09,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -14.698129653930664
2023-01-07 08:29:09,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.038917541503906
2023-01-07 08:29:09,334 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,334 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -4.60463809967041
2023-01-07 08:29:09,334 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:09,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,335 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -14.698129653930664
2023-01-07 08:29:09,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3535895347595215
2023-01-07 08:29:09,336 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,336 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -14.698129653930664
2023-01-07 08:29:09,336 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:09,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.089360237121582
2023-01-07 08:29:09,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.083554267883301
2023-01-07 08:29:09,338 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,338 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 33.6617546081543
2023-01-07 08:29:09,338 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:09,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,338 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.089360237121582
2023-01-07 08:29:09,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.376894474029541
2023-01-07 08:29:09,339 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,339 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -2.089360237121582
2023-01-07 08:29:09,339 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:09,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,340 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -55.05862808227539
2023-01-07 08:29:09,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.725545883178711
2023-01-07 08:29:09,341 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,341 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -7.753679275512695
2023-01-07 08:29:09,341 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,341 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,341 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:09,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,342 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -55.05862808227539
2023-01-07 08:29:09,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.854279518127441
2023-01-07 08:29:09,343 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,343 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -55.05862808227539
2023-01-07 08:29:09,343 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,343 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:29:09,343 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.13649332523345947
2023-01-07 08:29:09,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,344 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -57.0361328125
2023-01-07 08:29:09,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.775545120239258
2023-01-07 08:29:09,345 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,345 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.13649332523345947
2023-01-07 08:29:09,345 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,345 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 17.364486694335938
2023-01-07 08:29:09,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -57.0361328125
2023-01-07 08:29:09,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6099199652671814
2023-01-07 08:29:09,346 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,346 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -57.0361328125
2023-01-07 08:29:09,347 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:09,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 107.60653686523438
2023-01-07 08:29:09,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7908772230148315
2023-01-07 08:29:09,348 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,348 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -3.6098856925964355
2023-01-07 08:29:09,348 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,348 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:09,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 107.60653686523438
2023-01-07 08:29:09,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.498249053955078
2023-01-07 08:29:09,350 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,350 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 107.60653686523438
2023-01-07 08:29:09,350 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:09,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 118.5108871459961
2023-01-07 08:29:09,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08299203217029572
2023-01-07 08:29:09,351 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,351 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 4.162666320800781
2023-01-07 08:29:09,351 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:09,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 118.5108871459961
2023-01-07 08:29:09,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09484168887138367
2023-01-07 08:29:09,353 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,353 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 118.5108871459961
2023-01-07 08:29:09,353 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:09,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -41.17881774902344
2023-01-07 08:29:09,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.403850555419922
2023-01-07 08:29:09,355 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,355 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -3.794924736022949
2023-01-07 08:29:09,355 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:09,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -41.17881774902344
2023-01-07 08:29:09,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0995688438415527
2023-01-07 08:29:09,356 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,357 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -41.17881774902344
2023-01-07 08:29:09,357 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:09,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 57.9107666015625
2023-01-07 08:29:09,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2996206283569336
2023-01-07 08:29:09,358 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,358 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -3.9044625759124756
2023-01-07 08:29:09,358 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:09,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,359 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 57.9107666015625
2023-01-07 08:29:09,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06264138221740723
2023-01-07 08:29:09,360 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,360 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 57.9107666015625
2023-01-07 08:29:09,360 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:09,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -674.3261108398438
2023-01-07 08:29:09,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3672990798950195
2023-01-07 08:29:09,361 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,361 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 29.16504669189453
2023-01-07 08:29:09,362 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:09,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -674.3261108398438
2023-01-07 08:29:09,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.545995831489563
2023-01-07 08:29:09,363 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,363 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -674.3261108398438
2023-01-07 08:29:09,363 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:09,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1378.9188232421875
2023-01-07 08:29:09,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.389622688293457
2023-01-07 08:29:09,365 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,365 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -4.056302547454834
2023-01-07 08:29:09,365 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:09,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1378.9188232421875
2023-01-07 08:29:09,366 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5208983421325684
2023-01-07 08:29:09,366 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,367 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -1378.9188232421875
2023-01-07 08:29:09,367 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:09,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1957.2694091796875
2023-01-07 08:29:09,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7087899446487427
2023-01-07 08:29:09,368 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -2.7730765342712402
2023-01-07 08:29:09,368 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:09,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1957.2694091796875
2023-01-07 08:29:09,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0137324333190918
2023-01-07 08:29:09,370 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,370 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1957.2694091796875
2023-01-07 08:29:09,370 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:09,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 132.3009490966797
2023-01-07 08:29:09,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4332826137542725
2023-01-07 08:29:09,372 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,372 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 33.4207763671875
2023-01-07 08:29:09,372 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:09,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 132.3009490966797
2023-01-07 08:29:09,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20854169130325317
2023-01-07 08:29:09,373 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,373 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 132.3009490966797
2023-01-07 08:29:09,374 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:09,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,374 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 119.67164611816406
2023-01-07 08:29:09,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45600175857543945
2023-01-07 08:29:09,375 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,375 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -2.49204158782959
2023-01-07 08:29:09,375 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:09,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 119.67164611816406
2023-01-07 08:29:09,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9777219891548157
2023-01-07 08:29:09,377 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,377 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 119.67164611816406
2023-01-07 08:29:09,377 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:09,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 20.177635192871094
2023-01-07 08:29:09,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6388908624649048
2023-01-07 08:29:09,378 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,378 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -2.0082366466522217
2023-01-07 08:29:09,379 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:09,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 20.177635192871094
2023-01-07 08:29:09,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.042970895767212
2023-01-07 08:29:09,380 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,380 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 20.177635192871094
2023-01-07 08:29:09,380 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:09,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.90947151184082
2023-01-07 08:29:09,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7966675758361816
2023-01-07 08:29:09,382 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,382 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.13421738147735596
2023-01-07 08:29:09,382 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:09,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.90947151184082
2023-01-07 08:29:09,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011716075241565704
2023-01-07 08:29:09,384 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,384 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -10.90947151184082
2023-01-07 08:29:09,384 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:09,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 19.67485809326172
2023-01-07 08:29:09,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18463869392871857
2023-01-07 08:29:09,385 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,385 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -2.650426149368286
2023-01-07 08:29:09,385 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:09,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 19.67485809326172
2023-01-07 08:29:09,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.613593101501465
2023-01-07 08:29:09,387 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,387 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 19.67485809326172
2023-01-07 08:29:09,387 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:09,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,388 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -26.537639617919922
2023-01-07 08:29:09,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03694155067205429
2023-01-07 08:29:09,389 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,389 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -2.6355483531951904
2023-01-07 08:29:09,389 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:09,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -26.537639617919922
2023-01-07 08:29:09,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4279063940048218
2023-01-07 08:29:09,390 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,390 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -26.537639617919922
2023-01-07 08:29:09,391 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:09,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,391 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 27.70879554748535
2023-01-07 08:29:09,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.720149040222168
2023-01-07 08:29:09,392 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,392 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -3.908474922180176
2023-01-07 08:29:09,392 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:09,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,393 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 27.70879554748535
2023-01-07 08:29:09,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38995546102523804
2023-01-07 08:29:09,394 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,394 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 27.70879554748535
2023-01-07 08:29:09,394 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:09,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 77.98258209228516
2023-01-07 08:29:09,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0358214378356934
2023-01-07 08:29:09,395 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,395 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5489407777786255
2023-01-07 08:29:09,396 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:09,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 77.98258209228516
2023-01-07 08:29:09,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5718363523483276
2023-01-07 08:29:09,397 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,397 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 77.98258209228516
2023-01-07 08:29:09,397 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:09,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,398 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 14.899505615234375
2023-01-07 08:29:09,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2312294244766235
2023-01-07 08:29:09,399 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,399 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.5026172995567322
2023-01-07 08:29:09,399 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:09,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,399 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 14.899505615234375
2023-01-07 08:29:09,400 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4667387008666992
2023-01-07 08:29:09,400 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,401 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 14.899505615234375
2023-01-07 08:29:09,401 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:09,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,401 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.404861450195312
2023-01-07 08:29:09,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2733851373195648
2023-01-07 08:29:09,402 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,402 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -11.044183731079102
2023-01-07 08:29:09,402 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:09,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,403 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.404861450195312
2023-01-07 08:29:09,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07128085941076279
2023-01-07 08:29:09,404 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,404 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 27.404861450195312
2023-01-07 08:29:09,404 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:09,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,404 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.3719282150268555
2023-01-07 08:29:09,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9505870938301086
2023-01-07 08:29:09,405 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,405 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -13.220587730407715
2023-01-07 08:29:09,405 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:09,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,406 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1.3719282150268555
2023-01-07 08:29:09,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9656798243522644
2023-01-07 08:29:09,407 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,407 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1.3719282150268555
2023-01-07 08:29:09,407 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:09,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,408 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -41.50065231323242
2023-01-07 08:29:09,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3690031170845032
2023-01-07 08:29:09,409 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,409 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.22232267260551453
2023-01-07 08:29:09,409 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,409 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:09,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,409 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -41.50065231323242
2023-01-07 08:29:09,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1573612242937088
2023-01-07 08:29:09,410 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,410 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -41.50065231323242
2023-01-07 08:29:09,410 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:09,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,411 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 19.89347267150879
2023-01-07 08:29:09,411 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5145754814147949
2023-01-07 08:29:09,412 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,412 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.19414716958999634
2023-01-07 08:29:09,412 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:09,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,413 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 19.89347267150879
2023-01-07 08:29:09,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5930265188217163
2023-01-07 08:29:09,414 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,414 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 19.89347267150879
2023-01-07 08:29:09,414 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,414 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:09,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,414 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -10.745357513427734
2023-01-07 08:29:09,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3333894610404968
2023-01-07 08:29:09,415 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,415 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -8.39050006866455
2023-01-07 08:29:09,415 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:09,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,416 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -10.745357513427734
2023-01-07 08:29:09,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10958161950111389
2023-01-07 08:29:09,417 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,417 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -10.745357513427734
2023-01-07 08:29:09,417 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:09,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,418 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.7579278945922852
2023-01-07 08:29:09,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25173550844192505
2023-01-07 08:29:09,418 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,419 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.01764240860939026
2023-01-07 08:29:09,419 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,419 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,419 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:09,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,419 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.7579278945922852
2023-01-07 08:29:09,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3098773956298828
2023-01-07 08:29:09,420 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,420 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 1.7579278945922852
2023-01-07 08:29:09,420 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,421 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:09,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,421 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -57.727027893066406
2023-01-07 08:29:09,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08004500716924667
2023-01-07 08:29:09,422 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,422 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -83.07882690429688
2023-01-07 08:29:09,422 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:09,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,422 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -57.727027893066406
2023-01-07 08:29:09,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.337604820728302
2023-01-07 08:29:09,424 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,424 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -57.727027893066406
2023-01-07 08:29:09,424 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:09,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,424 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -198.70721435546875
2023-01-07 08:29:09,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2530369758605957
2023-01-07 08:29:09,425 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,425 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -315.84033203125
2023-01-07 08:29:09,426 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,426 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:09,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,426 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -198.70721435546875
2023-01-07 08:29:09,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5501903891563416
2023-01-07 08:29:09,427 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,427 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -198.70721435546875
2023-01-07 08:29:09,428 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,429 > [DEBUG] 0 :: 7.470500946044922
2023-01-07 08:29:09,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,432 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.001434326171875
2023-01-07 08:29:09,432 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -261.4449462890625
2023-01-07 08:29:09,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3428901433944702
2023-01-07 08:29:09,435 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,436 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.001434326171875
2023-01-07 08:29:09,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -314.7125244140625
2023-01-07 08:29:09,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.462858200073242
2023-01-07 08:29:09,440 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25467994809150696
2023-01-07 08:29:09,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.004005499184131622
2023-01-07 08:29:09,443 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,444 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.462858200073242
2023-01-07 08:29:09,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21342256665229797
2023-01-07 08:29:09,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -6.095987796783447
2023-01-07 08:29:09,445 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0478394441306591
2023-01-07 08:29:09,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,446 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.001675209030508995
2023-01-07 08:29:09,446 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,447 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -6.095987796783447
2023-01-07 08:29:09,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07255750894546509
2023-01-07 08:29:09,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,448 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.1276134252548218
2023-01-07 08:29:09,448 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.003498472273349762
2023-01-07 08:29:09,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2919134497642517
2023-01-07 08:29:09,450 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,450 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.1276134252548218
2023-01-07 08:29:09,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5058521628379822
2023-01-07 08:29:09,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.355127215385437
2023-01-07 08:29:09,451 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,452 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07935945689678192
2023-01-07 08:29:09,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,453 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.06819730997085571
2023-01-07 08:29:09,453 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,453 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.355127215385437
2023-01-07 08:29:09,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8852975368499756
2023-01-07 08:29:09,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,454 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -6.477919101715088
2023-01-07 08:29:09,455 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5666127800941467
2023-01-07 08:29:09,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.09435009211301804
2023-01-07 08:29:09,456 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -6.477919101715088
2023-01-07 08:29:09,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08790557831525803
2023-01-07 08:29:09,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8667969703674316
2023-01-07 08:29:09,458 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5534141659736633
2023-01-07 08:29:09,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.2800387740135193
2023-01-07 08:29:09,459 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8667969703674316
2023-01-07 08:29:09,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18086673319339752
2023-01-07 08:29:09,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.7900023460388184
2023-01-07 08:29:09,461 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6124066114425659
2023-01-07 08:29:09,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,462 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.07231985032558441
2023-01-07 08:29:09,462 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,462 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.7900023460388184
2023-01-07 08:29:09,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1451268345117569
2023-01-07 08:29:09,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,464 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.9583158493042
2023-01-07 08:29:09,464 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4105467200279236
2023-01-07 08:29:09,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,465 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.030115731060504913
2023-01-07 08:29:09,465 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,465 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.9583158493042
2023-01-07 08:29:09,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5125031471252441
2023-01-07 08:29:09,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,467 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -106.07177734375
2023-01-07 08:29:09,467 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2998731136322021
2023-01-07 08:29:09,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,468 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0058602988719940186
2023-01-07 08:29:09,468 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,469 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -106.07177734375
2023-01-07 08:29:09,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2632017135620117
2023-01-07 08:29:09,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,470 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.09471607208251953
2023-01-07 08:29:09,470 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4159367084503174
2023-01-07 08:29:09,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.045672185719013214
2023-01-07 08:29:09,471 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,472 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.09471607208251953
2023-01-07 08:29:09,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0187275409698486
2023-01-07 08:29:09,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,473 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.0989222526550293
2023-01-07 08:29:09,473 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5073114037513733
2023-01-07 08:29:09,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.014616772532463074
2023-01-07 08:29:09,475 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.0989222526550293
2023-01-07 08:29:09,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6200750470161438
2023-01-07 08:29:09,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -11.448150634765625
2023-01-07 08:29:09,477 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44276174902915955
2023-01-07 08:29:09,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.1330389678478241
2023-01-07 08:29:09,478 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -11.448150634765625
2023-01-07 08:29:09,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6749572157859802
2023-01-07 08:29:09,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -4.43686056137085
2023-01-07 08:29:09,480 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6428699493408203
2023-01-07 08:29:09,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.06293344497680664
2023-01-07 08:29:09,481 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -4.43686056137085
2023-01-07 08:29:09,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6035858392715454
2023-01-07 08:29:09,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,483 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.942406177520752
2023-01-07 08:29:09,483 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6705958843231201
2023-01-07 08:29:09,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.011017918586730957
2023-01-07 08:29:09,484 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.942406177520752
2023-01-07 08:29:09,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21655461192131042
2023-01-07 08:29:09,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 12.913528442382812
2023-01-07 08:29:09,486 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.47594165802001953
2023-01-07 08:29:09,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,487 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.0574512779712677
2023-01-07 08:29:09,487 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 12.913528442382812
2023-01-07 08:29:09,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1413743495941162
2023-01-07 08:29:09,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -6.191042900085449
2023-01-07 08:29:09,489 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25003865361213684
2023-01-07 08:29:09,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.09692172706127167
2023-01-07 08:29:09,490 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -6.191042900085449
2023-01-07 08:29:09,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6268211007118225
2023-01-07 08:29:09,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,492 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -4.081676483154297
2023-01-07 08:29:09,492 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07616199553012848
2023-01-07 08:29:09,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.05978219211101532
2023-01-07 08:29:09,493 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -4.081676483154297
2023-01-07 08:29:09,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1753339767456055
2023-01-07 08:29:09,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -22.078643798828125
2023-01-07 08:29:09,495 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2502797544002533
2023-01-07 08:29:09,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.12747332453727722
2023-01-07 08:29:09,497 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -22.078643798828125
2023-01-07 08:29:09,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.931567370891571
2023-01-07 08:29:09,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.724338531494141
2023-01-07 08:29:09,498 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21143518388271332
2023-01-07 08:29:09,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1709991842508316
2023-01-07 08:29:09,500 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.724338531494141
2023-01-07 08:29:09,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.5167107582092285
2023-01-07 08:29:09,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.990725994110107
2023-01-07 08:29:09,502 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,502 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3238279819488525
2023-01-07 08:29:09,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.08645472675561905
2023-01-07 08:29:09,503 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.990725994110107
2023-01-07 08:29:09,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.70465350151062
2023-01-07 08:29:09,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.111749649047852
2023-01-07 08:29:09,505 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9668121337890625
2023-01-07 08:29:09,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,506 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.18171854317188263
2023-01-07 08:29:09,506 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,506 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.111749649047852
2023-01-07 08:29:09,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7014123201370239
2023-01-07 08:29:09,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -13.012043952941895
2023-01-07 08:29:09,508 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29791638255119324
2023-01-07 08:29:09,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,509 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.15615709125995636
2023-01-07 08:29:09,509 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,509 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -13.012043952941895
2023-01-07 08:29:09,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1023550033569336
2023-01-07 08:29:09,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 75.56890869140625
2023-01-07 08:29:09,511 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,511 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.791316270828247
2023-01-07 08:29:09,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,512 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.3156103491783142
2023-01-07 08:29:09,512 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 75.56890869140625
2023-01-07 08:29:09,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6631383895874023
2023-01-07 08:29:09,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 332.287109375
2023-01-07 08:29:09,514 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7685080766677856
2023-01-07 08:29:09,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,516 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 332.287109375
2023-01-07 08:29:09,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0132558345794678
2023-01-07 08:29:09,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 90.73990631103516
2023-01-07 08:29:09,518 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.332573890686035
2023-01-07 08:29:09,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,519 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.4602067172527313
2023-01-07 08:29:09,519 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,519 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 90.73990631103516
2023-01-07 08:29:09,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3474997282028198
2023-01-07 08:29:09,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 108.89796447753906
2023-01-07 08:29:09,521 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9155861139297485
2023-01-07 08:29:09,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,522 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.27234432101249695
2023-01-07 08:29:09,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 108.89796447753906
2023-01-07 08:29:09,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.020137786865234
2023-01-07 08:29:09,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.62014102935791
2023-01-07 08:29:09,524 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.622299909591675
2023-01-07 08:29:09,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,525 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.33360058069229126
2023-01-07 08:29:09,525 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,526 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.62014102935791
2023-01-07 08:29:09,526 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.279082298278809
2023-01-07 08:29:09,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.9393310546875
2023-01-07 08:29:09,527 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.30823802947998
2023-01-07 08:29:09,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,528 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.9393310546875
2023-01-07 08:29:09,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.578601837158203
2023-01-07 08:29:09,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.843318939208984
2023-01-07 08:29:09,530 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.6351189613342285
2023-01-07 08:29:09,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,531 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.843318939208984
2023-01-07 08:29:09,531 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.383217811584473
2023-01-07 08:29:09,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,533 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.049781799316406
2023-01-07 08:29:09,533 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19839847087860107
2023-01-07 08:29:09,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.049781799316406
2023-01-07 08:29:09,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9434951543807983
2023-01-07 08:29:09,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.51539611816406
2023-01-07 08:29:09,535 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,536 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8069673776626587
2023-01-07 08:29:09,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,537 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.51539611816406
2023-01-07 08:29:09,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.804550170898438
2023-01-07 08:29:09,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,538 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.787486553192139
2023-01-07 08:29:09,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2421650886535645
2023-01-07 08:29:09,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.787486553192139
2023-01-07 08:29:09,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.742181777954102
2023-01-07 08:29:09,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.204429626464844
2023-01-07 08:29:09,541 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.6424174308776855
2023-01-07 08:29:09,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,542 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.204429626464844
2023-01-07 08:29:09,542 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37567567825317383
2023-01-07 08:29:09,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,543 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -181.8233184814453
2023-01-07 08:29:09,543 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.555708169937134
2023-01-07 08:29:09,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,545 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -181.8233184814453
2023-01-07 08:29:09,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.747262954711914
2023-01-07 08:29:09,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -18.393436431884766
2023-01-07 08:29:09,546 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.404470443725586
2023-01-07 08:29:09,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,547 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.8061530590057373
2023-01-07 08:29:09,547 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -18.393436431884766
2023-01-07 08:29:09,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.141684055328369
2023-01-07 08:29:09,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,549 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.2140464782714844
2023-01-07 08:29:09,549 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.033324718475342
2023-01-07 08:29:09,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.2140464782714844
2023-01-07 08:29:09,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.612290382385254
2023-01-07 08:29:09,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -69.08020782470703
2023-01-07 08:29:09,552 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.956104278564453
2023-01-07 08:29:09,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -69.08020782470703
2023-01-07 08:29:09,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.796707153320312
2023-01-07 08:29:09,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 64.85901641845703
2023-01-07 08:29:09,555 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9592242240905762
2023-01-07 08:29:09,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.0265637636184692
2023-01-07 08:29:09,556 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 64.85901641845703
2023-01-07 08:29:09,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0775089263916016
2023-01-07 08:29:09,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.957321166992188
2023-01-07 08:29:09,558 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8528367280960083
2023-01-07 08:29:09,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,559 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.957321166992188
2023-01-07 08:29:09,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.491898536682129
2023-01-07 08:29:09,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.506126403808594
2023-01-07 08:29:09,561 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.11899995803833
2023-01-07 08:29:09,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.506126403808594
2023-01-07 08:29:09,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.49074649810791
2023-01-07 08:29:09,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,563 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.606327056884766
2023-01-07 08:29:09,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.3677947521209717
2023-01-07 08:29:09,565 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.498563289642334
2023-01-07 08:29:09,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.27967071533203
2023-01-07 08:29:09,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,568 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.37249696254730225
2023-01-07 08:29:09,568 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,568 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -8.654638290405273
2023-01-07 08:29:09,568 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,568 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.003211975097656
2023-01-07 08:29:09,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45246386528015137
2023-01-07 08:29:09,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,571 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -8.654638290405273
2023-01-07 08:29:09,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.508323669433594
2023-01-07 08:29:09,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,573 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,573 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.782344818115234
2023-01-07 08:29:09,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,574 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.16047823429107666
2023-01-07 08:29:09,574 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,574 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.949044227600098
2023-01-07 08:29:09,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,576 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.353530883789062
2023-01-07 08:29:09,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,577 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 3.144944667816162
2023-01-07 08:29:09,577 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,577 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -122.53646087646484
2023-01-07 08:29:09,577 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.70421600341797
2023-01-07 08:29:09,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,579 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.4986572265625
2023-01-07 08:29:09,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,580 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -91.94000244140625
2023-01-07 08:29:09,580 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,581 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.26815414428711
2023-01-07 08:29:09,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,582 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.721267700195312
2023-01-07 08:29:09,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,583 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,584 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.95247459411621
2023-01-07 08:29:09,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,585 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.425615310668945
2023-01-07 08:29:09,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,586 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.478594779968262
2023-01-07 08:29:09,587 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,587 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -40.714210510253906
2023-01-07 08:29:09,587 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,587 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,587 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.30059051513672
2023-01-07 08:29:09,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.970691680908203
2023-01-07 08:29:09,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,590 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -138.07154846191406
2023-01-07 08:29:09,590 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.88710021972656
2023-01-07 08:29:09,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,592 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.428958892822266
2023-01-07 08:29:09,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,593 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -138.07154846191406
2023-01-07 08:29:09,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,593 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -79.29437255859375
2023-01-07 08:29:09,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,595 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,595 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.69425582885742
2023-01-07 08:29:09,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,596 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.1787831783294678
2023-01-07 08:29:09,596 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,596 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 221.98797607421875
2023-01-07 08:29:09,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,598 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -130.1142120361328
2023-01-07 08:29:09,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,599 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.57403564453125
2023-01-07 08:29:09,604 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:29:09,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,604 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -2206.63427734375
2023-01-07 08:29:09,605 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,605 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,606 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.506126403808594
2023-01-07 08:29:09,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.957321166992188
2023-01-07 08:29:09,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 64.85901641845703
2023-01-07 08:29:09,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,609 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -69.08020782470703
2023-01-07 08:29:09,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.2140464782714844
2023-01-07 08:29:09,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -18.393436431884766
2023-01-07 08:29:09,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -181.8233184814453
2023-01-07 08:29:09,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.204429626464844
2023-01-07 08:29:09,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.787486553192139
2023-01-07 08:29:09,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.51539611816406
2023-01-07 08:29:09,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.049781799316406
2023-01-07 08:29:09,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.843318939208984
2023-01-07 08:29:09,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.9393310546875
2023-01-07 08:29:09,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.62014102935791
2023-01-07 08:29:09,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 108.89796447753906
2023-01-07 08:29:09,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 90.73990631103516
2023-01-07 08:29:09,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 332.287109375
2023-01-07 08:29:09,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 75.56890869140625
2023-01-07 08:29:09,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -13.012043952941895
2023-01-07 08:29:09,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.111749649047852
2023-01-07 08:29:09,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.990725994110107
2023-01-07 08:29:09,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.724338531494141
2023-01-07 08:29:09,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2637.781005859375
2023-01-07 08:29:09,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -2381.851318359375
2023-01-07 08:29:09,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 169.57972717285156
2023-01-07 08:29:09,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -147.66876220703125
2023-01-07 08:29:09,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 222.70474243164062
2023-01-07 08:29:09,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1406.2171630859375
2023-01-07 08:29:09,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 499.2874755859375
2023-01-07 08:29:09,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 622.1962890625
2023-01-07 08:29:09,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 460.6595458984375
2023-01-07 08:29:09,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1007.6223754882812
2023-01-07 08:29:09,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 843.45947265625
2023-01-07 08:29:09,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -338.770263671875
2023-01-07 08:29:09,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.9583158493042
2023-01-07 08:29:09,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.7900023460388184
2023-01-07 08:29:09,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8667969703674316
2023-01-07 08:29:09,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -6.477919101715088
2023-01-07 08:29:09,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.76077651977539
2023-01-07 08:29:09,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.355127215385437
2023-01-07 08:29:09,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.1276134252548218
2023-01-07 08:29:09,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -6.095987796783447
2023-01-07 08:29:09,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.04605484008789
2023-01-07 08:29:09,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,625 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.462858200073242
2023-01-07 08:29:09,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,625 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.001434326171875
2023-01-07 08:29:09,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 571.8961181640625
2023-01-07 08:29:09,626 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,626 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -2206.63427734375
2023-01-07 08:29:09,626 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,627 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 2.513423204421997
2023-01-07 08:29:09,627 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,627 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 125.50627136230469
2023-01-07 08:29:09,628 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,628 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 2.513423204421997
2023-01-07 08:29:09,628 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,629 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.171111106872559
2023-01-07 08:29:09,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 147.0810546875
2023-01-07 08:29:09,629 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 157.23727416992188
2023-01-07 08:29:09,631 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,631 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 147.0810546875
2023-01-07 08:29:09,631 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:09,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,631 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -121.32404327392578
2023-01-07 08:29:09,632 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,632 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 2.1787831783294678
2023-01-07 08:29:09,632 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:09,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,633 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -26.418258666992188
2023-01-07 08:29:09,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.83628463745117
2023-01-07 08:29:09,634 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,634 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -26.418258666992188
2023-01-07 08:29:09,634 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,634 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -5.06932258605957
2023-01-07 08:29:09,635 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,635 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -39.85387420654297
2023-01-07 08:29:09,635 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,635 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -112.26599884033203
2023-01-07 08:29:09,636 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,636 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -5.06932258605957
2023-01-07 08:29:09,636 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,637 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9725303649902344
2023-01-07 08:29:09,638 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,638 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -39.85387420654297
2023-01-07 08:29:09,638 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,639 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -2.978118896484375
2023-01-07 08:29:09,639 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,639 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -40.714210510253906
2023-01-07 08:29:09,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.58136749267578
2023-01-07 08:29:09,640 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,640 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -2.978118896484375
2023-01-07 08:29:09,640 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,641 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.97193908691406
2023-01-07 08:29:09,641 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,642 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -138.07154846191406
2023-01-07 08:29:09,642 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,642 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.4111557006836
2023-01-07 08:29:09,643 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,643 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.478594779968262
2023-01-07 08:29:09,643 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 103.29238891601562
2023-01-07 08:29:09,645 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,645 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -40.714210510253906
2023-01-07 08:29:09,645 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,645 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.9314799308776855
2023-01-07 08:29:09,646 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,646 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.211761474609375
2023-01-07 08:29:09,647 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,647 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.9314799308776855
2023-01-07 08:29:09,647 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:09,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.58053588867188
2023-01-07 08:29:09,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,648 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.543701171875
2023-01-07 08:29:09,649 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,649 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 70.58053588867188
2023-01-07 08:29:09,649 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.9520061016082764
2023-01-07 08:29:09,650 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,650 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -91.94000244140625
2023-01-07 08:29:09,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.91897964477539
2023-01-07 08:29:09,651 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,651 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.9520061016082764
2023-01-07 08:29:09,651 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,651 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,652 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.176792621612549
2023-01-07 08:29:09,652 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,653 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -91.94000244140625
2023-01-07 08:29:09,653 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:09,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,653 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -122.53646087646484
2023-01-07 08:29:09,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.07244873046875
2023-01-07 08:29:09,654 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,654 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 3.144944667816162
2023-01-07 08:29:09,654 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:09,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,655 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -122.53646087646484
2023-01-07 08:29:09,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,655 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3889579772949219
2023-01-07 08:29:09,656 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,656 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -122.53646087646484
2023-01-07 08:29:09,656 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,657 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,657 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,657 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.05693817138672
2023-01-07 08:29:09,658 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,658 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.16047823429107666
2023-01-07 08:29:09,658 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:09,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.5576629638672
2023-01-07 08:29:09,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.41423797607422
2023-01-07 08:29:09,659 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,660 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -130.5576629638672
2023-01-07 08:29:09,660 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.6646865606307983
2023-01-07 08:29:09,660 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 4.984853744506836
2023-01-07 08:29:09,661 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,661 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -8.654638290405273
2023-01-07 08:29:09,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.896864891052246
2023-01-07 08:29:09,662 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,662 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.6646865606307983
2023-01-07 08:29:09,662 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,663 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.068439483642578
2023-01-07 08:29:09,664 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,664 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 4.984853744506836
2023-01-07 08:29:09,664 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,664 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,664 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.23777961730957
2023-01-07 08:29:09,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,664 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -8.654638290405273
2023-01-07 08:29:09,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7385828495025635
2023-01-07 08:29:09,665 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,665 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -0.37249696254730225
2023-01-07 08:29:09,665 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,666 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.857914924621582
2023-01-07 08:29:09,667 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,667 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -8.654638290405273
2023-01-07 08:29:09,667 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,667 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,668 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.680706024169922
2023-01-07 08:29:09,669 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,669 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 1.3677947521209717
2023-01-07 08:29:09,669 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:09,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,669 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -29.58863067626953
2023-01-07 08:29:09,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.267325401306152
2023-01-07 08:29:09,670 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,670 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -29.58863067626953
2023-01-07 08:29:09,671 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,671 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 1.2599421739578247
2023-01-07 08:29:09,671 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,671 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.506126403808594
2023-01-07 08:29:09,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.933467864990234
2023-01-07 08:29:09,672 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,672 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 1.2599421739578247
2023-01-07 08:29:09,673 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.787897109985352
2023-01-07 08:29:09,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.506126403808594
2023-01-07 08:29:09,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.128300428390503
2023-01-07 08:29:09,674 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,674 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 46.506126403808594
2023-01-07 08:29:09,674 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.762881875038147
2023-01-07 08:29:09,675 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.957321166992188
2023-01-07 08:29:09,675 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.22150421142578
2023-01-07 08:29:09,676 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,676 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.762881875038147
2023-01-07 08:29:09,676 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -20.516536712646484
2023-01-07 08:29:09,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.957321166992188
2023-01-07 08:29:09,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4520710706710815
2023-01-07 08:29:09,677 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,678 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 26.957321166992188
2023-01-07 08:29:09,678 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:09,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,678 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 64.85901641845703
2023-01-07 08:29:09,678 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.147222995758057
2023-01-07 08:29:09,679 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,679 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -1.0265637636184692
2023-01-07 08:29:09,679 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,679 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:09,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 64.85901641845703
2023-01-07 08:29:09,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.257957696914673
2023-01-07 08:29:09,681 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,681 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 64.85901641845703
2023-01-07 08:29:09,681 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,681 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.45273345708847046
2023-01-07 08:29:09,682 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -69.08020782470703
2023-01-07 08:29:09,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.66546630859375
2023-01-07 08:29:09,683 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,683 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.45273345708847046
2023-01-07 08:29:09,683 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,683 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.0566999912261963
2023-01-07 08:29:09,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,684 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -69.08020782470703
2023-01-07 08:29:09,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.600822448730469
2023-01-07 08:29:09,685 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,685 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -69.08020782470703
2023-01-07 08:29:09,685 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.3779486417770386
2023-01-07 08:29:09,685 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.2140464782714844
2023-01-07 08:29:09,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.63012409210205
2023-01-07 08:29:09,687 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,687 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.3779486417770386
2023-01-07 08:29:09,687 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 15.858409881591797
2023-01-07 08:29:09,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 2.2140464782714844
2023-01-07 08:29:09,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.8784589767456055
2023-01-07 08:29:09,688 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,688 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 2.2140464782714844
2023-01-07 08:29:09,689 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:09,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -18.393436431884766
2023-01-07 08:29:09,689 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.197994232177734
2023-01-07 08:29:09,690 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,690 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.8061530590057373
2023-01-07 08:29:09,690 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,690 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,690 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:09,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -18.393436431884766
2023-01-07 08:29:09,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2644549608230591
2023-01-07 08:29:09,692 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,692 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -18.393436431884766
2023-01-07 08:29:09,692 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,692 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.5291532874107361
2023-01-07 08:29:09,692 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -181.8233184814453
2023-01-07 08:29:09,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.810340881347656
2023-01-07 08:29:09,694 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,694 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.5291532874107361
2023-01-07 08:29:09,694 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,694 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 12.577384948730469
2023-01-07 08:29:09,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -181.8233184814453
2023-01-07 08:29:09,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.596370697021484
2023-01-07 08:29:09,695 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,695 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -181.8233184814453
2023-01-07 08:29:09,696 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,696 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.7637271285057068
2023-01-07 08:29:09,696 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.204429626464844
2023-01-07 08:29:09,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.254361152648926
2023-01-07 08:29:09,697 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,697 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.7637271285057068
2023-01-07 08:29:09,698 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,698 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.623693943023682
2023-01-07 08:29:09,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.204429626464844
2023-01-07 08:29:09,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.334787368774414
2023-01-07 08:29:09,699 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,699 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -58.204429626464844
2023-01-07 08:29:09,699 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.11792979389429092
2023-01-07 08:29:09,700 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.787486553192139
2023-01-07 08:29:09,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.434440612792969
2023-01-07 08:29:09,701 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,701 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.11792979389429092
2023-01-07 08:29:09,701 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -25.3018798828125
2023-01-07 08:29:09,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,702 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.787486553192139
2023-01-07 08:29:09,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.513802528381348
2023-01-07 08:29:09,703 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,703 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 5.787486553192139
2023-01-07 08:29:09,703 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,703 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.08771520853042603
2023-01-07 08:29:09,703 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,704 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.51539611816406
2023-01-07 08:29:09,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0309853553771973
2023-01-07 08:29:09,705 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,705 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.08771520853042603
2023-01-07 08:29:09,705 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.52092742919922
2023-01-07 08:29:09,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 68.51539611816406
2023-01-07 08:29:09,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4704737663269043
2023-01-07 08:29:09,707 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,707 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 68.51539611816406
2023-01-07 08:29:09,707 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:09,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.17199957370758057
2023-01-07 08:29:09,707 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.049781799316406
2023-01-07 08:29:09,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3343727588653564
2023-01-07 08:29:09,709 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,709 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.17199957370758057
2023-01-07 08:29:09,709 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.565258026123047
2023-01-07 08:29:09,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.049781799316406
2023-01-07 08:29:09,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.306554794311523
2023-01-07 08:29:09,710 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,711 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -20.049781799316406
2023-01-07 08:29:09,711 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:09,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,711 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.06865371763706207
2023-01-07 08:29:09,711 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,711 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.843318939208984
2023-01-07 08:29:09,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6791723966598511
2023-01-07 08:29:09,712 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,712 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.06865371763706207
2023-01-07 08:29:09,713 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.985057830810547
2023-01-07 08:29:09,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,713 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.843318939208984
2023-01-07 08:29:09,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8556511402130127
2023-01-07 08:29:09,714 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,714 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -9.843318939208984
2023-01-07 08:29:09,714 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,714 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.17547905445098877
2023-01-07 08:29:09,715 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.9393310546875
2023-01-07 08:29:09,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.683229446411133
2023-01-07 08:29:09,716 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,716 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.17547905445098877
2023-01-07 08:29:09,716 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.22715377807617188
2023-01-07 08:29:09,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.9393310546875
2023-01-07 08:29:09,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.932666778564453
2023-01-07 08:29:09,718 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,718 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -282.9393310546875
2023-01-07 08:29:09,718 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:09,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.62014102935791
2023-01-07 08:29:09,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1885986328125
2023-01-07 08:29:09,720 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,720 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.33360058069229126
2023-01-07 08:29:09,720 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:09,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.62014102935791
2023-01-07 08:29:09,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.316098690032959
2023-01-07 08:29:09,721 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,721 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 11.62014102935791
2023-01-07 08:29:09,722 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:09,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 108.89796447753906
2023-01-07 08:29:09,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7057852745056152
2023-01-07 08:29:09,723 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,723 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.27234432101249695
2023-01-07 08:29:09,723 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,723 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:09,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 108.89796447753906
2023-01-07 08:29:09,724 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.698979377746582
2023-01-07 08:29:09,724 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,725 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 108.89796447753906
2023-01-07 08:29:09,725 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,725 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,725 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:09,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 90.73990631103516
2023-01-07 08:29:09,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2919018268585205
2023-01-07 08:29:09,726 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,726 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.4602067172527313
2023-01-07 08:29:09,726 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:09,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 90.73990631103516
2023-01-07 08:29:09,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8068769574165344
2023-01-07 08:29:09,728 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,728 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 90.73990631103516
2023-01-07 08:29:09,728 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,728 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:09,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,728 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.22671344876289368
2023-01-07 08:29:09,729 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:29:09,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,729 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 332.287109375
2023-01-07 08:29:09,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.800052165985107
2023-01-07 08:29:09,730 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,730 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.22671344876289368
2023-01-07 08:29:09,730 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 17.364486694335938
2023-01-07 08:29:09,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 332.287109375
2023-01-07 08:29:09,731 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13108976185321808
2023-01-07 08:29:09,731 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,732 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 332.287109375
2023-01-07 08:29:09,732 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:09,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 75.56890869140625
2023-01-07 08:29:09,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4480929374694824
2023-01-07 08:29:09,733 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,733 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.3156103491783142
2023-01-07 08:29:09,733 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:09,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,734 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 75.56890869140625
2023-01-07 08:29:09,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6768298149108887
2023-01-07 08:29:09,736 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,736 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 75.56890869140625
2023-01-07 08:29:09,736 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:09,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -13.012043952941895
2023-01-07 08:29:09,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41025403141975403
2023-01-07 08:29:09,737 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,737 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.15615709125995636
2023-01-07 08:29:09,737 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:09,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -13.012043952941895
2023-01-07 08:29:09,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3920841217041016
2023-01-07 08:29:09,739 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,739 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -13.012043952941895
2023-01-07 08:29:09,739 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:09,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.111749649047852
2023-01-07 08:29:09,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.94711971282959
2023-01-07 08:29:09,741 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,741 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.18171854317188263
2023-01-07 08:29:09,741 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:09,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,741 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -13.111749649047852
2023-01-07 08:29:09,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0641348361968994
2023-01-07 08:29:09,742 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,742 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -13.111749649047852
2023-01-07 08:29:09,743 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,743 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:09,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.990725994110107
2023-01-07 08:29:09,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3681066036224365
2023-01-07 08:29:09,744 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,744 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.08645472675561905
2023-01-07 08:29:09,744 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:09,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.990725994110107
2023-01-07 08:29:09,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7849897146224976
2023-01-07 08:29:09,746 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,746 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -6.990725994110107
2023-01-07 08:29:09,746 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,746 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,746 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:09,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.724338531494141
2023-01-07 08:29:09,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2210773229599
2023-01-07 08:29:09,747 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,747 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.1709991842508316
2023-01-07 08:29:09,747 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,748 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:09,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.724338531494141
2023-01-07 08:29:09,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2436034232378006
2023-01-07 08:29:09,749 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,749 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -5.724338531494141
2023-01-07 08:29:09,749 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:09,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2637.781005859375
2023-01-07 08:29:09,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21297374367713928
2023-01-07 08:29:09,751 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,751 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.12747332453727722
2023-01-07 08:29:09,751 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:09,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,751 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2637.781005859375
2023-01-07 08:29:09,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6161898374557495
2023-01-07 08:29:09,752 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,752 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -2637.781005859375
2023-01-07 08:29:09,753 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:09,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 169.57972717285156
2023-01-07 08:29:09,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7527953386306763
2023-01-07 08:29:09,754 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,754 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.05978219211101532
2023-01-07 08:29:09,754 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:09,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 169.57972717285156
2023-01-07 08:29:09,755 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21875515580177307
2023-01-07 08:29:09,756 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,756 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 169.57972717285156
2023-01-07 08:29:09,756 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:09,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,756 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -147.66876220703125
2023-01-07 08:29:09,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.975035548210144
2023-01-07 08:29:09,757 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,757 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.09692172706127167
2023-01-07 08:29:09,757 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,757 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:09,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -147.66876220703125
2023-01-07 08:29:09,758 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.647420883178711
2023-01-07 08:29:09,759 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,759 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -147.66876220703125
2023-01-07 08:29:09,759 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,759 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,759 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:09,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 222.70474243164062
2023-01-07 08:29:09,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4673612117767334
2023-01-07 08:29:09,761 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,761 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.0574512779712677
2023-01-07 08:29:09,761 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,761 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:09,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 222.70474243164062
2023-01-07 08:29:09,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8066728115081787
2023-01-07 08:29:09,762 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,762 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 222.70474243164062
2023-01-07 08:29:09,763 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,763 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,763 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:09,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,763 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1406.2171630859375
2023-01-07 08:29:09,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7465760111808777
2023-01-07 08:29:09,764 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,764 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.011017918586730957
2023-01-07 08:29:09,764 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:09,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,765 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1406.2171630859375
2023-01-07 08:29:09,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09336278587579727
2023-01-07 08:29:09,766 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,766 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1406.2171630859375
2023-01-07 08:29:09,766 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:09,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 499.2874755859375
2023-01-07 08:29:09,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.226741313934326
2023-01-07 08:29:09,767 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,767 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.06293344497680664
2023-01-07 08:29:09,767 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,767 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:09,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,768 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 499.2874755859375
2023-01-07 08:29:09,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01645631343126297
2023-01-07 08:29:09,769 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,769 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 499.2874755859375
2023-01-07 08:29:09,769 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,769 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,769 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:09,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 622.1962890625
2023-01-07 08:29:09,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22081269323825836
2023-01-07 08:29:09,770 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,771 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.1330389678478241
2023-01-07 08:29:09,771 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:09,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,771 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 622.1962890625
2023-01-07 08:29:09,771 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46293124556541443
2023-01-07 08:29:09,772 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,772 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 622.1962890625
2023-01-07 08:29:09,772 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:09,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 460.6595458984375
2023-01-07 08:29:09,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8405064940452576
2023-01-07 08:29:09,774 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,774 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.014616772532463074
2023-01-07 08:29:09,774 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,774 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:09,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,774 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 460.6595458984375
2023-01-07 08:29:09,775 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6399582028388977
2023-01-07 08:29:09,776 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,776 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 460.6595458984375
2023-01-07 08:29:09,776 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:09,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,776 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1007.6223754882812
2023-01-07 08:29:09,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2453535795211792
2023-01-07 08:29:09,777 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,777 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.045672185719013214
2023-01-07 08:29:09,777 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:09,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,778 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1007.6223754882812
2023-01-07 08:29:09,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43964874744415283
2023-01-07 08:29:09,779 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,779 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1007.6223754882812
2023-01-07 08:29:09,779 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:09,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,780 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 843.45947265625
2023-01-07 08:29:09,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1813282072544098
2023-01-07 08:29:09,780 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,781 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0058602988719940186
2023-01-07 08:29:09,781 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,781 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,781 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:09,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,781 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 843.45947265625
2023-01-07 08:29:09,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4671192169189453
2023-01-07 08:29:09,782 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,782 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 843.45947265625
2023-01-07 08:29:09,782 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:09,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,783 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.9583158493042
2023-01-07 08:29:09,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7479208707809448
2023-01-07 08:29:09,784 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,784 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.030115731060504913
2023-01-07 08:29:09,784 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,784 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:09,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,785 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.9583158493042
2023-01-07 08:29:09,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3847494125366211
2023-01-07 08:29:09,786 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,786 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 12.9583158493042
2023-01-07 08:29:09,786 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,786 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,786 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:09,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,786 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.7900023460388184
2023-01-07 08:29:09,787 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1240870952606201
2023-01-07 08:29:09,787 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,787 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.07231985032558441
2023-01-07 08:29:09,787 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,788 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,788 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:09,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,788 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.7900023460388184
2023-01-07 08:29:09,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5426589250564575
2023-01-07 08:29:09,789 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,789 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 3.7900023460388184
2023-01-07 08:29:09,789 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:09,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,789 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8667969703674316
2023-01-07 08:29:09,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.056498147547245026
2023-01-07 08:29:09,790 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,790 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.2800387740135193
2023-01-07 08:29:09,791 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:09,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,791 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8667969703674316
2023-01-07 08:29:09,791 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17263904213905334
2023-01-07 08:29:09,792 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,792 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -1.8667969703674316
2023-01-07 08:29:09,792 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,793 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:09,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,793 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -6.477919101715088
2023-01-07 08:29:09,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21800626814365387
2023-01-07 08:29:09,794 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,794 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.09435009211301804
2023-01-07 08:29:09,794 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:09,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,794 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2.8747634887695312
2023-01-07 08:29:09,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1691724956035614
2023-01-07 08:29:09,796 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,796 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 2.8747634887695312
2023-01-07 08:29:09,796 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,796 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,796 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:09,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.355127215385437
2023-01-07 08:29:09,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6013160943984985
2023-01-07 08:29:09,797 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,797 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.06819730997085571
2023-01-07 08:29:09,797 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,798 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:09,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.355127215385437
2023-01-07 08:29:09,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.034981220960617065
2023-01-07 08:29:09,799 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,799 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1.355127215385437
2023-01-07 08:29:09,799 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:09,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,800 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.1276134252548218
2023-01-07 08:29:09,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3367200791835785
2023-01-07 08:29:09,800 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,801 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.2919134497642517
2023-01-07 08:29:09,801 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,801 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:09,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,801 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.1276134252548218
2023-01-07 08:29:09,801 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16420336067676544
2023-01-07 08:29:09,802 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,802 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -1.1276134252548218
2023-01-07 08:29:09,802 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,803 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,803 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:09,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,803 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -6.095987796783447
2023-01-07 08:29:09,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3467114269733429
2023-01-07 08:29:09,804 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,804 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.001675209030508995
2023-01-07 08:29:09,804 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:09,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,804 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -6.095987796783447
2023-01-07 08:29:09,805 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.033979933708906174
2023-01-07 08:29:09,806 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,806 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -6.095987796783447
2023-01-07 08:29:09,806 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:09,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,806 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.462858200073242
2023-01-07 08:29:09,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07171124219894409
2023-01-07 08:29:09,807 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,807 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.004005499184131622
2023-01-07 08:29:09,807 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,808 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:09,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,808 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.462858200073242
2023-01-07 08:29:09,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18244189023971558
2023-01-07 08:29:09,809 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,809 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -3.462858200073242
2023-01-07 08:29:09,809 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:09,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,810 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.001434326171875
2023-01-07 08:29:09,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.082859992980957
2023-01-07 08:29:09,810 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,811 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.3428901433944702
2023-01-07 08:29:09,811 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,811 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,811 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:09,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,811 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.001434326171875
2023-01-07 08:29:09,811 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5142874121665955
2023-01-07 08:29:09,812 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,813 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.001434326171875
2023-01-07 08:29:09,813 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,813 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,814 > [DEBUG] 0 :: 7.63010311126709
2023-01-07 08:29:09,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,817 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0081787109375
2023-01-07 08:29:09,818 > [DEBUG] 0 :: before allreduce fusion buffer :: -312.17962646484375
2023-01-07 08:29:09,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,820 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.320732980966568
2023-01-07 08:29:09,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,820 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0081787109375
2023-01-07 08:29:09,820 > [DEBUG] 0 :: before allreduce fusion buffer :: -386.4598693847656
2023-01-07 08:29:09,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,822 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.685201168060303
2023-01-07 08:29:09,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34331101179122925
2023-01-07 08:29:09,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,824 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.011651203036308289
2023-01-07 08:29:09,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,824 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.685201168060303
2023-01-07 08:29:09,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3801295757293701
2023-01-07 08:29:09,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,826 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.857125759124756
2023-01-07 08:29:09,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03910287469625473
2023-01-07 08:29:09,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,828 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.009521812200546265
2023-01-07 08:29:09,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,828 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.857125759124756
2023-01-07 08:29:09,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1489678919315338
2023-01-07 08:29:09,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,830 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.888062000274658
2023-01-07 08:29:09,830 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28180012106895447
2023-01-07 08:29:09,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,832 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.41751450300216675
2023-01-07 08:29:09,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,832 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.888062000274658
2023-01-07 08:29:09,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3470864295959473
2023-01-07 08:29:09,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,833 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.541734218597412
2023-01-07 08:29:09,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.38507723808288574
2023-01-07 08:29:09,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.013558628037571907
2023-01-07 08:29:09,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.541734218597412
2023-01-07 08:29:09,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1439306139945984
2023-01-07 08:29:09,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,836 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -16.53936004638672
2023-01-07 08:29:09,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8943989276885986
2023-01-07 08:29:09,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.015944771468639374
2023-01-07 08:29:09,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -16.53936004638672
2023-01-07 08:29:09,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.028001442551612854
2023-01-07 08:29:09,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,840 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 352.14215087890625
2023-01-07 08:29:09,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.538859486579895
2023-01-07 08:29:09,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.07710376381874084
2023-01-07 08:29:09,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 352.14215087890625
2023-01-07 08:29:09,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7369704246520996
2023-01-07 08:29:09,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1073.57568359375
2023-01-07 08:29:09,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8759379386901855
2023-01-07 08:29:09,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,844 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.2910899519920349
2023-01-07 08:29:09,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,844 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1073.57568359375
2023-01-07 08:29:09,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0861327648162842
2023-01-07 08:29:09,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,846 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 164.821044921875
2023-01-07 08:29:09,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6465142369270325
2023-01-07 08:29:09,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.03325112536549568
2023-01-07 08:29:09,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 164.821044921875
2023-01-07 08:29:09,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4613134860992432
2023-01-07 08:29:09,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,849 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -24.02878189086914
2023-01-07 08:29:09,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08383899927139282
2023-01-07 08:29:09,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.020458251237869263
2023-01-07 08:29:09,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -24.02878189086914
2023-01-07 08:29:09,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5921261310577393
2023-01-07 08:29:09,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.9705114364624023
2023-01-07 08:29:09,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.551362991333008
2023-01-07 08:29:09,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,853 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.09316813945770264
2023-01-07 08:29:09,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,853 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.9705114364624023
2023-01-07 08:29:09,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.200130820274353
2023-01-07 08:29:09,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,855 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.4548587799072266
2023-01-07 08:29:09,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5348150730133057
2023-01-07 08:29:09,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,856 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.05992592126131058
2023-01-07 08:29:09,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,857 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.4548587799072266
2023-01-07 08:29:09,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.848612368106842
2023-01-07 08:29:09,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,858 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -13.809151649475098
2023-01-07 08:29:09,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8772685527801514
2023-01-07 08:29:09,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,859 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.02653820812702179
2023-01-07 08:29:09,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,860 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -13.809151649475098
2023-01-07 08:29:09,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32100069522857666
2023-01-07 08:29:09,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,861 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.4125699996948242
2023-01-07 08:29:09,861 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05961211025714874
2023-01-07 08:29:09,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.034519292414188385
2023-01-07 08:29:09,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,863 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.4125699996948242
2023-01-07 08:29:09,863 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.288611888885498
2023-01-07 08:29:09,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,864 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.163569927215576
2023-01-07 08:29:09,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.790313720703125
2023-01-07 08:29:09,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.0919734537601471
2023-01-07 08:29:09,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,866 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.163569927215576
2023-01-07 08:29:09,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.591766595840454
2023-01-07 08:29:09,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,867 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.044458389282227
2023-01-07 08:29:09,867 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23410603404045105
2023-01-07 08:29:09,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,868 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.06739170849323273
2023-01-07 08:29:09,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,869 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.044458389282227
2023-01-07 08:29:09,869 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3930002450942993
2023-01-07 08:29:09,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,870 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.8848916292190552
2023-01-07 08:29:09,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0759330987930298
2023-01-07 08:29:09,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,871 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.034737423062324524
2023-01-07 08:29:09,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.8848916292190552
2023-01-07 08:29:09,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.479163646697998
2023-01-07 08:29:09,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,873 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.745321273803711
2023-01-07 08:29:09,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1275155544281006
2023-01-07 08:29:09,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.029132764786481857
2023-01-07 08:29:09,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.745321273803711
2023-01-07 08:29:09,875 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7813145518302917
2023-01-07 08:29:09,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.802162170410156
2023-01-07 08:29:09,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.032743215560913
2023-01-07 08:29:09,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.02733767032623291
2023-01-07 08:29:09,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.802162170410156
2023-01-07 08:29:09,878 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6172761917114258
2023-01-07 08:29:09,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,879 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.600234031677246
2023-01-07 08:29:09,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1550005078315735
2023-01-07 08:29:09,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.029736056923866272
2023-01-07 08:29:09,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.600234031677246
2023-01-07 08:29:09,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7645230293273926
2023-01-07 08:29:09,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4233665466308594
2023-01-07 08:29:09,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4806758761405945
2023-01-07 08:29:09,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.10683572292327881
2023-01-07 08:29:09,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4233665466308594
2023-01-07 08:29:09,884 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5433831810951233
2023-01-07 08:29:09,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,885 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -36.49057388305664
2023-01-07 08:29:09,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3294193744659424
2023-01-07 08:29:09,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.11143292486667633
2023-01-07 08:29:09,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -36.49057388305664
2023-01-07 08:29:09,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.35717511177063
2023-01-07 08:29:09,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,888 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -27.961673736572266
2023-01-07 08:29:09,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6449851989746094
2023-01-07 08:29:09,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.012995809316635132
2023-01-07 08:29:09,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -27.961673736572266
2023-01-07 08:29:09,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.059787094593048096
2023-01-07 08:29:09,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,891 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -19.77645492553711
2023-01-07 08:29:09,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36385995149612427
2023-01-07 08:29:09,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.04133245348930359
2023-01-07 08:29:09,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -19.77645492553711
2023-01-07 08:29:09,893 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6820626258850098
2023-01-07 08:29:09,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,895 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.94963264465332
2023-01-07 08:29:09,895 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2834796905517578
2023-01-07 08:29:09,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,896 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.94963264465332
2023-01-07 08:29:09,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1842055320739746
2023-01-07 08:29:09,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,897 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.802318572998047
2023-01-07 08:29:09,898 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3487385511398315
2023-01-07 08:29:09,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.1672848463058472
2023-01-07 08:29:09,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.802318572998047
2023-01-07 08:29:09,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.071818351745605
2023-01-07 08:29:09,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,901 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 45.595035552978516
2023-01-07 08:29:09,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2572104930877686
2023-01-07 08:29:09,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.6388099193572998
2023-01-07 08:29:09,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 45.595035552978516
2023-01-07 08:29:09,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6930685639381409
2023-01-07 08:29:09,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.319751024246216
2023-01-07 08:29:09,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2517833709716797
2023-01-07 08:29:09,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.05819857120513916
2023-01-07 08:29:09,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.319751024246216
2023-01-07 08:29:09,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8387563228607178
2023-01-07 08:29:09,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 30.059242248535156
2023-01-07 08:29:09,907 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2211380004882812
2023-01-07 08:29:09,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 30.059242248535156
2023-01-07 08:29:09,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.39214324951172
2023-01-07 08:29:09,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,909 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.0230712890625
2023-01-07 08:29:09,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.400318145751953
2023-01-07 08:29:09,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,911 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.0230712890625
2023-01-07 08:29:09,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.1223039627075195
2023-01-07 08:29:09,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,912 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1593571901321411
2023-01-07 08:29:09,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.085201740264893
2023-01-07 08:29:09,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,913 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1593571901321411
2023-01-07 08:29:09,913 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0459630489349365
2023-01-07 08:29:09,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,915 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 86.79830932617188
2023-01-07 08:29:09,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.526782989501953
2023-01-07 08:29:09,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,916 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 86.79830932617188
2023-01-07 08:29:09,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.611268043518066
2023-01-07 08:29:09,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,917 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -5.159395217895508
2023-01-07 08:29:09,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6846537590026855
2023-01-07 08:29:09,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,919 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -5.159395217895508
2023-01-07 08:29:09,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9591689705848694
2023-01-07 08:29:09,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,920 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.6943502426147461
2023-01-07 08:29:09,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9454004764556885
2023-01-07 08:29:09,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,921 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.6943502426147461
2023-01-07 08:29:09,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.451729774475098
2023-01-07 08:29:09,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,923 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.21533203125
2023-01-07 08:29:09,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.117761254310608
2023-01-07 08:29:09,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,924 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.21533203125
2023-01-07 08:29:09,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.730587005615234
2023-01-07 08:29:09,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,925 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.143526077270508
2023-01-07 08:29:09,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8918585777282715
2023-01-07 08:29:09,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,926 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.37892842292785645
2023-01-07 08:29:09,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,927 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.143526077270508
2023-01-07 08:29:09,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43689292669296265
2023-01-07 08:29:09,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,929 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 30.25216293334961
2023-01-07 08:29:09,929 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4851884841918945
2023-01-07 08:29:09,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,930 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 30.25216293334961
2023-01-07 08:29:09,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.185670852661133
2023-01-07 08:29:09,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,931 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -397.2954406738281
2023-01-07 08:29:09,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.567500114440918
2023-01-07 08:29:09,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,932 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -397.2954406738281
2023-01-07 08:29:09,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.708457946777344
2023-01-07 08:29:09,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,934 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.02313232421875
2023-01-07 08:29:09,934 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.785188674926758
2023-01-07 08:29:09,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,935 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.4980003833770752
2023-01-07 08:29:09,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,935 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.02313232421875
2023-01-07 08:29:09,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.456159591674805
2023-01-07 08:29:09,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,937 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 82.09976196289062
2023-01-07 08:29:09,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2648359537124634
2023-01-07 08:29:09,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,938 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 82.09976196289062
2023-01-07 08:29:09,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.40410614013672
2023-01-07 08:29:09,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,939 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 17.369285583496094
2023-01-07 08:29:09,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.572235584259033
2023-01-07 08:29:09,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,941 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 17.369285583496094
2023-01-07 08:29:09,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.555818557739258
2023-01-07 08:29:09,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,943 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.746711730957031
2023-01-07 08:29:09,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,944 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.7920513153076172
2023-01-07 08:29:09,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,944 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.65020751953125
2023-01-07 08:29:09,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,946 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.928184509277344
2023-01-07 08:29:09,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,947 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8912637233734131
2023-01-07 08:29:09,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,948 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -20.591991424560547
2023-01-07 08:29:09,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,948 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.303699493408203
2023-01-07 08:29:09,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,949 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.292924880981445
2023-01-07 08:29:09,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,951 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -20.591991424560547
2023-01-07 08:29:09,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.466957092285156
2023-01-07 08:29:09,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,952 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.12022590637207
2023-01-07 08:29:09,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,953 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.6217758655548096
2023-01-07 08:29:09,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,954 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,954 > [DEBUG] 0 :: before allreduce fusion buffer :: -113.296142578125
2023-01-07 08:29:09,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,955 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.66835021972656
2023-01-07 08:29:09,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,956 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -0.007174491882324219
2023-01-07 08:29:09,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,957 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 56.01264190673828
2023-01-07 08:29:09,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.527080535888672
2023-01-07 08:29:09,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,958 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,958 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.409975051879883
2023-01-07 08:29:09,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,959 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -28.816696166992188
2023-01-07 08:29:09,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,960 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.153270721435547
2023-01-07 08:29:09,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,961 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.747066497802734
2023-01-07 08:29:09,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,962 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:09,963 > [DEBUG] 0 :: before allreduce fusion buffer :: -100.3109130859375
2023-01-07 08:29:09,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,964 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.66323184967041
2023-01-07 08:29:09,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.0093138217926025
2023-01-07 08:29:09,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 41.93633270263672
2023-01-07 08:29:09,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,967 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,967 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.98952579498291
2023-01-07 08:29:09,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,968 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.018463134765625
2023-01-07 08:29:09,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -32.05262756347656
2023-01-07 08:29:09,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.761863708496094
2023-01-07 08:29:09,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,971 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:09,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5114450454711914
2023-01-07 08:29:09,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -32.05262756347656
2023-01-07 08:29:09,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,972 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.721052169799805
2023-01-07 08:29:09,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,974 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 109.22345733642578
2023-01-07 08:29:09,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,975 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 0.8615589141845703
2023-01-07 08:29:09,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,975 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.46427917480469
2023-01-07 08:29:09,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,977 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.880117416381836
2023-01-07 08:29:09,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,978 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -116.95954895019531
2023-01-07 08:29:09,980 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:29:09,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,980 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -2420.431396484375
2023-01-07 08:29:09,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,981 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,981 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:09,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,981 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 17.369285583496094
2023-01-07 08:29:09,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 82.09976196289062
2023-01-07 08:29:09,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.02313232421875
2023-01-07 08:29:09,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -397.2954406738281
2023-01-07 08:29:09,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 30.25216293334961
2023-01-07 08:29:09,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.143526077270508
2023-01-07 08:29:09,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.21533203125
2023-01-07 08:29:09,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.6943502426147461
2023-01-07 08:29:09,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,984 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -5.159395217895508
2023-01-07 08:29:09,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,984 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 86.79830932617188
2023-01-07 08:29:09,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,984 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1593571901321411
2023-01-07 08:29:09,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,984 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.0230712890625
2023-01-07 08:29:09,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 30.059242248535156
2023-01-07 08:29:09,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.319751024246216
2023-01-07 08:29:09,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 45.595035552978516
2023-01-07 08:29:09,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.802318572998047
2023-01-07 08:29:09,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.94963264465332
2023-01-07 08:29:09,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -19.77645492553711
2023-01-07 08:29:09,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -27.961673736572266
2023-01-07 08:29:09,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -36.49057388305664
2023-01-07 08:29:09,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4233665466308594
2023-01-07 08:29:09,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.600234031677246
2023-01-07 08:29:09,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.802162170410156
2023-01-07 08:29:09,988 > [DEBUG] 0 :: before allreduce fusion buffer :: -2506.658935546875
2023-01-07 08:29:09,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.745321273803711
2023-01-07 08:29:09,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.8848916292190552
2023-01-07 08:29:09,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.044458389282227
2023-01-07 08:29:09,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.163569927215576
2023-01-07 08:29:09,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.4125699996948242
2023-01-07 08:29:09,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -13.809151649475098
2023-01-07 08:29:09,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.4548587799072266
2023-01-07 08:29:09,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,992 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.9705114364624023
2023-01-07 08:29:09,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,992 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -24.02878189086914
2023-01-07 08:29:09,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.366127014160156
2023-01-07 08:29:09,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,993 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 164.821044921875
2023-01-07 08:29:09,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,994 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1073.57568359375
2023-01-07 08:29:09,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,994 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 352.14215087890625
2023-01-07 08:29:09,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,994 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 658.1552734375
2023-01-07 08:29:09,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 1574.05078125
2023-01-07 08:29:09,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,995 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -142.02197265625
2023-01-07 08:29:09,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,996 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 5045.1025390625
2023-01-07 08:29:09,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,996 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 709.531005859375
2023-01-07 08:29:09,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 6287.96484375
2023-01-07 08:29:09,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,997 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.685201168060303
2023-01-07 08:29:09,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,997 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0081787109375
2023-01-07 08:29:09,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 700.211669921875
2023-01-07 08:29:09,998 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:09,998 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -2420.431396484375
2023-01-07 08:29:09,998 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:09,998 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:09,999 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:09,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,999 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -4.448908805847168
2023-01-07 08:29:09,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:09,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:09,999 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:09,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.42928123474121
2023-01-07 08:29:10,000 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,000 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -4.448908805847168
2023-01-07 08:29:10,000 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,000 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,001 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.171111106872559
2023-01-07 08:29:10,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,001 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -25.890846252441406
2023-01-07 08:29:10,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,001 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:10,001 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.007797241210938
2023-01-07 08:29:10,002 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,003 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -25.890846252441406
2023-01-07 08:29:10,003 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,003 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,003 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:10,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,003 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:10,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 160.42953491210938
2023-01-07 08:29:10,004 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,004 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.8615589141845703
2023-01-07 08:29:10,004 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,004 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,004 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:10,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,005 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 693.0343017578125
2023-01-07 08:29:10,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.48506546020508
2023-01-07 08:29:10,006 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,006 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 693.0343017578125
2023-01-07 08:29:10,006 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,006 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,006 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,006 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 2.804152488708496
2023-01-07 08:29:10,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,007 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -33.95183563232422
2023-01-07 08:29:10,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,007 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,007 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.92755889892578
2023-01-07 08:29:10,008 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,008 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 2.804152488708496
2023-01-07 08:29:10,008 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,008 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,008 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,009 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,009 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7913789749145508
2023-01-07 08:29:10,010 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,010 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -33.95183563232422
2023-01-07 08:29:10,010 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,010 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,010 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,010 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 1.446925401687622
2023-01-07 08:29:10,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,011 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 41.93633270263672
2023-01-07 08:29:10,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.50822067260742
2023-01-07 08:29:10,012 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,012 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 1.446925401687622
2023-01-07 08:29:10,012 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,012 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,012 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,012 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:10,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.55146026611328
2023-01-07 08:29:10,013 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,013 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -32.05262756347656
2023-01-07 08:29:10,014 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,014 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,014 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,014 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:10,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.98812103271484
2023-01-07 08:29:10,015 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,015 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -3.0093138217926025
2023-01-07 08:29:10,015 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,015 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,016 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:10,016 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.78543853759766
2023-01-07 08:29:10,017 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,017 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 41.93633270263672
2023-01-07 08:29:10,017 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,017 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,017 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -0.5723586082458496
2023-01-07 08:29:10,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,018 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:10,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.573612213134766
2023-01-07 08:29:10,019 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,019 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -0.5723586082458496
2023-01-07 08:29:10,019 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,019 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -250.34603881835938
2023-01-07 08:29:10,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,020 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:10,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.0267219543457
2023-01-07 08:29:10,021 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,021 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -250.34603881835938
2023-01-07 08:29:10,021 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.6403698921203613
2023-01-07 08:29:10,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,022 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -28.816696166992188
2023-01-07 08:29:10,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.70064926147461
2023-01-07 08:29:10,023 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,023 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.6403698921203613
2023-01-07 08:29:10,023 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,023 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,023 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,023 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 142.50180053710938
2023-01-07 08:29:10,024 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,024 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -28.816696166992188
2023-01-07 08:29:10,025 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,025 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,025 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:10,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 56.01264190673828
2023-01-07 08:29:10,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.076950073242188
2023-01-07 08:29:10,026 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,026 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -0.007174491882324219
2023-01-07 08:29:10,026 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:10,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 56.01264190673828
2023-01-07 08:29:10,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 71.1518783569336
2023-01-07 08:29:10,028 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,028 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 56.01264190673828
2023-01-07 08:29:10,028 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,028 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,028 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 76.06097412109375
2023-01-07 08:29:10,029 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,030 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 1.6217758655548096
2023-01-07 08:29:10,030 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,030 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,030 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 133.28701782226562
2023-01-07 08:29:10,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.584442138671875
2023-01-07 08:29:10,031 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,031 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 133.28701782226562
2023-01-07 08:29:10,031 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,032 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,032 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.7370978593826294
2023-01-07 08:29:10,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,032 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -44.95705032348633
2023-01-07 08:29:10,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,032 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -20.591991424560547
2023-01-07 08:29:10,033 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.765262603759766
2023-01-07 08:29:10,033 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,034 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.7370978593826294
2023-01-07 08:29:10,034 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,034 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,034 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,034 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:10,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.894200325012207
2023-01-07 08:29:10,035 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,035 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -44.95705032348633
2023-01-07 08:29:10,035 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,036 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,036 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.23777961730957
2023-01-07 08:29:10,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,036 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,036 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -20.591991424560547
2023-01-07 08:29:10,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.40028381347656
2023-01-07 08:29:10,037 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,037 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -0.8912637233734131
2023-01-07 08:29:10,037 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:10,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.531044006347656
2023-01-07 08:29:10,039 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,039 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -20.591991424560547
2023-01-07 08:29:10,039 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,039 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:10,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.366806030273438
2023-01-07 08:29:10,040 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,040 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 1.7920513153076172
2023-01-07 08:29:10,040 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,040 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,041 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,041 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 134.59573364257812
2023-01-07 08:29:10,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.529403686523438
2023-01-07 08:29:10,042 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,042 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 134.59573364257812
2023-01-07 08:29:10,042 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,042 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,042 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,043 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.43713438510894775
2023-01-07 08:29:10,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,043 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 17.369285583496094
2023-01-07 08:29:10,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6656433343887329
2023-01-07 08:29:10,044 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,044 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.43713438510894775
2023-01-07 08:29:10,044 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,044 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,044 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.787897109985352
2023-01-07 08:29:10,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 17.369285583496094
2023-01-07 08:29:10,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.385330200195312
2023-01-07 08:29:10,046 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,046 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 17.369285583496094
2023-01-07 08:29:10,046 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,046 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,046 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,046 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1.698689579963684
2023-01-07 08:29:10,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,046 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 82.09976196289062
2023-01-07 08:29:10,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.105555534362793
2023-01-07 08:29:10,047 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,048 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -1.698689579963684
2023-01-07 08:29:10,048 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,048 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,048 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -20.516536712646484
2023-01-07 08:29:10,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,048 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 82.09976196289062
2023-01-07 08:29:10,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.229677200317383
2023-01-07 08:29:10,049 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,049 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 82.09976196289062
2023-01-07 08:29:10,049 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,049 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,049 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:10,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,050 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.02313232421875
2023-01-07 08:29:10,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.222177505493164
2023-01-07 08:29:10,051 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,051 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -1.4980003833770752
2023-01-07 08:29:10,051 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,051 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,051 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:10,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,051 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -18.02313232421875
2023-01-07 08:29:10,051 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.048027038574219
2023-01-07 08:29:10,052 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,052 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -18.02313232421875
2023-01-07 08:29:10,053 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,053 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,053 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.4815993309020996
2023-01-07 08:29:10,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -397.2954406738281
2023-01-07 08:29:10,053 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.801172256469727
2023-01-07 08:29:10,054 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,054 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.4815993309020996
2023-01-07 08:29:10,054 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,055 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,055 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.0566999912261963
2023-01-07 08:29:10,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,055 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -397.2954406738281
2023-01-07 08:29:10,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.066207885742188
2023-01-07 08:29:10,056 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,056 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -397.2954406738281
2023-01-07 08:29:10,056 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,056 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,057 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.26212722063064575
2023-01-07 08:29:10,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,057 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 30.25216293334961
2023-01-07 08:29:10,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18734407424926758
2023-01-07 08:29:10,058 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,058 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.26212722063064575
2023-01-07 08:29:10,058 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,058 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,058 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 15.858409881591797
2023-01-07 08:29:10,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 30.25216293334961
2023-01-07 08:29:10,059 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.608107566833496
2023-01-07 08:29:10,060 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,060 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 30.25216293334961
2023-01-07 08:29:10,060 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,060 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,060 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:10,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,060 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.143526077270508
2023-01-07 08:29:10,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.639160394668579
2023-01-07 08:29:10,061 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,061 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.37892842292785645
2023-01-07 08:29:10,061 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,062 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:10,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.143526077270508
2023-01-07 08:29:10,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.044109344482422
2023-01-07 08:29:10,063 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,063 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 4.143526077270508
2023-01-07 08:29:10,063 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,063 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,063 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.6284619569778442
2023-01-07 08:29:10,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,064 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.21533203125
2023-01-07 08:29:10,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1771154403686523
2023-01-07 08:29:10,065 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,065 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.6284619569778442
2023-01-07 08:29:10,065 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,065 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 12.577384948730469
2023-01-07 08:29:10,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.21533203125
2023-01-07 08:29:10,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0443291664123535
2023-01-07 08:29:10,066 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,067 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -90.21533203125
2023-01-07 08:29:10,067 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,067 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,067 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.2941691279411316
2023-01-07 08:29:10,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.6943502426147461
2023-01-07 08:29:10,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.012974739074707
2023-01-07 08:29:10,068 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,069 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.2941691279411316
2023-01-07 08:29:10,069 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,069 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.623693943023682
2023-01-07 08:29:10,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 0.6943502426147461
2023-01-07 08:29:10,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.976180076599121
2023-01-07 08:29:10,070 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,070 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 0.6943502426147461
2023-01-07 08:29:10,070 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,070 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.03008107841014862
2023-01-07 08:29:10,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -5.159395217895508
2023-01-07 08:29:10,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.519330978393555
2023-01-07 08:29:10,072 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,072 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.03008107841014862
2023-01-07 08:29:10,072 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,072 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -25.3018798828125
2023-01-07 08:29:10,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,073 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -5.159395217895508
2023-01-07 08:29:10,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5393872261047363
2023-01-07 08:29:10,074 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,074 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -5.159395217895508
2023-01-07 08:29:10,074 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,074 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.1880839467048645
2023-01-07 08:29:10,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 86.79830932617188
2023-01-07 08:29:10,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.923162460327148
2023-01-07 08:29:10,076 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,076 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.1880839467048645
2023-01-07 08:29:10,076 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,076 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,076 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.52092742919922
2023-01-07 08:29:10,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,076 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 86.79830932617188
2023-01-07 08:29:10,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3187806606292725
2023-01-07 08:29:10,077 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,077 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 86.79830932617188
2023-01-07 08:29:10,078 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,078 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.5084789395332336
2023-01-07 08:29:10,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1593571901321411
2023-01-07 08:29:10,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.236464500427246
2023-01-07 08:29:10,080 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,080 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.5084789395332336
2023-01-07 08:29:10,080 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.565258026123047
2023-01-07 08:29:10,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1593571901321411
2023-01-07 08:29:10,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3263554573059082
2023-01-07 08:29:10,081 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,081 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 0.1593571901321411
2023-01-07 08:29:10,082 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.15508732199668884
2023-01-07 08:29:10,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,082 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.0230712890625
2023-01-07 08:29:10,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5303096771240234
2023-01-07 08:29:10,083 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,083 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.15508732199668884
2023-01-07 08:29:10,083 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,084 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.985057830810547
2023-01-07 08:29:10,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,084 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.0230712890625
2023-01-07 08:29:10,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4345149993896484
2023-01-07 08:29:10,085 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,085 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -19.0230712890625
2023-01-07 08:29:10,085 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,085 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.20233534276485443
2023-01-07 08:29:10,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 30.059242248535156
2023-01-07 08:29:10,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.6054744720459
2023-01-07 08:29:10,087 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,087 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.20233534276485443
2023-01-07 08:29:10,087 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,087 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,087 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.22715377807617188
2023-01-07 08:29:10,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 30.059242248535156
2023-01-07 08:29:10,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4025516510009766
2023-01-07 08:29:10,089 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,089 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 30.059242248535156
2023-01-07 08:29:10,089 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,089 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,089 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:10,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,089 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.319751024246216
2023-01-07 08:29:10,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1818642616271973
2023-01-07 08:29:10,090 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,091 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.05819857120513916
2023-01-07 08:29:10,091 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,091 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,091 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:10,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 2.319751024246216
2023-01-07 08:29:10,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.154954671859741
2023-01-07 08:29:10,092 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,092 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 2.319751024246216
2023-01-07 08:29:10,092 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,093 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:10,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 45.595035552978516
2023-01-07 08:29:10,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.359123945236206
2023-01-07 08:29:10,094 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,094 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.6388099193572998
2023-01-07 08:29:10,094 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,094 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:10,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,094 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 45.595035552978516
2023-01-07 08:29:10,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37237879633903503
2023-01-07 08:29:10,095 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,095 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 45.595035552978516
2023-01-07 08:29:10,095 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:10,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,096 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.802318572998047
2023-01-07 08:29:10,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.402105450630188
2023-01-07 08:29:10,097 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,097 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.1672848463058472
2023-01-07 08:29:10,097 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,097 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,097 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:10,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 28.802318572998047
2023-01-07 08:29:10,098 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.557098865509033
2023-01-07 08:29:10,099 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,099 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 28.802318572998047
2023-01-07 08:29:10,099 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,099 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,099 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.17832934856414795
2023-01-07 08:29:10,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.94963264465332
2023-01-07 08:29:10,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.917014122009277
2023-01-07 08:29:10,101 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,101 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.17832934856414795
2023-01-07 08:29:10,101 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 17.364486694335938
2023-01-07 08:29:10,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,101 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -29.94963264465332
2023-01-07 08:29:10,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.939511299133301
2023-01-07 08:29:10,102 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,102 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -29.94963264465332
2023-01-07 08:29:10,102 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:10,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -19.77645492553711
2023-01-07 08:29:10,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3208954334259033
2023-01-07 08:29:10,104 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,104 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.04133245348930359
2023-01-07 08:29:10,104 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,104 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,104 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:10,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,104 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -19.77645492553711
2023-01-07 08:29:10,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11093676090240479
2023-01-07 08:29:10,106 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,106 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -19.77645492553711
2023-01-07 08:29:10,106 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,106 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,106 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:10,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -27.961673736572266
2023-01-07 08:29:10,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.945630073547363
2023-01-07 08:29:10,107 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,107 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.012995809316635132
2023-01-07 08:29:10,107 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,108 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:10,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -27.961673736572266
2023-01-07 08:29:10,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.80791437625885
2023-01-07 08:29:10,109 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,109 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -27.961673736572266
2023-01-07 08:29:10,109 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:10,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,110 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -36.49057388305664
2023-01-07 08:29:10,110 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.437315940856934
2023-01-07 08:29:10,110 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,111 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.11143292486667633
2023-01-07 08:29:10,111 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:10,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -36.49057388305664
2023-01-07 08:29:10,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.792486190795898
2023-01-07 08:29:10,112 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,112 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -36.49057388305664
2023-01-07 08:29:10,112 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,112 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,113 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:10,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4233665466308594
2023-01-07 08:29:10,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.839909553527832
2023-01-07 08:29:10,114 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,114 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.10683572292327881
2023-01-07 08:29:10,114 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:10,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -3.4233665466308594
2023-01-07 08:29:10,115 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4291974604129791
2023-01-07 08:29:10,115 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,116 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -3.4233665466308594
2023-01-07 08:29:10,116 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:10,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.600234031677246
2023-01-07 08:29:10,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 780924544.0
2023-01-07 08:29:10,117 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,117 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.029736056923866272
2023-01-07 08:29:10,117 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,117 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,117 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:10,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 8.600234031677246
2023-01-07 08:29:10,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 32284868.0
2023-01-07 08:29:10,119 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,119 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 8.600234031677246
2023-01-07 08:29:10,119 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:10,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,119 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.802162170410156
2023-01-07 08:29:10,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.303920030593872
2023-01-07 08:29:10,120 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,121 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.02733767032623291
2023-01-07 08:29:10,121 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:10,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,121 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.802162170410156
2023-01-07 08:29:10,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9302452802658081
2023-01-07 08:29:10,122 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,122 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -5.802162170410156
2023-01-07 08:29:10,122 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:10,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.745321273803711
2023-01-07 08:29:10,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.631048321723938
2023-01-07 08:29:10,124 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,124 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.029132764786481857
2023-01-07 08:29:10,124 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,124 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:10,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,124 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.745321273803711
2023-01-07 08:29:10,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32912343740463257
2023-01-07 08:29:10,125 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,126 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -14.745321273803711
2023-01-07 08:29:10,126 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:10,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.8848916292190552
2023-01-07 08:29:10,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 716883.125
2023-01-07 08:29:10,127 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,127 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.034737423062324524
2023-01-07 08:29:10,127 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:10,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,128 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.8848916292190552
2023-01-07 08:29:10,128 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8290354013442993
2023-01-07 08:29:10,129 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,129 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -0.8848916292190552
2023-01-07 08:29:10,129 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:10,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.044458389282227
2023-01-07 08:29:10,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.737587928771973
2023-01-07 08:29:10,130 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,130 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.06739170849323273
2023-01-07 08:29:10,131 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:10,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.044458389282227
2023-01-07 08:29:10,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9386128187179565
2023-01-07 08:29:10,132 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,132 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -12.044458389282227
2023-01-07 08:29:10,132 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:10,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.163569927215576
2023-01-07 08:29:10,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02361154556274414
2023-01-07 08:29:10,134 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,134 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.0919734537601471
2023-01-07 08:29:10,134 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,134 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,134 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:10,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -5.163569927215576
2023-01-07 08:29:10,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45863285660743713
2023-01-07 08:29:10,135 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,136 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -5.163569927215576
2023-01-07 08:29:10,136 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:10,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,136 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.4125699996948242
2023-01-07 08:29:10,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 170734032.0
2023-01-07 08:29:10,137 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,137 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.034519292414188385
2023-01-07 08:29:10,137 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:10,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.4125699996948242
2023-01-07 08:29:10,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5956780314445496
2023-01-07 08:29:10,139 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,139 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -0.4125699996948242
2023-01-07 08:29:10,139 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:10,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -13.809151649475098
2023-01-07 08:29:10,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24584943056106567
2023-01-07 08:29:10,140 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,140 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.02653820812702179
2023-01-07 08:29:10,141 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:10,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -13.809151649475098
2023-01-07 08:29:10,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8366793394088745
2023-01-07 08:29:10,142 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,142 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -13.809151649475098
2023-01-07 08:29:10,142 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,142 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,142 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:10,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.4548587799072266
2023-01-07 08:29:10,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7902829647064209
2023-01-07 08:29:10,144 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,144 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.05992592126131058
2023-01-07 08:29:10,144 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:10,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.4548587799072266
2023-01-07 08:29:10,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2447493076324463
2023-01-07 08:29:10,145 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,145 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 2.4548587799072266
2023-01-07 08:29:10,146 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:10,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,146 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.9705114364624023
2023-01-07 08:29:10,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 38782017536.0
2023-01-07 08:29:10,147 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,147 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.09316813945770264
2023-01-07 08:29:10,147 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,147 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:10,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,148 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 302792179712.0
2023-01-07 08:29:10,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 201861234688.0
2023-01-07 08:29:10,149 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,149 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 302792179712.0
2023-01-07 08:29:10,149 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:10,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,149 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1211167932416.0
2023-01-07 08:29:10,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 403723124736.0
2023-01-07 08:29:10,150 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,150 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.020458251237869263
2023-01-07 08:29:10,150 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:10,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,151 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1211167932416.0
2023-01-07 08:29:10,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 807445463040.0
2023-01-07 08:29:10,152 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,152 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 1211167932416.0
2023-01-07 08:29:10,152 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:10,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,153 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 4578294104064.0
2023-01-07 08:29:10,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 1614890401792.0
2023-01-07 08:29:10,154 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,155 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.03325112536549568
2023-01-07 08:29:10,155 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:10,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,155 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 4578294104064.0
2023-01-07 08:29:10,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 2963403702272.0
2023-01-07 08:29:10,156 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,157 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 4578294104064.0
2023-01-07 08:29:10,157 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:10,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,157 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17070091665408.0
2023-01-07 08:29:10,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 5926809501696.0
2023-01-07 08:29:10,158 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,158 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.2910899519920349
2023-01-07 08:29:10,158 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:10,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17070091665408.0
2023-01-07 08:29:10,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 11143280066560.0
2023-01-07 08:29:10,160 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,160 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 17070091665408.0
2023-01-07 08:29:10,160 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,160 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:10,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,160 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 66859688787968.0
2023-01-07 08:29:10,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 22286547550208.0
2023-01-07 08:29:10,161 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,161 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.07710376381874084
2023-01-07 08:29:10,161 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,162 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:10,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,162 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 66859688787968.0
2023-01-07 08:29:10,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 44573078323200.0
2023-01-07 08:29:10,163 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,163 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 66859688787968.0
2023-01-07 08:29:10,163 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:10,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,164 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 190088357085184.0
2023-01-07 08:29:10,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 84658754682880.0
2023-01-07 08:29:10,165 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,165 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -0.015944771468639374
2023-01-07 08:29:10,165 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:10,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,165 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 190088357085184.0
2023-01-07 08:29:10,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 105429652733952.0
2023-01-07 08:29:10,166 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,166 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 190088357085184.0
2023-01-07 08:29:10,166 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,167 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,167 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:10,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,167 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.926876143327642e+16
2023-01-07 08:29:10,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 195323737669632.0
2023-01-07 08:29:10,168 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,168 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 17899968266240.0
2023-01-07 08:29:10,168 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:10,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,169 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.944619082724147e+16
2023-01-07 08:29:10,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 11989535424512.0
2023-01-07 08:29:10,170 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,170 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 3.944619082724147e+16
2023-01-07 08:29:10,170 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:10,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,170 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.9997307600633856e+16
2023-01-07 08:29:10,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 361244196339712.0
2023-01-07 08:29:10,171 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,171 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 238883832856576.0
2023-01-07 08:29:10,171 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:10,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,172 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.9997307600633856e+16
2023-01-07 08:29:10,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -4199538491392.0
2023-01-07 08:29:10,173 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,173 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 1.9997307600633856e+16
2023-01-07 08:29:10,173 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,173 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:10,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,174 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.1881540619337728e+16
2023-01-07 08:29:10,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 393155669131264.0
2023-01-07 08:29:10,174 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,175 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 9762160016097280.0
2023-01-07 08:29:10,175 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,175 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,175 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:10,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,175 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1.1881540619337728e+16
2023-01-07 08:29:10,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 1561041431429120.0
2023-01-07 08:29:10,176 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,176 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 1.1881540619337728e+16
2023-01-07 08:29:10,176 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:10,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,177 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.36030741069824e+16
2023-01-07 08:29:10,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 2450752899383296.0
2023-01-07 08:29:10,178 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,178 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 4878842866958336.0
2023-01-07 08:29:10,178 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:10,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,178 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.36030741069824e+16
2023-01-07 08:29:10,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.016008879553642e+18
2023-01-07 08:29:10,180 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,180 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 1.36030741069824e+16
2023-01-07 08:29:10,180 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:10,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,180 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 3.419426616298701e+16
2023-01-07 08:29:10,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 11143470907392.0
2023-01-07 08:29:10,181 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,181 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 2.417581471091917e+16
2023-01-07 08:29:10,181 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:10,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,182 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 3.419426616298701e+16
2023-01-07 08:29:10,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1476073158017024e+16
2023-01-07 08:29:10,183 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,183 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 3.419426616298701e+16
2023-01-07 08:29:10,183 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,184 > [DEBUG] 0 :: 7.202641487121582
2023-01-07 08:29:10,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,188 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00244140625
2023-01-07 08:29:10,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.051194849583377e+22
2023-01-07 08:29:10,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,191 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2120128571987152
2023-01-07 08:29:10,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,191 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00244140625
2023-01-07 08:29:10,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.281289542008886e+23
2023-01-07 08:29:10,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,195 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.29530668258667
2023-01-07 08:29:10,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19419513642787933
2023-01-07 08:29:10,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,196 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.01937050372362137
2023-01-07 08:29:10,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,197 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.29530668258667
2023-01-07 08:29:10,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6520683773145075e+23
2023-01-07 08:29:10,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,198 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 25.803836822509766
2023-01-07 08:29:10,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27654388546943665
2023-01-07 08:29:10,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,199 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.003019854426383972
2023-01-07 08:29:10,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,200 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 25.803836822509766
2023-01-07 08:29:10,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 168081104.0
2023-01-07 08:29:10,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,201 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 15.382161140441895
2023-01-07 08:29:10,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 63622410240.0
2023-01-07 08:29:10,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,202 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.282329797744751
2023-01-07 08:29:10,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,203 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 15.382161140441895
2023-01-07 08:29:10,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 672324416.0
2023-01-07 08:29:10,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,204 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.321344375610352
2023-01-07 08:29:10,204 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3521893322467804
2023-01-07 08:29:10,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,205 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0568917840719223
2023-01-07 08:29:10,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,206 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.321344375610352
2023-01-07 08:29:10,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2550058364868164
2023-01-07 08:29:10,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,207 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.943046265700352e+16
2023-01-07 08:29:10,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.92469816541184e+16
2023-01-07 08:29:10,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,209 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 3.908443861680128e+16
2023-01-07 08:29:10,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,209 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.943046265700352e+16
2023-01-07 08:29:10,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.174013247183913e+17
2023-01-07 08:29:10,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,210 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.85358392393728e+16
2023-01-07 08:29:10,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.85358478293074e+16
2023-01-07 08:29:10,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,212 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 180622098169856.0
2023-01-07 08:29:10,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,212 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.85358392393728e+16
2023-01-07 08:29:10,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1294811536532439e+18
2023-01-07 08:29:10,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,213 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5725232480321536e+17
2023-01-07 08:29:10,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.57252290443477e+17
2023-01-07 08:29:10,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,215 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 1028770295185408.0
2023-01-07 08:29:10,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,215 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5725232480321536e+17
2023-01-07 08:29:10,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 1028870622937088.0
2023-01-07 08:29:10,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.565581206492283e+17
2023-01-07 08:29:10,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.564339960943739e+17
2023-01-07 08:29:10,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 72460443582464.0
2023-01-07 08:29:10,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.565581206492283e+17
2023-01-07 08:29:10,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.569221964369756e+17
2023-01-07 08:29:10,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,219 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.1551135558087475e+17
2023-01-07 08:29:10,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1403818179834675e+17
2023-01-07 08:29:10,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 3.130016172112609e+17
2023-01-07 08:29:10,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.1551135558087475e+17
2023-01-07 08:29:10,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.942644272454697e+17
2023-01-07 08:29:10,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,222 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.318120230715392e+17
2023-01-07 08:29:10,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.289497194265313e+17
2023-01-07 08:29:10,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,224 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 1.252194622772347e+18
2023-01-07 08:29:10,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,224 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.318120230715392e+17
2023-01-07 08:29:10,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0778954490762494e+19
2023-01-07 08:29:10,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,226 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.217036776401535e+18
2023-01-07 08:29:10,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2523151567345418e+18
2023-01-07 08:29:10,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 2.5438257039760425e+18
2023-01-07 08:29:10,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.217036776401535e+18
2023-01-07 08:29:10,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.508546773887222e+18
2023-01-07 08:29:10,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4297357045535867e+18
2023-01-07 08:29:10,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5047240468353516e+18
2023-01-07 08:29:10,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,231 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 5.088269333486895e+18
2023-01-07 08:29:10,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,231 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4297357045535867e+18
2023-01-07 08:29:10,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.01328099120513e+18
2023-01-07 08:29:10,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,232 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.0042526891245568e+19
2023-01-07 08:29:10,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.002656198241026e+19
2023-01-07 08:29:10,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 1.0018260669620552e+19
2023-01-07 08:29:10,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.0042526891245568e+19
2023-01-07 08:29:10,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0034226677967487e+19
2023-01-07 08:29:10,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,235 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.9422558444337496e+19
2023-01-07 08:29:10,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.003779457370607e+19
2023-01-07 08:29:10,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 4.070615906594167e+19
2023-01-07 08:29:10,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.9422558444337496e+19
2023-01-07 08:29:10,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.0090920737549844e+19
2023-01-07 08:29:10,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,238 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.880420186198219e+19
2023-01-07 08:29:10,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.006923397020359e+19
2023-01-07 08:29:10,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 8.141354078881343e+19
2023-01-07 08:29:10,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.880420186198219e+19
2023-01-07 08:29:10,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.014851307863854e+19
2023-01-07 08:29:10,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.6029245218890554e+20
2023-01-07 08:29:10,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6026097976807208e+20
2023-01-07 08:29:10,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 3.204746013713126e+20
2023-01-07 08:29:10,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.6029245218890554e+20
2023-01-07 08:29:10,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2050605619996e+20
2023-01-07 08:29:10,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.20381615088145e+20
2023-01-07 08:29:10,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.40993112838992e+20
2023-01-07 08:29:10,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 1.302616652621015e+21
2023-01-07 08:29:10,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.20381615088145e+20
2023-01-07 08:29:10,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2820053659764004e+21
2023-01-07 08:29:10,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.4814577804582626e+21
2023-01-07 08:29:10,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5639673848063874e+21
2023-01-07 08:29:10,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 5.21046661048406e+21
2023-01-07 08:29:10,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.4814577804582626e+21
2023-01-07 08:29:10,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.127957287610912e+21
2023-01-07 08:29:10,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,251 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.0256720719555122e+22
2023-01-07 08:29:10,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0255932589620333e+22
2023-01-07 08:29:10,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 1.0255187243882003e+22
2023-01-07 08:29:10,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.0256720719555122e+22
2023-01-07 08:29:10,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0255974247916886e+22
2023-01-07 08:29:10,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,254 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.985181536605343e+22
2023-01-07 08:29:10,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0511851668441783e+22
2023-01-07 08:29:10,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 4.168373288387248e+22
2023-01-07 08:29:10,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.985181536605343e+22
2023-01-07 08:29:10,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.102369883328394e+22
2023-01-07 08:29:10,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.10229197105484e+22
2023-01-07 08:29:10,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.10229197105484e+22
2023-01-07 08:29:10,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.13355499505996704
2023-01-07 08:29:10,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.10229197105484e+22
2023-01-07 08:29:10,258 > [DEBUG] 0 :: before allreduce fusion buffer :: -3441494784.0
2023-01-07 08:29:10,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 8.203223855022215e+22
2023-01-07 08:29:10,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.203223855022215e+22
2023-01-07 08:29:10,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 1.640644771004443e+23
2023-01-07 08:29:10,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 8.203223855022215e+22
2023-01-07 08:29:10,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.640644771004443e+23
2023-01-07 08:29:10,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.1768406182990286e+23
2023-01-07 08:29:10,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.282080013815482e+23
2023-01-07 08:29:10,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 6.669397261419596e+23
2023-01-07 08:29:10,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.1768406182990286e+23
2023-01-07 08:29:10,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.564160027630964e+23
2023-01-07 08:29:10,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.6256634345916333e+24
2023-01-07 08:29:10,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3128320055261928e+24
2023-01-07 08:29:10,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.6256634345916333e+24
2023-01-07 08:29:10,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3128320055261928e+24
2023-01-07 08:29:10,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,269 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.6256640110523856e+24
2023-01-07 08:29:10,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6256640110523856e+24
2023-01-07 08:29:10,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.6261315207225037e+24
2023-01-07 08:29:10,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.6256640110523856e+24
2023-01-07 08:29:10,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6261315207225037e+24
2023-01-07 08:29:10,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,272 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8201350135773537e+23
2023-01-07 08:29:10,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8201351937213388e+23
2023-01-07 08:29:10,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 5.252591047613068e+24
2023-01-07 08:29:10,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8201350135773537e+23
2023-01-07 08:29:10,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.252591047613068e+24
2023-01-07 08:29:10,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,275 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.068103490648081e+25
2023-01-07 08:29:10,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.068103490648081e+25
2023-01-07 08:29:10,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,276 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.01102466881275177
2023-01-07 08:29:10,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,276 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.068103490648081e+25
2023-01-07 08:29:10,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -33869524992.0
2023-01-07 08:29:10,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 43117142016.0
2023-01-07 08:29:10,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 87450096.0
2023-01-07 08:29:10,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,279 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 43117142016.0
2023-01-07 08:29:10,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 38900559872.0
2023-01-07 08:29:10,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,280 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 36786429952.0
2023-01-07 08:29:10,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 176008272.0
2023-01-07 08:29:10,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 36786429952.0
2023-01-07 08:29:10,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 34541248512.0
2023-01-07 08:29:10,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42855342080.0
2023-01-07 08:29:10,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 745735296.0
2023-01-07 08:29:10,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,284 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42855342080.0
2023-01-07 08:29:10,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 41592643584.0
2023-01-07 08:29:10,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77258907648.0
2023-01-07 08:29:10,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 1495245312.0
2023-01-07 08:29:10,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77258907648.0
2023-01-07 08:29:10,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 75505065984.0
2023-01-07 08:29:10,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 127538003968.0
2023-01-07 08:29:10,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 596457600.0
2023-01-07 08:29:10,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 127538003968.0
2023-01-07 08:29:10,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 126941396992.0
2023-01-07 08:29:10,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,290 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 263162298368.0
2023-01-07 08:29:10,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 2654569728.0
2023-01-07 08:29:10,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 263162298368.0
2023-01-07 08:29:10,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 260475437056.0
2023-01-07 08:29:10,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 530160254976.0
2023-01-07 08:29:10,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 7593168384.0
2023-01-07 08:29:10,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 530160254976.0
2023-01-07 08:29:10,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 520287354880.0
2023-01-07 08:29:10,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 522274209792.0
2023-01-07 08:29:10,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 20047339520.0
2023-01-07 08:29:10,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 9885446144.0
2023-01-07 08:29:10,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 522274209792.0
2023-01-07 08:29:10,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 511505563648.0
2023-01-07 08:29:10,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1066818142208.0
2023-01-07 08:29:10,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 19843158016.0
2023-01-07 08:29:10,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1066818142208.0
2023-01-07 08:29:10,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 1046815506432.0
2023-01-07 08:29:10,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,302 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.962465261571277e+16
2023-01-07 08:29:10,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 79372632064.0
2023-01-07 08:29:10,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.962465261571277e+16
2023-01-07 08:29:10,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.962457315881779e+16
2023-01-07 08:29:10,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.916025337951027e+16
2023-01-07 08:29:10,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.906850428813312e+16
2023-01-07 08:29:10,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 5.860891756278579e+16
2023-01-07 08:29:10,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.916025337951027e+16
2023-01-07 08:29:10,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.870067524409754e+16
2023-01-07 08:29:10,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6097446372101325e+17
2023-01-07 08:29:10,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2170653980635955e+17
2023-01-07 08:29:10,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6097446372101325e+17
2023-01-07 08:29:10,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.92679239146537e+16
2023-01-07 08:29:10,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.824629222679511e+17
2023-01-07 08:29:10,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8227777481775514e+17
2023-01-07 08:29:10,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.824629222679511e+17
2023-01-07 08:29:10,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 185141813051392.0
2023-01-07 08:29:10,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.6456492984408474e+17
2023-01-07 08:29:10,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.1192438757649285e+18
2023-01-07 08:29:10,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.131908188132606e+18
2023-01-07 08:29:10,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,316 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2634668690065326e+18
2023-01-07 08:29:10,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 2.323664993188315e+18
2023-01-07 08:29:10,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1.86885838209024e+16
2023-01-07 08:29:10,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3418385460059177e+18
2023-01-07 08:29:10,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.683309305372344e+18
2023-01-07 08:29:10,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,320 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1.86885838209024e+16
2023-01-07 08:29:10,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 514435378577408.0
2023-01-07 08:29:10,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,322 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 37582415069184.0
2023-01-07 08:29:10,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,323 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 13863845101568.0
2023-01-07 08:29:10,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,323 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 572493907623936.0
2023-01-07 08:29:10,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,325 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.820672448541491e+16
2023-01-07 08:29:10,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,326 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 7.95812428691538e+16
2023-01-07 08:29:10,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,327 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -220644012720128.0
2023-01-07 08:29:10,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.936060180922368e+16
2023-01-07 08:29:10,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,328 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 51894454059008.0
2023-01-07 08:29:10,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,329 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 7.84610982184878e+16
2023-01-07 08:29:10,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,330 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.84610982184878e+16
2023-01-07 08:29:10,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,331 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.594065429599355e+17
2023-01-07 08:29:10,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,332 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5652477452314214e+17
2023-01-07 08:29:10,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,334 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,334 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.752658252515246e+17
2023-01-07 08:29:10,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,335 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 9.547439482685358e+17
2023-01-07 08:29:10,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,336 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -4127495783710720.0
2023-01-07 08:29:10,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,336 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.506164503373414e+17
2023-01-07 08:29:10,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,338 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5156346882097152e+17
2023-01-07 08:29:10,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,339 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.4511004210453545e+18
2023-01-07 08:29:10,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0687013034076406e+18
2023-01-07 08:29:10,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0679232614920356e+18
2023-01-07 08:29:10,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,342 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.4511004210453545e+18
2023-01-07 08:29:10,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,342 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3823992550766674e+18
2023-01-07 08:29:10,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,343 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.761955173083906e+18
2023-01-07 08:29:10,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 9.733594360410276e+17
2023-01-07 08:29:10,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.390901112939217e+18
2023-01-07 08:29:10,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0777539419297546e+19
2023-01-07 08:29:10,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,348 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.155222670543264e+19
2023-01-07 08:29:10,352 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:29:10,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,353 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 2529.2041015625
2023-01-07 08:29:10,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,353 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,354 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,354 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.824629222679511e+17
2023-01-07 08:29:10,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,355 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6097446372101325e+17
2023-01-07 08:29:10,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,356 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.916025337951027e+16
2023-01-07 08:29:10,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,356 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.962465261571277e+16
2023-01-07 08:29:10,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,357 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1066818142208.0
2023-01-07 08:29:10,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,357 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 522274209792.0
2023-01-07 08:29:10,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,358 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 530160254976.0
2023-01-07 08:29:10,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,359 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 263162298368.0
2023-01-07 08:29:10,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,359 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 127538003968.0
2023-01-07 08:29:10,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,360 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77258907648.0
2023-01-07 08:29:10,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,360 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42855342080.0
2023-01-07 08:29:10,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 36786429952.0
2023-01-07 08:29:10,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 43117142016.0
2023-01-07 08:29:10,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.068103490648081e+25
2023-01-07 08:29:10,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8201350135773537e+23
2023-01-07 08:29:10,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.6256640110523856e+24
2023-01-07 08:29:10,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.6256634345916333e+24
2023-01-07 08:29:10,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.1768406182990286e+23
2023-01-07 08:29:10,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 8.203223855022215e+22
2023-01-07 08:29:10,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.10229197105484e+22
2023-01-07 08:29:10,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.985181536605343e+22
2023-01-07 08:29:10,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.0256720719555122e+22
2023-01-07 08:29:10,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.4814577804582626e+21
2023-01-07 08:29:10,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 2212.75830078125
2023-01-07 08:29:10,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.20381615088145e+20
2023-01-07 08:29:10,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.6029245218890554e+20
2023-01-07 08:29:10,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.880420186198219e+19
2023-01-07 08:29:10,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.9422558444337496e+19
2023-01-07 08:29:10,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.0042526891245568e+19
2023-01-07 08:29:10,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4297357045535867e+18
2023-01-07 08:29:10,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.217036776401535e+18
2023-01-07 08:29:10,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,372 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.318120230715392e+17
2023-01-07 08:29:10,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,372 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.1551135558087475e+17
2023-01-07 08:29:10,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.544523239135742
2023-01-07 08:29:10,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.565581206492283e+17
2023-01-07 08:29:10,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5725232480321536e+17
2023-01-07 08:29:10,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.85358392393728e+16
2023-01-07 08:29:10,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.943046265700352e+16
2023-01-07 08:29:10,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 101.00872802734375
2023-01-07 08:29:10,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,375 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.321344375610352
2023-01-07 08:29:10,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,376 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 15.382161140441895
2023-01-07 08:29:10,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,376 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 25.803836822509766
2023-01-07 08:29:10,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.89619064331055
2023-01-07 08:29:10,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,377 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.29530668258667
2023-01-07 08:29:10,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,377 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00244140625
2023-01-07 08:29:10,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 530.6142578125
2023-01-07 08:29:10,378 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,378 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 2529.2041015625
2023-01-07 08:29:10,378 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,379 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 2.8623297214508057
2023-01-07 08:29:10,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,379 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.706537246704102
2023-01-07 08:29:10,380 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,380 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 2.8623297214508057
2023-01-07 08:29:10,380 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.171111106872559
2023-01-07 08:29:10,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,381 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 291.4563903808594
2023-01-07 08:29:10,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,381 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 296.9892272949219
2023-01-07 08:29:10,382 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,383 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 291.4563903808594
2023-01-07 08:29:10,383 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:10,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,383 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 276.5920715332031
2023-01-07 08:29:10,384 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,384 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 9.733594360410276e+17
2023-01-07 08:29:10,384 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.95492172241211
2023-01-07 08:29:10,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,385 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 4.150926434910182e+19
2023-01-07 08:29:10,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.88152313232422
2023-01-07 08:29:10,386 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,386 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4.150926434910182e+19
2023-01-07 08:29:10,386 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,386 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 28.648033142089844
2023-01-07 08:29:10,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,387 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 335.13775634765625
2023-01-07 08:29:10,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,387 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 321.48785400390625
2023-01-07 08:29:10,388 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,388 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 28.648033142089844
2023-01-07 08:29:10,388 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,389 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 342.316162109375
2023-01-07 08:29:10,390 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,390 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 335.13775634765625
2023-01-07 08:29:10,390 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,390 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 23.979496002197266
2023-01-07 08:29:10,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,391 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -4127495783710720.0
2023-01-07 08:29:10,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 608.320556640625
2023-01-07 08:29:10,392 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,392 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 23.979496002197266
2023-01-07 08:29:10,392 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,392 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 991.90380859375
2023-01-07 08:29:10,393 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,393 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 4.4511004210453545e+18
2023-01-07 08:29:10,394 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,394 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 1531.92529296875
2023-01-07 08:29:10,395 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,395 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9.547439482685358e+17
2023-01-07 08:29:10,395 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,396 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 2233.576171875
2023-01-07 08:29:10,397 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,397 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -4127495783710720.0
2023-01-07 08:29:10,397 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,397 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 65.25479888916016
2023-01-07 08:29:10,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,398 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 4763.865234375
2023-01-07 08:29:10,398 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,399 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 65.25479888916016
2023-01-07 08:29:10,399 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 1.929361343383789
2023-01-07 08:29:10,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,399 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 1.5652477452314214e+17
2023-01-07 08:29:10,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,399 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 8793.361328125
2023-01-07 08:29:10,401 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,401 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 1.5652477452314214e+17
2023-01-07 08:29:10,401 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,401 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 121.50401306152344
2023-01-07 08:29:10,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,402 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 7.84610982184878e+16
2023-01-07 08:29:10,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 9282.5478515625
2023-01-07 08:29:10,402 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,403 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 121.50401306152344
2023-01-07 08:29:10,403 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,403 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,403 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 18172.7421875
2023-01-07 08:29:10,404 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,404 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 7.84610982184878e+16
2023-01-07 08:29:10,404 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:10,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,405 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -220644012720128.0
2023-01-07 08:29:10,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 15686.41796875
2023-01-07 08:29:10,406 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,406 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 7.95812428691538e+16
2023-01-07 08:29:10,406 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -11.16574478149414
2023-01-07 08:29:10,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,406 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -220644012720128.0
2023-01-07 08:29:10,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,407 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 29765.427734375
2023-01-07 08:29:10,408 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,408 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -220644012720128.0
2023-01-07 08:29:10,408 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,409 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 59985.9765625
2023-01-07 08:29:10,409 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,410 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 13863845101568.0
2023-01-07 08:29:10,410 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -7.883177757263184
2023-01-07 08:29:10,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,410 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 2.9330137875459604e+18
2023-01-07 08:29:10,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 108367.296875
2023-01-07 08:29:10,411 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,411 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 2.9330137875459604e+18
2023-01-07 08:29:10,412 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:29:10,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,412 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1962.916015625
2023-01-07 08:29:10,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,412 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 106317.6640625
2023-01-07 08:29:10,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1.86885838209024e+16
2023-01-07 08:29:10,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 108232.8125
2023-01-07 08:29:10,414 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,414 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1962.916015625
2023-01-07 08:29:10,414 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,414 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,414 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.9129638671875
2023-01-07 08:29:10,415 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,415 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 106317.6640625
2023-01-07 08:29:10,416 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.23777961730957
2023-01-07 08:29:10,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,416 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1.86885838209024e+16
2023-01-07 08:29:10,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 108821.625
2023-01-07 08:29:10,417 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,417 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 2.323664993188315e+18
2023-01-07 08:29:10,417 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,418 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 217488.25
2023-01-07 08:29:10,419 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,419 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 1.86885838209024e+16
2023-01-07 08:29:10,419 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,419 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,419 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,419 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 218596.671875
2023-01-07 08:29:10,420 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,420 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 1.1192438757649285e+18
2023-01-07 08:29:10,420 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,421 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -8.643059730529785
2023-01-07 08:29:10,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,421 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 7.52400644738279e+18
2023-01-07 08:29:10,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 434107.0625
2023-01-07 08:29:10,422 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,422 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 7.52400644738279e+18
2023-01-07 08:29:10,422 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,423 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 88170.421875
2023-01-07 08:29:10,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,423 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.824629222679511e+17
2023-01-07 08:29:10,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 460198.625
2023-01-07 08:29:10,424 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,424 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 88170.421875
2023-01-07 08:29:10,424 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.787897109985352
2023-01-07 08:29:10,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.824629222679511e+17
2023-01-07 08:29:10,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 870980.8125
2023-01-07 08:29:10,426 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,426 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 2.824629222679511e+17
2023-01-07 08:29:10,426 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,426 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,426 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -560423.875
2023-01-07 08:29:10,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,426 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6097446372101325e+17
2023-01-07 08:29:10,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 1742929.0
2023-01-07 08:29:10,427 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,428 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -560423.875
2023-01-07 08:29:10,428 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -20.516536712646484
2023-01-07 08:29:10,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,428 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6097446372101325e+17
2023-01-07 08:29:10,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 3484834.25
2023-01-07 08:29:10,429 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,429 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1.6097446372101325e+17
2023-01-07 08:29:10,429 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:10,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,430 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.916025337951027e+16
2023-01-07 08:29:10,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 6971438.5
2023-01-07 08:29:10,431 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,431 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 5.860891756278579e+16
2023-01-07 08:29:10,431 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -34.414127349853516
2023-01-07 08:29:10,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,431 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.916025337951027e+16
2023-01-07 08:29:10,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 13528259.0
2023-01-07 08:29:10,432 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,432 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 3.916025337951027e+16
2023-01-07 08:29:10,433 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,433 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 1410720.0
2023-01-07 08:29:10,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,433 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.962465261571277e+16
2023-01-07 08:29:10,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 27057946.0
2023-01-07 08:29:10,434 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,434 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 1410720.0
2023-01-07 08:29:10,434 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.0566999912261963
2023-01-07 08:29:10,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,435 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.962465261571277e+16
2023-01-07 08:29:10,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 52596632.0
2023-01-07 08:29:10,436 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,436 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 1.962465261571277e+16
2023-01-07 08:29:10,436 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,436 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 2821438.5
2023-01-07 08:29:10,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1066818142208.0
2023-01-07 08:29:10,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 54118488.0
2023-01-07 08:29:10,438 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,438 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 2821438.5
2023-01-07 08:29:10,438 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 15.858409881591797
2023-01-07 08:29:10,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,439 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1066818142208.0
2023-01-07 08:29:10,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 108234640.0
2023-01-07 08:29:10,440 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,440 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 1066818142208.0
2023-01-07 08:29:10,440 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:10,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,440 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 522274209792.0
2023-01-07 08:29:10,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 216231040.0
2023-01-07 08:29:10,441 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,441 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 9885446144.0
2023-01-07 08:29:10,441 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -46.32223129272461
2023-01-07 08:29:10,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 522274209792.0
2023-01-07 08:29:10,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 390462272.0
2023-01-07 08:29:10,443 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,443 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 522274209792.0
2023-01-07 08:29:10,443 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 45143024.0
2023-01-07 08:29:10,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 530160254976.0
2023-01-07 08:29:10,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 774969216.0
2023-01-07 08:29:10,445 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,445 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 45143024.0
2023-01-07 08:29:10,445 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 12.577384948730469
2023-01-07 08:29:10,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 530160254976.0
2023-01-07 08:29:10,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 1549933824.0
2023-01-07 08:29:10,446 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,447 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 530160254976.0
2023-01-07 08:29:10,447 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,447 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 16134028.0
2023-01-07 08:29:10,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,447 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 263162298368.0
2023-01-07 08:29:10,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 16144715.0
2023-01-07 08:29:10,448 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,449 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 16134028.0
2023-01-07 08:29:10,449 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.623693943023682
2023-01-07 08:29:10,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,449 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 263162298368.0
2023-01-07 08:29:10,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 32284304.0
2023-01-07 08:29:10,450 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,450 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 263162298368.0
2023-01-07 08:29:10,450 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,451 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 64524656.0
2023-01-07 08:29:10,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,451 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 127538003968.0
2023-01-07 08:29:10,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 64574440.0
2023-01-07 08:29:10,452 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,452 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 64524656.0
2023-01-07 08:29:10,452 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -25.3018798828125
2023-01-07 08:29:10,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 127538003968.0
2023-01-07 08:29:10,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 102348.703125
2023-01-07 08:29:10,454 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,454 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 127538003968.0
2023-01-07 08:29:10,454 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,454 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,454 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 129072240.0
2023-01-07 08:29:10,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,455 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77258907648.0
2023-01-07 08:29:10,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 129223000.0
2023-01-07 08:29:10,456 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,456 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 129072240.0
2023-01-07 08:29:10,456 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.52092742919922
2023-01-07 08:29:10,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,456 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77258907648.0
2023-01-07 08:29:10,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 258446464.0
2023-01-07 08:29:10,458 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,458 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 77258907648.0
2023-01-07 08:29:10,458 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:29:10,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,458 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.0948832780122757
2023-01-07 08:29:10,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,459 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42855342080.0
2023-01-07 08:29:10,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 33111.875
2023-01-07 08:29:10,460 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,460 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0948832780122757
2023-01-07 08:29:10,460 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.565258026123047
2023-01-07 08:29:10,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,460 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42855342080.0
2023-01-07 08:29:10,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 516926816.0
2023-01-07 08:29:10,461 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,462 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 42855342080.0
2023-01-07 08:29:10,462 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:29:10,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,462 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 1032394496.0
2023-01-07 08:29:10,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,462 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 36786429952.0
2023-01-07 08:29:10,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 1033860544.0
2023-01-07 08:29:10,463 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,463 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 1032394496.0
2023-01-07 08:29:10,464 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.985057830810547
2023-01-07 08:29:10,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,464 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 36786429952.0
2023-01-07 08:29:10,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 2067706624.0
2023-01-07 08:29:10,465 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,465 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 36786429952.0
2023-01-07 08:29:10,465 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,465 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,465 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 2065116032.0
2023-01-07 08:29:10,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 43117142016.0
2023-01-07 08:29:10,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 2064749440.0
2023-01-07 08:29:10,467 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,467 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 2065116032.0
2023-01-07 08:29:10,467 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.22715377807617188
2023-01-07 08:29:10,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 43117142016.0
2023-01-07 08:29:10,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 4129499136.0
2023-01-07 08:29:10,469 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,469 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 43117142016.0
2023-01-07 08:29:10,469 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:10,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,469 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.068103490648081e+25
2023-01-07 08:29:10,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 599612.4375
2023-01-07 08:29:10,470 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,470 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.01102466881275177
2023-01-07 08:29:10,471 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,471 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 8.128815650939941
2023-01-07 08:29:10,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.068103490648081e+25
2023-01-07 08:29:10,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 740274.25
2023-01-07 08:29:10,472 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,472 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 1.068103490648081e+25
2023-01-07 08:29:10,472 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:10,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,473 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8201350135773537e+23
2023-01-07 08:29:10,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9673359394073486
2023-01-07 08:29:10,474 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,474 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 5.252591047613068e+24
2023-01-07 08:29:10,474 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 67.7930679321289
2023-01-07 08:29:10,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8201350135773537e+23
2023-01-07 08:29:10,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 1491551.125
2023-01-07 08:29:10,475 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,475 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1.8201350135773537e+23
2023-01-07 08:29:10,475 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,475 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:10,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.6256640110523856e+24
2023-01-07 08:29:10,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 2937228.25
2023-01-07 08:29:10,477 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,477 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 2.6261315207225037e+24
2023-01-07 08:29:10,477 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,477 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,477 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 40.437259674072266
2023-01-07 08:29:10,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.6256640110523856e+24
2023-01-07 08:29:10,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 6137776.0
2023-01-07 08:29:10,478 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,479 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 2.6256640110523856e+24
2023-01-07 08:29:10,479 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,479 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,479 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:29:10,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 48058.15625
2023-01-07 08:29:10,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.6256634345916333e+24
2023-01-07 08:29:10,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 12000623.0
2023-01-07 08:29:10,480 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,481 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 48058.15625
2023-01-07 08:29:10,481 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 17.364486694335938
2023-01-07 08:29:10,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.6256634345916333e+24
2023-01-07 08:29:10,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 24007570.0
2023-01-07 08:29:10,482 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,482 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 2.6256634345916333e+24
2023-01-07 08:29:10,482 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,482 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:10,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,483 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.1768406182990286e+23
2023-01-07 08:29:10,483 > [DEBUG] 0 :: before allreduce fusion buffer :: -1603.341796875
2023-01-07 08:29:10,484 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,484 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 6.669397261419596e+23
2023-01-07 08:29:10,484 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.37203025817871
2023-01-07 08:29:10,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 3.1768406182990286e+23
2023-01-07 08:29:10,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 48014276.0
2023-01-07 08:29:10,486 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,486 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 3.1768406182990286e+23
2023-01-07 08:29:10,486 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:10,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 8.203223855022215e+22
2023-01-07 08:29:10,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 1346621.5
2023-01-07 08:29:10,487 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,487 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 1.640644771004443e+23
2023-01-07 08:29:10,487 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,488 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -14.687576293945312
2023-01-07 08:29:10,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 8.203223855022215e+22
2023-01-07 08:29:10,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 97366040.0
2023-01-07 08:29:10,489 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,489 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 8.203223855022215e+22
2023-01-07 08:29:10,489 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:10,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.10229197105484e+22
2023-01-07 08:29:10,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 194754656.0
2023-01-07 08:29:10,490 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,491 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.13355499505996704
2023-01-07 08:29:10,491 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -25.934267044067383
2023-01-07 08:29:10,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 4.10229197105484e+22
2023-01-07 08:29:10,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 333875008.0
2023-01-07 08:29:10,492 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,492 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 4.10229197105484e+22
2023-01-07 08:29:10,492 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,492 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:10,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.985181536605343e+22
2023-01-07 08:29:10,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 601477504.0
2023-01-07 08:29:10,494 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,494 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 4.168373288387248e+22
2023-01-07 08:29:10,494 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.774635314941406
2023-01-07 08:29:10,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 1.985181536605343e+22
2023-01-07 08:29:10,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 1001889536.0
2023-01-07 08:29:10,495 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,496 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 1.985181536605343e+22
2023-01-07 08:29:10,496 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:10,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.0256720719555122e+22
2023-01-07 08:29:10,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 1007911296.0
2023-01-07 08:29:10,497 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,497 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 1.0255187243882003e+22
2023-01-07 08:29:10,497 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 58.488250732421875
2023-01-07 08:29:10,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.0256720719555122e+22
2023-01-07 08:29:10,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 2015762560.0
2023-01-07 08:29:10,499 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,499 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 1.0256720719555122e+22
2023-01-07 08:29:10,499 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:10,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.4814577804582626e+21
2023-01-07 08:29:10,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 85368656.0
2023-01-07 08:29:10,500 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,501 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 5.21046661048406e+21
2023-01-07 08:29:10,501 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -14.990270614624023
2023-01-07 08:29:10,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.4814577804582626e+21
2023-01-07 08:29:10,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 4116915456.0
2023-01-07 08:29:10,502 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,502 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 2.4814577804582626e+21
2023-01-07 08:29:10,502 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:10,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.20381615088145e+20
2023-01-07 08:29:10,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 8233869312.0
2023-01-07 08:29:10,504 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,504 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 1.302616652621015e+21
2023-01-07 08:29:10,504 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,504 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 10.521615028381348
2023-01-07 08:29:10,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.20381615088145e+20
2023-01-07 08:29:10,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 16418732032.0
2023-01-07 08:29:10,505 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,506 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 6.20381615088145e+20
2023-01-07 08:29:10,506 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,506 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:10,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,506 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.6029245218890554e+20
2023-01-07 08:29:10,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 32837662720.0
2023-01-07 08:29:10,507 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,507 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 3.204746013713126e+20
2023-01-07 08:29:10,507 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -53.56951904296875
2023-01-07 08:29:10,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.6029245218890554e+20
2023-01-07 08:29:10,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05649509280920029
2023-01-07 08:29:10,509 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,509 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 1.6029245218890554e+20
2023-01-07 08:29:10,509 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:10,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,509 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.880420186198219e+19
2023-01-07 08:29:10,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 62947790848.0
2023-01-07 08:29:10,510 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,510 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 8.141354078881343e+19
2023-01-07 08:29:10,511 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 4.723761081695557
2023-01-07 08:29:10,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.880420186198219e+19
2023-01-07 08:29:10,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 374.5068054199219
2023-01-07 08:29:10,512 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,512 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 3.880420186198219e+19
2023-01-07 08:29:10,512 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:10,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.9422558444337496e+19
2023-01-07 08:29:10,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 125895581696.0
2023-01-07 08:29:10,514 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,514 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 4.070615906594167e+19
2023-01-07 08:29:10,514 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.061626434326172
2023-01-07 08:29:10,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1.9422558444337496e+19
2023-01-07 08:29:10,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 251791147008.0
2023-01-07 08:29:10,515 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,516 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 1.9422558444337496e+19
2023-01-07 08:29:10,516 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:10,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,516 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.0042526891245568e+19
2023-01-07 08:29:10,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 3154083840.0
2023-01-07 08:29:10,517 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,517 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 1.0018260669620552e+19
2023-01-07 08:29:10,517 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 48.568424224853516
2023-01-07 08:29:10,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1.0042526891245568e+19
2023-01-07 08:29:10,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 6308111360.0
2023-01-07 08:29:10,519 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,519 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 1.0042526891245568e+19
2023-01-07 08:29:10,519 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:10,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,519 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4297357045535867e+18
2023-01-07 08:29:10,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 11738.73046875
2023-01-07 08:29:10,520 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,520 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 5.088269333486895e+18
2023-01-07 08:29:10,521 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -41.97913360595703
2023-01-07 08:29:10,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4297357045535867e+18
2023-01-07 08:29:10,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 12616230912.0
2023-01-07 08:29:10,522 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,522 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 2.4297357045535867e+18
2023-01-07 08:29:10,522 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:10,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.217036776401535e+18
2023-01-07 08:29:10,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 15537205248.0
2023-01-07 08:29:10,524 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,524 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 2.5438257039760425e+18
2023-01-07 08:29:10,524 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -17.45341682434082
2023-01-07 08:29:10,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.217036776401535e+18
2023-01-07 08:29:10,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 19391008768.0
2023-01-07 08:29:10,525 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,525 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 1.217036776401535e+18
2023-01-07 08:29:10,526 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:10,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,526 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.318120230715392e+17
2023-01-07 08:29:10,526 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.834962010383606
2023-01-07 08:29:10,527 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,527 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 1.252194622772347e+18
2023-01-07 08:29:10,527 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.006181716918945
2023-01-07 08:29:10,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,528 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.318120230715392e+17
2023-01-07 08:29:10,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2476278394460678
2023-01-07 08:29:10,529 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,529 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 6.318120230715392e+17
2023-01-07 08:29:10,529 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,529 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:10,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,529 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.1551135558087475e+17
2023-01-07 08:29:10,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1916171312332153
2023-01-07 08:29:10,530 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,530 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 3.130016172112609e+17
2023-01-07 08:29:10,530 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,531 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 27.716506958007812
2023-01-07 08:29:10,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,531 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 3.1551135558087475e+17
2023-01-07 08:29:10,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3908030986785889
2023-01-07 08:29:10,532 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,532 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 3.1551135558087475e+17
2023-01-07 08:29:10,532 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:10,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,533 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.565581206492283e+17
2023-01-07 08:29:10,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6171578168869019
2023-01-07 08:29:10,534 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,534 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 72460443582464.0
2023-01-07 08:29:10,534 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,534 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -24.93017578125
2023-01-07 08:29:10,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,534 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.565581206492283e+17
2023-01-07 08:29:10,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.646246612071991
2023-01-07 08:29:10,535 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,536 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 1.565581206492283e+17
2023-01-07 08:29:10,536 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:10,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,536 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5725232480321536e+17
2023-01-07 08:29:10,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5936747789382935
2023-01-07 08:29:10,537 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,537 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 1028770295185408.0
2023-01-07 08:29:10,537 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -69.25334930419922
2023-01-07 08:29:10,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,538 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.5725232480321536e+17
2023-01-07 08:29:10,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42297279834747314
2023-01-07 08:29:10,539 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,539 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1.5725232480321536e+17
2023-01-07 08:29:10,539 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:10,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,539 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.85358392393728e+16
2023-01-07 08:29:10,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7590804696083069
2023-01-07 08:29:10,540 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,540 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 180622098169856.0
2023-01-07 08:29:10,540 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -13.785552978515625
2023-01-07 08:29:10,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,541 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.85358392393728e+16
2023-01-07 08:29:10,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8242514729499817
2023-01-07 08:29:10,542 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,542 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 7.85358392393728e+16
2023-01-07 08:29:10,542 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:10,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,543 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.943046265700352e+16
2023-01-07 08:29:10,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3449113368988037
2023-01-07 08:29:10,543 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,544 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 3.908443861680128e+16
2023-01-07 08:29:10,544 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,544 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,544 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -25.73187255859375
2023-01-07 08:29:10,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,544 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.943046265700352e+16
2023-01-07 08:29:10,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17122328281402588
2023-01-07 08:29:10,545 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,545 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 3.943046265700352e+16
2023-01-07 08:29:10,545 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:10,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,546 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.321344375610352
2023-01-07 08:29:10,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12400393187999725
2023-01-07 08:29:10,547 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,547 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.0568917840719223
2023-01-07 08:29:10,547 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -16.042068481445312
2023-01-07 08:29:10,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,547 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.321344375610352
2023-01-07 08:29:10,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4232463240623474
2023-01-07 08:29:10,549 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,549 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 14.321344375610352
2023-01-07 08:29:10,549 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,549 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,549 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:10,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,549 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 15.382161140441895
2023-01-07 08:29:10,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3642979562282562
2023-01-07 08:29:10,550 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,550 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.282329797744751
2023-01-07 08:29:10,550 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.24941635131836
2023-01-07 08:29:10,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,551 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 15.382161140441895
2023-01-07 08:29:10,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01190393790602684
2023-01-07 08:29:10,552 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,552 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 15.382161140441895
2023-01-07 08:29:10,552 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,552 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:10,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,552 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 25.803836822509766
2023-01-07 08:29:10,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11776624619960785
2023-01-07 08:29:10,553 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,554 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.003019854426383972
2023-01-07 08:29:10,554 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.0634317398071289
2023-01-07 08:29:10,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,554 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 25.803836822509766
2023-01-07 08:29:10,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5668952465057373
2023-01-07 08:29:10,555 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,555 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 25.803836822509766
2023-01-07 08:29:10,555 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,555 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:10,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,556 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.29530668258667
2023-01-07 08:29:10,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15335580706596375
2023-01-07 08:29:10,557 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,557 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.01937050372362137
2023-01-07 08:29:10,557 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.21839141845703
2023-01-07 08:29:10,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,557 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.29530668258667
2023-01-07 08:29:10,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38227415084838867
2023-01-07 08:29:10,558 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,559 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 7.29530668258667
2023-01-07 08:29:10,559 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:10,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,559 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00244140625
2023-01-07 08:29:10,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.012871503829956
2023-01-07 08:29:10,560 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,560 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.2120128571987152
2023-01-07 08:29:10,560 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -12.843450546264648
2023-01-07 08:29:10,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,561 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00244140625
2023-01-07 08:29:10,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4829287528991699
2023-01-07 08:29:10,562 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:29:10,562 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.00244140625
2023-01-07 08:29:10,562 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:29:10,563 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:29:10,563 > [DEBUG] 0 :: 7.536781311035156
2023-01-07 08:29:10,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,567 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.032196044921875
2023-01-07 08:29:10,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -321.10235595703125
2023-01-07 08:29:10,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,570 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.39259999990463257
2023-01-07 08:29:10,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,571 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.032196044921875
2023-01-07 08:29:10,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -349.62957763671875
2023-01-07 08:29:10,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.9520721435546875
2023-01-07 08:29:10,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02174948900938034
2023-01-07 08:29:10,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.0026746876537799835
2023-01-07 08:29:10,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,578 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.9520721435546875
2023-01-07 08:29:10,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8157380819320679
2023-01-07 08:29:10,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,580 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -9.324568748474121
2023-01-07 08:29:10,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1729142665863037
2023-01-07 08:29:10,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,581 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.04003041982650757
2023-01-07 08:29:10,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,581 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -9.324568748474121
2023-01-07 08:29:10,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14216266572475433
2023-01-07 08:29:10,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,583 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -6.210546493530273
2023-01-07 08:29:10,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19429296255111694
2023-01-07 08:29:10,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,584 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2728731632232666
2023-01-07 08:29:10,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,584 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -6.210546493530273
2023-01-07 08:29:10,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05775481462478638
2023-01-07 08:29:10,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,586 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 1.4137858152389526
2023-01-07 08:29:10,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3080369830131531
2023-01-07 08:29:10,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,587 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.005335971713066101
2023-01-07 08:29:10,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,588 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 1.4137858152389526
2023-01-07 08:29:10,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.377936840057373
2023-01-07 08:29:10,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,589 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.916817665100098
2023-01-07 08:29:10,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13325035572052002
2023-01-07 08:29:10,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,590 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.04586605727672577
2023-01-07 08:29:10,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,592 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.916817665100098
2023-01-07 08:29:10,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3996732532978058
2023-01-07 08:29:10,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,593 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.846057653427124
2023-01-07 08:29:10,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9356164336204529
2023-01-07 08:29:10,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,594 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.5084009170532227
2023-01-07 08:29:10,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,595 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.846057653427124
2023-01-07 08:29:10,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.435607671737671
2023-01-07 08:29:10,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,596 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -19.236778259277344
2023-01-07 08:29:10,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7578385472297668
2023-01-07 08:29:10,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,598 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.24182318150997162
2023-01-07 08:29:10,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,598 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -19.236778259277344
2023-01-07 08:29:10,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24246272444725037
2023-01-07 08:29:10,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,599 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.466245412826538
2023-01-07 08:29:10,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13102975487709045
2023-01-07 08:29:10,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,601 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.057924337685108185
2023-01-07 08:29:10,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,601 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.466245412826538
2023-01-07 08:29:10,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3213627338409424
2023-01-07 08:29:10,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,602 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2456.914794921875
2023-01-07 08:29:10,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5085861682891846
2023-01-07 08:29:10,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.02310916781425476
2023-01-07 08:29:10,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2456.914794921875
2023-01-07 08:29:10,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0053530931472778
2023-01-07 08:29:10,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,606 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2959.0322265625
2023-01-07 08:29:10,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2523438930511475
2023-01-07 08:29:10,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,607 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.031328313052654266
2023-01-07 08:29:10,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,607 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2959.0322265625
2023-01-07 08:29:10,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6527117490768433
2023-01-07 08:29:10,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1352.447021484375
2023-01-07 08:29:10,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4266996383666992
2023-01-07 08:29:10,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,610 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.10315579921007156
2023-01-07 08:29:10,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1352.447021484375
2023-01-07 08:29:10,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2758982181549072
2023-01-07 08:29:10,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 3890.021240234375
2023-01-07 08:29:10,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5755720138549805
2023-01-07 08:29:10,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.09765912592411041
2023-01-07 08:29:10,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 3890.021240234375
2023-01-07 08:29:10,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7229163646697998
2023-01-07 08:29:10,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1949.244873046875
2023-01-07 08:29:10,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4916260242462158
2023-01-07 08:29:10,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.0706426352262497
2023-01-07 08:29:10,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 1949.244873046875
2023-01-07 08:29:10,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45233774185180664
2023-01-07 08:29:10,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4278.80078125
2023-01-07 08:29:10,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0374794006347656
2023-01-07 08:29:10,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.05061455816030502
2023-01-07 08:29:10,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4278.80078125
2023-01-07 08:29:10,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9525513648986816
2023-01-07 08:29:10,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,621 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 1716.0185546875
2023-01-07 08:29:10,621 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.017512768507003784
2023-01-07 08:29:10,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.11949886381626129
2023-01-07 08:29:10,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 1716.0185546875
2023-01-07 08:29:10,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5032612085342407
2023-01-07 08:29:10,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,624 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -647.1238403320312
2023-01-07 08:29:10,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.238199383020401
2023-01-07 08:29:10,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.09637409448623657
2023-01-07 08:29:10,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,626 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -647.1238403320312
2023-01-07 08:29:10,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0601565837860107
2023-01-07 08:29:10,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,627 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -7.888667106628418
2023-01-07 08:29:10,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24834713339805603
2023-01-07 08:29:10,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.001916937530040741
2023-01-07 08:29:10,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -7.888667106628418
2023-01-07 08:29:10,629 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4044007658958435
2023-01-07 08:29:10,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 13.086947441101074
2023-01-07 08:29:10,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.784787654876709
2023-01-07 08:29:10,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.08747285604476929
2023-01-07 08:29:10,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,632 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 13.086947441101074
2023-01-07 08:29:10,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.529646635055542
2023-01-07 08:29:10,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -0.22751998901367188
2023-01-07 08:29:10,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4368197917938232
2023-01-07 08:29:10,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.3321027159690857
2023-01-07 08:29:10,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -0.22751998901367188
2023-01-07 08:29:10,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0258069038391113
2023-01-07 08:29:10,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,636 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.277394771575928
2023-01-07 08:29:10,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37739136815071106
2023-01-07 08:29:10,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.15693062543869019
2023-01-07 08:29:10,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -6.277394771575928
2023-01-07 08:29:10,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7972079515457153
2023-01-07 08:29:10,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -3.202914237976074
2023-01-07 08:29:10,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5283575654029846
2023-01-07 08:29:10,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.08434849977493286
2023-01-07 08:29:10,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -3.202914237976074
2023-01-07 08:29:10,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.344805717468262
2023-01-07 08:29:10,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 3.5548298358917236
2023-01-07 08:29:10,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2875233888626099
2023-01-07 08:29:10,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.25036484003067017
2023-01-07 08:29:10,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 3.5548298358917236
2023-01-07 08:29:10,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.152862310409546
2023-01-07 08:29:10,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -27.89588737487793
2023-01-07 08:29:10,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2545863687992096
2023-01-07 08:29:10,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.2494657039642334
2023-01-07 08:29:10,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -27.89588737487793
2023-01-07 08:29:10,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6777389645576477
2023-01-07 08:29:10,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,649 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -22.413177490234375
2023-01-07 08:29:10,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8423107862472534
2023-01-07 08:29:10,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -22.413177490234375
2023-01-07 08:29:10,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3339429795742035
2023-01-07 08:29:10,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,652 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -46.93887710571289
2023-01-07 08:29:10,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8783174753189087
2023-01-07 08:29:10,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.6038140058517456
2023-01-07 08:29:10,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -46.93887710571289
2023-01-07 08:29:10,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4583185315132141
2023-01-07 08:29:10,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -114.44596099853516
2023-01-07 08:29:10,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.097484588623047
2023-01-07 08:29:10,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.6447591781616211
2023-01-07 08:29:10,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -114.44596099853516
2023-01-07 08:29:10,656 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.550935745239258
2023-01-07 08:29:10,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -76.70427703857422
2023-01-07 08:29:10,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5838266611099243
2023-01-07 08:29:10,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.4016427993774414
2023-01-07 08:29:10,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -76.70427703857422
2023-01-07 08:29:10,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.71113395690918
2023-01-07 08:29:10,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -93.76675415039062
2023-01-07 08:29:10,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.916728496551514
2023-01-07 08:29:10,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -93.76675415039062
2023-01-07 08:29:10,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.03510093688965
2023-01-07 08:29:10,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -39.77537155151367
2023-01-07 08:29:10,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1341259479522705
2023-01-07 08:29:10,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -39.77537155151367
2023-01-07 08:29:10,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.132314682006836
2023-01-07 08:29:10,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,666 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -11.333372116088867
2023-01-07 08:29:10,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3912791013717651
2023-01-07 08:29:10,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,667 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -11.333372116088867
2023-01-07 08:29:10,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2646429538726807
2023-01-07 08:29:10,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,669 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 17.562849044799805
2023-01-07 08:29:10,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.248358726501465
2023-01-07 08:29:10,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,670 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 17.562849044799805
2023-01-07 08:29:10,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.818340301513672
2023-01-07 08:29:10,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,672 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 15.19881820678711
2023-01-07 08:29:10,672 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.4758100509643555
2023-01-07 08:29:10,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 15.19881820678711
2023-01-07 08:29:10,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.466825008392334
2023-01-07 08:29:10,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,674 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -33.39212417602539
2023-01-07 08:29:10,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.264080047607422
2023-01-07 08:29:10,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -33.39212417602539
2023-01-07 08:29:10,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9445838928222656
2023-01-07 08:29:10,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -26.068788528442383
2023-01-07 08:29:10,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9261133670806885
2023-01-07 08:29:10,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,678 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -26.068788528442383
2023-01-07 08:29:10,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.319647789001465
2023-01-07 08:29:10,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -29.879680633544922
2023-01-07 08:29:10,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.197718620300293
2023-01-07 08:29:10,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,681 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.9123004078865051
2023-01-07 08:29:10,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,681 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -29.879680633544922
2023-01-07 08:29:10,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.394213676452637
2023-01-07 08:29:10,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,683 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 8.741393089294434
2023-01-07 08:29:10,683 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35193130373954773
2023-01-07 08:29:10,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,684 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 8.741393089294434
2023-01-07 08:29:10,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.938127040863037
2023-01-07 08:29:10,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -73.36515045166016
2023-01-07 08:29:10,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44774580001831055
2023-01-07 08:29:10,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -73.36515045166016
2023-01-07 08:29:10,687 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.583812713623047
2023-01-07 08:29:10,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 28.394546508789062
2023-01-07 08:29:10,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.293994426727295
2023-01-07 08:29:10,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.7109527587890625
2023-01-07 08:29:10,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 28.394546508789062
2023-01-07 08:29:10,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.759357452392578
2023-01-07 08:29:10,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.739822387695312
2023-01-07 08:29:10,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9679784774780273
2023-01-07 08:29:10,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.739822387695312
2023-01-07 08:29:10,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6125822067260742
2023-01-07 08:29:10,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -4.232198715209961
2023-01-07 08:29:10,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0262562036514282
2023-01-07 08:29:10,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -4.232198715209961
2023-01-07 08:29:10,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.356829643249512
2023-01-07 08:29:10,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.62877655029297
2023-01-07 08:29:10,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.8716991543769836
2023-01-07 08:29:10,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 145.12245178222656
2023-01-07 08:29:10,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.696824073791504
2023-01-07 08:29:10,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,701 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.2517332136631012
2023-01-07 08:29:10,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.122747421264648
2023-01-07 08:29:10,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.735687255859375
2023-01-07 08:29:10,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.015352249145508
2023-01-07 08:29:10,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,704 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.122747421264648
2023-01-07 08:29:10,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.506209373474121
2023-01-07 08:29:10,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.839378356933594
2023-01-07 08:29:10,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,707 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -4.318779468536377
2023-01-07 08:29:10,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,707 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.883727073669434
2023-01-07 08:29:10,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,709 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.355255126953125
2023-01-07 08:29:10,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.6552633047103882
2023-01-07 08:29:10,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 72.80635070800781
2023-01-07 08:29:10,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.581987380981445
2023-01-07 08:29:10,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,712 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.201114654541016
2023-01-07 08:29:10,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 67.3372802734375
2023-01-07 08:29:10,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,714 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.031200408935547
2023-01-07 08:29:10,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,715 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.376138687133789
2023-01-07 08:29:10,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,716 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 107.85496520996094
2023-01-07 08:29:10,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.405452728271484
2023-01-07 08:29:10,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,718 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,718 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.935335159301758
2023-01-07 08:29:10,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,719 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.4083006381988525
2023-01-07 08:29:10,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,720 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 73.75836181640625
2023-01-07 08:29:10,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,720 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,720 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.741180419921875
2023-01-07 08:29:10,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,722 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.88734245300293
2023-01-07 08:29:10,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,723 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 35.577423095703125
2023-01-07 08:29:10,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 83.2939453125
2023-01-07 08:29:10,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,724 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 169.55145263671875
2023-01-07 08:29:10,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0474207401275635
2023-01-07 08:29:10,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,726 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 35.577423095703125
2023-01-07 08:29:10,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,726 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 358.2369689941406
2023-01-07 08:29:10,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.29267120361328
2023-01-07 08:29:10,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,727 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 273.970458984375
2023-01-07 08:29:10,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 203.3003387451172
2023-01-07 08:29:10,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,729 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -4.506017684936523
2023-01-07 08:29:10,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,729 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 273.970458984375
2023-01-07 08:29:10,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 135.76132202148438
2023-01-07 08:29:10,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,730 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 273.970458984375
2023-01-07 08:29:10,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 93.67514038085938
2023-01-07 08:29:10,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:29:10,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:29:10,732 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 273.970458984375
2023-01-07 08:29:10,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 97.68130493164062
