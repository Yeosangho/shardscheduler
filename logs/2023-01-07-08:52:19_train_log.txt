2023-01-07 08:52:26,984 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:52:26,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,024 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:52:27,024 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,024 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:52:27,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,881 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,881 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,881 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,881 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:52:27,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,884 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:52:27,884 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,884 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:52:27,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,885 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,885 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,885 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,885 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:52:27,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,886 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:52:27,886 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,886 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,886 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:52:27,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,923 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,924 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,924 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,924 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:52:27,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,925 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,925 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,925 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,925 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:52:27,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,926 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,926 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,927 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,927 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:52:27,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,928 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,928 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,928 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,928 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:52:27,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,929 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,929 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,929 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,929 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:52:27,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,930 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,930 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,930 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,930 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:52:27,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,931 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,931 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,931 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,931 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:52:27,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,932 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:52:27,932 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,933 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:52:27,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,933 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,934 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,934 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,934 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:52:27,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,935 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,935 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,935 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,935 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:52:27,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,936 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,936 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,936 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,936 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:52:27,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,937 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,937 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,937 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,937 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:52:27,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,938 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,938 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,938 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,938 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:52:27,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,940 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:52:27,940 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,940 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,940 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:52:27,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,941 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:52:27,941 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,941 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,941 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:52:27,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,942 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:52:27,942 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,942 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,942 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:52:27,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,943 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,943 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,943 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,943 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:52:27,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,945 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:52:27,945 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,945 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,945 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:52:27,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,946 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,946 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,946 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,946 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:52:27,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,947 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:52:27,947 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,947 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,947 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:52:27,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,948 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,949 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,949 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,949 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:52:27,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,950 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,950 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,950 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,950 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:52:27,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,951 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:27,951 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,951 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,951 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:52:27,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,952 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:52:27,952 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,952 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,952 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:52:27,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,953 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:27,953 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,954 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:52:27,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,955 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,955 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,955 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,955 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:52:27,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,956 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,956 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,956 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:52:27,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,957 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:52:27,957 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,957 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:52:27,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,959 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,959 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,959 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,959 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:52:27,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,960 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,960 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,960 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,960 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:52:27,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,961 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:27,961 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,961 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,961 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:52:27,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,962 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,962 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,962 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:52:27,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,963 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,963 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,964 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,964 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:52:27,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,965 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:52:27,965 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,965 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:52:27,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,966 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,966 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,966 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,966 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:52:27,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,967 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,967 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,967 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:52:27,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,968 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:27,968 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,968 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,968 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:52:27,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,970 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,970 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,970 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:52:27,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,971 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,971 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,971 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:52:27,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,972 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:52:27,972 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,972 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,972 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:52:27,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,973 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:52:27,973 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,973 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:52:27,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,974 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:52:27,974 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,974 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,975 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:52:27,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,975 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:27,976 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,976 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:52:27,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,977 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:52:27,977 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,977 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,977 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:52:27,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,978 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,978 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,978 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:52:27,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,979 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:27,980 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,980 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,980 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:52:27,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,981 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,981 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,981 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,981 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:52:27,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,982 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:27,982 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,982 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,982 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:52:27,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,983 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:27,983 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,983 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,983 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:52:27,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,984 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:52:27,984 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,984 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,984 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:52:27,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,985 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:27,985 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,986 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,986 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:52:27,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,987 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:27,987 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,987 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,987 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:52:27,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,988 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,988 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,988 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,988 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:52:27,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,989 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:27,989 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,989 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,989 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:52:27,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,991 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,991 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,991 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,991 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:52:27,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,992 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:27,992 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,992 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,992 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:52:27,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,993 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:27,993 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,993 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,993 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:52:27,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,994 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:27,994 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,994 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,994 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:52:27,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,995 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,995 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,995 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,995 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:52:27,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,997 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:27,997 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,997 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,997 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:52:27,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,998 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:27,998 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,998 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,998 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:52:27,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:27,999 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:27,999 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:27,999 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:27,999 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:52:27,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,000 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:28,000 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,000 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,000 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:52:28,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,001 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,001 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,001 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,001 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:52:28,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,002 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,003 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,003 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,003 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:52:28,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,004 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:28,004 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,004 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,004 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:52:28,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,005 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,005 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,005 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,005 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:52:28,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,006 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,006 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,006 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,006 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:52:28,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,007 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:28,007 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,007 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,007 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:52:28,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,008 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,009 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,009 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,009 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:52:28,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,010 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,010 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,010 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,010 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:52:28,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,011 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:28,011 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,011 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,011 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:52:28,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,012 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,012 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,012 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,012 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:52:28,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,013 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,013 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,013 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,013 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:52:28,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,014 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:28,014 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,014 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,014 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:52:28,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,016 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,016 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,016 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,016 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:52:28,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,017 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,017 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,017 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,017 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:52:28,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,018 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:52:28,018 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,018 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,018 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:52:28,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,019 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:52:28,019 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,019 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,019 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:52:28,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,020 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:52:28,020 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,020 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,020 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:52:28,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,021 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:52:28,021 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,021 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,022 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:52:28,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,023 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:52:28,023 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,023 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,023 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:52:28,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,024 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,024 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,024 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:52:28,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,025 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:52:28,025 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,025 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,025 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:52:28,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,026 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,026 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,027 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,027 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:52:28,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,028 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:52:28,028 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,028 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,028 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:52:28,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,029 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:52:28,029 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,029 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:52:28,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,030 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:52:28,030 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,030 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,030 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:52:28,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,031 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:52:28,031 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,031 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,031 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:52:28,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,032 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:52:28,032 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,032 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:52:28,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,033 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,033 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,033 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,034 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:52:28,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,035 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:52:28,035 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,035 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,035 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:52:28,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,036 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,036 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,036 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,036 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:52:28,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,037 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:52:28,037 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,037 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,037 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:52:28,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,038 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:52:28,038 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,038 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,039 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:52:28,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,040 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:52:28,040 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,040 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,040 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:52:28,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,041 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,041 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,041 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,041 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:52:28,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,042 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:52:28,042 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,042 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,042 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:52:28,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,043 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:52:28,043 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,043 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,043 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:52:28,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,044 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:52:28,044 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,044 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,044 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:52:28,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,045 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:52:28,045 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,045 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,045 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:52:28,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:52:28,047 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:52:28,047 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,047 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:52:28,047 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:52:28,048 > [DEBUG] 0 :: 7.943153381347656
2023-01-07 08:52:28,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,054 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,054 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.038360595703125
2023-01-07 08:52:28,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -370.0415954589844
2023-01-07 08:52:28,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,057 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,057 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.501874566078186
2023-01-07 08:52:28,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,058 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -178.13360595703125
2023-01-07 08:52:28,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -362.71173095703125
2023-01-07 08:52:28,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,068 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,068 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.816044330596924
2023-01-07 08:52:28,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2532932758331299
2023-01-07 08:52:28,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,069 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,070 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.037850506603717804
2023-01-07 08:52:28,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,070 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.965510845184326
2023-01-07 08:52:28,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03430645167827606
2023-01-07 08:52:28,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,072 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,072 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.714324474334717
2023-01-07 08:52:28,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11180668324232101
2023-01-07 08:52:28,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,073 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,074 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.01051270216703415
2023-01-07 08:52:28,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,074 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.93245267868042
2023-01-07 08:52:28,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4418802559375763
2023-01-07 08:52:28,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,076 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,076 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.45110023021698
2023-01-07 08:52:28,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5193107724189758
2023-01-07 08:52:28,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,077 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.5882007479667664
2023-01-07 08:52:28,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -2.9766836166381836
2023-01-07 08:52:28,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.056253302842378616
2023-01-07 08:52:28,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,079 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,079 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -6.279018402099609
2023-01-07 08:52:28,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1298217475414276
2023-01-07 08:52:28,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,080 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.0229632630944252
2023-01-07 08:52:28,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,080 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -5.640895843505859
2023-01-07 08:52:28,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11020474880933762
2023-01-07 08:52:28,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,082 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11.391979217529297
2023-01-07 08:52:28,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7643570303916931
2023-01-07 08:52:28,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,083 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,083 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.032234326004981995
2023-01-07 08:52:28,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,084 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11.20455551147461
2023-01-07 08:52:28,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18004503846168518
2023-01-07 08:52:28,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,085 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,085 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 9.749242782592773
2023-01-07 08:52:28,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07537700980901718
2023-01-07 08:52:28,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,086 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,086 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.4786345660686493
2023-01-07 08:52:28,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,087 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 11.864717483520508
2023-01-07 08:52:28,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17958346009254456
2023-01-07 08:52:28,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,088 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,089 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 19.232036590576172
2023-01-07 08:52:28,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2283181995153427
2023-01-07 08:52:28,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,090 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,090 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.37050914764404297
2023-01-07 08:52:28,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,090 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 21.380931854248047
2023-01-07 08:52:28,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35428550839424133
2023-01-07 08:52:28,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,092 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.8357658386230469
2023-01-07 08:52:28,092 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14445127546787262
2023-01-07 08:52:28,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,093 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,093 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.03354337811470032
2023-01-07 08:52:28,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,093 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.67887544631958
2023-01-07 08:52:28,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4825479984283447
2023-01-07 08:52:28,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,095 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,095 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -45.049346923828125
2023-01-07 08:52:28,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.38307881355285645
2023-01-07 08:52:28,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,096 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,097 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0491141676902771
2023-01-07 08:52:28,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,097 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -44.698524475097656
2023-01-07 08:52:28,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1403927505016327
2023-01-07 08:52:28,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,099 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,099 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -14.761212348937988
2023-01-07 08:52:28,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5099688172340393
2023-01-07 08:52:28,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,100 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.18052536249160767
2023-01-07 08:52:28,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,100 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -19.142333984375
2023-01-07 08:52:28,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7096691131591797
2023-01-07 08:52:28,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,102 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.085563659667969
2023-01-07 08:52:28,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0559651851654053
2023-01-07 08:52:28,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,103 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.036325402557849884
2023-01-07 08:52:28,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,104 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.068335771560669
2023-01-07 08:52:28,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.165539026260376
2023-01-07 08:52:28,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,106 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -7.5075578689575195
2023-01-07 08:52:28,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9277446269989014
2023-01-07 08:52:28,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,107 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,107 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.1154438853263855
2023-01-07 08:52:28,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,107 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -9.45611572265625
2023-01-07 08:52:28,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0787353515625
2023-01-07 08:52:28,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -9.3870849609375
2023-01-07 08:52:28,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6060006618499756
2023-01-07 08:52:28,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,110 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,110 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.005536630749702454
2023-01-07 08:52:28,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.466909408569336
2023-01-07 08:52:28,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2580702304840088
2023-01-07 08:52:28,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.867560386657715
2023-01-07 08:52:28,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.113804817199707
2023-01-07 08:52:28,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,113 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.0899994820356369
2023-01-07 08:52:28,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.59592342376709
2023-01-07 08:52:28,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6477311849594116
2023-01-07 08:52:28,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,115 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 9.334091186523438
2023-01-07 08:52:28,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.822206974029541
2023-01-07 08:52:28,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,116 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.14188966155052185
2023-01-07 08:52:28,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 9.882915496826172
2023-01-07 08:52:28,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0189976692199707
2023-01-07 08:52:28,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1.6086266040802002
2023-01-07 08:52:28,118 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3857985734939575
2023-01-07 08:52:28,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,119 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.35425230860710144
2023-01-07 08:52:28,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -3.0020105838775635
2023-01-07 08:52:28,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4669530391693115
2023-01-07 08:52:28,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,121 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.622559547424316
2023-01-07 08:52:28,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6344266533851624
2023-01-07 08:52:28,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,122 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.015326842665672302
2023-01-07 08:52:28,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.559288024902344
2023-01-07 08:52:28,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3091237545013428
2023-01-07 08:52:28,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,124 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,124 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -25.171756744384766
2023-01-07 08:52:28,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.053943708539009094
2023-01-07 08:52:28,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,126 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.05050685256719589
2023-01-07 08:52:28,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -28.094852447509766
2023-01-07 08:52:28,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.095243215560913
2023-01-07 08:52:28,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,127 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -15.974021911621094
2023-01-07 08:52:28,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27919042110443115
2023-01-07 08:52:28,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,129 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.189832866191864
2023-01-07 08:52:28,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -16.508331298828125
2023-01-07 08:52:28,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0890941172838211
2023-01-07 08:52:28,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,130 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -13.264406204223633
2023-01-07 08:52:28,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09304860234260559
2023-01-07 08:52:28,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.08615165948867798
2023-01-07 08:52:28,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 4.8465046882629395
2023-01-07 08:52:28,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2559735774993896
2023-01-07 08:52:28,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,133 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 24.573701858520508
2023-01-07 08:52:28,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4016382098197937
2023-01-07 08:52:28,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,135 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.05447566509246826
2023-01-07 08:52:28,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 45.17412567138672
2023-01-07 08:52:28,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.105040550231934
2023-01-07 08:52:28,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,137 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.61210823059082
2023-01-07 08:52:28,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.659731149673462
2023-01-07 08:52:28,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,138 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2954521179199219
2023-01-07 08:52:28,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 1.2980070114135742
2023-01-07 08:52:28,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.6345930099487305
2023-01-07 08:52:28,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,140 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -6.973657608032227
2023-01-07 08:52:28,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7898162603378296
2023-01-07 08:52:28,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,141 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.5298802852630615
2023-01-07 08:52:28,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 11.13577651977539
2023-01-07 08:52:28,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.250255107879639
2023-01-07 08:52:28,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -27.145938873291016
2023-01-07 08:52:28,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04689127206802368
2023-01-07 08:52:28,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 0.873291015625
2023-01-07 08:52:28,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.415556907653809
2023-01-07 08:52:28,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,146 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,146 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 25.94719886779785
2023-01-07 08:52:28,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.167552947998047
2023-01-07 08:52:28,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,147 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.8781173229217529
2023-01-07 08:52:28,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 48.550376892089844
2023-01-07 08:52:28,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.782106399536133
2023-01-07 08:52:28,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,149 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 28.496232986450195
2023-01-07 08:52:28,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8410325050354004
2023-01-07 08:52:28,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,151 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.005602136254310608
2023-01-07 08:52:28,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,152 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 49.830894470214844
2023-01-07 08:52:28,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.641473770141602
2023-01-07 08:52:28,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,153 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,153 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 25.332387924194336
2023-01-07 08:52:28,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9856542348861694
2023-01-07 08:52:28,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,154 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.2223091423511505
2023-01-07 08:52:28,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 45.62853240966797
2023-01-07 08:52:28,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.341798782348633
2023-01-07 08:52:28,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,157 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,157 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.22581100463867
2023-01-07 08:52:28,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0603909492492676
2023-01-07 08:52:28,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,158 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -34.57955551147461
2023-01-07 08:52:28,158 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18914222717285156
2023-01-07 08:52:28,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,160 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,160 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 19.227205276489258
2023-01-07 08:52:28,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.0224609375
2023-01-07 08:52:28,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,161 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 35.076316833496094
2023-01-07 08:52:28,161 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.099942684173584
2023-01-07 08:52:28,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,163 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,163 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 11.229779243469238
2023-01-07 08:52:28,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.807223320007324
2023-01-07 08:52:28,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,164 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 47.920494079589844
2023-01-07 08:52:28,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.5704779624938965
2023-01-07 08:52:28,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,165 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,166 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -100.25810241699219
2023-01-07 08:52:28,166 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.69788122177124
2023-01-07 08:52:28,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,167 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -87.14940643310547
2023-01-07 08:52:28,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.108932495117188
2023-01-07 08:52:28,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,168 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,169 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -32.29278564453125
2023-01-07 08:52:28,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1811933517456055
2023-01-07 08:52:28,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,170 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -31.329317092895508
2023-01-07 08:52:28,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.228235244750977
2023-01-07 08:52:28,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,171 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,171 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -21.44442367553711
2023-01-07 08:52:28,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7982815504074097
2023-01-07 08:52:28,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -77.07564544677734
2023-01-07 08:52:28,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7614197731018066
2023-01-07 08:52:28,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,174 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,174 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 115.02717590332031
2023-01-07 08:52:28,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7032434940338135
2023-01-07 08:52:28,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 88.10892486572266
2023-01-07 08:52:28,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.1537971496582
2023-01-07 08:52:28,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,177 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,177 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 44.97315979003906
2023-01-07 08:52:28,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.313926696777344
2023-01-07 08:52:28,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,178 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,178 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.4910793900489807
2023-01-07 08:52:28,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,178 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 89.71885681152344
2023-01-07 08:52:28,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.10573673248291
2023-01-07 08:52:28,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,180 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,180 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -54.89015197753906
2023-01-07 08:52:28,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6318817138671875
2023-01-07 08:52:28,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,181 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -64.27822875976562
2023-01-07 08:52:28,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.067785739898682
2023-01-07 08:52:28,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,182 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,183 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -196.95721435546875
2023-01-07 08:52:28,183 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.683710098266602
2023-01-07 08:52:28,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,184 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -177.14312744140625
2023-01-07 08:52:28,184 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.614410400390625
2023-01-07 08:52:28,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,185 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,185 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -50.78657150268555
2023-01-07 08:52:28,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.618247032165527
2023-01-07 08:52:28,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,186 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,186 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.28081566095352173
2023-01-07 08:52:28,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,187 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -47.846736907958984
2023-01-07 08:52:28,187 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.484504222869873
2023-01-07 08:52:28,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,189 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,189 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.636398315429688
2023-01-07 08:52:28,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5359728336334229
2023-01-07 08:52:28,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,190 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.933862686157227
2023-01-07 08:52:28,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.396306037902832
2023-01-07 08:52:28,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,191 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,191 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.471277236938477
2023-01-07 08:52:28,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.059365272521973
2023-01-07 08:52:28,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,193 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -20.11252212524414
2023-01-07 08:52:28,193 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.842082977294922
2023-01-07 08:52:28,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,194 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,194 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -231.47488403320312
2023-01-07 08:52:28,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.45120620727539
2023-01-07 08:52:28,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,196 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,196 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.3269994258880615
2023-01-07 08:52:28,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,196 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -225.10818481445312
2023-01-07 08:52:28,196 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.04712438583374
2023-01-07 08:52:28,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,198 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -195.16213989257812
2023-01-07 08:52:28,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.664390563964844
2023-01-07 08:52:28,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,199 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,199 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.3691632747650146
2023-01-07 08:52:28,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,199 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,199 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 7.293275833129883
2023-01-07 08:52:28,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,200 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -198.903076171875
2023-01-07 08:52:28,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.803238868713379
2023-01-07 08:52:28,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,202 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -205.25592041015625
2023-01-07 08:52:28,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9629616737365723
2023-01-07 08:52:28,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,203 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 4.50568962097168
2023-01-07 08:52:28,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.901752471923828
2023-01-07 08:52:28,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,205 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -113.69013214111328
2023-01-07 08:52:28,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.284835815429688
2023-01-07 08:52:28,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,206 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,206 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 2.0794780254364014
2023-01-07 08:52:28,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,206 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -115.89004516601562
2023-01-07 08:52:28,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.206932067871094
2023-01-07 08:52:28,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -144.85166931152344
2023-01-07 08:52:28,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.91819381713867
2023-01-07 08:52:28,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,209 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 2.912097454071045
2023-01-07 08:52:28,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,209 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,210 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -17.490745544433594
2023-01-07 08:52:28,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.723796844482422
2023-01-07 08:52:28,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -182.3551788330078
2023-01-07 08:52:28,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.109655380249023
2023-01-07 08:52:28,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,213 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,214 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -38.62065124511719
2023-01-07 08:52:28,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,214 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -212.08494567871094
2023-01-07 08:52:28,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.383739471435547
2023-01-07 08:52:28,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,215 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -199.71298217773438
2023-01-07 08:52:28,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.063680648803711
2023-01-07 08:52:28,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,216 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,217 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 296.2752685546875
2023-01-07 08:52:28,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 144.94195556640625
2023-01-07 08:52:28,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,218 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -243.81907653808594
2023-01-07 08:52:28,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4671204090118408
2023-01-07 08:52:28,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,219 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,219 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.9655358791351318
2023-01-07 08:52:28,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,219 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,220 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -42.71330261230469
2023-01-07 08:52:28,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,220 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -218.4875030517578
2023-01-07 08:52:28,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,220 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -243.21705627441406
2023-01-07 08:52:28,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.259376525878906
2023-01-07 08:52:28,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,222 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -218.9270477294922
2023-01-07 08:52:28,222 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.83934783935547
2023-01-07 08:52:28,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,223 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,223 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.102622985839844
2023-01-07 08:52:28,223 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.35462188720703
2023-01-07 08:52:28,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,224 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -191.5357208251953
2023-01-07 08:52:28,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.13675308227539
2023-01-07 08:52:28,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,226 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.0253982543945312
2023-01-07 08:52:28,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,226 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.89749145507812
2023-01-07 08:52:28,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.59020233154297
2023-01-07 08:52:28,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,227 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,227 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -310.0491943359375
2023-01-07 08:52:28,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.575571060180664
2023-01-07 08:52:28,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,228 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,229 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -9.607272148132324
2023-01-07 08:52:28,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,229 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -287.18841552734375
2023-01-07 08:52:28,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -121.29975128173828
2023-01-07 08:52:28,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,231 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -286.6010437011719
2023-01-07 08:52:28,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.182514190673828
2023-01-07 08:52:28,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,232 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -346.38031005859375
2023-01-07 08:52:28,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.26696014404297
2023-01-07 08:52:28,234 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:52:28,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,234 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,234 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 547.9267578125
2023-01-07 08:52:28,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -452.26495361328125
2023-01-07 08:52:28,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,235 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,236 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.7802734375
2023-01-07 08:52:28,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,237 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 48.67527770996094
2023-01-07 08:52:28,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,237 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -31.226930618286133
2023-01-07 08:52:28,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,238 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -138.08489990234375
2023-01-07 08:52:28,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,239 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -89.53296661376953
2023-01-07 08:52:28,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,239 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 183.28993225097656
2023-01-07 08:52:28,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,240 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -20.837844848632812
2023-01-07 08:52:28,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,241 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -173.67486572265625
2023-01-07 08:52:28,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,241 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -110.45838165283203
2023-01-07 08:52:28,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,242 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -82.06676483154297
2023-01-07 08:52:28,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,243 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 99.84132385253906
2023-01-07 08:52:28,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 87.03369140625
2023-01-07 08:52:28,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 1527.8375244140625
2023-01-07 08:52:28,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 100.82858276367188
2023-01-07 08:52:28,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 104.65645599365234
2023-01-07 08:52:28,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -82.7322006225586
2023-01-07 08:52:28,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 63.93785858154297
2023-01-07 08:52:28,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8.828278541564941
2023-01-07 08:52:28,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -30.867067337036133
2023-01-07 08:52:28,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.7873477935791
2023-01-07 08:52:28,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 16.474536895751953
2023-01-07 08:52:28,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -24.12883949279785
2023-01-07 08:52:28,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -35.156761169433594
2023-01-07 08:52:28,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 157.31317138671875
2023-01-07 08:52:28,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.859582901000977
2023-01-07 08:52:28,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.083629608154297
2023-01-07 08:52:28,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -11.286885261535645
2023-01-07 08:52:28,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -11.769815444946289
2023-01-07 08:52:28,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.263635635375977
2023-01-07 08:52:28,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -9.499246597290039
2023-01-07 08:52:28,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.4318900108337402
2023-01-07 08:52:28,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,251 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -17.134624481201172
2023-01-07 08:52:28,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,251 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -45.61425018310547
2023-01-07 08:52:28,251 > [DEBUG] 0 :: before allreduce fusion buffer :: -93.18281555175781
2023-01-07 08:52:28,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,252 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2.6932759284973145
2023-01-07 08:52:28,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,252 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.851459503173828
2023-01-07 08:52:28,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.412065505981445
2023-01-07 08:52:28,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -15.714523315429688
2023-01-07 08:52:28,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.731792449951172
2023-01-07 08:52:28,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,254 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -10.93570327758789
2023-01-07 08:52:28,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,254 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -2.8018674850463867
2023-01-07 08:52:28,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,255 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3.1876327991485596
2023-01-07 08:52:28,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.82158088684082
2023-01-07 08:52:28,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,256 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.02846622467041
2023-01-07 08:52:28,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,256 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -178.53826904296875
2023-01-07 08:52:28,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 735.9114990234375
2023-01-07 08:52:28,257 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,257 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 547.9267578125
2023-01-07 08:52:28,257 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,257 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,257 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -0.33893871307373047
2023-01-07 08:52:28,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,258 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -452.26495361328125
2023-01-07 08:52:28,258 > [DEBUG] 0 :: before allreduce fusion buffer :: -79.43697357177734
2023-01-07 08:52:28,259 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,259 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -0.33893871307373047
2023-01-07 08:52:28,259 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7820339202880859
2023-01-07 08:52:28,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,259 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,259 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 78.25837707519531
2023-01-07 08:52:28,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -452.26495361328125
2023-01-07 08:52:28,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.36022186279297
2023-01-07 08:52:28,261 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,261 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 78.25837707519531
2023-01-07 08:52:28,261 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:28,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,262 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -452.26495361328125
2023-01-07 08:52:28,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -127.98739624023438
2023-01-07 08:52:28,263 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,263 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 48.30706787109375
2023-01-07 08:52:28,263 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:28,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,263 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -452.26495361328125
2023-01-07 08:52:28,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.991174697875977
2023-01-07 08:52:28,264 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,264 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -452.26495361328125
2023-01-07 08:52:28,264 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,265 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -5.021859169006348
2023-01-07 08:52:28,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,265 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -0.1152801513671875
2023-01-07 08:52:28,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.12897872924805
2023-01-07 08:52:28,267 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,267 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -5.021859169006348
2023-01-07 08:52:28,267 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,267 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.087326049804688
2023-01-07 08:52:28,268 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,268 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -0.1152801513671875
2023-01-07 08:52:28,268 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,269 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,269 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -1.5873382091522217
2023-01-07 08:52:28,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,269 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -40.264827728271484
2023-01-07 08:52:28,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.937339782714844
2023-01-07 08:52:28,270 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,270 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -1.5873382091522217
2023-01-07 08:52:28,270 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,271 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 267.40673828125
2023-01-07 08:52:28,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.983570098876953
2023-01-07 08:52:28,272 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,272 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -16.405025482177734
2023-01-07 08:52:28,272 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,272 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 267.40673828125
2023-01-07 08:52:28,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 76.28457641601562
2023-01-07 08:52:28,273 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,273 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 19.21490478515625
2023-01-07 08:52:28,274 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,274 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 267.40673828125
2023-01-07 08:52:28,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 100.91704559326172
2023-01-07 08:52:28,275 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,275 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -40.264827728271484
2023-01-07 08:52:28,275 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,276 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,276 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 2.332854747772217
2023-01-07 08:52:28,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,276 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 267.40673828125
2023-01-07 08:52:28,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.802291870117188
2023-01-07 08:52:28,277 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,277 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 2.332854747772217
2023-01-07 08:52:28,277 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,278 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 267.40673828125
2023-01-07 08:52:28,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.415334701538086
2023-01-07 08:52:28,279 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,279 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 267.40673828125
2023-01-07 08:52:28,279 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,280 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,280 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.720332145690918
2023-01-07 08:52:28,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,280 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -45.42966842651367
2023-01-07 08:52:28,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.1752815246582
2023-01-07 08:52:28,281 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,281 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.720332145690918
2023-01-07 08:52:28,281 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,282 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.778669357299805
2023-01-07 08:52:28,283 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,283 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -45.42966842651367
2023-01-07 08:52:28,283 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:28,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,283 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -94.04490661621094
2023-01-07 08:52:28,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.492290496826172
2023-01-07 08:52:28,284 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,284 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -20.614721298217773
2023-01-07 08:52:28,285 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:28,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,285 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -94.04490661621094
2023-01-07 08:52:28,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,285 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,285 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.73136901855469
2023-01-07 08:52:28,286 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,286 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -94.04490661621094
2023-01-07 08:52:28,287 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,287 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,287 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.16209030151367
2023-01-07 08:52:28,288 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,288 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 2.306825876235962
2023-01-07 08:52:28,288 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,289 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -216.79742431640625
2023-01-07 08:52:28,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.716156005859375
2023-01-07 08:52:28,290 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,290 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -216.79742431640625
2023-01-07 08:52:28,290 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,290 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,290 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.6476407051086426
2023-01-07 08:52:28,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,290 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,291 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -42.79195022583008
2023-01-07 08:52:28,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,291 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -16.56559944152832
2023-01-07 08:52:28,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.28733825683594
2023-01-07 08:52:28,292 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,292 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.6476407051086426
2023-01-07 08:52:28,292 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,293 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.426586151123047
2023-01-07 08:52:28,294 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,294 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -42.79195022583008
2023-01-07 08:52:28,294 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 6.527358055114746
2023-01-07 08:52:28,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -16.56559944152832
2023-01-07 08:52:28,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.276123046875
2023-01-07 08:52:28,295 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,295 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -12.063582420349121
2023-01-07 08:52:28,295 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,296 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.977226257324219
2023-01-07 08:52:28,297 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,297 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -16.56559944152832
2023-01-07 08:52:28,297 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.061614990234375
2023-01-07 08:52:28,299 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,299 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -12.29188060760498
2023-01-07 08:52:28,299 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -250.97650146484375
2023-01-07 08:52:28,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -67.11061096191406
2023-01-07 08:52:28,300 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,300 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -250.97650146484375
2023-01-07 08:52:28,301 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,301 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.027231693267822266
2023-01-07 08:52:28,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.7802734375
2023-01-07 08:52:28,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.3076491355896
2023-01-07 08:52:28,302 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,302 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.027231693267822266
2023-01-07 08:52:28,302 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -3.782032012939453
2023-01-07 08:52:28,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 46.7802734375
2023-01-07 08:52:28,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.097005367279053
2023-01-07 08:52:28,304 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,304 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 46.7802734375
2023-01-07 08:52:28,304 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,304 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.7778267860412598
2023-01-07 08:52:28,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 48.67527770996094
2023-01-07 08:52:28,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.367250442504883
2023-01-07 08:52:28,306 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,306 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.7778267860412598
2023-01-07 08:52:28,306 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -27.419443130493164
2023-01-07 08:52:28,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 48.67527770996094
2023-01-07 08:52:28,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8585472106933594
2023-01-07 08:52:28,307 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,308 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 48.67527770996094
2023-01-07 08:52:28,308 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:28,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -31.226930618286133
2023-01-07 08:52:28,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.270193099975586
2023-01-07 08:52:28,309 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,309 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 11.77621841430664
2023-01-07 08:52:28,309 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:28,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -31.226930618286133
2023-01-07 08:52:28,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.260561466217041
2023-01-07 08:52:28,311 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,311 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -31.226930618286133
2023-01-07 08:52:28,311 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,311 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.8079578876495361
2023-01-07 08:52:28,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,312 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -138.08489990234375
2023-01-07 08:52:28,312 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8398494720458984
2023-01-07 08:52:28,313 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,313 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.8079578876495361
2023-01-07 08:52:28,313 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.335868000984192
2023-01-07 08:52:28,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -138.08489990234375
2023-01-07 08:52:28,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.05345916748047
2023-01-07 08:52:28,314 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,314 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -138.08489990234375
2023-01-07 08:52:28,315 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,315 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,315 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.2453368902206421
2023-01-07 08:52:28,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -89.53296661376953
2023-01-07 08:52:28,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.746150493621826
2023-01-07 08:52:28,316 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,316 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.2453368902206421
2023-01-07 08:52:28,317 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,317 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.009390830993652
2023-01-07 08:52:28,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -89.53296661376953
2023-01-07 08:52:28,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.133853912353516
2023-01-07 08:52:28,318 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,318 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -89.53296661376953
2023-01-07 08:52:28,318 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:28,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 183.28993225097656
2023-01-07 08:52:28,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3163137435913086
2023-01-07 08:52:28,319 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,320 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 8.066877365112305
2023-01-07 08:52:28,320 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:28,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,320 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 183.28993225097656
2023-01-07 08:52:28,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.53972339630127
2023-01-07 08:52:28,322 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,322 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 183.28993225097656
2023-01-07 08:52:28,322 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,322 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.73732990026474
2023-01-07 08:52:28,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,323 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -20.837844848632812
2023-01-07 08:52:28,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.225046157836914
2023-01-07 08:52:28,324 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,324 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.73732990026474
2023-01-07 08:52:28,324 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.902972221374512
2023-01-07 08:52:28,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -20.837844848632812
2023-01-07 08:52:28,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.52132797241211
2023-01-07 08:52:28,325 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,325 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -20.837844848632812
2023-01-07 08:52:28,326 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,326 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.1992362141609192
2023-01-07 08:52:28,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -173.67486572265625
2023-01-07 08:52:28,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.331171989440918
2023-01-07 08:52:28,327 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,327 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.1992362141609192
2023-01-07 08:52:28,327 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,328 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,328 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.7452926635742188
2023-01-07 08:52:28,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -173.67486572265625
2023-01-07 08:52:28,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5333609580993652
2023-01-07 08:52:28,329 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,329 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -173.67486572265625
2023-01-07 08:52:28,329 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,329 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,330 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.5911432504653931
2023-01-07 08:52:28,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,330 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -110.45838165283203
2023-01-07 08:52:28,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08679962158203125
2023-01-07 08:52:28,331 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,331 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.5911432504653931
2023-01-07 08:52:28,331 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 4.1814117431640625
2023-01-07 08:52:28,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,331 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -110.45838165283203
2023-01-07 08:52:28,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0831680297851562
2023-01-07 08:52:28,333 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,333 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -110.45838165283203
2023-01-07 08:52:28,333 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,333 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,333 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.11088073253631592
2023-01-07 08:52:28,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,333 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -82.06676483154297
2023-01-07 08:52:28,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2125816345214844
2023-01-07 08:52:28,334 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,335 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.11088073253631592
2023-01-07 08:52:28,335 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.66290283203125
2023-01-07 08:52:28,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,335 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -82.06676483154297
2023-01-07 08:52:28,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.8392972946167
2023-01-07 08:52:28,336 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,336 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -82.06676483154297
2023-01-07 08:52:28,336 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,337 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,337 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.0040960609912872314
2023-01-07 08:52:28,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 99.84132385253906
2023-01-07 08:52:28,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.2710418701171875
2023-01-07 08:52:28,338 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,338 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0040960609912872314
2023-01-07 08:52:28,338 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -4.248758316040039
2023-01-07 08:52:28,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,339 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 99.84132385253906
2023-01-07 08:52:28,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9198163747787476
2023-01-07 08:52:28,340 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,340 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 99.84132385253906
2023-01-07 08:52:28,340 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,340 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.3128305673599243
2023-01-07 08:52:28,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,341 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 87.03369140625
2023-01-07 08:52:28,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4925031661987305
2023-01-07 08:52:28,342 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,342 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.3128305673599243
2023-01-07 08:52:28,342 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.61199951171875
2023-01-07 08:52:28,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,342 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 87.03369140625
2023-01-07 08:52:28,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9162144660949707
2023-01-07 08:52:28,344 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,344 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 87.03369140625
2023-01-07 08:52:28,344 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,344 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,344 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.11955234408378601
2023-01-07 08:52:28,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 1527.8375244140625
2023-01-07 08:52:28,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.96765899658203
2023-01-07 08:52:28,346 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,346 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.11955234408378601
2023-01-07 08:52:28,346 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.29677963256836
2023-01-07 08:52:28,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 1527.8375244140625
2023-01-07 08:52:28,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7855982780456543
2023-01-07 08:52:28,348 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,348 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 1527.8375244140625
2023-01-07 08:52:28,348 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,348 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:28,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,348 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 100.82858276367188
2023-01-07 08:52:28,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.960951805114746
2023-01-07 08:52:28,349 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,349 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -4.671212196350098
2023-01-07 08:52:28,349 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:28,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 100.82858276367188
2023-01-07 08:52:28,350 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.088621139526367
2023-01-07 08:52:28,351 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,351 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 100.82858276367188
2023-01-07 08:52:28,351 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:28,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 104.65645599365234
2023-01-07 08:52:28,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.798465728759766
2023-01-07 08:52:28,353 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,353 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 12.559350967407227
2023-01-07 08:52:28,353 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:28,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 104.65645599365234
2023-01-07 08:52:28,353 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9334900975227356
2023-01-07 08:52:28,354 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,354 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 104.65645599365234
2023-01-07 08:52:28,354 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:28,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -82.7322006225586
2023-01-07 08:52:28,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.33540153503418
2023-01-07 08:52:28,356 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,356 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 21.068096160888672
2023-01-07 08:52:28,356 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,356 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:28,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -82.7322006225586
2023-01-07 08:52:28,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8183965086936951
2023-01-07 08:52:28,358 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,358 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -82.7322006225586
2023-01-07 08:52:28,358 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,358 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:52:28,358 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.17090392112731934
2023-01-07 08:52:28,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,359 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 63.93785858154297
2023-01-07 08:52:28,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.783979415893555
2023-01-07 08:52:28,360 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,360 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.17090392112731934
2023-01-07 08:52:28,360 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.882823944091797
2023-01-07 08:52:28,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 63.93785858154297
2023-01-07 08:52:28,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3863813877105713
2023-01-07 08:52:28,361 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,361 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 63.93785858154297
2023-01-07 08:52:28,362 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:28,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8.828278541564941
2023-01-07 08:52:28,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.268549919128418
2023-01-07 08:52:28,363 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,363 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -2.1934423446655273
2023-01-07 08:52:28,363 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:28,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8.828278541564941
2023-01-07 08:52:28,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8711206912994385
2023-01-07 08:52:28,365 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,365 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -8.828278541564941
2023-01-07 08:52:28,365 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:28,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -30.867067337036133
2023-01-07 08:52:28,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7213006019592285
2023-01-07 08:52:28,366 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,366 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 26.593448638916016
2023-01-07 08:52:28,366 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:28,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -30.867067337036133
2023-01-07 08:52:28,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.985172748565674
2023-01-07 08:52:28,368 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -30.867067337036133
2023-01-07 08:52:28,368 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:28,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.7873477935791
2023-01-07 08:52:28,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.761833667755127
2023-01-07 08:52:28,370 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,370 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -1.2079079151153564
2023-01-07 08:52:28,370 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:28,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.7873477935791
2023-01-07 08:52:28,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4335535764694214
2023-01-07 08:52:28,372 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,372 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 27.7873477935791
2023-01-07 08:52:28,373 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:28,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 16.474536895751953
2023-01-07 08:52:28,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1383283138275146
2023-01-07 08:52:28,374 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,374 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -1.3057599067687988
2023-01-07 08:52:28,374 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:28,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 16.474536895751953
2023-01-07 08:52:28,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.731688380241394
2023-01-07 08:52:28,376 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,376 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 16.474536895751953
2023-01-07 08:52:28,376 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,376 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:28,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -24.12883949279785
2023-01-07 08:52:28,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6564702987670898
2023-01-07 08:52:28,377 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,377 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.49490439891815186
2023-01-07 08:52:28,377 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:28,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -24.12883949279785
2023-01-07 08:52:28,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3306527137756348
2023-01-07 08:52:28,379 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,379 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -24.12883949279785
2023-01-07 08:52:28,379 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:28,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -35.156761169433594
2023-01-07 08:52:28,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6693340539932251
2023-01-07 08:52:28,381 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,381 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.6011485457420349
2023-01-07 08:52:28,381 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:28,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -35.156761169433594
2023-01-07 08:52:28,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6731667518615723
2023-01-07 08:52:28,382 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,383 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -35.156761169433594
2023-01-07 08:52:28,383 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:28,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.859582901000977
2023-01-07 08:52:28,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5250185132026672
2023-01-07 08:52:28,384 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,384 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.41041696071624756
2023-01-07 08:52:28,384 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:28,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.859582901000977
2023-01-07 08:52:28,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2687366008758545
2023-01-07 08:52:28,386 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,386 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -5.859582901000977
2023-01-07 08:52:28,386 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:28,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.083629608154297
2023-01-07 08:52:28,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9344959259033203
2023-01-07 08:52:28,387 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,388 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 2.8104987144470215
2023-01-07 08:52:28,388 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:28,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,388 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.083629608154297
2023-01-07 08:52:28,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5188528895378113
2023-01-07 08:52:28,389 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,389 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -24.083629608154297
2023-01-07 08:52:28,389 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:28,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -11.286885261535645
2023-01-07 08:52:28,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7084956169128418
2023-01-07 08:52:28,391 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,391 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.23173460364341736
2023-01-07 08:52:28,391 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:28,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -11.286885261535645
2023-01-07 08:52:28,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1013031005859375
2023-01-07 08:52:28,393 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,393 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -11.286885261535645
2023-01-07 08:52:28,393 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:28,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -11.769815444946289
2023-01-07 08:52:28,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7203712463378906
2023-01-07 08:52:28,394 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,394 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.7669532895088196
2023-01-07 08:52:28,394 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:28,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -11.769815444946289
2023-01-07 08:52:28,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6422953605651855
2023-01-07 08:52:28,396 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,396 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -11.769815444946289
2023-01-07 08:52:28,396 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:28,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.263635635375977
2023-01-07 08:52:28,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8984818458557129
2023-01-07 08:52:28,398 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,398 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.216252863407135
2023-01-07 08:52:28,398 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:28,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,398 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.263635635375977
2023-01-07 08:52:28,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5913135409355164
2023-01-07 08:52:28,400 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,400 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -12.263635635375977
2023-01-07 08:52:28,400 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:28,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,400 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -9.499246597290039
2023-01-07 08:52:28,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3333055973052979
2023-01-07 08:52:28,401 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,401 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.9149235486984253
2023-01-07 08:52:28,401 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:28,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -9.499246597290039
2023-01-07 08:52:28,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1424329280853271
2023-01-07 08:52:28,403 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,403 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -9.499246597290039
2023-01-07 08:52:28,403 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,403 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:28,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.4318900108337402
2023-01-07 08:52:28,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49478697776794434
2023-01-07 08:52:28,404 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,405 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -1.472203016281128
2023-01-07 08:52:28,405 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:28,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.4318900108337402
2023-01-07 08:52:28,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3338780999183655
2023-01-07 08:52:28,406 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,406 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -3.4318900108337402
2023-01-07 08:52:28,406 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:28,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,407 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -17.134624481201172
2023-01-07 08:52:28,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4970500469207764
2023-01-07 08:52:28,408 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,408 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -1.001621127128601
2023-01-07 08:52:28,408 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:28,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,408 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -17.134624481201172
2023-01-07 08:52:28,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5851972103118896
2023-01-07 08:52:28,409 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,410 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -17.134624481201172
2023-01-07 08:52:28,410 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:28,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,410 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -45.61425018310547
2023-01-07 08:52:28,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4744304418563843
2023-01-07 08:52:28,411 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,411 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.1985911875963211
2023-01-07 08:52:28,411 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:28,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,412 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -45.61425018310547
2023-01-07 08:52:28,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.001422643661499
2023-01-07 08:52:28,413 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,413 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -45.61425018310547
2023-01-07 08:52:28,413 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:28,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,414 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2.6932759284973145
2023-01-07 08:52:28,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5552748441696167
2023-01-07 08:52:28,415 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,415 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.11198314279317856
2023-01-07 08:52:28,415 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:28,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,415 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2.6932759284973145
2023-01-07 08:52:28,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7930480241775513
2023-01-07 08:52:28,416 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,416 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -2.6932759284973145
2023-01-07 08:52:28,417 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:28,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,417 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.851459503173828
2023-01-07 08:52:28,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.377179741859436
2023-01-07 08:52:28,418 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,418 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 3.530031204223633
2023-01-07 08:52:28,418 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:28,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,419 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.851459503173828
2023-01-07 08:52:28,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8211997747421265
2023-01-07 08:52:28,419 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,420 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 16.851459503173828
2023-01-07 08:52:28,420 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:28,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,420 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.412065505981445
2023-01-07 08:52:28,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.613919734954834
2023-01-07 08:52:28,421 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,421 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 2.3235950469970703
2023-01-07 08:52:28,421 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,421 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,421 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:28,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,422 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.412065505981445
2023-01-07 08:52:28,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5052534937858582
2023-01-07 08:52:28,423 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,423 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 12.412065505981445
2023-01-07 08:52:28,423 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,423 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,423 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:28,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,423 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -15.714523315429688
2023-01-07 08:52:28,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16124677658081055
2023-01-07 08:52:28,424 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,424 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -0.09773781895637512
2023-01-07 08:52:28,425 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:28,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,425 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -15.714523315429688
2023-01-07 08:52:28,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40788695216178894
2023-01-07 08:52:28,426 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,426 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -15.714523315429688
2023-01-07 08:52:28,426 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,426 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:28,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,427 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -10.93570327758789
2023-01-07 08:52:28,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3383471667766571
2023-01-07 08:52:28,428 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,428 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.1951025426387787
2023-01-07 08:52:28,428 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:28,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,428 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -10.93570327758789
2023-01-07 08:52:28,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15394437313079834
2023-01-07 08:52:28,429 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,430 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -10.93570327758789
2023-01-07 08:52:28,430 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,430 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,430 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:28,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,430 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -2.8018674850463867
2023-01-07 08:52:28,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33431145548820496
2023-01-07 08:52:28,431 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,431 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -0.6558793187141418
2023-01-07 08:52:28,431 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:28,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -2.8018674850463867
2023-01-07 08:52:28,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35726800560951233
2023-01-07 08:52:28,433 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,433 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -2.8018674850463867
2023-01-07 08:52:28,433 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:28,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,433 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3.1876327991485596
2023-01-07 08:52:28,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02362874522805214
2023-01-07 08:52:28,434 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,434 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.17920473217964172
2023-01-07 08:52:28,435 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:28,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -3.1876327991485596
2023-01-07 08:52:28,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03572218865156174
2023-01-07 08:52:28,436 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,436 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -3.1876327991485596
2023-01-07 08:52:28,436 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:28,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,437 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.02846622467041
2023-01-07 08:52:28,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4124637246131897
2023-01-07 08:52:28,438 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,438 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.12369777262210846
2023-01-07 08:52:28,438 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:28,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,438 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.02846622467041
2023-01-07 08:52:28,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06860720366239548
2023-01-07 08:52:28,439 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,440 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 7.02846622467041
2023-01-07 08:52:28,440 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:28,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,440 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -178.53826904296875
2023-01-07 08:52:28,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7561469078063965
2023-01-07 08:52:28,441 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,441 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -293.63629150390625
2023-01-07 08:52:28,441 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:28,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,442 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -178.53826904296875
2023-01-07 08:52:28,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5815374851226807
2023-01-07 08:52:28,443 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,444 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -178.53826904296875
2023-01-07 08:52:28,444 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,445 > [DEBUG] 0 :: 7.780207633972168
2023-01-07 08:52:28,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,448 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:52:28,448 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,449 > [DEBUG] 0 :: before allreduce fusion buffer :: -272.8447570800781
2023-01-07 08:52:28,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.5116667151451111
2023-01-07 08:52:28,452 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,452 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:52:28,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -311.93255615234375
2023-01-07 08:52:28,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,455 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1877079010009766
2023-01-07 08:52:28,456 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1480134278535843
2023-01-07 08:52:28,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,457 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.009145371615886688
2023-01-07 08:52:28,457 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1877079010009766
2023-01-07 08:52:28,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11726179718971252
2023-01-07 08:52:28,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -0.6805820465087891
2023-01-07 08:52:28,459 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04197859764099121
2023-01-07 08:52:28,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.01946328580379486
2023-01-07 08:52:28,461 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -0.6805820465087891
2023-01-07 08:52:28,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.028913894668221474
2023-01-07 08:52:28,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,463 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.18653297424316406
2023-01-07 08:52:28,463 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6709132194519043
2023-01-07 08:52:28,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,464 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.5041651129722595
2023-01-07 08:52:28,464 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,464 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.18653297424316406
2023-01-07 08:52:28,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05477948486804962
2023-01-07 08:52:28,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,466 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.118487358093262
2023-01-07 08:52:28,466 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2580746114253998
2023-01-07 08:52:28,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,467 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.02302417904138565
2023-01-07 08:52:28,467 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,468 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.118487358093262
2023-01-07 08:52:28,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39197611808776855
2023-01-07 08:52:28,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,469 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -10.584234237670898
2023-01-07 08:52:28,469 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45386552810668945
2023-01-07 08:52:28,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,470 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.020444059744477272
2023-01-07 08:52:28,471 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,471 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -10.584234237670898
2023-01-07 08:52:28,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08107741177082062
2023-01-07 08:52:28,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,472 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.253800392150879
2023-01-07 08:52:28,472 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45653894543647766
2023-01-07 08:52:28,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,474 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.339240163564682
2023-01-07 08:52:28,474 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,474 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.253800392150879
2023-01-07 08:52:28,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3614475727081299
2023-01-07 08:52:28,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,475 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.867344856262207
2023-01-07 08:52:28,476 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21212491393089294
2023-01-07 08:52:28,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,477 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.24687334895133972
2023-01-07 08:52:28,477 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,477 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.867344856262207
2023-01-07 08:52:28,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44644591212272644
2023-01-07 08:52:28,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,479 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.977278232574463
2023-01-07 08:52:28,479 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12774385511875153
2023-01-07 08:52:28,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,480 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.015128865838050842
2023-01-07 08:52:28,480 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,480 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.977278232574463
2023-01-07 08:52:28,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.301750898361206
2023-01-07 08:52:28,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,482 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 18.194162368774414
2023-01-07 08:52:28,482 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1595382690429688
2023-01-07 08:52:28,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.016315333545207977
2023-01-07 08:52:28,483 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 18.194162368774414
2023-01-07 08:52:28,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6621139049530029
2023-01-07 08:52:28,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -10.023994445800781
2023-01-07 08:52:28,485 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8953847289085388
2023-01-07 08:52:28,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.1549271047115326
2023-01-07 08:52:28,486 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,486 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -10.023994445800781
2023-01-07 08:52:28,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.531213641166687
2023-01-07 08:52:28,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -6.077132701873779
2023-01-07 08:52:28,488 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4460485577583313
2023-01-07 08:52:28,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.07173182815313339
2023-01-07 08:52:28,490 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -6.077132701873779
2023-01-07 08:52:28,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2344624549150467
2023-01-07 08:52:28,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.7009429931640625
2023-01-07 08:52:28,492 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9467819929122925
2023-01-07 08:52:28,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.04966271296143532
2023-01-07 08:52:28,493 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.7009429931640625
2023-01-07 08:52:28,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4903451204299927
2023-01-07 08:52:28,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 2.000302314758301
2023-01-07 08:52:28,495 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8528476357460022
2023-01-07 08:52:28,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.10633593052625656
2023-01-07 08:52:28,496 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 2.000302314758301
2023-01-07 08:52:28,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4636311531066895
2023-01-07 08:52:28,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.8390941619873047
2023-01-07 08:52:28,498 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6720020771026611
2023-01-07 08:52:28,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.08500096201896667
2023-01-07 08:52:28,499 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.8390941619873047
2023-01-07 08:52:28,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46984389424324036
2023-01-07 08:52:28,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -10.455297470092773
2023-01-07 08:52:28,501 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4869471788406372
2023-01-07 08:52:28,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.06528444588184357
2023-01-07 08:52:28,502 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -10.455297470092773
2023-01-07 08:52:28,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0801987648010254
2023-01-07 08:52:28,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.9926953315734863
2023-01-07 08:52:28,504 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.248488426208496
2023-01-07 08:52:28,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.20606201887130737
2023-01-07 08:52:28,505 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.9926953315734863
2023-01-07 08:52:28,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8169277906417847
2023-01-07 08:52:28,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,507 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -2.101335287094116
2023-01-07 08:52:28,507 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2427634000778198
2023-01-07 08:52:28,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.08289623260498047
2023-01-07 08:52:28,508 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -2.101335287094116
2023-01-07 08:52:28,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9497652053833008
2023-01-07 08:52:28,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,510 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 6.606768608093262
2023-01-07 08:52:28,510 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5873163938522339
2023-01-07 08:52:28,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.045543596148490906
2023-01-07 08:52:28,511 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 6.606768608093262
2023-01-07 08:52:28,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25403904914855957
2023-01-07 08:52:28,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.854259490966797
2023-01-07 08:52:28,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7369375228881836
2023-01-07 08:52:28,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.01487039029598236
2023-01-07 08:52:28,514 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,515 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.854259490966797
2023-01-07 08:52:28,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3660962581634521
2023-01-07 08:52:28,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,516 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.613455772399902
2023-01-07 08:52:28,516 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9815120697021484
2023-01-07 08:52:28,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.06885696202516556
2023-01-07 08:52:28,517 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.613455772399902
2023-01-07 08:52:28,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.818626880645752
2023-01-07 08:52:28,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,519 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.504852294921875
2023-01-07 08:52:28,519 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,519 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2813674211502075
2023-01-07 08:52:28,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.10813255608081818
2023-01-07 08:52:28,521 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.504852294921875
2023-01-07 08:52:28,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0348308086395264
2023-01-07 08:52:28,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,522 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.366442680358887
2023-01-07 08:52:28,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7885234951972961
2023-01-07 08:52:28,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.18679776787757874
2023-01-07 08:52:28,524 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.366442680358887
2023-01-07 08:52:28,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.624990463256836
2023-01-07 08:52:28,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,525 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 50.80482482910156
2023-01-07 08:52:28,525 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5759339332580566
2023-01-07 08:52:28,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.49643754959106445
2023-01-07 08:52:28,527 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 50.80482482910156
2023-01-07 08:52:28,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6732139587402344
2023-01-07 08:52:28,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,529 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 223.83282470703125
2023-01-07 08:52:28,529 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2599672079086304
2023-01-07 08:52:28,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 223.83282470703125
2023-01-07 08:52:28,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.397160053253174
2023-01-07 08:52:28,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,532 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 102.53419494628906
2023-01-07 08:52:28,532 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7844862937927246
2023-01-07 08:52:28,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,533 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.04788518697023392
2023-01-07 08:52:28,533 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,533 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 102.53419494628906
2023-01-07 08:52:28,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8609824180603027
2023-01-07 08:52:28,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,535 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 224.5889892578125
2023-01-07 08:52:28,535 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2236047387123108
2023-01-07 08:52:28,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,536 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7817401885986328
2023-01-07 08:52:28,536 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,536 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 224.5889892578125
2023-01-07 08:52:28,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.807575702667236
2023-01-07 08:52:28,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,538 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.85197067260742
2023-01-07 08:52:28,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22138366103172302
2023-01-07 08:52:28,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,539 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.05754369497299194
2023-01-07 08:52:28,539 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,539 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.85197067260742
2023-01-07 08:52:28,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6952335834503174
2023-01-07 08:52:28,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,541 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -459.9798583984375
2023-01-07 08:52:28,541 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.632021903991699
2023-01-07 08:52:28,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -459.9798583984375
2023-01-07 08:52:28,542 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.082772254943848
2023-01-07 08:52:28,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.01077938079834
2023-01-07 08:52:28,544 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6478962898254395
2023-01-07 08:52:28,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,545 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.01077938079834
2023-01-07 08:52:28,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.209043502807617
2023-01-07 08:52:28,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0113956928253174
2023-01-07 08:52:28,547 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.114038348197937
2023-01-07 08:52:28,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0113956928253174
2023-01-07 08:52:28,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.366909503936768
2023-01-07 08:52:28,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,549 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 10.71603775024414
2023-01-07 08:52:28,549 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.370908737182617
2023-01-07 08:52:28,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 10.71603775024414
2023-01-07 08:52:28,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.611860275268555
2023-01-07 08:52:28,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.119552612304688
2023-01-07 08:52:28,552 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9361497163772583
2023-01-07 08:52:28,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.119552612304688
2023-01-07 08:52:28,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.90479850769043
2023-01-07 08:52:28,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,554 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.07331657409668
2023-01-07 08:52:28,555 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1218794584274292
2023-01-07 08:52:28,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.07331657409668
2023-01-07 08:52:28,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5849709510803223
2023-01-07 08:52:28,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,557 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.884986877441406
2023-01-07 08:52:28,557 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.092775344848633
2023-01-07 08:52:28,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.884986877441406
2023-01-07 08:52:28,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.5391206741333
2023-01-07 08:52:28,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.578182220458984
2023-01-07 08:52:28,560 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3803951740264893
2023-01-07 08:52:28,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.7669427990913391
2023-01-07 08:52:28,561 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.578182220458984
2023-01-07 08:52:28,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5353518724441528
2023-01-07 08:52:28,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -26.017581939697266
2023-01-07 08:52:28,563 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9278254508972168
2023-01-07 08:52:28,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -26.017581939697266
2023-01-07 08:52:28,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1864562034606934
2023-01-07 08:52:28,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -41.73194122314453
2023-01-07 08:52:28,566 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4742159843444824
2023-01-07 08:52:28,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,567 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -41.73194122314453
2023-01-07 08:52:28,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.996334075927734
2023-01-07 08:52:28,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,569 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.0786590576171875
2023-01-07 08:52:28,569 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.696178674697876
2023-01-07 08:52:28,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.5423759818077087
2023-01-07 08:52:28,570 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.0786590576171875
2023-01-07 08:52:28,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.99588394165039
2023-01-07 08:52:28,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.61479568481445
2023-01-07 08:52:28,572 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5813727974891663
2023-01-07 08:52:28,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,573 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.61479568481445
2023-01-07 08:52:28,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.454856872558594
2023-01-07 08:52:28,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,574 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.16387367248535
2023-01-07 08:52:28,574 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.0037689208984375
2023-01-07 08:52:28,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,576 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.16387367248535
2023-01-07 08:52:28,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.243602752685547
2023-01-07 08:52:28,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,577 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,577 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.795005798339844
2023-01-07 08:52:28,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,578 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.36130285263061523
2023-01-07 08:52:28,579 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,579 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.970344543457031
2023-01-07 08:52:28,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,580 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.412521362304688
2023-01-07 08:52:28,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,582 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.9625788331031799
2023-01-07 08:52:28,582 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,582 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -35.85394287109375
2023-01-07 08:52:28,583 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.19884490966797
2023-01-07 08:52:28,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.053108215332031
2023-01-07 08:52:28,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,586 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -35.85394287109375
2023-01-07 08:52:28,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.23044204711914
2023-01-07 08:52:28,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,587 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,588 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.755183219909668
2023-01-07 08:52:28,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.395061731338501
2023-01-07 08:52:28,589 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 105.906494140625
2023-01-07 08:52:28,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,591 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.103801727294922
2023-01-07 08:52:28,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,592 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 2.3827574253082275
2023-01-07 08:52:28,592 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,592 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 24.536216735839844
2023-01-07 08:52:28,592 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.807516098022461
2023-01-07 08:52:28,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,594 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,594 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.3427734375
2023-01-07 08:52:28,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,595 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 26.83971405029297
2023-01-07 08:52:28,595 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.293041229248047
2023-01-07 08:52:28,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,597 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.861234664916992
2023-01-07 08:52:28,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,598 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,598 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.911006927490234
2023-01-07 08:52:28,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,600 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.580020904541016
2023-01-07 08:52:28,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,601 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 6.809615135192871
2023-01-07 08:52:28,601 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,601 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 8.752939224243164
2023-01-07 08:52:28,601 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,602 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,602 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.55232048034668
2023-01-07 08:52:28,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,604 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.867448329925537
2023-01-07 08:52:28,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,605 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -236.0964813232422
2023-01-07 08:52:28,605 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -237.00738525390625
2023-01-07 08:52:28,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,606 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.025115966796875
2023-01-07 08:52:28,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,608 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -236.0964813232422
2023-01-07 08:52:28,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8586158752441406
2023-01-07 08:52:28,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,609 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,609 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.761987686157227
2023-01-07 08:52:28,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,611 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 0.11651468276977539
2023-01-07 08:52:28,611 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,611 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -352.16912841796875
2023-01-07 08:52:28,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,612 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 70.53461456298828
2023-01-07 08:52:28,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,614 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.053924560546875
2023-01-07 08:52:28,619 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:52:28,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,619 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 1385.494384765625
2023-01-07 08:52:28,619 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,620 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.16387367248535
2023-01-07 08:52:28,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,622 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.61479568481445
2023-01-07 08:52:28,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,622 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.0786590576171875
2023-01-07 08:52:28,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,623 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -41.73194122314453
2023-01-07 08:52:28,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -26.017581939697266
2023-01-07 08:52:28,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.578182220458984
2023-01-07 08:52:28,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,625 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.884986877441406
2023-01-07 08:52:28,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,626 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.07331657409668
2023-01-07 08:52:28,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,626 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.119552612304688
2023-01-07 08:52:28,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,627 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 10.71603775024414
2023-01-07 08:52:28,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,627 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0113956928253174
2023-01-07 08:52:28,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,627 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.01077938079834
2023-01-07 08:52:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -459.9798583984375
2023-01-07 08:52:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.85197067260742
2023-01-07 08:52:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 224.5889892578125
2023-01-07 08:52:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 102.53419494628906
2023-01-07 08:52:28,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 223.83282470703125
2023-01-07 08:52:28,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 50.80482482910156
2023-01-07 08:52:28,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.366442680358887
2023-01-07 08:52:28,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.504852294921875
2023-01-07 08:52:28,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.613455772399902
2023-01-07 08:52:28,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.854259490966797
2023-01-07 08:52:28,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1332.1929931640625
2023-01-07 08:52:28,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 1406.491455078125
2023-01-07 08:52:28,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -56.15372848510742
2023-01-07 08:52:28,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 105.88877868652344
2023-01-07 08:52:28,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 59.50096893310547
2023-01-07 08:52:28,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1398.093017578125
2023-01-07 08:52:28,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 377.22265625
2023-01-07 08:52:28,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 828.45703125
2023-01-07 08:52:28,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 501.6304931640625
2023-01-07 08:52:28,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,635 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 984.3370971679688
2023-01-07 08:52:28,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,635 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 889.29443359375
2023-01-07 08:52:28,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 3607.33349609375
2023-01-07 08:52:28,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.977278232574463
2023-01-07 08:52:28,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.867344856262207
2023-01-07 08:52:28,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.253800392150879
2023-01-07 08:52:28,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,638 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -10.584234237670898
2023-01-07 08:52:28,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.045082092285156
2023-01-07 08:52:28,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,639 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.118487358093262
2023-01-07 08:52:28,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,639 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.18653297424316406
2023-01-07 08:52:28,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,639 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -0.6805820465087891
2023-01-07 08:52:28,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.434969902038574
2023-01-07 08:52:28,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,640 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1877079010009766
2023-01-07 08:52:28,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,641 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:52:28,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 583.1272583007812
2023-01-07 08:52:28,641 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,641 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 1385.494384765625
2023-01-07 08:52:28,642 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,642 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -0.5015053749084473
2023-01-07 08:52:28,642 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,642 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -143.12619018554688
2023-01-07 08:52:28,643 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,644 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -0.5015053749084473
2023-01-07 08:52:28,644 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7820339202880859
2023-01-07 08:52:28,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 10.520393371582031
2023-01-07 08:52:28,644 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7988128662109375
2023-01-07 08:52:28,646 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,646 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 10.520393371582031
2023-01-07 08:52:28,646 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:28,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.324678421020508
2023-01-07 08:52:28,648 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,648 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.11651468276977539
2023-01-07 08:52:28,649 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:28,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -637.8452758789062
2023-01-07 08:52:28,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.1011734008789
2023-01-07 08:52:28,652 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,652 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -637.8452758789062
2023-01-07 08:52:28,652 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,652 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -2.61795711517334
2023-01-07 08:52:28,652 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,652 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -123.79680633544922
2023-01-07 08:52:28,653 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,653 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -162.38467407226562
2023-01-07 08:52:28,654 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,654 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -2.61795711517334
2023-01-07 08:52:28,654 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,655 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.863158702850342
2023-01-07 08:52:28,656 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,656 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -123.79680633544922
2023-01-07 08:52:28,656 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,656 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -2.2648351192474365
2023-01-07 08:52:28,656 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,657 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 8.752939224243164
2023-01-07 08:52:28,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.09864044189453
2023-01-07 08:52:28,658 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,658 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -2.2648351192474365
2023-01-07 08:52:28,658 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.38282775878906
2023-01-07 08:52:28,659 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,659 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -236.0964813232422
2023-01-07 08:52:28,659 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.287951469421387
2023-01-07 08:52:28,661 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,661 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 6.809615135192871
2023-01-07 08:52:28,661 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,661 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,661 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,661 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 84.77936553955078
2023-01-07 08:52:28,662 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,663 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 8.752939224243164
2023-01-07 08:52:28,663 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.7145538330078125
2023-01-07 08:52:28,663 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -239.80528259277344
2023-01-07 08:52:28,664 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,664 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.7145538330078125
2023-01-07 08:52:28,665 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:28,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,665 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -283.0618591308594
2023-01-07 08:52:28,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,665 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -70.51727294921875
2023-01-07 08:52:28,666 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,667 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -283.0618591308594
2023-01-07 08:52:28,667 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,667 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,667 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.12379336357116699
2023-01-07 08:52:28,667 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,667 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 26.83971405029297
2023-01-07 08:52:28,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.7274169921875
2023-01-07 08:52:28,668 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,669 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.12379336357116699
2023-01-07 08:52:28,669 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,669 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.15423583984375
2023-01-07 08:52:28,670 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,670 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 26.83971405029297
2023-01-07 08:52:28,670 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:28,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,671 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 24.536216735839844
2023-01-07 08:52:28,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.943791389465332
2023-01-07 08:52:28,672 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,672 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 2.3827574253082275
2023-01-07 08:52:28,672 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,672 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 24.536216735839844
2023-01-07 08:52:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,673 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.056427001953125
2023-01-07 08:52:28,674 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,674 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 24.536216735839844
2023-01-07 08:52:28,674 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,674 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.97763061523438
2023-01-07 08:52:28,675 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,676 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -1.395061731338501
2023-01-07 08:52:28,676 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:28,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,676 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -52.05615234375
2023-01-07 08:52:28,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.84292984008789
2023-01-07 08:52:28,677 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,677 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -52.05615234375
2023-01-07 08:52:28,677 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,677 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:28,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,678 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.8642498850822449
2023-01-07 08:52:28,678 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,678 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -22.64188003540039
2023-01-07 08:52:28,678 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,678 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -35.85394287109375
2023-01-07 08:52:28,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.2237777709961
2023-01-07 08:52:28,679 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,680 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8642498850822449
2023-01-07 08:52:28,680 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.704994201660156
2023-01-07 08:52:28,681 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,681 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -22.64188003540039
2023-01-07 08:52:28,681 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 6.527358055114746
2023-01-07 08:52:28,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -35.85394287109375
2023-01-07 08:52:28,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.750465393066406
2023-01-07 08:52:28,683 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,683 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 0.9625788331031799
2023-01-07 08:52:28,683 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,683 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,683 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.85603713989258
2023-01-07 08:52:28,685 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,685 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -35.85394287109375
2023-01-07 08:52:28,685 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.017770767211914
2023-01-07 08:52:28,686 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,686 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.36130285263061523
2023-01-07 08:52:28,686 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,686 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:28,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 0.5328445434570312
2023-01-07 08:52:28,687 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.520803451538086
2023-01-07 08:52:28,688 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,688 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 0.5328445434570312
2023-01-07 08:52:28,688 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,688 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.38562506437301636
2023-01-07 08:52:28,689 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.16387367248535
2023-01-07 08:52:28,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.364529132843018
2023-01-07 08:52:28,690 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,690 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.38562506437301636
2023-01-07 08:52:28,690 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,690 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,690 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -3.782032012939453
2023-01-07 08:52:28,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.16387367248535
2023-01-07 08:52:28,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3132271766662598
2023-01-07 08:52:28,692 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,692 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -16.16387367248535
2023-01-07 08:52:28,692 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,692 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 1.8635627031326294
2023-01-07 08:52:28,692 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.61479568481445
2023-01-07 08:52:28,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.768083572387695
2023-01-07 08:52:28,693 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,694 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 1.8635627031326294
2023-01-07 08:52:28,694 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,694 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -27.419443130493164
2023-01-07 08:52:28,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.61479568481445
2023-01-07 08:52:28,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.379032135009766
2023-01-07 08:52:28,695 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,695 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -60.61479568481445
2023-01-07 08:52:28,695 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:28,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.0786590576171875
2023-01-07 08:52:28,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.893275737762451
2023-01-07 08:52:28,697 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,697 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.5423759818077087
2023-01-07 08:52:28,697 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:28,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 1.0786590576171875
2023-01-07 08:52:28,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.4442219734191895
2023-01-07 08:52:28,698 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,699 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 1.0786590576171875
2023-01-07 08:52:28,699 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.883522093296051
2023-01-07 08:52:28,699 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -41.73194122314453
2023-01-07 08:52:28,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1435916423797607
2023-01-07 08:52:28,700 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,701 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.883522093296051
2023-01-07 08:52:28,701 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.335868000984192
2023-01-07 08:52:28,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -41.73194122314453
2023-01-07 08:52:28,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.800162315368652
2023-01-07 08:52:28,702 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,702 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -41.73194122314453
2023-01-07 08:52:28,702 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.2095094919204712
2023-01-07 08:52:28,703 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -26.017581939697266
2023-01-07 08:52:28,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.978891372680664
2023-01-07 08:52:28,704 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,704 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.2095094919204712
2023-01-07 08:52:28,704 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.009390830993652
2023-01-07 08:52:28,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -26.017581939697266
2023-01-07 08:52:28,705 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7249139547348022
2023-01-07 08:52:28,706 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,706 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -26.017581939697266
2023-01-07 08:52:28,706 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:28,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.578182220458984
2023-01-07 08:52:28,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8798844814300537
2023-01-07 08:52:28,707 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,708 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.7669427990913391
2023-01-07 08:52:28,708 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:28,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.578182220458984
2023-01-07 08:52:28,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.783871173858643
2023-01-07 08:52:28,709 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,709 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 42.578182220458984
2023-01-07 08:52:28,709 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,710 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,710 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.7392605543136597
2023-01-07 08:52:28,710 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,710 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.884986877441406
2023-01-07 08:52:28,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.265185356140137
2023-01-07 08:52:28,711 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,711 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.7392605543136597
2023-01-07 08:52:28,711 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.902972221374512
2023-01-07 08:52:28,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,712 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -58.884986877441406
2023-01-07 08:52:28,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.395223617553711
2023-01-07 08:52:28,713 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,713 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -58.884986877441406
2023-01-07 08:52:28,713 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.24781399965286255
2023-01-07 08:52:28,714 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.07331657409668
2023-01-07 08:52:28,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.643561840057373
2023-01-07 08:52:28,715 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,715 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.24781399965286255
2023-01-07 08:52:28,715 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.7452926635742188
2023-01-07 08:52:28,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,716 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.07331657409668
2023-01-07 08:52:28,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3996317386627197
2023-01-07 08:52:28,717 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,717 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -22.07331657409668
2023-01-07 08:52:28,717 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,717 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.6553125381469727
2023-01-07 08:52:28,717 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,718 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.119552612304688
2023-01-07 08:52:28,718 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.452652931213379
2023-01-07 08:52:28,718 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,719 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.6553125381469727
2023-01-07 08:52:28,719 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,719 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,719 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 4.1814117431640625
2023-01-07 08:52:28,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,719 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.119552612304688
2023-01-07 08:52:28,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.933132171630859
2023-01-07 08:52:28,720 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,720 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -24.119552612304688
2023-01-07 08:52:28,720 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,721 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.1143808662891388
2023-01-07 08:52:28,721 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 10.71603775024414
2023-01-07 08:52:28,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.901098251342773
2023-01-07 08:52:28,722 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,722 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.1143808662891388
2023-01-07 08:52:28,722 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.66290283203125
2023-01-07 08:52:28,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 10.71603775024414
2023-01-07 08:52:28,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.531761169433594
2023-01-07 08:52:28,724 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,724 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 10.71603775024414
2023-01-07 08:52:28,725 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,725 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,725 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:28,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.14478014409542084
2023-01-07 08:52:28,725 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0113956928253174
2023-01-07 08:52:28,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.37109899520874
2023-01-07 08:52:28,726 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,726 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.14478014409542084
2023-01-07 08:52:28,727 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,727 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -4.248758316040039
2023-01-07 08:52:28,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0113956928253174
2023-01-07 08:52:28,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.063153028488159
2023-01-07 08:52:28,728 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,728 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 2.0113956928253174
2023-01-07 08:52:28,728 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,728 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:28,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.7863061428070068
2023-01-07 08:52:28,729 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,729 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.01077938079834
2023-01-07 08:52:28,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8084473609924316
2023-01-07 08:52:28,730 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,730 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.7863061428070068
2023-01-07 08:52:28,730 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.61199951171875
2023-01-07 08:52:28,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,731 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.01077938079834
2023-01-07 08:52:28,731 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.722806453704834
2023-01-07 08:52:28,732 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,732 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -7.01077938079834
2023-01-07 08:52:28,732 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.1716391146183014
2023-01-07 08:52:28,732 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -459.9798583984375
2023-01-07 08:52:28,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.445412635803223
2023-01-07 08:52:28,734 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,734 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.1716391146183014
2023-01-07 08:52:28,734 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,734 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.29677963256836
2023-01-07 08:52:28,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,734 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -459.9798583984375
2023-01-07 08:52:28,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.403046131134033
2023-01-07 08:52:28,736 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,736 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -459.9798583984375
2023-01-07 08:52:28,736 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:28,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.85197067260742
2023-01-07 08:52:28,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2463297843933105
2023-01-07 08:52:28,737 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,737 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.05754369497299194
2023-01-07 08:52:28,737 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:28,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.85197067260742
2023-01-07 08:52:28,738 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.272068977355957
2023-01-07 08:52:28,739 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,739 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 47.85197067260742
2023-01-07 08:52:28,739 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:28,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 224.5889892578125
2023-01-07 08:52:28,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.984750509262085
2023-01-07 08:52:28,740 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,741 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.7817401885986328
2023-01-07 08:52:28,741 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:28,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,741 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 224.5889892578125
2023-01-07 08:52:28,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.067755699157715
2023-01-07 08:52:28,742 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,742 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 224.5889892578125
2023-01-07 08:52:28,742 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:28,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 102.53419494628906
2023-01-07 08:52:28,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8389199376106262
2023-01-07 08:52:28,744 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,744 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.04788518697023392
2023-01-07 08:52:28,744 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:28,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,744 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 102.53419494628906
2023-01-07 08:52:28,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.107038736343384
2023-01-07 08:52:28,745 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,746 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 102.53419494628906
2023-01-07 08:52:28,746 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,746 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,746 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:28,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.11930783092975616
2023-01-07 08:52:28,746 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:52:28,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 223.83282470703125
2023-01-07 08:52:28,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0166752338409424
2023-01-07 08:52:28,747 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,747 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.11930783092975616
2023-01-07 08:52:28,748 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,748 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,748 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.882823944091797
2023-01-07 08:52:28,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 223.83282470703125
2023-01-07 08:52:28,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3072757720947266
2023-01-07 08:52:28,749 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,749 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 223.83282470703125
2023-01-07 08:52:28,749 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:28,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 50.80482482910156
2023-01-07 08:52:28,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8342013359069824
2023-01-07 08:52:28,751 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,751 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.49643754959106445
2023-01-07 08:52:28,751 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:28,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,751 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 50.80482482910156
2023-01-07 08:52:28,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1364158391952515
2023-01-07 08:52:28,752 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,753 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 50.80482482910156
2023-01-07 08:52:28,753 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:28,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.366442680358887
2023-01-07 08:52:28,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.694547176361084
2023-01-07 08:52:28,754 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,754 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.18679776787757874
2023-01-07 08:52:28,754 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:28,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.366442680358887
2023-01-07 08:52:28,755 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0526185035705566
2023-01-07 08:52:28,756 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,756 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 12.366442680358887
2023-01-07 08:52:28,756 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:28,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,756 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.504852294921875
2023-01-07 08:52:28,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9409899711608887
2023-01-07 08:52:28,757 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,757 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.10813255608081818
2023-01-07 08:52:28,758 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:28,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 26.504852294921875
2023-01-07 08:52:28,758 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.095624327659607
2023-01-07 08:52:28,759 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,759 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 26.504852294921875
2023-01-07 08:52:28,759 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,759 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,759 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:28,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.613455772399902
2023-01-07 08:52:28,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.078853130340576
2023-01-07 08:52:28,761 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,761 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.06885696202516556
2023-01-07 08:52:28,761 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,761 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:28,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.613455772399902
2023-01-07 08:52:28,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2668418884277344
2023-01-07 08:52:28,762 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,763 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -4.613455772399902
2023-01-07 08:52:28,763 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,763 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,763 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:28,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,763 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.854259490966797
2023-01-07 08:52:28,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3984222412109375
2023-01-07 08:52:28,764 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,764 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.01487039029598236
2023-01-07 08:52:28,764 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:28,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,765 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -14.854259490966797
2023-01-07 08:52:28,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6554033160209656
2023-01-07 08:52:28,766 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,766 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -14.854259490966797
2023-01-07 08:52:28,766 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:28,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1332.1929931640625
2023-01-07 08:52:28,767 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3266831040382385
2023-01-07 08:52:28,767 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,767 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.045543596148490906
2023-01-07 08:52:28,768 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:28,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,768 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1332.1929931640625
2023-01-07 08:52:28,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1054359674453735
2023-01-07 08:52:28,769 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,769 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 1332.1929931640625
2023-01-07 08:52:28,769 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,769 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,769 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:28,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -56.15372848510742
2023-01-07 08:52:28,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32565295696258545
2023-01-07 08:52:28,771 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,771 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.08289623260498047
2023-01-07 08:52:28,771 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:28,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,771 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -56.15372848510742
2023-01-07 08:52:28,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5847662687301636
2023-01-07 08:52:28,772 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,773 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -56.15372848510742
2023-01-07 08:52:28,773 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:28,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 105.88877868652344
2023-01-07 08:52:28,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.064016580581665
2023-01-07 08:52:28,774 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,774 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.20606201887130737
2023-01-07 08:52:28,774 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,774 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:28,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,775 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 105.88877868652344
2023-01-07 08:52:28,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5383950471878052
2023-01-07 08:52:28,776 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,776 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 105.88877868652344
2023-01-07 08:52:28,776 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:28,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,776 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 59.50096893310547
2023-01-07 08:52:28,777 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.363789439201355
2023-01-07 08:52:28,777 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,778 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.06528444588184357
2023-01-07 08:52:28,778 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:28,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,778 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 59.50096893310547
2023-01-07 08:52:28,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8445755243301392
2023-01-07 08:52:28,779 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,779 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 59.50096893310547
2023-01-07 08:52:28,779 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,780 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:28,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,780 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1398.093017578125
2023-01-07 08:52:28,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4674628973007202
2023-01-07 08:52:28,781 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,781 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.08500096201896667
2023-01-07 08:52:28,781 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,781 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,781 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:28,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,781 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1398.093017578125
2023-01-07 08:52:28,782 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0346695184707642
2023-01-07 08:52:28,782 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,783 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1398.093017578125
2023-01-07 08:52:28,783 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:28,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,783 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 377.22265625
2023-01-07 08:52:28,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2360206842422485
2023-01-07 08:52:28,784 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,784 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.10633593052625656
2023-01-07 08:52:28,784 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,784 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:28,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 377.22265625
2023-01-07 08:52:28,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1147549152374268
2023-01-07 08:52:28,786 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,786 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 377.22265625
2023-01-07 08:52:28,786 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,786 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,786 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:28,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 828.45703125
2023-01-07 08:52:28,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7746646404266357
2023-01-07 08:52:28,787 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,788 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.04966271296143532
2023-01-07 08:52:28,788 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,788 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,788 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:28,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 828.45703125
2023-01-07 08:52:28,788 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09469211101531982
2023-01-07 08:52:28,789 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,789 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 828.45703125
2023-01-07 08:52:28,789 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,790 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:28,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 501.6304931640625
2023-01-07 08:52:28,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1265708208084106
2023-01-07 08:52:28,791 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,791 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.07173182815313339
2023-01-07 08:52:28,791 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:28,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 501.6304931640625
2023-01-07 08:52:28,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40832602977752686
2023-01-07 08:52:28,792 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,793 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 501.6304931640625
2023-01-07 08:52:28,793 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,793 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,793 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:28,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,793 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 984.3370971679688
2023-01-07 08:52:28,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2282545566558838
2023-01-07 08:52:28,794 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,794 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.1549271047115326
2023-01-07 08:52:28,794 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:28,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 984.3370971679688
2023-01-07 08:52:28,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2918789386749268
2023-01-07 08:52:28,796 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,796 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 984.3370971679688
2023-01-07 08:52:28,796 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,796 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,796 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:28,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 889.29443359375
2023-01-07 08:52:28,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3673408031463623
2023-01-07 08:52:28,797 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,797 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.016315333545207977
2023-01-07 08:52:28,798 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,798 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:28,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 889.29443359375
2023-01-07 08:52:28,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.841428279876709
2023-01-07 08:52:28,799 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,799 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 889.29443359375
2023-01-07 08:52:28,799 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:28,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,800 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.977278232574463
2023-01-07 08:52:28,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13029535114765167
2023-01-07 08:52:28,801 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,801 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.015128865838050842
2023-01-07 08:52:28,801 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,801 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:28,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,802 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.977278232574463
2023-01-07 08:52:28,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2822352945804596
2023-01-07 08:52:28,803 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,803 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -5.977278232574463
2023-01-07 08:52:28,803 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,803 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,803 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:28,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,803 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.867344856262207
2023-01-07 08:52:28,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46750903129577637
2023-01-07 08:52:28,804 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,804 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.24687334895133972
2023-01-07 08:52:28,804 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,805 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:28,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,805 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -11.867344856262207
2023-01-07 08:52:28,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4337555766105652
2023-01-07 08:52:28,806 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,806 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -11.867344856262207
2023-01-07 08:52:28,806 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:28,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,807 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.253800392150879
2023-01-07 08:52:28,807 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02360587567090988
2023-01-07 08:52:28,808 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,808 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.339240163564682
2023-01-07 08:52:28,808 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,808 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:28,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,809 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 732.4356079101562
2023-01-07 08:52:28,809 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44494616985321045
2023-01-07 08:52:28,810 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,810 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 732.4356079101562
2023-01-07 08:52:28,810 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,810 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,810 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:28,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,811 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -18.919118881225586
2023-01-07 08:52:28,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8030213713645935
2023-01-07 08:52:28,812 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,812 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.020444059744477272
2023-01-07 08:52:28,812 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,812 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,812 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:28,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,812 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -18.919118881225586
2023-01-07 08:52:28,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46387410163879395
2023-01-07 08:52:28,813 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,813 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -18.919118881225586
2023-01-07 08:52:28,814 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,814 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,814 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:28,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,814 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.118487358093262
2023-01-07 08:52:28,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03701375424861908
2023-01-07 08:52:28,815 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,815 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.02302417904138565
2023-01-07 08:52:28,815 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,815 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,815 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:28,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,816 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.118487358093262
2023-01-07 08:52:28,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8421477675437927
2023-01-07 08:52:28,817 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,817 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -4.118487358093262
2023-01-07 08:52:28,817 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,817 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,817 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:28,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,817 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.18653297424316406
2023-01-07 08:52:28,818 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20336702466011047
2023-01-07 08:52:28,818 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,818 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.5041651129722595
2023-01-07 08:52:28,818 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,819 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,819 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:28,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,819 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.18653297424316406
2023-01-07 08:52:28,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18111659586429596
2023-01-07 08:52:28,820 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,820 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.18653297424316406
2023-01-07 08:52:28,820 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,820 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:28,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,821 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -0.6805820465087891
2023-01-07 08:52:28,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1894199550151825
2023-01-07 08:52:28,822 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,822 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.01946328580379486
2023-01-07 08:52:28,822 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,822 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,822 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:28,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,822 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -0.6805820465087891
2023-01-07 08:52:28,822 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.046021826565265656
2023-01-07 08:52:28,823 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,823 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -0.6805820465087891
2023-01-07 08:52:28,824 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,824 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,824 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:28,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,824 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1877079010009766
2023-01-07 08:52:28,824 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25465887784957886
2023-01-07 08:52:28,825 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,825 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.009145371615886688
2023-01-07 08:52:28,825 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,825 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,825 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:28,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,826 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1877079010009766
2023-01-07 08:52:28,826 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24319018423557281
2023-01-07 08:52:28,827 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,827 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 3.1877079010009766
2023-01-07 08:52:28,827 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,827 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,827 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:28,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,827 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:52:28,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.639186382293701
2023-01-07 08:52:28,828 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,828 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.5116667151451111
2023-01-07 08:52:28,828 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,829 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:28,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,829 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:52:28,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.506577730178833
2023-01-07 08:52:28,830 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:28,830 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -0.027069091796875
2023-01-07 08:52:28,830 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:28,831 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:28,831 > [DEBUG] 0 :: 7.555277347564697
2023-01-07 08:52:28,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,835 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.004180908203125
2023-01-07 08:52:28,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -313.9917297363281
2023-01-07 08:52:28,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,837 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4569450616836548
2023-01-07 08:52:28,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,838 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.004180908203125
2023-01-07 08:52:28,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -398.55059814453125
2023-01-07 08:52:28,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,840 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.098460674285889
2023-01-07 08:52:28,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21268828213214874
2023-01-07 08:52:28,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.0201420858502388
2023-01-07 08:52:28,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.098460674285889
2023-01-07 08:52:28,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05505365878343582
2023-01-07 08:52:28,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,846 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6.802179336547852
2023-01-07 08:52:28,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09076006710529327
2023-01-07 08:52:28,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.03140222281217575
2023-01-07 08:52:28,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6.802179336547852
2023-01-07 08:52:28,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40520331263542175
2023-01-07 08:52:28,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.3560152053833008
2023-01-07 08:52:28,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2760668396949768
2023-01-07 08:52:28,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,851 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.33160871267318726
2023-01-07 08:52:28,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,851 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.3560152053833008
2023-01-07 08:52:28,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8044965267181396
2023-01-07 08:52:28,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,853 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 7.258631706237793
2023-01-07 08:52:28,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2267482429742813
2023-01-07 08:52:28,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,854 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.003277920186519623
2023-01-07 08:52:28,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,854 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 7.258631706237793
2023-01-07 08:52:28,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16769033670425415
2023-01-07 08:52:28,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,856 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 0.2617216110229492
2023-01-07 08:52:28,856 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20234137773513794
2023-01-07 08:52:28,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,857 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.012575007975101471
2023-01-07 08:52:28,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,857 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 0.2617216110229492
2023-01-07 08:52:28,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13879473507404327
2023-01-07 08:52:28,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,859 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.867097854614258
2023-01-07 08:52:28,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18927742540836334
2023-01-07 08:52:28,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.3712242841720581
2023-01-07 08:52:28,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.867097854614258
2023-01-07 08:52:28,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08642619103193283
2023-01-07 08:52:28,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,862 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1190.4210205078125
2023-01-07 08:52:28,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2285221517086029
2023-01-07 08:52:28,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,863 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1763181984424591
2023-01-07 08:52:28,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,863 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1190.4210205078125
2023-01-07 08:52:28,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.760132908821106
2023-01-07 08:52:28,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,865 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 114.54248046875
2023-01-07 08:52:28,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2698713541030884
2023-01-07 08:52:28,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,866 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.07465416193008423
2023-01-07 08:52:28,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,866 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 114.54248046875
2023-01-07 08:52:28,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12763184309005737
2023-01-07 08:52:28,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,868 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -15.52926254272461
2023-01-07 08:52:28,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.199115753173828
2023-01-07 08:52:28,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,869 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.12175893038511276
2023-01-07 08:52:28,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,870 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -15.52926254272461
2023-01-07 08:52:28,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0815792083740234
2023-01-07 08:52:28,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,871 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -20.997207641601562
2023-01-07 08:52:28,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46302714943885803
2023-01-07 08:52:28,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.16211259365081787
2023-01-07 08:52:28,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,873 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -20.997207641601562
2023-01-07 08:52:28,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.475540041923523
2023-01-07 08:52:28,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.161581039428711
2023-01-07 08:52:28,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20805829763412476
2023-01-07 08:52:28,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.10171712934970856
2023-01-07 08:52:28,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.161581039428711
2023-01-07 08:52:28,876 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37880855798721313
2023-01-07 08:52:28,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -15.399375915527344
2023-01-07 08:52:28,878 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9974191188812256
2023-01-07 08:52:28,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,879 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.00977657362818718
2023-01-07 08:52:28,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,879 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -15.399375915527344
2023-01-07 08:52:28,879 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.980933904647827
2023-01-07 08:52:28,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.361075401306152
2023-01-07 08:52:28,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07842129468917847
2023-01-07 08:52:28,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.13415440917015076
2023-01-07 08:52:28,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.361075401306152
2023-01-07 08:52:28,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.938845694065094
2023-01-07 08:52:28,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.2479915618896484
2023-01-07 08:52:28,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4474259316921234
2023-01-07 08:52:28,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,885 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.054134614765644073
2023-01-07 08:52:28,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,885 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.2479915618896484
2023-01-07 08:52:28,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6945311427116394
2023-01-07 08:52:28,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 6.848174095153809
2023-01-07 08:52:28,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46671605110168457
2023-01-07 08:52:28,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,888 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.01889774203300476
2023-01-07 08:52:28,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,888 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 6.848174095153809
2023-01-07 08:52:28,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7422475814819336
2023-01-07 08:52:28,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.179058074951172
2023-01-07 08:52:28,890 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16179019212722778
2023-01-07 08:52:28,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,891 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.07144492864608765
2023-01-07 08:52:28,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,891 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.179058074951172
2023-01-07 08:52:28,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10511541366577148
2023-01-07 08:52:28,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8130519390106201
2023-01-07 08:52:28,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4891813397407532
2023-01-07 08:52:28,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,894 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.05016700178384781
2023-01-07 08:52:28,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,894 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8130519390106201
2023-01-07 08:52:28,895 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.774817943572998
2023-01-07 08:52:28,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,896 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.691570281982422
2023-01-07 08:52:28,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0968866348266602
2023-01-07 08:52:28,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,897 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.13014622032642365
2023-01-07 08:52:28,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,898 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.691570281982422
2023-01-07 08:52:28,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8339149951934814
2023-01-07 08:52:28,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.3523976802825928
2023-01-07 08:52:28,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.443857431411743
2023-01-07 08:52:28,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,900 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.19611787796020508
2023-01-07 08:52:28,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,901 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.3523976802825928
2023-01-07 08:52:28,901 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6329861283302307
2023-01-07 08:52:28,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.575687408447266
2023-01-07 08:52:28,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2516438961029053
2023-01-07 08:52:28,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.1445143073797226
2023-01-07 08:52:28,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.575687408447266
2023-01-07 08:52:28,904 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.548477053642273
2023-01-07 08:52:28,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.126192092895508
2023-01-07 08:52:28,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34068384766578674
2023-01-07 08:52:28,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,906 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.10156912356615067
2023-01-07 08:52:28,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.126192092895508
2023-01-07 08:52:28,907 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5173639059066772
2023-01-07 08:52:28,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.317920207977295
2023-01-07 08:52:28,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6050233244895935
2023-01-07 08:52:28,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,909 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.11085262894630432
2023-01-07 08:52:28,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,910 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.317920207977295
2023-01-07 08:52:28,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.326766014099121
2023-01-07 08:52:28,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,911 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.7697434425354
2023-01-07 08:52:28,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3639814555644989
2023-01-07 08:52:28,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,912 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.23710086941719055
2023-01-07 08:52:28,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,913 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.7697434425354
2023-01-07 08:52:28,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9450726509094238
2023-01-07 08:52:28,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,915 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.221303939819336
2023-01-07 08:52:28,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24113066494464874
2023-01-07 08:52:28,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,916 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.221303939819336
2023-01-07 08:52:28,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2962918281555176
2023-01-07 08:52:28,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,917 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 22.066211700439453
2023-01-07 08:52:28,917 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3817622661590576
2023-01-07 08:52:28,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,919 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.31727635860443115
2023-01-07 08:52:28,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,919 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 22.066211700439453
2023-01-07 08:52:28,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3672930002212524
2023-01-07 08:52:28,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,920 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.76263427734375
2023-01-07 08:52:28,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07298172265291214
2023-01-07 08:52:28,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,922 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.431196928024292
2023-01-07 08:52:28,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,922 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.76263427734375
2023-01-07 08:52:28,922 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2083138227462769
2023-01-07 08:52:28,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,923 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.02133560180664
2023-01-07 08:52:28,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5724848508834839
2023-01-07 08:52:28,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,925 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.15536227822303772
2023-01-07 08:52:28,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,925 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.02133560180664
2023-01-07 08:52:28,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4314996004104614
2023-01-07 08:52:28,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.13911056518555
2023-01-07 08:52:28,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7251882553100586
2023-01-07 08:52:28,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,928 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.13911056518555
2023-01-07 08:52:28,928 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9855942726135254
2023-01-07 08:52:28,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,929 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -16.20087432861328
2023-01-07 08:52:28,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.260202407836914
2023-01-07 08:52:28,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,930 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -16.20087432861328
2023-01-07 08:52:28,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.079408645629883
2023-01-07 08:52:28,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,932 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 9.980828285217285
2023-01-07 08:52:28,932 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1523184776306152
2023-01-07 08:52:28,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,933 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 9.980828285217285
2023-01-07 08:52:28,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3313400745391846
2023-01-07 08:52:28,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,934 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -73.79924011230469
2023-01-07 08:52:28,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.085723876953125
2023-01-07 08:52:28,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,936 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -73.79924011230469
2023-01-07 08:52:28,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.35956573486328
2023-01-07 08:52:28,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,937 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.4687662124633789
2023-01-07 08:52:28,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.237074851989746
2023-01-07 08:52:28,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,938 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.4687662124633789
2023-01-07 08:52:28,939 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6071634888648987
2023-01-07 08:52:28,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,940 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.108116149902344
2023-01-07 08:52:28,940 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2782466411590576
2023-01-07 08:52:28,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,941 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.108116149902344
2023-01-07 08:52:28,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.763689637184143
2023-01-07 08:52:28,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,942 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -60.922157287597656
2023-01-07 08:52:28,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.20090651512146
2023-01-07 08:52:28,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,944 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -60.922157287597656
2023-01-07 08:52:28,944 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.832374572753906
2023-01-07 08:52:28,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,945 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -35.646629333496094
2023-01-07 08:52:28,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.366670608520508
2023-01-07 08:52:28,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,946 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.020236879587173462
2023-01-07 08:52:28,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,947 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -35.646629333496094
2023-01-07 08:52:28,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4241037368774414
2023-01-07 08:52:28,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,948 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.7081379890441895
2023-01-07 08:52:28,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23112954199314117
2023-01-07 08:52:28,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,950 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.7081379890441895
2023-01-07 08:52:28,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.31320571899414
2023-01-07 08:52:28,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,951 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 108.32154846191406
2023-01-07 08:52:28,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.190784454345703
2023-01-07 08:52:28,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,952 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 108.32154846191406
2023-01-07 08:52:28,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.775249481201172
2023-01-07 08:52:28,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,954 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -29.770118713378906
2023-01-07 08:52:28,954 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.870316743850708
2023-01-07 08:52:28,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,955 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.39478856325149536
2023-01-07 08:52:28,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,955 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -29.770118713378906
2023-01-07 08:52:28,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.91292953491211
2023-01-07 08:52:28,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,957 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.696474075317383
2023-01-07 08:52:28,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07048803567886353
2023-01-07 08:52:28,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,958 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.696474075317383
2023-01-07 08:52:28,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.241323471069336
2023-01-07 08:52:28,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,959 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.70334243774414
2023-01-07 08:52:28,959 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5495692491531372
2023-01-07 08:52:28,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,961 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.70334243774414
2023-01-07 08:52:28,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.077191352844238
2023-01-07 08:52:28,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,962 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,962 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.242437362670898
2023-01-07 08:52:28,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,963 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.8158180713653564
2023-01-07 08:52:28,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,964 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.546710014343262
2023-01-07 08:52:28,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,965 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.087618827819824
2023-01-07 08:52:28,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.3185447454452515
2023-01-07 08:52:28,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,967 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 19.01219940185547
2023-01-07 08:52:28,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,967 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,967 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.709436416625977
2023-01-07 08:52:28,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,969 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,969 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.740694046020508
2023-01-07 08:52:28,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,970 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 19.01219940185547
2023-01-07 08:52:28,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.55170440673828
2023-01-07 08:52:28,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,971 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.72768211364746
2023-01-07 08:52:28,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.9675228595733643
2023-01-07 08:52:28,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,973 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.405616760253906
2023-01-07 08:52:28,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,974 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.699566841125488
2023-01-07 08:52:28,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,976 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 4.400345802307129
2023-01-07 08:52:28,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,976 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 27.61028289794922
2023-01-07 08:52:28,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.357481002807617
2023-01-07 08:52:28,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,977 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.708648681640625
2023-01-07 08:52:28,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,979 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -31.828704833984375
2023-01-07 08:52:28,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,979 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.659971237182617
2023-01-07 08:52:28,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,980 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,981 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.798668146133423
2023-01-07 08:52:28,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,982 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:28,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.849288940429688
2023-01-07 08:52:28,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,983 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.72275161743164
2023-01-07 08:52:28,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,985 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.238060474395752
2023-01-07 08:52:28,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,985 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 8.490056991577148
2023-01-07 08:52:28,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,985 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.071014404296875
2023-01-07 08:52:28,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,987 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.239990234375
2023-01-07 08:52:28,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,989 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 70.01274108886719
2023-01-07 08:52:28,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2628135681152344
2023-01-07 08:52:28,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,990 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:28,990 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.087238311767578
2023-01-07 08:52:28,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,991 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 70.01274108886719
2023-01-07 08:52:28,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,992 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:28,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 71.5276870727539
2023-01-07 08:52:28,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,993 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:28,993 > [DEBUG] 0 :: before allreduce fusion buffer :: -168.62071228027344
2023-01-07 08:52:28,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,994 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.0143399238586426
2023-01-07 08:52:28,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,995 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:28,995 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.89512634277344
2023-01-07 08:52:28,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,996 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:28,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 290.63653564453125
2023-01-07 08:52:28,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:28,997 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:28,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.971435546875
2023-01-07 08:52:28,999 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:52:28,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:28,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,000 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1216.7464599609375
2023-01-07 08:52:29,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,000 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:29,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,000 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,000 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.70334243774414
2023-01-07 08:52:29,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.696474075317383
2023-01-07 08:52:29,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -29.770118713378906
2023-01-07 08:52:29,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 108.32154846191406
2023-01-07 08:52:29,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.7081379890441895
2023-01-07 08:52:29,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -35.646629333496094
2023-01-07 08:52:29,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -60.922157287597656
2023-01-07 08:52:29,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.108116149902344
2023-01-07 08:52:29,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.4687662124633789
2023-01-07 08:52:29,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -73.79924011230469
2023-01-07 08:52:29,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 9.980828285217285
2023-01-07 08:52:29,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -16.20087432861328
2023-01-07 08:52:29,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.13911056518555
2023-01-07 08:52:29,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.02133560180664
2023-01-07 08:52:29,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.76263427734375
2023-01-07 08:52:29,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 22.066211700439453
2023-01-07 08:52:29,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.221303939819336
2023-01-07 08:52:29,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.7697434425354
2023-01-07 08:52:29,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.317920207977295
2023-01-07 08:52:29,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.126192092895508
2023-01-07 08:52:29,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.575687408447266
2023-01-07 08:52:29,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,007 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.3523976802825928
2023-01-07 08:52:29,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,007 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.691570281982422
2023-01-07 08:52:29,007 > [DEBUG] 0 :: before allreduce fusion buffer :: -1493.276123046875
2023-01-07 08:52:29,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,009 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8130519390106201
2023-01-07 08:52:29,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,009 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.179058074951172
2023-01-07 08:52:29,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,010 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 6.848174095153809
2023-01-07 08:52:29,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,010 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.2479915618896484
2023-01-07 08:52:29,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,010 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.361075401306152
2023-01-07 08:52:29,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,011 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -15.399375915527344
2023-01-07 08:52:29,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,011 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.161581039428711
2023-01-07 08:52:29,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,011 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -20.997207641601562
2023-01-07 08:52:29,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,011 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -15.52926254272461
2023-01-07 08:52:29,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -261.7879638671875
2023-01-07 08:52:29,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,013 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 114.54248046875
2023-01-07 08:52:29,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,013 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1190.4210205078125
2023-01-07 08:52:29,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,013 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.867097854614258
2023-01-07 08:52:29,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,014 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 546.2763671875
2023-01-07 08:52:29,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 1313.319580078125
2023-01-07 08:52:29,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,015 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 330.824951171875
2023-01-07 08:52:29,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,015 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 5191.46630859375
2023-01-07 08:52:29,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,016 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 697.1783447265625
2023-01-07 08:52:29,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 6760.5830078125
2023-01-07 08:52:29,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,017 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.098460674285889
2023-01-07 08:52:29,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,017 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.004180908203125
2023-01-07 08:52:29,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 715.4396362304688
2023-01-07 08:52:29,018 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,018 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -1216.7464599609375
2023-01-07 08:52:29,019 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,019 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 1.3214430809020996
2023-01-07 08:52:29,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,019 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:29,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.03630065917969
2023-01-07 08:52:29,020 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,020 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 1.3214430809020996
2023-01-07 08:52:29,021 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7820339202880859
2023-01-07 08:52:29,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -19.71373748779297
2023-01-07 08:52:29,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:29,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.483535766601562
2023-01-07 08:52:29,023 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,023 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -19.71373748779297
2023-01-07 08:52:29,023 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,023 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,023 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:29,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,023 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:29,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.990055084228516
2023-01-07 08:52:29,024 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,024 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -2.0143399238586426
2023-01-07 08:52:29,024 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,025 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:29,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -159.18112182617188
2023-01-07 08:52:29,025 > [DEBUG] 0 :: before allreduce fusion buffer :: -74.22068786621094
2023-01-07 08:52:29,026 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,026 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -159.18112182617188
2023-01-07 08:52:29,026 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.6412532329559326
2023-01-07 08:52:29,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 31.57659339904785
2023-01-07 08:52:29,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.782629013061523
2023-01-07 08:52:29,028 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,028 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.6412532329559326
2023-01-07 08:52:29,028 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,029 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.096099853515625
2023-01-07 08:52:29,030 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,030 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 31.57659339904785
2023-01-07 08:52:29,030 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,030 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -4.008869647979736
2023-01-07 08:52:29,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 8.490056991577148
2023-01-07 08:52:29,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.460530281066895
2023-01-07 08:52:29,032 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,032 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -4.008869647979736
2023-01-07 08:52:29,032 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,032 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,033 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:29,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 70.39102172851562
2023-01-07 08:52:29,033 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,034 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 70.01274108886719
2023-01-07 08:52:29,034 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,034 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,034 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:29,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 76.4808349609375
2023-01-07 08:52:29,035 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,035 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -3.238060474395752
2023-01-07 08:52:29,035 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,035 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,036 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,036 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:29,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.53150939941406
2023-01-07 08:52:29,037 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,037 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 8.490056991577148
2023-01-07 08:52:29,037 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,038 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 2.177140712738037
2023-01-07 08:52:29,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,038 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:29,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -269.58905029296875
2023-01-07 08:52:29,039 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,039 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 2.177140712738037
2023-01-07 08:52:29,039 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,039 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -190.04879760742188
2023-01-07 08:52:29,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,040 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.15968322753906
2023-01-07 08:52:29,041 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,041 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -190.04879760742188
2023-01-07 08:52:29,041 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,041 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,041 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,041 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.3766515254974365
2023-01-07 08:52:29,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,042 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -31.828704833984375
2023-01-07 08:52:29,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -74.76215362548828
2023-01-07 08:52:29,043 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,043 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -1.3766515254974365
2023-01-07 08:52:29,043 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,043 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,043 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,043 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.0759048461914
2023-01-07 08:52:29,044 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,045 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -31.828704833984375
2023-01-07 08:52:29,045 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,045 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,045 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:29,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,045 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 27.61028289794922
2023-01-07 08:52:29,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.81488609313965
2023-01-07 08:52:29,046 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,046 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 4.400345802307129
2023-01-07 08:52:29,046 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,046 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,046 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:29,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,047 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 27.61028289794922
2023-01-07 08:52:29,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,047 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.524463653564453
2023-01-07 08:52:29,048 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,048 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 27.61028289794922
2023-01-07 08:52:29,048 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,048 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,049 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,049 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.13481521606445
2023-01-07 08:52:29,050 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,050 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -0.9675228595733643
2023-01-07 08:52:29,050 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,050 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,050 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 181.14187622070312
2023-01-07 08:52:29,051 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.906009674072266
2023-01-07 08:52:29,051 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,052 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 181.14187622070312
2023-01-07 08:52:29,052 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,052 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,052 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,052 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.5194414854049683
2023-01-07 08:52:29,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,052 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 28.209522247314453
2023-01-07 08:52:29,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 19.01219940185547
2023-01-07 08:52:29,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.114391326904297
2023-01-07 08:52:29,054 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,054 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.5194414854049683
2023-01-07 08:52:29,054 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,054 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,054 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,054 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8741531372070312
2023-01-07 08:52:29,055 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,056 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 28.209522247314453
2023-01-07 08:52:29,056 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,056 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 6.527358055114746
2023-01-07 08:52:29,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 19.01219940185547
2023-01-07 08:52:29,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40503931045532227
2023-01-07 08:52:29,057 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,057 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -1.3185447454452515
2023-01-07 08:52:29,057 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,057 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,057 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.87457847595215
2023-01-07 08:52:29,059 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,059 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 19.01219940185547
2023-01-07 08:52:29,059 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,059 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,059 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,059 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3363075256347656
2023-01-07 08:52:29,060 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,061 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.8158180713653564
2023-01-07 08:52:29,061 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,061 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,061 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -61.23128890991211
2023-01-07 08:52:29,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.482245922088623
2023-01-07 08:52:29,062 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,062 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -61.23128890991211
2023-01-07 08:52:29,062 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,062 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,063 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -1.0686089992523193
2023-01-07 08:52:29,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.70334243774414
2023-01-07 08:52:29,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.492692947387695
2023-01-07 08:52:29,064 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,064 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -1.0686089992523193
2023-01-07 08:52:29,064 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,064 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,064 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -3.782032012939453
2023-01-07 08:52:29,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -16.70334243774414
2023-01-07 08:52:29,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4367215633392334
2023-01-07 08:52:29,066 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,066 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -16.70334243774414
2023-01-07 08:52:29,066 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,066 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,066 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.0255277156829834
2023-01-07 08:52:29,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.696474075317383
2023-01-07 08:52:29,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.633960723876953
2023-01-07 08:52:29,068 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,068 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0255277156829834
2023-01-07 08:52:29,068 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,068 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,068 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -27.419443130493164
2023-01-07 08:52:29,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,068 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.696474075317383
2023-01-07 08:52:29,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.72161865234375
2023-01-07 08:52:29,069 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,069 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 26.696474075317383
2023-01-07 08:52:29,069 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,070 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:29,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,070 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -29.770118713378906
2023-01-07 08:52:29,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.4467315673828125
2023-01-07 08:52:29,071 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,071 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.39478856325149536
2023-01-07 08:52:29,071 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,071 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,071 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:29,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -29.770118713378906
2023-01-07 08:52:29,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.616962432861328
2023-01-07 08:52:29,073 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,073 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -29.770118713378906
2023-01-07 08:52:29,073 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,073 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,073 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,073 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.5504086017608643
2023-01-07 08:52:29,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,074 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 108.32154846191406
2023-01-07 08:52:29,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.927831649780273
2023-01-07 08:52:29,075 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,075 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.5504086017608643
2023-01-07 08:52:29,075 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,075 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,075 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.335868000984192
2023-01-07 08:52:29,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 108.32154846191406
2023-01-07 08:52:29,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.018779754638672
2023-01-07 08:52:29,076 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,076 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 108.32154846191406
2023-01-07 08:52:29,077 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,077 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,077 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -1.1566119194030762
2023-01-07 08:52:29,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,077 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.7081379890441895
2023-01-07 08:52:29,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.229238033294678
2023-01-07 08:52:29,078 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,078 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -1.1566119194030762
2023-01-07 08:52:29,078 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,079 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.009390830993652
2023-01-07 08:52:29,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,079 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -7.7081379890441895
2023-01-07 08:52:29,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.4445977210998535
2023-01-07 08:52:29,080 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,080 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -7.7081379890441895
2023-01-07 08:52:29,080 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:29,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,081 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -35.646629333496094
2023-01-07 08:52:29,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.981873512268066
2023-01-07 08:52:29,081 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,082 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.020236879587173462
2023-01-07 08:52:29,082 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:29,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -35.646629333496094
2023-01-07 08:52:29,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.618447303771973
2023-01-07 08:52:29,083 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,083 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -35.646629333496094
2023-01-07 08:52:29,083 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,084 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.1168099045753479
2023-01-07 08:52:29,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -60.922157287597656
2023-01-07 08:52:29,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.146766662597656
2023-01-07 08:52:29,085 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,085 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.1168099045753479
2023-01-07 08:52:29,085 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,086 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.902972221374512
2023-01-07 08:52:29,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,086 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -60.922157287597656
2023-01-07 08:52:29,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.962137222290039
2023-01-07 08:52:29,087 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,087 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -60.922157287597656
2023-01-07 08:52:29,087 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,087 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,087 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,088 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.3435872197151184
2023-01-07 08:52:29,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,088 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.108116149902344
2023-01-07 08:52:29,088 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.23142147064209
2023-01-07 08:52:29,089 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,089 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.3435872197151184
2023-01-07 08:52:29,089 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,089 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,089 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.7452926635742188
2023-01-07 08:52:29,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,089 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.108116149902344
2023-01-07 08:52:29,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1933499574661255
2023-01-07 08:52:29,090 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,091 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 11.108116149902344
2023-01-07 08:52:29,091 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,091 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,091 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,091 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8803040981292725
2023-01-07 08:52:29,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,091 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.4687662124633789
2023-01-07 08:52:29,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.879288673400879
2023-01-07 08:52:29,092 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,093 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.8803040981292725
2023-01-07 08:52:29,093 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,093 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,093 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 4.1814117431640625
2023-01-07 08:52:29,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,093 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.4687662124633789
2023-01-07 08:52:29,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.842745780944824
2023-01-07 08:52:29,094 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,094 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 0.4687662124633789
2023-01-07 08:52:29,094 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,095 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,095 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.13584661483764648
2023-01-07 08:52:29,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,095 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -73.79924011230469
2023-01-07 08:52:29,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.488534927368164
2023-01-07 08:52:29,096 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,096 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.13584661483764648
2023-01-07 08:52:29,096 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.66290283203125
2023-01-07 08:52:29,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,097 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -73.79924011230469
2023-01-07 08:52:29,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.682954788208008
2023-01-07 08:52:29,098 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,098 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -73.79924011230469
2023-01-07 08:52:29,098 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,098 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,098 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,098 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.1706567406654358
2023-01-07 08:52:29,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,099 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 9.980828285217285
2023-01-07 08:52:29,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3975815176963806
2023-01-07 08:52:29,100 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,100 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.1706567406654358
2023-01-07 08:52:29,100 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,100 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -4.248758316040039
2023-01-07 08:52:29,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,101 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 9.980828285217285
2023-01-07 08:52:29,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.392565727233887
2023-01-07 08:52:29,102 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,102 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 9.980828285217285
2023-01-07 08:52:29,102 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,102 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,102 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,102 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.6033518314361572
2023-01-07 08:52:29,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -16.20087432861328
2023-01-07 08:52:29,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8704166412353516
2023-01-07 08:52:29,104 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,104 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.6033518314361572
2023-01-07 08:52:29,104 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,104 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,104 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.61199951171875
2023-01-07 08:52:29,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,104 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -16.20087432861328
2023-01-07 08:52:29,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8033642768859863
2023-01-07 08:52:29,105 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,106 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -16.20087432861328
2023-01-07 08:52:29,106 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,106 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,106 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.30488187074661255
2023-01-07 08:52:29,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.13911056518555
2023-01-07 08:52:29,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.844562530517578
2023-01-07 08:52:29,107 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,108 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.30488187074661255
2023-01-07 08:52:29,108 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,108 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,108 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.29677963256836
2023-01-07 08:52:29,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -55.13911056518555
2023-01-07 08:52:29,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.564573287963867
2023-01-07 08:52:29,109 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,109 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -55.13911056518555
2023-01-07 08:52:29,109 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,110 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:29,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,110 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.02133560180664
2023-01-07 08:52:29,110 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.665926218032837
2023-01-07 08:52:29,111 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,111 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.15536227822303772
2023-01-07 08:52:29,111 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:29,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.02133560180664
2023-01-07 08:52:29,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44400641322135925
2023-01-07 08:52:29,113 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,113 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 41.02133560180664
2023-01-07 08:52:29,113 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,113 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,113 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:29,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.76263427734375
2023-01-07 08:52:29,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.26068115234375
2023-01-07 08:52:29,114 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,114 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.431196928024292
2023-01-07 08:52:29,114 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,115 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:29,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 26.76263427734375
2023-01-07 08:52:29,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.186995506286621
2023-01-07 08:52:29,116 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,116 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 26.76263427734375
2023-01-07 08:52:29,116 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:29,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 22.066211700439453
2023-01-07 08:52:29,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 2167.9951171875
2023-01-07 08:52:29,117 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,117 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.31727635860443115
2023-01-07 08:52:29,118 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:29,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 22.066211700439453
2023-01-07 08:52:29,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6448042392730713
2023-01-07 08:52:29,119 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,119 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 22.066211700439453
2023-01-07 08:52:29,119 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.18635690212249756
2023-01-07 08:52:29,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.221303939819336
2023-01-07 08:52:29,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -8137.0810546875
2023-01-07 08:52:29,121 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,121 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.18635690212249756
2023-01-07 08:52:29,121 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.882823944091797
2023-01-07 08:52:29,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,122 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.221303939819336
2023-01-07 08:52:29,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 1518.87548828125
2023-01-07 08:52:29,123 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,123 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -18.221303939819336
2023-01-07 08:52:29,123 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:29,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.7697434425354
2023-01-07 08:52:29,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6782302856445312
2023-01-07 08:52:29,124 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,124 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -0.23710086941719055
2023-01-07 08:52:29,125 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:29,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.7697434425354
2023-01-07 08:52:29,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26161861419677734
2023-01-07 08:52:29,126 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,126 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 4.7697434425354
2023-01-07 08:52:29,126 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:29,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.317920207977295
2023-01-07 08:52:29,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6708574295043945
2023-01-07 08:52:29,128 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,128 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.11085262894630432
2023-01-07 08:52:29,128 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:29,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,128 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.317920207977295
2023-01-07 08:52:29,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7968506813049316
2023-01-07 08:52:29,129 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,129 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 2.317920207977295
2023-01-07 08:52:29,130 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:29,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,130 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.126192092895508
2023-01-07 08:52:29,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.327998161315918
2023-01-07 08:52:29,131 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,131 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.10156912356615067
2023-01-07 08:52:29,131 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:29,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.126192092895508
2023-01-07 08:52:29,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5811447501182556
2023-01-07 08:52:29,133 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,133 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -8.126192092895508
2023-01-07 08:52:29,133 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:29,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.575687408447266
2023-01-07 08:52:29,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1625055074691772
2023-01-07 08:52:29,134 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,134 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.1445143073797226
2023-01-07 08:52:29,134 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:29,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.575687408447266
2023-01-07 08:52:29,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08194604516029358
2023-01-07 08:52:29,136 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,136 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -4.575687408447266
2023-01-07 08:52:29,136 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:29,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.3523976802825928
2023-01-07 08:52:29,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.779987096786499
2023-01-07 08:52:29,138 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,138 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.19611787796020508
2023-01-07 08:52:29,138 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,138 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:29,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.3523976802825928
2023-01-07 08:52:29,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4634780883789062
2023-01-07 08:52:29,139 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,139 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 3.3523976802825928
2023-01-07 08:52:29,140 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:29,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.691570281982422
2023-01-07 08:52:29,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -59830194176.0
2023-01-07 08:52:29,141 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,141 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.13014622032642365
2023-01-07 08:52:29,141 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:29,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.691570281982422
2023-01-07 08:52:29,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1376428604125977
2023-01-07 08:52:29,143 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,143 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 17.691570281982422
2023-01-07 08:52:29,143 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:29,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8130519390106201
2023-01-07 08:52:29,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10997872054576874
2023-01-07 08:52:29,144 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,144 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.05016700178384781
2023-01-07 08:52:29,144 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,145 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:29,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,145 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8130519390106201
2023-01-07 08:52:29,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30446282029151917
2023-01-07 08:52:29,146 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,146 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1.8130519390106201
2023-01-07 08:52:29,146 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:29,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.179058074951172
2023-01-07 08:52:29,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6328092813491821
2023-01-07 08:52:29,147 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,148 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.07144492864608765
2023-01-07 08:52:29,148 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,148 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,148 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:29,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.179058074951172
2023-01-07 08:52:29,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 6423266197504.0
2023-01-07 08:52:29,149 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,149 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 10.179058074951172
2023-01-07 08:52:29,149 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:29,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 6.848174095153809
2023-01-07 08:52:29,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7453370690345764
2023-01-07 08:52:29,151 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,151 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.01889774203300476
2023-01-07 08:52:29,151 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:29,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 6.848174095153809
2023-01-07 08:52:29,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.487579345703125
2023-01-07 08:52:29,153 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,153 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 6.848174095153809
2023-01-07 08:52:29,153 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:29,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,153 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.2479915618896484
2023-01-07 08:52:29,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -21902.865234375
2023-01-07 08:52:29,154 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,154 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.054134614765644073
2023-01-07 08:52:29,154 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:29,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.2479915618896484
2023-01-07 08:52:29,155 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.975527286529541
2023-01-07 08:52:29,156 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,156 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -3.2479915618896484
2023-01-07 08:52:29,156 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:29,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,157 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.361075401306152
2023-01-07 08:52:29,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0669679343700409
2023-01-07 08:52:29,157 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,158 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.13415440917015076
2023-01-07 08:52:29,158 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:29,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,158 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -7.361075401306152
2023-01-07 08:52:29,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.482310175895691
2023-01-07 08:52:29,159 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,159 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -1.721101843431424e+16
2023-01-07 08:52:29,159 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:29,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,160 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -2.2948036032331776e+16
2023-01-07 08:52:29,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2948027442397184e+16
2023-01-07 08:52:29,161 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,161 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.00977657362818718
2023-01-07 08:52:29,161 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:29,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,161 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -6.884411668692992e+16
2023-01-07 08:52:29,162 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.589608924453274e+16
2023-01-07 08:52:29,162 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,163 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -6.884411668692992e+16
2023-01-07 08:52:29,163 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:29,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,163 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -2.6299916127463014e+17
2023-01-07 08:52:29,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0855227474942362e+17
2023-01-07 08:52:29,164 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,164 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.10171712934970856
2023-01-07 08:52:29,164 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,164 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,164 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:29,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,165 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -2.6299916127463014e+17
2023-01-07 08:52:29,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6998002774953165e+17
2023-01-07 08:52:29,166 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,166 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -2.6299916127463014e+17
2023-01-07 08:52:29,166 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:29,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,166 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -9.266810442733322e+17
2023-01-07 08:52:29,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.088936356114596e+17
2023-01-07 08:52:29,167 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,167 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.16211259365081787
2023-01-07 08:52:29,168 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:29,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,168 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -9.266810442733322e+17
2023-01-07 08:52:29,168 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5290547567666921e+19
2023-01-07 08:52:29,169 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,169 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -9.266810442733322e+17
2023-01-07 08:52:29,169 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:29,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,170 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3.232788898350367e+17
2023-01-07 08:52:29,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.810065731773006e+18
2023-01-07 08:52:29,171 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,171 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.12175893038511276
2023-01-07 08:52:29,171 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,171 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:29,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,171 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3.232788898350367e+17
2023-01-07 08:52:29,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.155193286095012e+17
2023-01-07 08:52:29,172 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,173 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -3.232788898350367e+17
2023-01-07 08:52:29,173 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,173 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:29,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,173 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.1099083348503429e+18
2023-01-07 08:52:29,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3428101533953897e+21
2023-01-07 08:52:29,174 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,174 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.07465416193008423
2023-01-07 08:52:29,174 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:29,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,175 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.1099083348503429e+18
2023-01-07 08:52:29,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.241559830297313e+17
2023-01-07 08:52:29,176 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,176 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -1.1099083348503429e+18
2023-01-07 08:52:29,176 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:29,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,177 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8363391746475295e+18
2023-01-07 08:52:29,177 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4798574388353434e+18
2023-01-07 08:52:29,177 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,178 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.1763181984424591
2023-01-07 08:52:29,178 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:29,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,178 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.8363391746475295e+18
2023-01-07 08:52:29,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5648256044590694e+17
2023-01-07 08:52:29,179 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,179 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1.8363391746475295e+18
2023-01-07 08:52:29,179 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:29,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,180 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.1386600671870976e+18
2023-01-07 08:52:29,180 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.129673199150694e+17
2023-01-07 08:52:29,181 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,181 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.3712242841720581
2023-01-07 08:52:29,181 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:29,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,181 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.1386600671870976e+18
2023-01-07 08:52:29,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4256927472720282e+18
2023-01-07 08:52:29,182 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,183 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -2.1386600671870976e+18
2023-01-07 08:52:29,183 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:29,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,183 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8.553699086795014e+18
2023-01-07 08:52:29,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4518261556446233e+19
2023-01-07 08:52:29,184 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,184 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -9.957284396738595e+23
2023-01-07 08:52:29,184 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:29,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,185 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 1.997236307654177e+20
2023-01-07 08:52:29,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.702313592250958e+18
2023-01-07 08:52:29,186 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,186 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 1.997236307654177e+20
2023-01-07 08:52:29,186 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:29,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,186 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.977404969479666e+23
2023-01-07 08:52:29,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0531013499693826e+19
2023-01-07 08:52:29,187 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,187 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -4.9783799087269994e+23
2023-01-07 08:52:29,188 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:29,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,188 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -4.977404969479666e+23
2023-01-07 08:52:29,188 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1619990726057656e+19
2023-01-07 08:52:29,189 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,189 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -4.977404969479666e+23
2023-01-07 08:52:29,189 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:29,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,190 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 6.887158757263304e+19
2023-01-07 08:52:29,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.324005182085949e+19
2023-01-07 08:52:29,191 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,191 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -2.4885992622363738e+23
2023-01-07 08:52:29,191 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:29,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,191 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 6.887158757263304e+19
2023-01-07 08:52:29,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.54912687764207e+19
2023-01-07 08:52:29,192 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,193 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 6.887158757263304e+19
2023-01-07 08:52:29,193 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,193 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,193 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:29,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,193 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 5.2479419074083396e+20
2023-01-07 08:52:29,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.709825375528414e+20
2023-01-07 08:52:29,194 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,194 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -1.2444638323606008e+23
2023-01-07 08:52:29,194 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:29,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,195 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 5.2479419074083396e+20
2023-01-07 08:52:29,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9930287894153166e+21
2023-01-07 08:52:29,196 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,196 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 5.2479419074083396e+20
2023-01-07 08:52:29,196 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:29,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,196 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.7512057999845824e+22
2023-01-07 08:52:29,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.839290243114588e+20
2023-01-07 08:52:29,197 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,197 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -6.2219912997501315e+22
2023-01-07 08:52:29,198 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:29,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,198 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.7512057999845824e+22
2023-01-07 08:52:29,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7097596007934853e+21
2023-01-07 08:52:29,199 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,199 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -1.7512057999845824e+22
2023-01-07 08:52:29,199 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:29,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,200 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4.0995259727537e+21
2023-01-07 08:52:29,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.366781688311976e+21
2023-01-07 08:52:29,201 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,201 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 5.466388162909015e+21
2023-01-07 08:52:29,201 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:29,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,201 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4.0995259727537e+21
2023-01-07 08:52:29,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.733563376623952e+21
2023-01-07 08:52:29,203 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,203 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 4.0995259727537e+21
2023-01-07 08:52:29,203 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,204 > [DEBUG] 0 :: 7.9861578941345215
2023-01-07 08:52:29,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,207 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.02117919921875
2023-01-07 08:52:29,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.507914075813517e+21
2023-01-07 08:52:29,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,210 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.6348865032196045
2023-01-07 08:52:29,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,211 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.02117919921875
2023-01-07 08:52:29,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.870659222119005e+25
2023-01-07 08:52:29,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,213 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.717001438140869
2023-01-07 08:52:29,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.691733921171415e+26
2023-01-07 08:52:29,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,214 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.03684471920132637
2023-01-07 08:52:29,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,215 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.717001438140869
2023-01-07 08:52:29,215 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35516685247421265
2023-01-07 08:52:29,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.402017593383789
2023-01-07 08:52:29,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 522075890515968.0
2023-01-07 08:52:29,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.026054181158542633
2023-01-07 08:52:29,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.402017593383789
2023-01-07 08:52:29,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4413734482909769e+28
2023-01-07 08:52:29,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,220 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.0106048583984375
2023-01-07 08:52:29,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.056628805153311e+28
2023-01-07 08:52:29,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.5818096399307251
2023-01-07 08:52:29,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.0106048583984375
2023-01-07 08:52:29,222 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.306197517265563e+29
2023-01-07 08:52:29,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,223 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.264432907104492
2023-01-07 08:52:29,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.790917140408551e+30
2023-01-07 08:52:29,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,224 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.031309813261032104
2023-01-07 08:52:29,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,225 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.264432907104492
2023-01-07 08:52:29,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7719962000846863
2023-01-07 08:52:29,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,226 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.07155704498291
2023-01-07 08:52:29,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3551434426692403e+17
2023-01-07 08:52:29,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,227 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.06408169865608215
2023-01-07 08:52:29,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,228 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.07155704498291
2023-01-07 08:52:29,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07982059568166733
2023-01-07 08:52:29,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,229 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.95554708814634e+23
2023-01-07 08:52:29,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.95554708814634e+23
2023-01-07 08:52:29,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,230 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: -9.955575190608015e+23
2023-01-07 08:52:29,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,231 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.95554708814634e+23
2023-01-07 08:52:29,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6939184052769917e+24
2023-01-07 08:52:29,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,232 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0265508948870169e+19
2023-01-07 08:52:29,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0265506749846913e+19
2023-01-07 08:52:29,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,234 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: -1.9911222438810068e+24
2023-01-07 08:52:29,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,235 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0265508948870169e+19
2023-01-07 08:52:29,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.790302437922315e+25
2023-01-07 08:52:29,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.9905332451073408e+24
2023-01-07 08:52:29,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9911233968025114e+24
2023-01-07 08:52:29,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -1.9917196013355813e+24
2023-01-07 08:52:29,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,238 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.9905332451073408e+24
2023-01-07 08:52:29,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9911293055252226e+24
2023-01-07 08:52:29,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,239 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.148548579286067e+20
2023-01-07 08:52:29,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4554114030787206e+20
2023-01-07 08:52:29,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,241 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -4.210686211293761e+20
2023-01-07 08:52:29,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,241 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.148548579286067e+20
2023-01-07 08:52:29,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.482448853851261e+20
2023-01-07 08:52:29,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,243 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6067850767886356e+21
2023-01-07 08:52:29,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.800855870535033e+20
2023-01-07 08:52:29,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -8.665281045218932e+20
2023-01-07 08:52:29,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,244 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6067850767886356e+21
2023-01-07 08:52:29,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1601711741070066e+21
2023-01-07 08:52:29,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.933488568410955e+21
2023-01-07 08:52:29,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1501355556969864e+21
2023-01-07 08:52:29,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -1.411594334191661e+21
2023-01-07 08:52:29,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.933488568410955e+21
2023-01-07 08:52:29,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3717576933598885e+21
2023-01-07 08:52:29,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.130687031935051e+21
2023-01-07 08:52:29,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.663923632143744e+20
2023-01-07 08:52:29,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -3.226923145653182e+21
2023-01-07 08:52:29,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,251 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.130687031935051e+21
2023-01-07 08:52:29,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0373726489674022e+21
2023-01-07 08:52:29,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.956533397294588e+21
2023-01-07 08:52:29,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0747452979348043e+21
2023-01-07 08:52:29,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,253 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -1.39867308464827e+21
2023-01-07 08:52:29,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,254 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.956533397294588e+21
2023-01-07 08:52:29,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.483116422086397e+21
2023-01-07 08:52:29,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.04212207859332e+22
2023-01-07 08:52:29,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.320159007348441e+21
2023-01-07 08:52:29,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -2.7430321948404985e+22
2023-01-07 08:52:29,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.04212207859332e+22
2023-01-07 08:52:29,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.670744333779397e+21
2023-01-07 08:52:29,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.751602884389502e+22
2023-01-07 08:52:29,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.497207062716603e+27
2023-01-07 08:52:29,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -1.6155836126760125e+21
2023-01-07 08:52:29,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.751602884389502e+22
2023-01-07 08:52:29,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.078208499507145e+22
2023-01-07 08:52:29,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.0589066105856742e+23
2023-01-07 08:52:29,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.015641699901429e+23
2023-01-07 08:52:29,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -2.7978829420771274e+21
2023-01-07 08:52:29,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.0589066105856742e+23
2023-01-07 08:52:29,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.528617415021628e+21
2023-01-07 08:52:29,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.184499980932548e+21
2023-01-07 08:52:29,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.865825924659387e+20
2023-01-07 08:52:29,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -3.231167225352025e+21
2023-01-07 08:52:29,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.184499980932548e+21
2023-01-07 08:52:29,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3667501631145844e+21
2023-01-07 08:52:29,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.6449527117460024e+22
2023-01-07 08:52:29,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.7094434037389056e+21
2023-01-07 08:52:29,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,269 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -1.29246689014081e+22
2023-01-07 08:52:29,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,269 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.6449527117460024e+22
2023-01-07 08:52:29,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1845908171865163e+21
2023-01-07 08:52:29,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.745683013581169e+22
2023-01-07 08:52:29,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3691816343730326e+21
2023-01-07 08:52:29,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,272 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -5.275113080975187e+22
2023-01-07 08:52:29,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,272 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.745683013581169e+22
2023-01-07 08:52:29,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2925123543166773e+22
2023-01-07 08:52:29,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,274 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4114259614900923e+23
2023-01-07 08:52:29,274 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6434344807576213e+23
2023-01-07 08:52:29,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,275 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -2.067947024225296e+23
2023-01-07 08:52:29,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,275 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4114259614900923e+23
2023-01-07 08:52:29,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.835938504957767e+23
2023-01-07 08:52:29,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,277 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -4.3547070208099255e+23
2023-01-07 08:52:29,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.100450517058832e+23
2023-01-07 08:52:29,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -8.271788096901184e+23
2023-01-07 08:52:29,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -4.3547070208099255e+23
2023-01-07 08:52:29,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.526043159500397e+23
2023-01-07 08:52:29,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,280 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.502954670358573e+24
2023-01-07 08:52:29,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5052086319000794e+24
2023-01-07 08:52:29,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -14.129179954528809
2023-01-07 08:52:29,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.502954670358573e+24
2023-01-07 08:52:29,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2539570379067585e+21
2023-01-07 08:52:29,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,283 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7353111557477112e+24
2023-01-07 08:52:29,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.671162040619062e+24
2023-01-07 08:52:29,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -6.617430477520947e+24
2023-01-07 08:52:29,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7353111557477112e+24
2023-01-07 08:52:29,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.681579880879973e+24
2023-01-07 08:52:29,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,286 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.298561459069401e+25
2023-01-07 08:52:29,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0808973452925526e+25
2023-01-07 08:52:29,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,287 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.298561459069401e+25
2023-01-07 08:52:29,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2176643443611494e+25
2023-01-07 08:52:29,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,289 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.549221192476391e+25
2023-01-07 08:52:29,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.435330533396706e+25
2023-01-07 08:52:29,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,290 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -4.321437568474012e+25
2023-01-07 08:52:29,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,290 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.549221192476391e+25
2023-01-07 08:52:29,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.4353296110595025e+25
2023-01-07 08:52:29,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,292 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.5376847993009252e+26
2023-01-07 08:52:29,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.514906575251268e+26
2023-01-07 08:52:29,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,293 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -2.3791939044786297e+26
2023-01-07 08:52:29,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,293 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.5376847993009252e+26
2023-01-07 08:52:29,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4019723129957277e+26
2023-01-07 08:52:29,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,295 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.7218231501237458e+26
2023-01-07 08:52:29,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5953408357730674e+25
2023-01-07 08:52:29,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,296 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -2.117577752806703e+26
2023-01-07 08:52:29,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,296 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.7218231501237458e+26
2023-01-07 08:52:29,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3622005793704877e+25
2023-01-07 08:52:29,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,298 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 3.360227172067125e+26
2023-01-07 08:52:29,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.853048173131061e+26
2023-01-07 08:52:29,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,299 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 3.360227172067125e+26
2023-01-07 08:52:29,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.928197097918504e+25
2023-01-07 08:52:29,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,301 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.856394195837008e+25
2023-01-07 08:52:29,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.856394195837008e+25
2023-01-07 08:52:29,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,302 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.856394195837008e+25
2023-01-07 08:52:29,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 261038062698496.0
2023-01-07 08:52:29,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.1861954233874086e+26
2023-01-07 08:52:29,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1002768028634891e+26
2023-01-07 08:52:29,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.1861954233874086e+26
2023-01-07 08:52:29,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.085915023408825e+26
2023-01-07 08:52:29,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.875615011355431e+27
2023-01-07 08:52:29,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.5703435445442715e+26
2023-01-07 08:52:29,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.875615011355431e+27
2023-01-07 08:52:29,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.332649181342417e+27
2023-01-07 08:52:29,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.533059672536967e+28
2023-01-07 08:52:29,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2665298362684835e+28
2023-01-07 08:52:29,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.533059672536967e+28
2023-01-07 08:52:29,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2665298362684835e+28
2023-01-07 08:52:29,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,312 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.051884243547972e+28
2023-01-07 08:52:29,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.523569368734668e+28
2023-01-07 08:52:29,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.051884243547972e+28
2023-01-07 08:52:29,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5283144025766554e+28
2023-01-07 08:52:29,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 1.5155231967790291e+29
2023-01-07 08:52:29,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.049302525791787e+28
2023-01-07 08:52:29,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,316 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 1.5155231967790291e+29
2023-01-07 08:52:29,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0105931330945098e+29
2023-01-07 08:52:29,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.501668808276383e+29
2023-01-07 08:52:29,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.174284835927142e+29
2023-01-07 08:52:29,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -2.306197517265563e+29
2023-01-07 08:52:29,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.501668808276383e+29
2023-01-07 08:52:29,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0211862661890195e+29
2023-01-07 08:52:29,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,321 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.211933173706648e+30
2023-01-07 08:52:29,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.0384801690281985e+29
2023-01-07 08:52:29,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.211933173706648e+30
2023-01-07 08:52:29,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.080852323616919e+29
2023-01-07 08:52:29,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,323 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 5.010850637815555e+30
2023-01-07 08:52:29,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3954585702042753e+30
2023-01-07 08:52:29,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 5.010850637815555e+30
2023-01-07 08:52:29,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.615391916495552e+30
2023-01-07 08:52:29,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.230783832991104e+30
2023-01-07 08:52:29,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.230783832991104e+30
2023-01-07 08:52:29,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -2.409961984360448e+16
2023-01-07 08:52:29,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.230783832991104e+30
2023-01-07 08:52:29,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -5737054105239552.0
2023-01-07 08:52:29,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,329 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.890448487232307e+16
2023-01-07 08:52:29,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.371517974098739e+16
2023-01-07 08:52:29,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,330 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.890448487232307e+16
2023-01-07 08:52:29,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.518930513133568e+16
2023-01-07 08:52:29,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,332 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4.315210335859507e+16
2023-01-07 08:52:29,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.775717213346202e+16
2023-01-07 08:52:29,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,333 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4.315210335859507e+16
2023-01-07 08:52:29,333 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.460506447989965e+16
2023-01-07 08:52:29,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.404568660947763e+16
2023-01-07 08:52:29,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,335 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -2.223688050142085e+17
2023-01-07 08:52:29,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5248174843940045e+17
2023-01-07 08:52:29,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.044084137361408e+17
2023-01-07 08:52:29,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,339 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -2.0688621556695777e+23
2023-01-07 08:52:29,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,339 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.886764195830204e+23
2023-01-07 08:52:29,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,339 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.9778387561957745e+23
2023-01-07 08:52:29,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,341 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.977565657914371e+23
2023-01-07 08:52:29,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,342 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.886764195830204e+23
2023-01-07 08:52:29,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.9777875953040076e+23
2023-01-07 08:52:29,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,343 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.346940611779234e+24
2023-01-07 08:52:29,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 6.327116483980035e+23
2023-01-07 08:52:29,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.693899670302542e+24
2023-01-07 08:52:29,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.387977466977545e+24
2023-01-07 08:52:29,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,348 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.2593636859755626e+19
2023-01-07 08:52:29,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,348 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 1.7760314566442484e+19
2023-01-07 08:52:29,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.166675507663602e+18
2023-01-07 08:52:29,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,350 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,350 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0775571011094057e+25
2023-01-07 08:52:29,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,351 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -2.1551349548058943e+25
2023-01-07 08:52:29,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,351 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1551349548058943e+25
2023-01-07 08:52:29,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,353 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,353 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0195970992017867e+24
2023-01-07 08:52:29,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.3951512189611575e+25
2023-01-07 08:52:29,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,355 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.790338409073259e+25
2023-01-07 08:52:29,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -8.110154381854978e+25
2023-01-07 08:52:29,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -9.470565786140113e+25
2023-01-07 08:52:29,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,358 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7580721090332295e+26
2023-01-07 08:52:29,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,359 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.325532535813292e+26
2023-01-07 08:52:29,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -3.440749423558156e+26
2023-01-07 08:52:29,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4307940847164566e+26
2023-01-07 08:52:29,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,362 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.430792608976931e+26
2023-01-07 08:52:29,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -3.440749423558156e+26
2023-01-07 08:52:29,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,363 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.955646527626113e+23
2023-01-07 08:52:29,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,365 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.871540556795561e+26
2023-01-07 08:52:29,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 3.3196406457561494e+26
2023-01-07 08:52:29,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3743084065070174e+27
2023-01-07 08:52:29,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,368 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.646662390754543e+20
2023-01-07 08:52:29,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,369 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7486162227182244e+27
2023-01-07 08:52:29,374 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:52:29,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,375 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -3196.833251953125
2023-01-07 08:52:29,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,376 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4.315210335859507e+16
2023-01-07 08:52:29,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,378 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.890448487232307e+16
2023-01-07 08:52:29,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,379 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.230783832991104e+30
2023-01-07 08:52:29,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,380 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 5.010850637815555e+30
2023-01-07 08:52:29,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,380 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.211933173706648e+30
2023-01-07 08:52:29,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,381 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.501668808276383e+29
2023-01-07 08:52:29,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 1.5155231967790291e+29
2023-01-07 08:52:29,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.051884243547972e+28
2023-01-07 08:52:29,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,383 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.533059672536967e+28
2023-01-07 08:52:29,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,383 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.875615011355431e+27
2023-01-07 08:52:29,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,384 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.1861954233874086e+26
2023-01-07 08:52:29,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.856394195837008e+25
2023-01-07 08:52:29,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 3.360227172067125e+26
2023-01-07 08:52:29,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.7218231501237458e+26
2023-01-07 08:52:29,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.5376847993009252e+26
2023-01-07 08:52:29,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.549221192476391e+25
2023-01-07 08:52:29,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.298561459069401e+25
2023-01-07 08:52:29,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7353111557477112e+24
2023-01-07 08:52:29,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.502954670358573e+24
2023-01-07 08:52:29,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -4.3547070208099255e+23
2023-01-07 08:52:29,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4114259614900923e+23
2023-01-07 08:52:29,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.745683013581169e+22
2023-01-07 08:52:29,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.6449527117460024e+22
2023-01-07 08:52:29,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -2703.9541015625
2023-01-07 08:52:29,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.184499980932548e+21
2023-01-07 08:52:29,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.0589066105856742e+23
2023-01-07 08:52:29,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.751602884389502e+22
2023-01-07 08:52:29,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.04212207859332e+22
2023-01-07 08:52:29,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.956533397294588e+21
2023-01-07 08:52:29,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.130687031935051e+21
2023-01-07 08:52:29,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.933488568410955e+21
2023-01-07 08:52:29,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,391 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6067850767886356e+21
2023-01-07 08:52:29,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,392 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.148548579286067e+20
2023-01-07 08:52:29,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.02219009399414
2023-01-07 08:52:29,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,393 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.9905332451073408e+24
2023-01-07 08:52:29,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,393 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0265508948870169e+19
2023-01-07 08:52:29,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.95554708814634e+23
2023-01-07 08:52:29,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.07155704498291
2023-01-07 08:52:29,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.42645263671875
2023-01-07 08:52:29,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,395 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.264432907104492
2023-01-07 08:52:29,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,395 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.0106048583984375
2023-01-07 08:52:29,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.402017593383789
2023-01-07 08:52:29,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7103257179260254
2023-01-07 08:52:29,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.717001438140869
2023-01-07 08:52:29,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,397 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.02117919921875
2023-01-07 08:52:29,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 522.043212890625
2023-01-07 08:52:29,398 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,398 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -3196.833251953125
2023-01-07 08:52:29,398 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,398 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 16.545318603515625
2023-01-07 08:52:29,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,398 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.09101104736328
2023-01-07 08:52:29,400 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,400 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 16.545318603515625
2023-01-07 08:52:29,400 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -0.7820339202880859
2023-01-07 08:52:29,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,400 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 60.882843017578125
2023-01-07 08:52:29,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,400 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.093231201171875
2023-01-07 08:52:29,402 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,402 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 60.882843017578125
2023-01-07 08:52:29,402 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:29,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,402 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.5286865234375
2023-01-07 08:52:29,403 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,404 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3.3196406457561494e+26
2023-01-07 08:52:29,404 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -10.904928207397461
2023-01-07 08:52:29,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,404 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -5.142043192202271e+27
2023-01-07 08:52:29,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.4744873046875
2023-01-07 08:52:29,405 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,405 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -5.142043192202271e+27
2023-01-07 08:52:29,405 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,406 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 79.96952819824219
2023-01-07 08:52:29,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,406 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -201.73138427734375
2023-01-07 08:52:29,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,406 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,407 > [DEBUG] 0 :: before allreduce fusion buffer :: -100.5318603515625
2023-01-07 08:52:29,407 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,408 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 79.96952819824219
2023-01-07 08:52:29,408 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,408 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2858734130859375
2023-01-07 08:52:29,409 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,409 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -201.73138427734375
2023-01-07 08:52:29,409 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,410 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -254.22964477539062
2023-01-07 08:52:29,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,410 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -9.470565786140113e+25
2023-01-07 08:52:29,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -372.4698486328125
2023-01-07 08:52:29,411 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,411 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -254.22964477539062
2023-01-07 08:52:29,411 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,412 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -580.363037109375
2023-01-07 08:52:29,413 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,413 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -3.440749423558156e+26
2023-01-07 08:52:29,413 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,413 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 291.849853515625
2023-01-07 08:52:29,414 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,414 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -8.110154381854978e+25
2023-01-07 08:52:29,414 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,415 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -1227.05419921875
2023-01-07 08:52:29,416 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,416 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -9.470565786140113e+25
2023-01-07 08:52:29,416 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,416 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.12109720706939697
2023-01-07 08:52:29,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,417 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.815044403076172
2023-01-07 08:52:29,418 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,418 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.12109720706939697
2023-01-07 08:52:29,418 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.560659408569336
2023-01-07 08:52:29,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,418 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -4.3951512189611575e+25
2023-01-07 08:52:29,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,419 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -1200.7967529296875
2023-01-07 08:52:29,420 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,420 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -4.3951512189611575e+25
2023-01-07 08:52:29,420 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,420 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 2610.85986328125
2023-01-07 08:52:29,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,421 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -2.1551349548058943e+25
2023-01-07 08:52:29,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -2613.103759765625
2023-01-07 08:52:29,422 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,422 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 2610.85986328125
2023-01-07 08:52:29,422 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,422 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -806.884765625
2023-01-07 08:52:29,423 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,424 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -2.1551349548058943e+25
2023-01-07 08:52:29,424 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:29,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,424 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 1.7760314566442484e+19
2023-01-07 08:52:29,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -5576.6484375
2023-01-07 08:52:29,425 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,425 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -1.2593636859755626e+19
2023-01-07 08:52:29,425 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -24.63249969482422
2023-01-07 08:52:29,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,426 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 1.7760314566442484e+19
2023-01-07 08:52:29,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,426 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,426 > [DEBUG] 0 :: before allreduce fusion buffer :: -5200.986328125
2023-01-07 08:52:29,427 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,427 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 1.7760314566442484e+19
2023-01-07 08:52:29,427 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,428 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 846.967529296875
2023-01-07 08:52:29,429 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,429 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 6.327116483980035e+23
2023-01-07 08:52:29,429 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.145297050476074
2023-01-07 08:52:29,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,429 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.863925950687017e+26
2023-01-07 08:52:29,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -9818.2939453125
2023-01-07 08:52:29,430 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,430 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -7.863925950687017e+26
2023-01-07 08:52:29,431 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:52:29,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,431 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 21222.046875
2023-01-07 08:52:29,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,431 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -41187.25
2023-01-07 08:52:29,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,432 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.886764195830204e+23
2023-01-07 08:52:29,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -20017.22265625
2023-01-07 08:52:29,433 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,433 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 21222.046875
2023-01-07 08:52:29,433 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,433 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -18314.595703125
2023-01-07 08:52:29,434 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,434 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -41187.25
2023-01-07 08:52:29,435 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 6.527358055114746
2023-01-07 08:52:29,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,435 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.886764195830204e+23
2023-01-07 08:52:29,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -3327.59375
2023-01-07 08:52:29,436 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,436 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -2.0688621556695777e+23
2023-01-07 08:52:29,436 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,436 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 7261.75390625
2023-01-07 08:52:29,438 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,438 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -7.886764195830204e+23
2023-01-07 08:52:29,438 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,438 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 197.59375
2023-01-07 08:52:29,439 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,439 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -2.223688050142085e+17
2023-01-07 08:52:29,439 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -27.338253021240234
2023-01-07 08:52:29,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,440 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4.9775725033858044e+23
2023-01-07 08:52:29,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -72635.71875
2023-01-07 08:52:29,441 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,441 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -4.9775725033858044e+23
2023-01-07 08:52:29,441 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -404486.1875
2023-01-07 08:52:29,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4.315210335859507e+16
2023-01-07 08:52:29,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -117650.3125
2023-01-07 08:52:29,443 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,443 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -404486.1875
2023-01-07 08:52:29,443 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -3.782032012939453
2023-01-07 08:52:29,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 4.315210335859507e+16
2023-01-07 08:52:29,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -189286.5625
2023-01-07 08:52:29,445 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,445 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 4.315210335859507e+16
2023-01-07 08:52:29,445 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1996593.875
2023-01-07 08:52:29,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,446 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.890448487232307e+16
2023-01-07 08:52:29,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -372047.25
2023-01-07 08:52:29,447 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,448 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -1996593.875
2023-01-07 08:52:29,448 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,448 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,448 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -27.419443130493164
2023-01-07 08:52:29,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.890448487232307e+16
2023-01-07 08:52:29,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -626389.0
2023-01-07 08:52:29,449 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,449 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -7.890448487232307e+16
2023-01-07 08:52:29,449 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:29,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,450 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.230783832991104e+30
2023-01-07 08:52:29,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -1252448.0
2023-01-07 08:52:29,451 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,451 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -2.409961984360448e+16
2023-01-07 08:52:29,451 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,451 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.568113327026367
2023-01-07 08:52:29,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,451 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 3.230783832991104e+30
2023-01-07 08:52:29,451 > [DEBUG] 0 :: before allreduce fusion buffer :: -2515500.0
2023-01-07 08:52:29,452 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,452 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 3.230783832991104e+30
2023-01-07 08:52:29,453 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,453 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,453 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -6471785.5
2023-01-07 08:52:29,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 5.010850637815555e+30
2023-01-07 08:52:29,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -2984504.0
2023-01-07 08:52:29,454 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,454 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -6471785.5
2023-01-07 08:52:29,454 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,455 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.335868000984192
2023-01-07 08:52:29,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,455 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 5.010850637815555e+30
2023-01-07 08:52:29,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -5028810.0
2023-01-07 08:52:29,456 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,456 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 5.010850637815555e+30
2023-01-07 08:52:29,456 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,457 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -12943570.0
2023-01-07 08:52:29,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,457 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.211933173706648e+30
2023-01-07 08:52:29,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -10054720.0
2023-01-07 08:52:29,458 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,458 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -12943570.0
2023-01-07 08:52:29,458 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.009390830993652
2023-01-07 08:52:29,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,459 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.211933173706648e+30
2023-01-07 08:52:29,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -20109472.0
2023-01-07 08:52:29,460 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,460 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 1.211933173706648e+30
2023-01-07 08:52:29,460 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:29,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,460 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.501668808276383e+29
2023-01-07 08:52:29,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 31061576.0
2023-01-07 08:52:29,461 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,461 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -2.306197517265563e+29
2023-01-07 08:52:29,461 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -32.58728790283203
2023-01-07 08:52:29,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,462 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.501668808276383e+29
2023-01-07 08:52:29,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -9220256.0
2023-01-07 08:52:29,463 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,463 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 7.501668808276383e+29
2023-01-07 08:52:29,463 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -103548544.0
2023-01-07 08:52:29,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 1.5155231967790291e+29
2023-01-07 08:52:29,464 > [DEBUG] 0 :: before allreduce fusion buffer :: -18377568.0
2023-01-07 08:52:29,465 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,465 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -103548544.0
2023-01-07 08:52:29,465 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,465 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,465 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -5.902972221374512
2023-01-07 08:52:29,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,465 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 1.5155231967790291e+29
2023-01-07 08:52:29,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -36649088.0
2023-01-07 08:52:29,467 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,467 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 1.5155231967790291e+29
2023-01-07 08:52:29,467 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,467 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -207097088.0
2023-01-07 08:52:29,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,468 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.051884243547972e+28
2023-01-07 08:52:29,468 > [DEBUG] 0 :: before allreduce fusion buffer :: -36702400.0
2023-01-07 08:52:29,469 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,469 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -207097088.0
2023-01-07 08:52:29,469 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.7452926635742188
2023-01-07 08:52:29,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,469 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.051884243547972e+28
2023-01-07 08:52:29,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -73397760.0
2023-01-07 08:52:29,470 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,470 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 5.051884243547972e+28
2023-01-07 08:52:29,470 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,471 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,471 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8606541752815247
2023-01-07 08:52:29,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,471 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.533059672536967e+28
2023-01-07 08:52:29,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 993678720.0
2023-01-07 08:52:29,472 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,472 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.8606541752815247
2023-01-07 08:52:29,472 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,473 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 4.1814117431640625
2023-01-07 08:52:29,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.533059672536967e+28
2023-01-07 08:52:29,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -146804736.0
2023-01-07 08:52:29,474 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,474 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 2.533059672536967e+28
2023-01-07 08:52:29,474 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,475 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -1656776704.0
2023-01-07 08:52:29,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,475 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.875615011355431e+27
2023-01-07 08:52:29,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -293606400.0
2023-01-07 08:52:29,476 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,476 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -1656776704.0
2023-01-07 08:52:29,476 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.66290283203125
2023-01-07 08:52:29,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,476 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.875615011355431e+27
2023-01-07 08:52:29,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -1316844544.0
2023-01-07 08:52:29,478 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,478 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 5.875615011355431e+27
2023-01-07 08:52:29,478 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:52:29,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,479 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -3313553408.0
2023-01-07 08:52:29,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,479 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.1861954233874086e+26
2023-01-07 08:52:29,479 > [DEBUG] 0 :: before allreduce fusion buffer :: -3891417088.0
2023-01-07 08:52:29,480 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,480 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -3313553408.0
2023-01-07 08:52:29,480 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -4.248758316040039
2023-01-07 08:52:29,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,481 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.1861954233874086e+26
2023-01-07 08:52:29,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -5149140992.0
2023-01-07 08:52:29,482 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,482 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -3.1861954233874086e+26
2023-01-07 08:52:29,482 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,482 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,482 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:52:29,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,482 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -16356083712.0
2023-01-07 08:52:29,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,483 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.856394195837008e+25
2023-01-07 08:52:29,483 > [DEBUG] 0 :: before allreduce fusion buffer :: -5149143040.0
2023-01-07 08:52:29,484 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,484 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -16356083712.0
2023-01-07 08:52:29,484 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.61199951171875
2023-01-07 08:52:29,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.856394195837008e+25
2023-01-07 08:52:29,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -7660662784.0
2023-01-07 08:52:29,485 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,486 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -9.856394195837008e+25
2023-01-07 08:52:29,486 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -17485078528.0
2023-01-07 08:52:29,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 3.360227172067125e+26
2023-01-07 08:52:29,487 > [DEBUG] 0 :: before allreduce fusion buffer :: -15321337856.0
2023-01-07 08:52:29,487 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,488 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -17485078528.0
2023-01-07 08:52:29,488 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,488 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.29677963256836
2023-01-07 08:52:29,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 3.360227172067125e+26
2023-01-07 08:52:29,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -15321325568.0
2023-01-07 08:52:29,489 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,489 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 3.360227172067125e+26
2023-01-07 08:52:29,489 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:29,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.7218231501237458e+26
2023-01-07 08:52:29,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -38599917568.0
2023-01-07 08:52:29,491 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,491 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -2.117577752806703e+26
2023-01-07 08:52:29,491 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 26.955684661865234
2023-01-07 08:52:29,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.7218231501237458e+26
2023-01-07 08:52:29,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 42460536832.0
2023-01-07 08:52:29,492 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,493 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 1.7218231501237458e+26
2023-01-07 08:52:29,493 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:29,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.5376847993009252e+26
2023-01-07 08:52:29,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -34739355648.0
2023-01-07 08:52:29,494 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,494 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -2.3791939044786297e+26
2023-01-07 08:52:29,494 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 6.144536972045898
2023-01-07 08:52:29,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1.5376847993009252e+26
2023-01-07 08:52:29,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -69478711296.0
2023-01-07 08:52:29,496 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,496 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1.5376847993009252e+26
2023-01-07 08:52:29,496 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:29,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.549221192476391e+25
2023-01-07 08:52:29,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -138957422592.0
2023-01-07 08:52:29,497 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,497 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -4.321437568474012e+25
2023-01-07 08:52:29,498 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,498 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -8.796521186828613
2023-01-07 08:52:29,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.549221192476391e+25
2023-01-07 08:52:29,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 200726806528.0
2023-01-07 08:52:29,499 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,499 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -4.549221192476391e+25
2023-01-07 08:52:29,499 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:52:29,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -559522512896.0
2023-01-07 08:52:29,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.298561459069401e+25
2023-01-07 08:52:29,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 401453613056.0
2023-01-07 08:52:29,501 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,501 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -559522512896.0
2023-01-07 08:52:29,501 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.882823944091797
2023-01-07 08:52:29,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.298561459069401e+25
2023-01-07 08:52:29,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 1018530758656.0
2023-01-07 08:52:29,503 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,503 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -3.298561459069401e+25
2023-01-07 08:52:29,503 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:29,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7353111557477112e+24
2023-01-07 08:52:29,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 1821438377984.0
2023-01-07 08:52:29,504 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,505 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -6.617430477520947e+24
2023-01-07 08:52:29,505 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 19.358606338500977
2023-01-07 08:52:29,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7353111557477112e+24
2023-01-07 08:52:29,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 1605814452224.0
2023-01-07 08:52:29,506 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,506 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -1.7353111557477112e+24
2023-01-07 08:52:29,506 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:29,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,507 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.502954670358573e+24
2023-01-07 08:52:29,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 1605815500800.0
2023-01-07 08:52:29,508 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,508 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -14.129179954528809
2023-01-07 08:52:29,508 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,508 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,508 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 0.04617118835449219
2023-01-07 08:52:29,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.502954670358573e+24
2023-01-07 08:52:29,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 3211631001600.0
2023-01-07 08:52:29,509 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,510 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -1.502954670358573e+24
2023-01-07 08:52:29,510 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,510 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:29,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,510 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -4.3547070208099255e+23
2023-01-07 08:52:29,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 3211633098752.0
2023-01-07 08:52:29,511 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,511 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -8.271788096901184e+23
2023-01-07 08:52:29,511 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.333494186401367
2023-01-07 08:52:29,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,512 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -4.3547070208099255e+23
2023-01-07 08:52:29,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -3574526377984.0
2023-01-07 08:52:29,513 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,513 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -4.3547070208099255e+23
2023-01-07 08:52:29,513 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,513 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:29,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4114259614900923e+23
2023-01-07 08:52:29,514 > [DEBUG] 0 :: before allreduce fusion buffer :: -7149052755968.0
2023-01-07 08:52:29,514 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,515 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -2.067947024225296e+23
2023-01-07 08:52:29,515 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 33.58697509765625
2023-01-07 08:52:29,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,515 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4114259614900923e+23
2023-01-07 08:52:29,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 999200653312.0
2023-01-07 08:52:29,516 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,516 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1.4114259614900923e+23
2023-01-07 08:52:29,516 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:29,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.745683013581169e+22
2023-01-07 08:52:29,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 1998401306624.0
2023-01-07 08:52:29,518 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,518 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -5.275113080975187e+22
2023-01-07 08:52:29,518 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 38.536842346191406
2023-01-07 08:52:29,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.745683013581169e+22
2023-01-07 08:52:29,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 3996769058816.0
2023-01-07 08:52:29,520 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,520 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 2.745683013581169e+22
2023-01-07 08:52:29,520 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:29,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.6449527117460024e+22
2023-01-07 08:52:29,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 3996802613248.0
2023-01-07 08:52:29,521 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,521 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -1.29246689014081e+22
2023-01-07 08:52:29,521 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 10.412580490112305
2023-01-07 08:52:29,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,522 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.6449527117460024e+22
2023-01-07 08:52:29,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 7993605226496.0
2023-01-07 08:52:29,523 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,523 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 1.6449527117460024e+22
2023-01-07 08:52:29,523 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:29,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.184499980932548e+21
2023-01-07 08:52:29,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -10951.2958984375
2023-01-07 08:52:29,524 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,525 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -3.231167225352025e+21
2023-01-07 08:52:29,525 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,525 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,525 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 16.41494369506836
2023-01-07 08:52:29,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,525 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 5.184499980932548e+21
2023-01-07 08:52:29,525 > [DEBUG] 0 :: before allreduce fusion buffer :: -114382629502976.0
2023-01-07 08:52:29,526 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,526 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 5.184499980932548e+21
2023-01-07 08:52:29,526 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:29,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.0589066105856742e+23
2023-01-07 08:52:29,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -359139528146944.0
2023-01-07 08:52:29,528 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,528 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -2.7978829420771274e+21
2023-01-07 08:52:29,528 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,528 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 43.77464294433594
2023-01-07 08:52:29,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,528 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.0589066105856742e+23
2023-01-07 08:52:29,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -358849617854464.0
2023-01-07 08:52:29,530 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,530 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 1.0589066105856742e+23
2023-01-07 08:52:29,530 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:29,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.751602884389502e+22
2023-01-07 08:52:29,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -717699235708928.0
2023-01-07 08:52:29,531 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,531 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -1.6155836126760125e+21
2023-01-07 08:52:29,531 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 7.484190940856934
2023-01-07 08:52:29,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,532 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.751602884389502e+22
2023-01-07 08:52:29,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 261327989768192.0
2023-01-07 08:52:29,533 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,533 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 7.751602884389502e+22
2023-01-07 08:52:29,533 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:29,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,534 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.04212207859332e+22
2023-01-07 08:52:29,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -1435398471417856.0
2023-01-07 08:52:29,534 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,535 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -2.7430321948404985e+22
2023-01-07 08:52:29,535 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 33.637542724609375
2023-01-07 08:52:29,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,535 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.04212207859332e+22
2023-01-07 08:52:29,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 2263519974129664.0
2023-01-07 08:52:29,536 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,536 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 4.04212207859332e+22
2023-01-07 08:52:29,536 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:29,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,537 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.956533397294588e+21
2023-01-07 08:52:29,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8447192907333374
2023-01-07 08:52:29,538 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,538 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -1.39867308464827e+21
2023-01-07 08:52:29,538 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 36.43920135498047
2023-01-07 08:52:29,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,538 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.956533397294588e+21
2023-01-07 08:52:29,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2766997814178467
2023-01-07 08:52:29,540 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,540 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 4.956533397294588e+21
2023-01-07 08:52:29,540 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:29,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,540 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.130687031935051e+21
2023-01-07 08:52:29,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3747227191925049
2023-01-07 08:52:29,541 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,541 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -3.226923145653182e+21
2023-01-07 08:52:29,541 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 16.279495239257812
2023-01-07 08:52:29,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5.130687031935051e+21
2023-01-07 08:52:29,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1023293286561966
2023-01-07 08:52:29,543 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,543 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 5.130687031935051e+21
2023-01-07 08:52:29,543 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:29,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.933488568410955e+21
2023-01-07 08:52:29,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03655388951301575
2023-01-07 08:52:29,544 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,545 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -1.411594334191661e+21
2023-01-07 08:52:29,545 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -14.608198165893555
2023-01-07 08:52:29,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,545 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.933488568410955e+21
2023-01-07 08:52:29,545 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9309955835342407
2023-01-07 08:52:29,546 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,546 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 5.933488568410955e+21
2023-01-07 08:52:29,546 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:29,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,547 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6067850767886356e+21
2023-01-07 08:52:29,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0075063705444336
2023-01-07 08:52:29,548 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,548 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -8.665281045218932e+20
2023-01-07 08:52:29,548 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.3158674240112305
2023-01-07 08:52:29,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,548 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.6067850767886356e+21
2023-01-07 08:52:29,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3649847209453583
2023-01-07 08:52:29,550 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,550 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 2.6067850767886356e+21
2023-01-07 08:52:29,550 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:29,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,550 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.148548579286067e+20
2023-01-07 08:52:29,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4248085021972656
2023-01-07 08:52:29,551 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,551 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -4.210686211293761e+20
2023-01-07 08:52:29,551 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 46.818939208984375
2023-01-07 08:52:29,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,552 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.148548579286067e+20
2023-01-07 08:52:29,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.601531982421875
2023-01-07 08:52:29,553 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,553 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 8.148548579286067e+20
2023-01-07 08:52:29,553 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:29,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,554 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.9905332451073408e+24
2023-01-07 08:52:29,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37211528420448303
2023-01-07 08:52:29,555 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,555 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -1.9917196013355813e+24
2023-01-07 08:52:29,555 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,555 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,555 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.624189376831055
2023-01-07 08:52:29,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,555 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1.9905332451073408e+24
2023-01-07 08:52:29,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9597915410995483
2023-01-07 08:52:29,556 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,557 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -1.9905332451073408e+24
2023-01-07 08:52:29,557 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:29,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,557 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0265508948870169e+19
2023-01-07 08:52:29,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6849675178527832
2023-01-07 08:52:29,558 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,558 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -1.9911222438810068e+24
2023-01-07 08:52:29,558 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -39.08067321777344
2023-01-07 08:52:29,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,559 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0265508948870169e+19
2023-01-07 08:52:29,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2555502951145172
2023-01-07 08:52:29,560 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,560 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1.0265508948870169e+19
2023-01-07 08:52:29,560 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:29,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,560 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.95554708814634e+23
2023-01-07 08:52:29,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.009233340620994568
2023-01-07 08:52:29,561 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,561 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -9.955575190608015e+23
2023-01-07 08:52:29,561 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,561 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -61.14878463745117
2023-01-07 08:52:29,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,562 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.95554708814634e+23
2023-01-07 08:52:29,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0628187358379364
2023-01-07 08:52:29,563 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,563 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -9.95554708814634e+23
2023-01-07 08:52:29,563 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,563 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,563 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:29,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,564 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.07155704498291
2023-01-07 08:52:29,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.028219405561685562
2023-01-07 08:52:29,564 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,565 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.06408169865608215
2023-01-07 08:52:29,565 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,565 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 60.91974639892578
2023-01-07 08:52:29,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,565 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 7.07155704498291
2023-01-07 08:52:29,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40967246890068054
2023-01-07 08:52:29,566 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,566 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 7.07155704498291
2023-01-07 08:52:29,566 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:29,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,567 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.264432907104492
2023-01-07 08:52:29,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22908051311969757
2023-01-07 08:52:29,568 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,568 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.031309813261032104
2023-01-07 08:52:29,568 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 36.41448211669922
2023-01-07 08:52:29,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,569 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.264432907104492
2023-01-07 08:52:29,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3773268163204193
2023-01-07 08:52:29,570 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,570 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 6.264432907104492
2023-01-07 08:52:29,570 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,570 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,570 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:29,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,570 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.0106048583984375
2023-01-07 08:52:29,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16668105125427246
2023-01-07 08:52:29,571 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,571 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.5818096399307251
2023-01-07 08:52:29,571 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.122623443603516
2023-01-07 08:52:29,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,572 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.0106048583984375
2023-01-07 08:52:29,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07747264206409454
2023-01-07 08:52:29,573 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,573 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -1.0106048583984375
2023-01-07 08:52:29,573 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:29,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.402017593383789
2023-01-07 08:52:29,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02139311656355858
2023-01-07 08:52:29,574 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,575 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.026054181158542633
2023-01-07 08:52:29,575 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -8.743721961975098
2023-01-07 08:52:29,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.402017593383789
2023-01-07 08:52:29,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08515828847885132
2023-01-07 08:52:29,576 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,576 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -4.402017593383789
2023-01-07 08:52:29,576 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,576 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:29,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.717001438140869
2023-01-07 08:52:29,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.013830378651618958
2023-01-07 08:52:29,578 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,578 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.03684471920132637
2023-01-07 08:52:29,578 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -8.253673553466797
2023-01-07 08:52:29,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,578 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.717001438140869
2023-01-07 08:52:29,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.034320663660764694
2023-01-07 08:52:29,580 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,580 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 3.717001438140869
2023-01-07 08:52:29,580 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:29,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,580 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.02117919921875
2023-01-07 08:52:29,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.6422295570373535
2023-01-07 08:52:29,581 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,581 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.6348865032196045
2023-01-07 08:52:29,581 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 3.132833480834961
2023-01-07 08:52:29,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,582 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.02117919921875
2023-01-07 08:52:29,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.49945035576820374
2023-01-07 08:52:29,583 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:52:29,583 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -0.02117919921875
2023-01-07 08:52:29,584 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:52:29,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:52:29,585 > [DEBUG] 0 :: 7.5506815910339355
2023-01-07 08:52:29,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,588 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.030609130859375
2023-01-07 08:52:29,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -340.8158874511719
2023-01-07 08:52:29,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,590 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4001823663711548
2023-01-07 08:52:29,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,591 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.030609130859375
2023-01-07 08:52:29,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -337.98406982421875
2023-01-07 08:52:29,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,594 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.937177658081055
2023-01-07 08:52:29,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.037991419434547424
2023-01-07 08:52:29,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,596 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.04276119917631149
2023-01-07 08:52:29,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,597 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.937177658081055
2023-01-07 08:52:29,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44670993089675903
2023-01-07 08:52:29,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,600 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.25260591506958
2023-01-07 08:52:29,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1322951316833496
2023-01-07 08:52:29,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,601 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.048213742673397064
2023-01-07 08:52:29,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,601 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.25260591506958
2023-01-07 08:52:29,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.050666578114032745
2023-01-07 08:52:29,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,603 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.8310055732727051
2023-01-07 08:52:29,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1134987473487854
2023-01-07 08:52:29,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.3948257863521576
2023-01-07 08:52:29,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.8310055732727051
2023-01-07 08:52:29,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6055370569229126
2023-01-07 08:52:29,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,606 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.177079677581787
2023-01-07 08:52:29,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11672570556402206
2023-01-07 08:52:29,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,607 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.06565116345882416
2023-01-07 08:52:29,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,607 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.177079677581787
2023-01-07 08:52:29,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5859719514846802
2023-01-07 08:52:29,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,609 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -2.3443219661712646
2023-01-07 08:52:29,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36849501729011536
2023-01-07 08:52:29,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.04414132237434387
2023-01-07 08:52:29,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -2.3443219661712646
2023-01-07 08:52:29,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.219490185379982
2023-01-07 08:52:29,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 3.6752548217773438
2023-01-07 08:52:29,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1492339372634888
2023-01-07 08:52:29,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.3474666476249695
2023-01-07 08:52:29,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 3.6752548217773438
2023-01-07 08:52:29,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.178476333618164
2023-01-07 08:52:29,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.264676094055176
2023-01-07 08:52:29,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4910244941711426
2023-01-07 08:52:29,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,616 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1311236321926117
2023-01-07 08:52:29,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,617 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.264676094055176
2023-01-07 08:52:29,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02472599595785141
2023-01-07 08:52:29,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,618 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.0285773277282715
2023-01-07 08:52:29,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14255067706108093
2023-01-07 08:52:29,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,619 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.05099974572658539
2023-01-07 08:52:29,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.0285773277282715
2023-01-07 08:52:29,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.579339325428009
2023-01-07 08:52:29,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,621 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2536.898193359375
2023-01-07 08:52:29,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6570054292678833
2023-01-07 08:52:29,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.08685328811407089
2023-01-07 08:52:29,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2536.898193359375
2023-01-07 08:52:29,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6251792907714844
2023-01-07 08:52:29,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2870.24267578125
2023-01-07 08:52:29,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4941548705101013
2023-01-07 08:52:29,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.09157221019268036
2023-01-07 08:52:29,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2870.24267578125
2023-01-07 08:52:29,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1914427280426025
2023-01-07 08:52:29,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,627 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1436.554931640625
2023-01-07 08:52:29,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3008939027786255
2023-01-07 08:52:29,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.07681702077388763
2023-01-07 08:52:29,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1436.554931640625
2023-01-07 08:52:29,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5870763063430786
2023-01-07 08:52:29,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5929.30224609375
2023-01-07 08:52:29,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4259492754936218
2023-01-07 08:52:29,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,632 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.08855590224266052
2023-01-07 08:52:29,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,632 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 5929.30224609375
2023-01-07 08:52:29,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7630378603935242
2023-01-07 08:52:29,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 898.6549072265625
2023-01-07 08:52:29,634 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.186598777770996
2023-01-07 08:52:29,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.15471675992012024
2023-01-07 08:52:29,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 898.6549072265625
2023-01-07 08:52:29,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9559670686721802
2023-01-07 08:52:29,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4254.466796875
2023-01-07 08:52:29,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.652641236782074
2023-01-07 08:52:29,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.03157716244459152
2023-01-07 08:52:29,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4254.466796875
2023-01-07 08:52:29,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15475371479988098
2023-01-07 08:52:29,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 492.0118408203125
2023-01-07 08:52:29,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08141481876373291
2023-01-07 08:52:29,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.083150215446949
2023-01-07 08:52:29,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 492.0118408203125
2023-01-07 08:52:29,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5705742835998535
2023-01-07 08:52:29,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 582.0672607421875
2023-01-07 08:52:29,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30641627311706543
2023-01-07 08:52:29,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.09991142153739929
2023-01-07 08:52:29,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 582.0672607421875
2023-01-07 08:52:29,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5180156230926514
2023-01-07 08:52:29,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,646 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.079431533813477
2023-01-07 08:52:29,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1758390665054321
2023-01-07 08:52:29,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.17467908561229706
2023-01-07 08:52:29,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.079431533813477
2023-01-07 08:52:29,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7822867631912231
2023-01-07 08:52:29,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,649 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 12.344830513000488
2023-01-07 08:52:29,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1840656995773315
2023-01-07 08:52:29,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.0016501843929290771
2023-01-07 08:52:29,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 12.344830513000488
2023-01-07 08:52:29,651 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8589490652084351
2023-01-07 08:52:29,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,652 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 14.66955852508545
2023-01-07 08:52:29,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0258617401123047
2023-01-07 08:52:29,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.39024683833122253
2023-01-07 08:52:29,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 14.66955852508545
2023-01-07 08:52:29,654 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5905930995941162
2023-01-07 08:52:29,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.8513448238372803
2023-01-07 08:52:29,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.253596782684326
2023-01-07 08:52:29,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.022979021072387695
2023-01-07 08:52:29,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.8513448238372803
2023-01-07 08:52:29,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1546261310577393
2023-01-07 08:52:29,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 1.1929054260253906
2023-01-07 08:52:29,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6931266784667969
2023-01-07 08:52:29,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.009735405445098877
2023-01-07 08:52:29,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 1.1929054260253906
2023-01-07 08:52:29,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.405792236328125
2023-01-07 08:52:29,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.9085235595703125
2023-01-07 08:52:29,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0716029405593872
2023-01-07 08:52:29,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.15096783638000488
2023-01-07 08:52:29,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.9085235595703125
2023-01-07 08:52:29,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8876956701278687
2023-01-07 08:52:29,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 16.335309982299805
2023-01-07 08:52:29,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1449711322784424
2023-01-07 08:52:29,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,666 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.09233815968036652
2023-01-07 08:52:29,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 16.335309982299805
2023-01-07 08:52:29,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1032034158706665
2023-01-07 08:52:29,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,668 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 57.94342803955078
2023-01-07 08:52:29,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.313265323638916
2023-01-07 08:52:29,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 57.94342803955078
2023-01-07 08:52:29,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06778848171234131
2023-01-07 08:52:29,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,671 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 9.677722930908203
2023-01-07 08:52:29,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4076443016529083
2023-01-07 08:52:29,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,672 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.30045169591903687
2023-01-07 08:52:29,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,673 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 9.677722930908203
2023-01-07 08:52:29,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9669766426086426
2023-01-07 08:52:29,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,674 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 31.860734939575195
2023-01-07 08:52:29,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23705029487609863
2023-01-07 08:52:29,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,675 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.3371095657348633
2023-01-07 08:52:29,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,676 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 31.860734939575195
2023-01-07 08:52:29,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.242157459259033
2023-01-07 08:52:29,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,677 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 9.571242332458496
2023-01-07 08:52:29,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8619685173034668
2023-01-07 08:52:29,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,678 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.16882243752479553
2023-01-07 08:52:29,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,679 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 9.571242332458496
2023-01-07 08:52:29,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.212228775024414
2023-01-07 08:52:29,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,680 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 57.761817932128906
2023-01-07 08:52:29,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.228873252868652
2023-01-07 08:52:29,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,681 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 57.761817932128906
2023-01-07 08:52:29,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.786357879638672
2023-01-07 08:52:29,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,683 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -3.862936019897461
2023-01-07 08:52:29,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2554188668727875
2023-01-07 08:52:29,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,684 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -3.862936019897461
2023-01-07 08:52:29,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.681031227111816
2023-01-07 08:52:29,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0727176666259766
2023-01-07 08:52:29,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4877946376800537
2023-01-07 08:52:29,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.0727176666259766
2023-01-07 08:52:29,687 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.012411594390869
2023-01-07 08:52:29,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 164.2832794189453
2023-01-07 08:52:29,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5678107738494873
2023-01-07 08:52:29,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 164.2832794189453
2023-01-07 08:52:29,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.542248725891113
2023-01-07 08:52:29,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 23.36446762084961
2023-01-07 08:52:29,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.015871286392212
2023-01-07 08:52:29,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 23.36446762084961
2023-01-07 08:52:29,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.44734001159668
2023-01-07 08:52:29,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.319984436035156
2023-01-07 08:52:29,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9845248460769653
2023-01-07 08:52:29,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 11.319984436035156
2023-01-07 08:52:29,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.067002296447754
2023-01-07 08:52:29,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 49.62834930419922
2023-01-07 08:52:29,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28266358375549316
2023-01-07 08:52:29,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 49.62834930419922
2023-01-07 08:52:29,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.636789798736572
2023-01-07 08:52:29,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -28.628334045410156
2023-01-07 08:52:29,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5779175758361816
2023-01-07 08:52:29,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -1.2119872570037842
2023-01-07 08:52:29,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -28.628334045410156
2023-01-07 08:52:29,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6522172689437866
2023-01-07 08:52:29,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,702 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 38.520538330078125
2023-01-07 08:52:29,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.710294723510742
2023-01-07 08:52:29,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 38.520538330078125
2023-01-07 08:52:29,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.242892026901245
2023-01-07 08:52:29,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.8235397338867188
2023-01-07 08:52:29,705 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.09169006347656
2023-01-07 08:52:29,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,706 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1.8235397338867188
2023-01-07 08:52:29,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.026065826416016
2023-01-07 08:52:29,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -31.38011360168457
2023-01-07 08:52:29,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.492769718170166
2023-01-07 08:52:29,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.9296085834503174
2023-01-07 08:52:29,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -31.38011360168457
2023-01-07 08:52:29,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.991952896118164
2023-01-07 08:52:29,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,710 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6153526306152344
2023-01-07 08:52:29,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.83127212524414
2023-01-07 08:52:29,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,712 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.6153526306152344
2023-01-07 08:52:29,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.315301895141602
2023-01-07 08:52:29,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -31.528732299804688
2023-01-07 08:52:29,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.567466735839844
2023-01-07 08:52:29,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -31.528732299804688
2023-01-07 08:52:29,714 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.8456854820251465
2023-01-07 08:52:29,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,716 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.200002670288086
2023-01-07 08:52:29,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,717 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 2.0076942443847656
2023-01-07 08:52:29,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,717 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,718 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.312255859375
2023-01-07 08:52:29,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,719 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.798847198486328
2023-01-07 08:52:29,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,720 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.923546552658081
2023-01-07 08:52:29,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,720 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -53.5009880065918
2023-01-07 08:52:29,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.437294006347656
2023-01-07 08:52:29,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,722 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.48902893066406
2023-01-07 08:52:29,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,724 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -53.5009880065918
2023-01-07 08:52:29,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.848474979400635
2023-01-07 08:52:29,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,725 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.939358711242676
2023-01-07 08:52:29,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,726 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.6591479778289795
2023-01-07 08:52:29,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,727 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.376991271972656
2023-01-07 08:52:29,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,728 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.65578031539917
2023-01-07 08:52:29,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,729 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -3.0181946754455566
2023-01-07 08:52:29,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,730 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -81.39726257324219
2023-01-07 08:52:29,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.04783248901367
2023-01-07 08:52:29,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,731 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,731 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6974935531616211
2023-01-07 08:52:29,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,732 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -25.855810165405273
2023-01-07 08:52:29,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.524730682373047
2023-01-07 08:52:29,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,734 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.535444259643555
2023-01-07 08:52:29,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,735 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 207.24913024902344
2023-01-07 08:52:29,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.735471725463867
2023-01-07 08:52:29,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,737 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.235431671142578
2023-01-07 08:52:29,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,739 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -7.632549285888672
2023-01-07 08:52:29,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,739 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -15.590803146362305
2023-01-07 08:52:29,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,739 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.743297576904297
2023-01-07 08:52:29,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,741 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.558014869689941
2023-01-07 08:52:29,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,742 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 162.35533142089844
2023-01-07 08:52:29,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 116.6475601196289
2023-01-07 08:52:29,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,744 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -53.66881561279297
2023-01-07 08:52:29,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.749210357666016
2023-01-07 08:52:29,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,745 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 162.35533142089844
2023-01-07 08:52:29,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -38.52390670776367
2023-01-07 08:52:29,745 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.79450607299805
2023-01-07 08:52:29,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,747 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 32.76957702636719
2023-01-07 08:52:29,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.305812835693359
2023-01-07 08:52:29,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,748 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -5.7614617347717285
2023-01-07 08:52:29,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,748 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 32.76957702636719
2023-01-07 08:52:29,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -74.99649047851562
2023-01-07 08:52:29,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,750 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 32.76957702636719
2023-01-07 08:52:29,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.310760498046875
2023-01-07 08:52:29,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:52:29,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:52:29,751 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 32.76957702636719
2023-01-07 08:52:29,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.546192169189453
