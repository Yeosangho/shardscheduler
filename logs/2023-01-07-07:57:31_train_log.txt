2023-01-07 07:57:38,497 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:57:38,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:38,535 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.13284309208393097
2023-01-07 07:57:38,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:38,535 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:57:38,535 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:38,535 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:38,535 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 07:57:38,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,405 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,406 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,406 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,406 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,406 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 07:57:39,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,408 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 5.791226863861084
2023-01-07 07:57:39,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,408 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:57:39,409 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,409 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,409 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 07:57:39,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,410 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,410 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,410 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,410 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,410 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 07:57:39,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,411 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -5.585012912750244
2023-01-07 07:57:39,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,412 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:57:39,412 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,412 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,412 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 07:57:39,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,450 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,451 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,451 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,451 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,451 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 07:57:39,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,453 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 7.001418113708496
2023-01-07 07:57:39,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,453 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,453 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,453 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,453 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 07:57:39,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,454 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,455 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,455 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,455 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,455 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 07:57:39,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,456 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -13.992088317871094
2023-01-07 07:57:39,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,456 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,457 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,457 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,457 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 07:57:39,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,458 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,458 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,458 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,458 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,458 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 07:57:39,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,459 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -30.501585006713867
2023-01-07 07:57:39,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,460 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,460 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,460 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,460 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 07:57:39,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,461 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,461 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,461 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,461 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,462 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 07:57:39,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,463 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.1827199459075928
2023-01-07 07:57:39,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,463 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:57:39,463 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,463 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,463 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 07:57:39,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,464 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,465 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,465 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,465 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,465 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 07:57:39,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,466 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -12.021247863769531
2023-01-07 07:57:39,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,466 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,467 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,467 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,467 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 07:57:39,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,468 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,468 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,468 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,468 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,468 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 07:57:39,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,469 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -17.160531997680664
2023-01-07 07:57:39,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,470 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,470 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,470 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,470 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 07:57:39,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,471 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,471 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,471 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,471 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,472 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 07:57:39,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,473 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -5.949265956878662
2023-01-07 07:57:39,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,473 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:57:39,473 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,473 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,473 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 07:57:39,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,474 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:57:39,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,475 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:57:39,475 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,475 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,475 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 07:57:39,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,476 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -1.8480854034423828
2023-01-07 07:57:39,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,477 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:57:39,477 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,477 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,477 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 07:57:39,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,478 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,478 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,478 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,478 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,478 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 07:57:39,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,479 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -25.623701095581055
2023-01-07 07:57:39,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,480 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:57:39,480 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,480 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,480 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 07:57:39,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,481 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,482 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,482 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,482 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,482 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 07:57:39,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,483 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: -17.432403564453125
2023-01-07 07:57:39,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,483 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:57:39,484 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,484 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,484 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 07:57:39,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,485 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,485 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,485 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,485 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,485 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 07:57:39,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,487 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.029879570007324
2023-01-07 07:57:39,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,487 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,487 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,487 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,487 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 07:57:39,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,488 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,489 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,489 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,489 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,489 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 07:57:39,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,490 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -10.26220989227295
2023-01-07 07:57:39,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,490 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:57:39,490 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,490 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,490 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 07:57:39,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,492 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,492 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,492 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,492 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,492 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 07:57:39,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,493 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 27.125289916992188
2023-01-07 07:57:39,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,494 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,494 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,494 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,494 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 07:57:39,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,495 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,495 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,496 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,496 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,496 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 07:57:39,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,497 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -18.01710319519043
2023-01-07 07:57:39,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,497 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:57:39,497 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,497 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,497 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 07:57:39,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,499 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,499 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,499 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,499 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,499 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 07:57:39,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,500 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -7.786376953125
2023-01-07 07:57:39,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,501 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,501 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,501 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,501 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 07:57:39,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,502 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,502 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,502 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,502 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,502 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 07:57:39,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,504 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -13.727725982666016
2023-01-07 07:57:39,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,504 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,504 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,504 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,504 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 07:57:39,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,505 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,506 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,506 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,506 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,506 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 07:57:39,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,507 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -9.844511985778809
2023-01-07 07:57:39,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,508 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:57:39,508 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,508 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,508 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 07:57:39,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,509 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,509 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,509 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,509 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,509 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 07:57:39,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,510 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 6.59615421295166
2023-01-07 07:57:39,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,511 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,511 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,511 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,511 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 07:57:39,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,512 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,512 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,512 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,512 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,513 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 07:57:39,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,514 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 15.237881660461426
2023-01-07 07:57:39,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,514 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,514 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,514 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,514 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 07:57:39,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,515 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,516 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,516 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,516 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,516 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 07:57:39,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,517 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -9.673160552978516
2023-01-07 07:57:39,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,518 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:57:39,518 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,518 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,518 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 07:57:39,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,519 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:57:39,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,519 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:57:39,519 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,519 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,519 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 07:57:39,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,520 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -24.8615665435791
2023-01-07 07:57:39,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,521 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:57:39,521 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,521 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,521 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 07:57:39,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,522 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,522 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,522 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,522 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,522 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 07:57:39,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,524 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 12.279350280761719
2023-01-07 07:57:39,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,524 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:57:39,524 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,524 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,524 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 07:57:39,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,525 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,526 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,526 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,526 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,526 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 07:57:39,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,527 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -12.231971740722656
2023-01-07 07:57:39,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,528 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,528 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,528 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,528 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 07:57:39,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,529 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,529 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,529 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,529 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,529 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 07:57:39,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,531 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 21.67206382751465
2023-01-07 07:57:39,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,531 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,531 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,531 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,531 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 07:57:39,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,532 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,533 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,533 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,533 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,533 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 07:57:39,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,534 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -16.804777145385742
2023-01-07 07:57:39,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,534 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:57:39,534 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,534 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,534 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 07:57:39,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,535 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,536 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,536 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,536 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,536 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 07:57:39,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,537 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -16.295696258544922
2023-01-07 07:57:39,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,538 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,538 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,538 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,538 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 07:57:39,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,539 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,539 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,539 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,539 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,540 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 07:57:39,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,541 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -42.445552825927734
2023-01-07 07:57:39,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,541 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,541 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,541 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,541 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 07:57:39,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,543 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,543 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,543 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,543 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,543 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 07:57:39,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,544 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: -7.268904685974121
2023-01-07 07:57:39,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,545 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,545 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,545 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,545 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 07:57:39,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,546 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,546 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,546 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,546 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,546 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 07:57:39,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,548 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 17.74041175842285
2023-01-07 07:57:39,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,548 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,548 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,548 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,548 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 07:57:39,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,549 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,550 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,550 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,550 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,550 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 07:57:39,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,551 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: -29.108219146728516
2023-01-07 07:57:39,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,551 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,551 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,551 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,551 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 07:57:39,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,552 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,553 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,553 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,553 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,553 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 07:57:39,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,554 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 24.388256072998047
2023-01-07 07:57:39,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,555 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,555 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,555 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,555 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 07:57:39,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,556 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,556 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,556 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,556 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,556 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 07:57:39,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,557 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 10.171612739562988
2023-01-07 07:57:39,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,558 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,558 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,558 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,558 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 07:57:39,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,559 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,559 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,559 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,560 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,560 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 07:57:39,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,561 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: 40.93818664550781
2023-01-07 07:57:39,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,561 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,561 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,561 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,561 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 07:57:39,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,562 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,563 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,563 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,563 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,563 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 07:57:39,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,564 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -10.738574981689453
2023-01-07 07:57:39,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,564 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,565 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,565 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,565 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 07:57:39,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,566 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,566 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,566 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,566 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,566 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 07:57:39,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,567 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 19.385414123535156
2023-01-07 07:57:39,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,568 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,568 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,568 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,568 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 07:57:39,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,569 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,569 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,569 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,569 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,569 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 07:57:39,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,570 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 12.631711959838867
2023-01-07 07:57:39,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,571 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,571 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,571 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,571 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 07:57:39,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,572 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,573 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,573 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,573 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,573 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 07:57:39,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,574 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -9.76319408416748
2023-01-07 07:57:39,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,574 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,574 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,574 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,574 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 07:57:39,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,575 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,576 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,576 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,576 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,576 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 07:57:39,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,577 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 90.18097686767578
2023-01-07 07:57:39,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,578 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,578 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,578 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,578 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 07:57:39,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,579 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,579 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,579 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,579 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,579 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 07:57:39,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,580 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -3.338642120361328
2023-01-07 07:57:39,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,581 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:57:39,581 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,581 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,581 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 07:57:39,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,582 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:57:39,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,582 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:57:39,582 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,582 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,583 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 07:57:39,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,584 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -33.3038215637207
2023-01-07 07:57:39,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,584 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:57:39,584 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,584 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,584 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 07:57:39,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,585 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:57:39,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,586 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:57:39,586 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,586 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,586 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 07:57:39,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,587 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -47.220123291015625
2023-01-07 07:57:39,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,587 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:57:39,587 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,587 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,588 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 07:57:39,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,589 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,589 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,589 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,589 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,589 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 07:57:39,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,590 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: 9.24878978729248
2023-01-07 07:57:39,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,591 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:57:39,591 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,591 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,591 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 07:57:39,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,592 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,592 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,592 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,592 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,592 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 07:57:39,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,594 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -28.780466079711914
2023-01-07 07:57:39,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,594 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:57:39,594 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,594 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,594 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 07:57:39,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,595 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:57:39,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,596 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:57:39,596 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,596 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,596 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 07:57:39,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,597 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 20.619970321655273
2023-01-07 07:57:39,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,597 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:57:39,597 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,597 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,597 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 07:57:39,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,598 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:57:39,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,599 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:57:39,599 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,599 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,599 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 07:57:39,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,600 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -17.029775619506836
2023-01-07 07:57:39,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,600 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:57:39,600 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,601 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,601 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 07:57:39,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,602 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,602 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,602 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,602 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,602 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 07:57:39,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,603 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: 43.10231018066406
2023-01-07 07:57:39,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,604 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:57:39,604 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,604 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,604 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 07:57:39,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,605 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,606 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,606 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,606 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,606 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 07:57:39,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,607 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -6.619914531707764
2023-01-07 07:57:39,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,607 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:57:39,607 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,607 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,608 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 07:57:39,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,609 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:57:39,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,609 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:57:39,609 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,609 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,609 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 07:57:39,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,610 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -77.86361694335938
2023-01-07 07:57:39,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,611 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:57:39,611 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,611 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,611 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 07:57:39,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,612 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,612 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,612 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,612 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,612 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 07:57:39,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,613 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -17.798227310180664
2023-01-07 07:57:39,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,614 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:57:39,614 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,614 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,614 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 07:57:39,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,615 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:57:39,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,615 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:57:39,616 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,616 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,616 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 07:57:39,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,617 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -2.0706939697265625
2023-01-07 07:57:39,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,617 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:57:39,617 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,617 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,617 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 07:57:39,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,618 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:57:39,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,619 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:57:39,619 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,619 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,619 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 07:57:39,619 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,620 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: -4.9523773193359375
2023-01-07 07:57:39,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:57:39,621 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:57:39,621 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:57:39,621 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:57:39,621 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 07:57:39,622 > [DEBUG] 0 :: 7.3850178718566895
2023-01-07 07:57:39,627 > [DEBUG] 0 :: do all reduce async
