2023-01-07 08:40:36,960 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:40:36,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:36,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:36,996 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:40:36,996 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:36,996 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:36,996 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:40:36,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,840 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,840 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,840 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,840 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:40:37,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,842 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:40:37,842 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,842 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,843 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:40:37,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,844 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,844 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,844 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,844 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:40:37,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,846 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:40:37,846 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,846 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,846 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:40:37,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,883 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,884 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,884 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:40:37,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,885 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,885 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,885 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,885 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:40:37,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,887 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,887 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,887 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:40:37,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,888 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,888 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,889 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:40:37,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,890 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,890 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,890 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,890 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:40:37,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,892 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,892 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,892 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:40:37,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,893 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,893 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,893 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,893 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:40:37,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,895 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:40:37,895 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,895 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,895 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:40:37,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,896 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,896 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,896 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,896 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:40:37,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,898 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,898 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,898 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,898 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:40:37,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,899 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,899 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,899 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,899 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:40:37,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,901 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,901 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,901 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,901 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:40:37,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,902 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,902 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,902 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,902 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:40:37,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,904 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:40:37,904 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,904 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,904 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:40:37,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,905 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:40:37,905 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,905 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,905 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:40:37,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,907 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:40:37,907 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,907 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,907 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:40:37,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,908 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,908 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,908 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,908 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:40:37,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,910 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:40:37,910 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,910 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,910 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:40:37,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,911 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,912 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,912 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,912 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:40:37,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,913 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:40:37,913 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,913 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,913 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:40:37,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,915 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,915 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,915 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,915 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:40:37,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,917 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,917 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,917 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:40:37,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,918 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:37,918 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,918 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,918 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:40:37,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,920 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:40:37,920 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,920 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,920 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:40:37,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,921 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:37,921 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,921 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,921 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:40:37,922 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,923 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,923 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,923 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,923 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:40:37,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,924 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,924 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,924 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,925 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:40:37,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,926 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:40:37,926 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,926 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,926 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:40:37,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,928 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,928 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,928 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,928 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:40:37,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,929 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,929 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,929 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,929 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:40:37,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,931 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:37,931 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,931 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,931 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:40:37,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,932 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,932 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,933 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:40:37,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,934 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,934 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,934 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,934 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:40:37,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,935 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:40:37,936 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,936 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,936 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:40:37,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,937 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,937 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,937 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,937 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:40:37,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,939 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,939 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,939 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,939 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:40:37,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,940 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:37,940 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,940 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,940 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:40:37,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,942 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,942 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,942 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,942 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:40:37,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,943 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,943 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,943 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,943 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:40:37,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,945 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:40:37,945 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,945 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,945 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:40:37,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,946 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:40:37,946 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,946 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,946 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:40:37,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,948 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:40:37,948 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,948 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,948 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:40:37,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,949 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:37,949 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,949 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,949 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:40:37,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,951 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:40:37,951 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,951 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,951 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:40:37,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,953 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,953 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,953 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,953 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:40:37,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,954 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:37,954 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,954 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:40:37,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,956 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,956 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,956 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:40:37,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,957 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,957 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,958 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:40:37,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,959 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,959 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,959 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,959 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:40:37,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,960 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:40:37,960 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,960 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,961 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:40:37,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,962 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,962 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,962 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:40:37,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,963 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,964 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,964 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,964 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:40:37,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,965 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,965 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,965 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:40:37,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,967 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:37,967 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,967 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:40:37,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,968 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,968 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,968 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,968 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:40:37,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,970 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,970 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,970 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:40:37,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,971 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,971 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,971 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:40:37,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,973 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,973 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,973 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:40:37,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,974 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,974 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,975 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,975 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:40:37,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,976 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:37,976 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,976 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:40:37,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,977 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,978 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,978 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:40:37,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,979 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,979 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,979 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,979 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:40:37,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,980 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,981 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,981 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,981 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:40:37,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,982 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,982 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,982 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,982 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:40:37,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,984 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,984 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,984 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,984 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:40:37,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,985 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:37,985 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,985 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,985 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:40:37,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,987 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,987 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,987 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,987 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:40:37,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,988 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,988 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,988 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,988 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:40:37,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,990 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,990 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,990 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,990 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:40:37,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,991 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,991 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,991 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,991 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:40:37,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,993 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,993 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,993 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,993 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:40:37,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,994 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:37,994 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,994 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,994 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:40:37,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,996 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:37,996 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,996 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,996 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:40:37,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,997 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:37,997 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,997 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,997 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:40:37,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:37,999 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:37,999 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:37,999 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:37,999 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:40:37,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,000 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:38,000 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,000 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,001 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:40:38,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,002 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:38,002 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,002 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,002 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:40:38,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,003 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:40:38,003 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,004 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,004 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:40:38,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,005 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:40:38,005 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,005 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,005 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:40:38,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,006 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:40:38,006 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,006 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,007 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:40:38,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,008 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:40:38,008 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,008 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,008 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:40:38,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,009 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:40:38,010 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,010 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,010 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:40:38,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,011 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,011 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,011 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,011 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:40:38,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,013 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:40:38,013 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,013 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,013 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:40:38,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,014 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,014 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,014 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,014 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:40:38,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,016 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:40:38,016 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,016 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,016 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:40:38,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,017 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:40:38,017 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,017 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,017 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:40:38,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,018 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:40:38,019 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,019 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,019 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:40:38,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,020 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:40:38,020 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,020 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,020 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:40:38,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,022 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:40:38,022 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,022 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,022 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:40:38,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,023 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,023 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,023 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,023 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:40:38,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,025 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:40:38,025 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,025 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,025 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:40:38,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,026 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,026 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,026 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,026 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:40:38,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,028 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:40:38,028 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,028 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,028 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:40:38,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,029 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:40:38,029 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,029 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:40:38,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,031 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:40:38,031 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,031 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,031 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:40:38,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,032 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,032 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,032 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:40:38,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,034 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:40:38,034 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,034 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,034 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:40:38,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,035 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:40:38,035 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,035 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,036 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:40:38,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,037 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:40:38,037 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,037 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,037 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:40:38,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,038 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:40:38,038 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,038 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,038 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:40:38,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:40:38,040 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:40:38,040 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,040 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:40:38,040 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:40:38,042 > [DEBUG] 0 :: 7.212255954742432
2023-01-07 08:40:38,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,047 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,047 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.01715087890625
2023-01-07 08:40:38,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -367.652587890625
2023-01-07 08:40:38,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,050 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,050 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2123936414718628
2023-01-07 08:40:38,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,051 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -187.938720703125
2023-01-07 08:40:38,051 > [DEBUG] 0 :: before allreduce fusion buffer :: -347.9674377441406
2023-01-07 08:40:38,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,062 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,062 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.3324456214904785
2023-01-07 08:40:38,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16040247678756714
2023-01-07 08:40:38,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,063 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,063 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.02858194336295128
2023-01-07 08:40:38,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,063 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.360652923583984
2023-01-07 08:40:38,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002320513129234314
2023-01-07 08:40:38,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,066 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,066 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -5.4045891761779785
2023-01-07 08:40:38,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1895238310098648
2023-01-07 08:40:38,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,067 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,067 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.0840427577495575
2023-01-07 08:40:38,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,067 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -5.098363399505615
2023-01-07 08:40:38,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7006667852401733
2023-01-07 08:40:38,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,069 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,069 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.6773438453674316
2023-01-07 08:40:38,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15995579957962036
2023-01-07 08:40:38,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,070 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,070 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.22233840823173523
2023-01-07 08:40:38,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,071 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 4.436058044433594
2023-01-07 08:40:38,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.05019211769104
2023-01-07 08:40:38,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,072 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,072 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.388168334960938
2023-01-07 08:40:38,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1408885419368744
2023-01-07 08:40:38,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,073 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,073 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.014899156987667084
2023-01-07 08:40:38,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,074 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.595584869384766
2023-01-07 08:40:38,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6072496175765991
2023-01-07 08:40:38,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,075 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,075 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2.3238911628723145
2023-01-07 08:40:38,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7808717489242554
2023-01-07 08:40:38,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,076 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.038139257580041885
2023-01-07 08:40:38,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,077 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.302217483520508
2023-01-07 08:40:38,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6457874774932861
2023-01-07 08:40:38,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,078 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,078 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.31598949432373
2023-01-07 08:40:38,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5773524045944214
2023-01-07 08:40:38,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,080 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.12284177541732788
2023-01-07 08:40:38,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,080 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -7.9066901206970215
2023-01-07 08:40:38,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.826727867126465
2023-01-07 08:40:38,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,082 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 15.910987854003906
2023-01-07 08:40:38,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8757422566413879
2023-01-07 08:40:38,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,083 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,083 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.16053417325019836
2023-01-07 08:40:38,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,083 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 18.39364242553711
2023-01-07 08:40:38,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20426133275032043
2023-01-07 08:40:38,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,085 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,085 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 15.629307746887207
2023-01-07 08:40:38,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21034224331378937
2023-01-07 08:40:38,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,086 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,086 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.12840913236141205
2023-01-07 08:40:38,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,086 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 16.65280532836914
2023-01-07 08:40:38,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.275481641292572
2023-01-07 08:40:38,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,089 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,089 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 99.68656158447266
2023-01-07 08:40:38,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8905692100524902
2023-01-07 08:40:38,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,090 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,091 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.11149419844150543
2023-01-07 08:40:38,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,091 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 100.73670196533203
2023-01-07 08:40:38,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25489431619644165
2023-01-07 08:40:38,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,093 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,093 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 9.104275703430176
2023-01-07 08:40:38,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2300308495759964
2023-01-07 08:40:38,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,094 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,094 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.03613045811653137
2023-01-07 08:40:38,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,094 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 7.4619951248168945
2023-01-07 08:40:38,095 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.133228063583374
2023-01-07 08:40:38,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,096 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,096 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.4503726959228516
2023-01-07 08:40:38,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24961024522781372
2023-01-07 08:40:38,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,097 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.02035241574048996
2023-01-07 08:40:38,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.101546764373779
2023-01-07 08:40:38,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3508017063140869
2023-01-07 08:40:38,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,100 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 0.660789966583252
2023-01-07 08:40:38,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3516286015510559
2023-01-07 08:40:38,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,101 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.08525784313678741
2023-01-07 08:40:38,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,101 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 0.11540603637695312
2023-01-07 08:40:38,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5577104091644287
2023-01-07 08:40:38,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,103 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.22592544555664
2023-01-07 08:40:38,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09368222951889038
2023-01-07 08:40:38,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,104 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,104 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.07350926846265793
2023-01-07 08:40:38,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.733246803283691
2023-01-07 08:40:38,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22374771535396576
2023-01-07 08:40:38,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,106 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 8.419394493103027
2023-01-07 08:40:38,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7627251744270325
2023-01-07 08:40:38,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,107 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,107 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.12651526927947998
2023-01-07 08:40:38,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 7.289477825164795
2023-01-07 08:40:38,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2425687313079834
2023-01-07 08:40:38,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -35.48917770385742
2023-01-07 08:40:38,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.728202223777771
2023-01-07 08:40:38,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,110 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,110 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.15503227710723877
2023-01-07 08:40:38,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -37.97686004638672
2023-01-07 08:40:38,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.214202642440796
2023-01-07 08:40:38,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -6.239794731140137
2023-01-07 08:40:38,112 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17494884133338928
2023-01-07 08:40:38,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,113 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.12557876110076904
2023-01-07 08:40:38,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -8.650683403015137
2023-01-07 08:40:38,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8672072887420654
2023-01-07 08:40:38,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,115 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.5841498374938965
2023-01-07 08:40:38,115 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3078372478485107
2023-01-07 08:40:38,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,116 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.06868550181388855
2023-01-07 08:40:38,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1.5386595726013184
2023-01-07 08:40:38,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1160433292388916
2023-01-07 08:40:38,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -7.194494247436523
2023-01-07 08:40:38,118 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9304214715957642
2023-01-07 08:40:38,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,119 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.03932659327983856
2023-01-07 08:40:38,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -12.553977966308594
2023-01-07 08:40:38,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3677294254302979
2023-01-07 08:40:38,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,121 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 20.86675453186035
2023-01-07 08:40:38,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04814543575048447
2023-01-07 08:40:38,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,122 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,122 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.14607837796211243
2023-01-07 08:40:38,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 15.681633949279785
2023-01-07 08:40:38,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4938546419143677
2023-01-07 08:40:38,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,124 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,124 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 2.798990249633789
2023-01-07 08:40:38,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5364503264427185
2023-01-07 08:40:38,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,125 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.17322534322738647
2023-01-07 08:40:38,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -0.2120676040649414
2023-01-07 08:40:38,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4081659317016602
2023-01-07 08:40:38,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,127 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 21.341096878051758
2023-01-07 08:40:38,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.262314558029175
2023-01-07 08:40:38,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.06085282564163208
2023-01-07 08:40:38,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 18.81080436706543
2023-01-07 08:40:38,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5611292123794556
2023-01-07 08:40:38,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,130 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -0.6157100200653076
2023-01-07 08:40:38,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22096532583236694
2023-01-07 08:40:38,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.47820475697517395
2023-01-07 08:40:38,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.0793557167053223
2023-01-07 08:40:38,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.696029186248779
2023-01-07 08:40:38,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,133 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,134 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 28.99239158630371
2023-01-07 08:40:38,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.131812572479248
2023-01-07 08:40:38,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,135 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.48117560148239136
2023-01-07 08:40:38,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 32.92115783691406
2023-01-07 08:40:38,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2511231899261475
2023-01-07 08:40:38,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,136 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 30.94474983215332
2023-01-07 08:40:38,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6851052641868591
2023-01-07 08:40:38,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 37.766700744628906
2023-01-07 08:40:38,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7906270027160645
2023-01-07 08:40:38,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,139 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,139 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.092352867126465
2023-01-07 08:40:38,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3735731840133667
2023-01-07 08:40:38,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,141 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.4247462749481201
2023-01-07 08:40:38,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,141 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.855330467224121
2023-01-07 08:40:38,141 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1571474075317383
2023-01-07 08:40:38,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 104.27110290527344
2023-01-07 08:40:38,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.519011974334717
2023-01-07 08:40:38,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,144 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.04319797456264496
2023-01-07 08:40:38,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 95.9188232421875
2023-01-07 08:40:38,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.892709493637085
2023-01-07 08:40:38,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,146 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,146 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 34.064212799072266
2023-01-07 08:40:38,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0917720794677734
2023-01-07 08:40:38,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,147 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.29976773262023926
2023-01-07 08:40:38,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,147 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 31.12955093383789
2023-01-07 08:40:38,148 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.169378280639648
2023-01-07 08:40:38,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,149 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,149 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 54.34666442871094
2023-01-07 08:40:38,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.871534824371338
2023-01-07 08:40:38,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 61.069435119628906
2023-01-07 08:40:38,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.597345352172852
2023-01-07 08:40:38,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,152 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,152 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -2.0020031929016113
2023-01-07 08:40:38,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5222159624099731
2023-01-07 08:40:38,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -34.845977783203125
2023-01-07 08:40:38,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.090149402618408
2023-01-07 08:40:38,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,155 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,155 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -12.12188720703125
2023-01-07 08:40:38,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3226404190063477
2023-01-07 08:40:38,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,157 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -14.565414428710938
2023-01-07 08:40:38,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.07069206237793
2023-01-07 08:40:38,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,158 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,158 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -49.00941467285156
2023-01-07 08:40:38,158 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.096220016479492
2023-01-07 08:40:38,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,159 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -51.75084686279297
2023-01-07 08:40:38,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.03304672241211
2023-01-07 08:40:38,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,161 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,161 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 0.922734260559082
2023-01-07 08:40:38,161 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9924094676971436
2023-01-07 08:40:38,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,162 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -6.419776439666748
2023-01-07 08:40:38,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.754592180252075
2023-01-07 08:40:38,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,164 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,164 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 13.65820598602295
2023-01-07 08:40:38,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.342215061187744
2023-01-07 08:40:38,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,165 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 16.662952423095703
2023-01-07 08:40:38,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.717809677124023
2023-01-07 08:40:38,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,166 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,167 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 98.05132293701172
2023-01-07 08:40:38,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.942310333251953
2023-01-07 08:40:38,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,168 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 93.8364028930664
2023-01-07 08:40:38,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.35916519165039
2023-01-07 08:40:38,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,169 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,169 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -45.06622314453125
2023-01-07 08:40:38,169 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5683358907699585
2023-01-07 08:40:38,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,170 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,170 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.5744187831878662
2023-01-07 08:40:38,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,171 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 0.6453151702880859
2023-01-07 08:40:38,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.64883804321289
2023-01-07 08:40:38,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,172 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,172 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 25.53870391845703
2023-01-07 08:40:38,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.798465251922607
2023-01-07 08:40:38,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -9.892671585083008
2023-01-07 08:40:38,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.283075332641602
2023-01-07 08:40:38,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,175 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 65.48492431640625
2023-01-07 08:40:38,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.940761089324951
2023-01-07 08:40:38,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,176 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 18.539655685424805
2023-01-07 08:40:38,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.212770462036133
2023-01-07 08:40:38,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,177 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,178 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -4.868805885314941
2023-01-07 08:40:38,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.093490600585938
2023-01-07 08:40:38,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,179 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,179 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.7460659146308899
2023-01-07 08:40:38,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,179 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -109.29059600830078
2023-01-07 08:40:38,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.047389507293701
2023-01-07 08:40:38,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,181 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,181 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -26.15644073486328
2023-01-07 08:40:38,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9422416090965271
2023-01-07 08:40:38,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,182 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -30.281112670898438
2023-01-07 08:40:38,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.173835277557373
2023-01-07 08:40:38,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,184 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,184 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 16.248470306396484
2023-01-07 08:40:38,184 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5567409992218018
2023-01-07 08:40:38,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,185 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -18.355457305908203
2023-01-07 08:40:38,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.854637145996094
2023-01-07 08:40:38,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,187 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,187 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -326.6423645019531
2023-01-07 08:40:38,187 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.684371948242188
2023-01-07 08:40:38,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,188 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.2765592634677887
2023-01-07 08:40:38,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,188 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -385.3577880859375
2023-01-07 08:40:38,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.28977584838867
2023-01-07 08:40:38,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,190 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -526.79541015625
2023-01-07 08:40:38,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.620474338531494
2023-01-07 08:40:38,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,191 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,191 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.01613032817840576
2023-01-07 08:40:38,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,191 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,192 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -54.78876876831055
2023-01-07 08:40:38,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,192 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -609.103271484375
2023-01-07 08:40:38,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.835840225219727
2023-01-07 08:40:38,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,194 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -609.266845703125
2023-01-07 08:40:38,194 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.587482452392578
2023-01-07 08:40:38,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,195 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -52.386993408203125
2023-01-07 08:40:38,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.215536117553711
2023-01-07 08:40:38,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,197 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,197 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -507.20892333984375
2023-01-07 08:40:38,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.772146224975586
2023-01-07 08:40:38,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,198 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,198 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 4.067802429199219
2023-01-07 08:40:38,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,199 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -536.3551025390625
2023-01-07 08:40:38,199 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5779781341552734
2023-01-07 08:40:38,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,200 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -569.1776733398438
2023-01-07 08:40:38,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.52043914794922
2023-01-07 08:40:38,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,201 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 3.824490547180176
2023-01-07 08:40:38,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,202 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -57.273048400878906
2023-01-07 08:40:38,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.528175354003906
2023-01-07 08:40:38,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -652.387939453125
2023-01-07 08:40:38,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.70015335083008
2023-01-07 08:40:38,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,206 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,206 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -117.25250244140625
2023-01-07 08:40:38,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,206 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -614.177001953125
2023-01-07 08:40:38,206 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.41860580444336
2023-01-07 08:40:38,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -697.7037963867188
2023-01-07 08:40:38,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.08539581298828
2023-01-07 08:40:38,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,209 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -72.8492431640625
2023-01-07 08:40:38,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.266311645507812
2023-01-07 08:40:38,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,210 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -798.612548828125
2023-01-07 08:40:38,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.71693420410156
2023-01-07 08:40:38,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,211 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,211 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.621606349945068
2023-01-07 08:40:38,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,212 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 8.388468742370605
2023-01-07 08:40:38,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -944.3951416015625
2023-01-07 08:40:38,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,212 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -633.5128173828125
2023-01-07 08:40:38,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.592775344848633
2023-01-07 08:40:38,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,214 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -918.9706420898438
2023-01-07 08:40:38,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.033374786376953
2023-01-07 08:40:38,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,215 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,215 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -98.44834899902344
2023-01-07 08:40:38,215 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.38191223144531
2023-01-07 08:40:38,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,216 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1095.788818359375
2023-01-07 08:40:38,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.952227592468262
2023-01-07 08:40:38,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,218 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -284.70806884765625
2023-01-07 08:40:38,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,218 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -637.4810180664062
2023-01-07 08:40:38,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.93616485595703
2023-01-07 08:40:38,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,219 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,219 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -226.18594360351562
2023-01-07 08:40:38,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.605766296386719
2023-01-07 08:40:38,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,221 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,221 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.0886783599853516
2023-01-07 08:40:38,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,221 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -256.95489501953125
2023-01-07 08:40:38,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.480682373046875
2023-01-07 08:40:38,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,223 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -233.30126953125
2023-01-07 08:40:38,223 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.06271362304688
2023-01-07 08:40:38,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,224 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -233.30126953125
2023-01-07 08:40:38,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.4166488647461
2023-01-07 08:40:38,227 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:40:38,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,228 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,228 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -3965.43701171875
2023-01-07 08:40:38,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,229 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -229.35791015625
2023-01-07 08:40:38,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,230 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,231 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -119.90013122558594
2023-01-07 08:40:38,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,233 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1044.6707763671875
2023-01-07 08:40:38,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,234 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -681.3104248046875
2023-01-07 08:40:38,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,235 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -644.6970825195312
2023-01-07 08:40:38,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,235 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -41.19046401977539
2023-01-07 08:40:38,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,236 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 2.8206253051757812
2023-01-07 08:40:38,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,236 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 113.17436218261719
2023-01-07 08:40:38,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,237 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 113.70063018798828
2023-01-07 08:40:38,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,237 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.82568359375
2023-01-07 08:40:38,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,238 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.80314636230469
2023-01-07 08:40:38,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,238 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -63.61921691894531
2023-01-07 08:40:38,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,239 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -41.02004623413086
2023-01-07 08:40:38,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,239 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 34.03031921386719
2023-01-07 08:40:38,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 45.170772552490234
2023-01-07 08:40:38,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 100.25556945800781
2023-01-07 08:40:38,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.588686943054199
2023-01-07 08:40:38,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 68.67298889160156
2023-01-07 08:40:38,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 46.20701599121094
2023-01-07 08:40:38,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 14.013908386230469
2023-01-07 08:40:38,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 13.954545974731445
2023-01-07 08:40:38,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -12.954334259033203
2023-01-07 08:40:38,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 6.644968032836914
2023-01-07 08:40:38,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -27.848419189453125
2023-01-07 08:40:38,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -4015.09228515625
2023-01-07 08:40:38,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.652904510498047
2023-01-07 08:40:38,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -12.179479598999023
2023-01-07 08:40:38,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -44.712730407714844
2023-01-07 08:40:38,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 6.42746114730835
2023-01-07 08:40:38,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.93901252746582
2023-01-07 08:40:38,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.740821361541748
2023-01-07 08:40:38,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.845700263977051
2023-01-07 08:40:38,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,249 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 5.3247575759887695
2023-01-07 08:40:38,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,249 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 99.39401245117188
2023-01-07 08:40:38,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.89537811279297
2023-01-07 08:40:38,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,250 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 18.535564422607422
2023-01-07 08:40:38,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,251 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.745773315429688
2023-01-07 08:40:38,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,251 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -8.18985366821289
2023-01-07 08:40:38,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,252 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.424870491027832
2023-01-07 08:40:38,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.540937423706055
2023-01-07 08:40:38,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.843985557556152
2023-01-07 08:40:38,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.751387596130371
2023-01-07 08:40:38,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.430682182312012
2023-01-07 08:40:38,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.253440856933594
2023-01-07 08:40:38,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,254 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.152726173400879
2023-01-07 08:40:38,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,255 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -205.94781494140625
2023-01-07 08:40:38,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 715.9999389648438
2023-01-07 08:40:38,256 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,256 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -3965.43701171875
2023-01-07 08:40:38,256 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,256 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,256 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 4.472931861877441
2023-01-07 08:40:38,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,257 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -229.35791015625
2023-01-07 08:40:38,257 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.00306701660156
2023-01-07 08:40:38,258 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,258 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 4.472931861877441
2023-01-07 08:40:38,258 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.8936614990234375
2023-01-07 08:40:38,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,259 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,259 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 102.0841064453125
2023-01-07 08:40:38,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,259 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -229.35791015625
2023-01-07 08:40:38,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 86.93294525146484
2023-01-07 08:40:38,261 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,261 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 102.0841064453125
2023-01-07 08:40:38,261 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:38,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,261 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -229.35791015625
2023-01-07 08:40:38,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -128.971923828125
2023-01-07 08:40:38,262 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,263 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -20.837230682373047
2023-01-07 08:40:38,263 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:38,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,263 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -229.35791015625
2023-01-07 08:40:38,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.092750549316406
2023-01-07 08:40:38,264 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,265 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -229.35791015625
2023-01-07 08:40:38,265 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,265 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.2915276288986206
2023-01-07 08:40:38,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,265 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -19.529205322265625
2023-01-07 08:40:38,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,266 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.96668243408203
2023-01-07 08:40:38,267 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,267 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.2915276288986206
2023-01-07 08:40:38,267 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,268 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -105.0756607055664
2023-01-07 08:40:38,269 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,269 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -19.529205322265625
2023-01-07 08:40:38,269 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,270 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,270 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.8415381908416748
2023-01-07 08:40:38,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,270 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 14.343770980834961
2023-01-07 08:40:38,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.970184326171875
2023-01-07 08:40:38,271 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,271 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -0.8415381908416748
2023-01-07 08:40:38,272 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,272 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -193.11544799804688
2023-01-07 08:40:38,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.69083023071289
2023-01-07 08:40:38,273 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,273 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -329.2358703613281
2023-01-07 08:40:38,273 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,274 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -193.11544799804688
2023-01-07 08:40:38,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 191.49354553222656
2023-01-07 08:40:38,275 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,275 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -97.03848266601562
2023-01-07 08:40:38,275 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,276 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -193.11544799804688
2023-01-07 08:40:38,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -108.80760955810547
2023-01-07 08:40:38,277 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,277 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 14.343770980834961
2023-01-07 08:40:38,277 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,277 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,278 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.7006652355194092
2023-01-07 08:40:38,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,278 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -193.11544799804688
2023-01-07 08:40:38,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.05548095703125
2023-01-07 08:40:38,279 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,279 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.7006652355194092
2023-01-07 08:40:38,279 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,280 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -193.11544799804688
2023-01-07 08:40:38,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8907203674316406
2023-01-07 08:40:38,282 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,282 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -193.11544799804688
2023-01-07 08:40:38,282 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,282 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,282 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,282 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,282 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.5896801948547363
2023-01-07 08:40:38,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,283 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -248.1814727783203
2023-01-07 08:40:38,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.23390197753906
2023-01-07 08:40:38,284 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,284 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.5896801948547363
2023-01-07 08:40:38,284 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,284 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,285 > [DEBUG] 0 :: before allreduce fusion buffer :: -99.86604309082031
2023-01-07 08:40:38,286 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,286 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -248.1814727783203
2023-01-07 08:40:38,286 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:38,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,286 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.366058349609375
2023-01-07 08:40:38,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.433385848999023
2023-01-07 08:40:38,287 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,288 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -19.659067153930664
2023-01-07 08:40:38,288 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:38,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,288 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.366058349609375
2023-01-07 08:40:38,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,289 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -71.59466552734375
2023-01-07 08:40:38,290 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,290 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -39.366058349609375
2023-01-07 08:40:38,290 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,291 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.769493103027344
2023-01-07 08:40:38,292 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,292 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -10.499603271484375
2023-01-07 08:40:38,292 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,292 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1114.41650390625
2023-01-07 08:40:38,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5220870971679688
2023-01-07 08:40:38,294 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,294 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -1114.41650390625
2023-01-07 08:40:38,294 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,294 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,294 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1.1355230808258057
2023-01-07 08:40:38,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,294 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,295 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -1.0364513397216797
2023-01-07 08:40:38,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -122.61583709716797
2023-01-07 08:40:38,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -86.57843017578125
2023-01-07 08:40:38,296 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,296 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.1355230808258057
2023-01-07 08:40:38,296 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.86844825744629
2023-01-07 08:40:38,298 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,298 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -1.0364513397216797
2023-01-07 08:40:38,298 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -9.872394561767578
2023-01-07 08:40:38,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -122.61583709716797
2023-01-07 08:40:38,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.03438949584961
2023-01-07 08:40:38,300 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,300 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -13.202587127685547
2023-01-07 08:40:38,300 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.73264503479004
2023-01-07 08:40:38,302 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,302 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -122.61583709716797
2023-01-07 08:40:38,302 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 74.87324523925781
2023-01-07 08:40:38,304 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,304 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -14.068607330322266
2023-01-07 08:40:38,304 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -635.10009765625
2023-01-07 08:40:38,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.24428653717041
2023-01-07 08:40:38,306 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,306 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -635.10009765625
2023-01-07 08:40:38,306 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,306 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.7973078489303589
2023-01-07 08:40:38,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -119.90013122558594
2023-01-07 08:40:38,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.288591384887695
2023-01-07 08:40:38,308 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,308 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.7973078489303589
2023-01-07 08:40:38,308 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 4.868085861206055
2023-01-07 08:40:38,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -119.90013122558594
2023-01-07 08:40:38,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0341553688049316
2023-01-07 08:40:38,310 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,310 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -119.90013122558594
2023-01-07 08:40:38,310 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,310 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,310 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,311 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1.7780985832214355
2023-01-07 08:40:38,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1044.6707763671875
2023-01-07 08:40:38,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.29526138305664
2023-01-07 08:40:38,312 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,312 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -1.7780985832214355
2023-01-07 08:40:38,312 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -12.447158813476562
2023-01-07 08:40:38,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1044.6707763671875
2023-01-07 08:40:38,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.728470802307129
2023-01-07 08:40:38,314 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,314 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1044.6707763671875
2023-01-07 08:40:38,314 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:38,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -681.3104248046875
2023-01-07 08:40:38,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.037593841552734
2023-01-07 08:40:38,316 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,316 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -21.842947006225586
2023-01-07 08:40:38,316 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:38,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -681.3104248046875
2023-01-07 08:40:38,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.283362627029419
2023-01-07 08:40:38,318 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,318 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -681.3104248046875
2023-01-07 08:40:38,318 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,318 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.3934175372123718
2023-01-07 08:40:38,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -644.6970825195312
2023-01-07 08:40:38,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07356917858123779
2023-01-07 08:40:38,320 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,320 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.3934175372123718
2023-01-07 08:40:38,320 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -32.070369720458984
2023-01-07 08:40:38,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -644.6970825195312
2023-01-07 08:40:38,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.327558517456055
2023-01-07 08:40:38,323 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,323 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -644.6970825195312
2023-01-07 08:40:38,323 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,323 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.7328392863273621
2023-01-07 08:40:38,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -41.19046401977539
2023-01-07 08:40:38,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.344566345214844
2023-01-07 08:40:38,325 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,325 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.7328392863273621
2023-01-07 08:40:38,325 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,325 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,325 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.4443514347076416
2023-01-07 08:40:38,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -41.19046401977539
2023-01-07 08:40:38,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.850086212158203
2023-01-07 08:40:38,327 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,327 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -41.19046401977539
2023-01-07 08:40:38,327 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:38,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 2.8206253051757812
2023-01-07 08:40:38,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.365081787109375
2023-01-07 08:40:38,329 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,329 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 5.366054534912109
2023-01-07 08:40:38,329 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:38,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,329 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 2.8206253051757812
2023-01-07 08:40:38,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.2267427444458
2023-01-07 08:40:38,331 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,331 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 2.8206253051757812
2023-01-07 08:40:38,331 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,332 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,332 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.8672671318054199
2023-01-07 08:40:38,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,332 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 113.17436218261719
2023-01-07 08:40:38,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.26184844970703
2023-01-07 08:40:38,333 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,333 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.8672671318054199
2023-01-07 08:40:38,333 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,334 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,334 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.05154252052307129
2023-01-07 08:40:38,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 113.17436218261719
2023-01-07 08:40:38,334 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4226045608520508
2023-01-07 08:40:38,335 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,335 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 113.17436218261719
2023-01-07 08:40:38,335 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,336 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.04091441631317139
2023-01-07 08:40:38,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 113.70063018798828
2023-01-07 08:40:38,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.146205425262451
2023-01-07 08:40:38,337 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,338 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.04091441631317139
2023-01-07 08:40:38,338 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.814935684204102
2023-01-07 08:40:38,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,338 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 113.70063018798828
2023-01-07 08:40:38,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2325453758239746
2023-01-07 08:40:38,339 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,340 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 113.70063018798828
2023-01-07 08:40:38,340 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,340 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.4085853397846222
2023-01-07 08:40:38,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.82568359375
2023-01-07 08:40:38,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3505445718765259
2023-01-07 08:40:38,342 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,342 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.4085853397846222
2023-01-07 08:40:38,342 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 64.07452392578125
2023-01-07 08:40:38,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,342 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.82568359375
2023-01-07 08:40:38,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7585105895996094
2023-01-07 08:40:38,344 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,344 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -20.82568359375
2023-01-07 08:40:38,344 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,344 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,344 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.7977210283279419
2023-01-07 08:40:38,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,345 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.80314636230469
2023-01-07 08:40:38,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.021392822265625
2023-01-07 08:40:38,346 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,346 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.7977210283279419
2023-01-07 08:40:38,346 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.62215805053711
2023-01-07 08:40:38,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,347 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.80314636230469
2023-01-07 08:40:38,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.02227783203125
2023-01-07 08:40:38,348 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,348 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -107.80314636230469
2023-01-07 08:40:38,348 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,348 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,348 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,349 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.05460961163043976
2023-01-07 08:40:38,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,349 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -63.61921691894531
2023-01-07 08:40:38,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7331463098526001
2023-01-07 08:40:38,350 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,350 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.05460961163043976
2023-01-07 08:40:38,350 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 7.18283748626709
2023-01-07 08:40:38,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,351 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -63.61921691894531
2023-01-07 08:40:38,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9523533582687378
2023-01-07 08:40:38,352 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,352 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -63.61921691894531
2023-01-07 08:40:38,352 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,353 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,353 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.5063228011131287
2023-01-07 08:40:38,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -41.02004623413086
2023-01-07 08:40:38,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06142020225524902
2023-01-07 08:40:38,354 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,354 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.5063228011131287
2023-01-07 08:40:38,354 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.088607788085938
2023-01-07 08:40:38,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -41.02004623413086
2023-01-07 08:40:38,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2445758581161499
2023-01-07 08:40:38,356 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,357 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -41.02004623413086
2023-01-07 08:40:38,357 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,357 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.598927915096283
2023-01-07 08:40:38,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,358 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 34.03031921386719
2023-01-07 08:40:38,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.729942321777344
2023-01-07 08:40:38,359 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,359 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.598927915096283
2023-01-07 08:40:38,359 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -19.012502670288086
2023-01-07 08:40:38,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 34.03031921386719
2023-01-07 08:40:38,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.857282638549805
2023-01-07 08:40:38,361 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,361 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 34.03031921386719
2023-01-07 08:40:38,361 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:38,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 45.170772552490234
2023-01-07 08:40:38,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3602589964866638
2023-01-07 08:40:38,363 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,363 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 1.3747676610946655
2023-01-07 08:40:38,363 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:38,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 45.170772552490234
2023-01-07 08:40:38,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.542640209197998
2023-01-07 08:40:38,365 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,365 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 45.170772552490234
2023-01-07 08:40:38,365 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:38,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 100.25556945800781
2023-01-07 08:40:38,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.383876323699951
2023-01-07 08:40:38,366 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,367 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 2.2705395221710205
2023-01-07 08:40:38,367 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:38,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 100.25556945800781
2023-01-07 08:40:38,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.042072296142578
2023-01-07 08:40:38,368 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 100.25556945800781
2023-01-07 08:40:38,369 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:38,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.588686943054199
2023-01-07 08:40:38,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8935248851776123
2023-01-07 08:40:38,370 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,370 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.32195234298706055
2023-01-07 08:40:38,371 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:38,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,371 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.588686943054199
2023-01-07 08:40:38,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.157401084899902
2023-01-07 08:40:38,372 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,373 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -5.588686943054199
2023-01-07 08:40:38,373 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,373 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:40:38,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.12707941234111786
2023-01-07 08:40:38,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 68.67298889160156
2023-01-07 08:40:38,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.757977485656738
2023-01-07 08:40:38,375 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,375 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.12707941234111786
2023-01-07 08:40:38,375 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 18.518516540527344
2023-01-07 08:40:38,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 68.67298889160156
2023-01-07 08:40:38,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8076283931732178
2023-01-07 08:40:38,377 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,377 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 68.67298889160156
2023-01-07 08:40:38,377 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:38,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 46.20701599121094
2023-01-07 08:40:38,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1402206420898438
2023-01-07 08:40:38,378 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,379 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -0.8861538171768188
2023-01-07 08:40:38,379 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:38,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 46.20701599121094
2023-01-07 08:40:38,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3171536326408386
2023-01-07 08:40:38,380 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,381 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 46.20701599121094
2023-01-07 08:40:38,381 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:38,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 14.013908386230469
2023-01-07 08:40:38,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41626304388046265
2023-01-07 08:40:38,382 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,382 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 1.6822195053100586
2023-01-07 08:40:38,383 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:38,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 14.013908386230469
2023-01-07 08:40:38,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4945123791694641
2023-01-07 08:40:38,384 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,385 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 14.013908386230469
2023-01-07 08:40:38,385 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:38,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 13.954545974731445
2023-01-07 08:40:38,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.110069990158081
2023-01-07 08:40:38,386 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,386 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -1.5936403274536133
2023-01-07 08:40:38,386 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:38,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 13.954545974731445
2023-01-07 08:40:38,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9959508180618286
2023-01-07 08:40:38,388 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,389 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 13.954545974731445
2023-01-07 08:40:38,389 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:38,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -12.954334259033203
2023-01-07 08:40:38,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3090002536773682
2023-01-07 08:40:38,390 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,390 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -1.808672308921814
2023-01-07 08:40:38,390 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:38,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -12.954334259033203
2023-01-07 08:40:38,391 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5615167617797852
2023-01-07 08:40:38,392 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,392 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -12.954334259033203
2023-01-07 08:40:38,392 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:38,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 6.644968032836914
2023-01-07 08:40:38,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5142504572868347
2023-01-07 08:40:38,394 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,394 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -5.292990684509277
2023-01-07 08:40:38,394 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:38,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 6.644968032836914
2023-01-07 08:40:38,395 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1075310707092285
2023-01-07 08:40:38,396 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,396 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 6.644968032836914
2023-01-07 08:40:38,396 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:38,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -27.848419189453125
2023-01-07 08:40:38,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6177597045898438
2023-01-07 08:40:38,398 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,398 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -2.0861406326293945
2023-01-07 08:40:38,398 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:38,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,399 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -27.848419189453125
2023-01-07 08:40:38,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.393948554992676
2023-01-07 08:40:38,400 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,400 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -27.848419189453125
2023-01-07 08:40:38,400 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:38,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,401 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.652904510498047
2023-01-07 08:40:38,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4186744689941406
2023-01-07 08:40:38,402 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,402 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -1.5094523429870605
2023-01-07 08:40:38,402 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:38,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.652904510498047
2023-01-07 08:40:38,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6578257083892822
2023-01-07 08:40:38,404 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,404 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -8.652904510498047
2023-01-07 08:40:38,404 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:38,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,404 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -12.179479598999023
2023-01-07 08:40:38,405 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2961132526397705
2023-01-07 08:40:38,406 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,406 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.6917935609817505
2023-01-07 08:40:38,406 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:38,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,406 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -12.179479598999023
2023-01-07 08:40:38,407 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07679493725299835
2023-01-07 08:40:38,408 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,408 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -12.179479598999023
2023-01-07 08:40:38,408 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:38,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,408 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -44.712730407714844
2023-01-07 08:40:38,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9810937643051147
2023-01-07 08:40:38,409 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,410 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -1.0565203428268433
2023-01-07 08:40:38,410 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:38,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -44.712730407714844
2023-01-07 08:40:38,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9959930181503296
2023-01-07 08:40:38,411 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,412 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -44.712730407714844
2023-01-07 08:40:38,412 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:38,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,412 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 6.42746114730835
2023-01-07 08:40:38,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04094293713569641
2023-01-07 08:40:38,413 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,413 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.38545387983322144
2023-01-07 08:40:38,414 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,414 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:38,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,414 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 6.42746114730835
2023-01-07 08:40:38,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5012081861495972
2023-01-07 08:40:38,415 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,416 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 6.42746114730835
2023-01-07 08:40:38,416 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:38,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,416 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.93901252746582
2023-01-07 08:40:38,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5030465126037598
2023-01-07 08:40:38,417 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,417 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.13203337788581848
2023-01-07 08:40:38,417 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:38,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.93901252746582
2023-01-07 08:40:38,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3228391110897064
2023-01-07 08:40:38,420 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,420 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -11.93901252746582
2023-01-07 08:40:38,420 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:38,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.740821361541748
2023-01-07 08:40:38,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7959297299385071
2023-01-07 08:40:38,421 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,421 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.2479318231344223
2023-01-07 08:40:38,422 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:38,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,422 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.740821361541748
2023-01-07 08:40:38,422 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1127517223358154
2023-01-07 08:40:38,423 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,423 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 2.740821361541748
2023-01-07 08:40:38,424 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:38,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,424 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.845700263977051
2023-01-07 08:40:38,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7146754264831543
2023-01-07 08:40:38,425 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,425 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.08408822119235992
2023-01-07 08:40:38,425 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:38,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,426 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.845700263977051
2023-01-07 08:40:38,426 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0607004165649414
2023-01-07 08:40:38,427 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,427 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 3.845700263977051
2023-01-07 08:40:38,427 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:38,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,428 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 5.3247575759887695
2023-01-07 08:40:38,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4181697964668274
2023-01-07 08:40:38,429 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,429 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.0657123327255249
2023-01-07 08:40:38,429 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:38,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,430 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 5.3247575759887695
2023-01-07 08:40:38,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27829599380493164
2023-01-07 08:40:38,431 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,431 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 5.3247575759887695
2023-01-07 08:40:38,431 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:38,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 99.39401245117188
2023-01-07 08:40:38,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7385177612304688
2023-01-07 08:40:38,433 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,433 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.4767366051673889
2023-01-07 08:40:38,433 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:38,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,434 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 99.39401245117188
2023-01-07 08:40:38,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5347075462341309
2023-01-07 08:40:38,435 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,435 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 99.39401245117188
2023-01-07 08:40:38,435 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:38,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,436 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 18.535564422607422
2023-01-07 08:40:38,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4681344032287598
2023-01-07 08:40:38,437 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,437 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.2992916703224182
2023-01-07 08:40:38,437 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:38,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,438 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 18.535564422607422
2023-01-07 08:40:38,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1894124746322632
2023-01-07 08:40:38,439 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,439 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 18.535564422607422
2023-01-07 08:40:38,439 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,439 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:38,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.745773315429688
2023-01-07 08:40:38,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.732940673828125
2023-01-07 08:40:38,441 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,441 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 1.7656886577606201
2023-01-07 08:40:38,441 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:38,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,441 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.745773315429688
2023-01-07 08:40:38,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10801532864570618
2023-01-07 08:40:38,442 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,442 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 17.745773315429688
2023-01-07 08:40:38,443 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:38,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -8.18985366821289
2023-01-07 08:40:38,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30660194158554077
2023-01-07 08:40:38,444 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,444 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 2.093355655670166
2023-01-07 08:40:38,444 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:38,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -8.18985366821289
2023-01-07 08:40:38,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19160345196723938
2023-01-07 08:40:38,446 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,446 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -8.18985366821289
2023-01-07 08:40:38,446 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,446 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:38,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,447 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.424870491027832
2023-01-07 08:40:38,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17709732055664062
2023-01-07 08:40:38,448 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,448 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.48440253734588623
2023-01-07 08:40:38,448 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,448 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,448 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:38,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3.424870491027832
2023-01-07 08:40:38,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0875551700592041
2023-01-07 08:40:38,450 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,450 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 3.424870491027832
2023-01-07 08:40:38,450 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,450 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:38,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.843985557556152
2023-01-07 08:40:38,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14520731568336487
2023-01-07 08:40:38,452 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,452 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.2556424140930176
2023-01-07 08:40:38,452 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:38,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,453 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 13.843985557556152
2023-01-07 08:40:38,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2786693871021271
2023-01-07 08:40:38,454 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,454 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 13.843985557556152
2023-01-07 08:40:38,454 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,454 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:38,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,455 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.751387596130371
2023-01-07 08:40:38,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2274114042520523
2023-01-07 08:40:38,456 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,456 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 1.534598469734192
2023-01-07 08:40:38,456 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:38,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.751387596130371
2023-01-07 08:40:38,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12932147085666656
2023-01-07 08:40:38,458 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,458 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 3.751387596130371
2023-01-07 08:40:38,458 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:38,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.430682182312012
2023-01-07 08:40:38,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17220498621463776
2023-01-07 08:40:38,459 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,460 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.2505756616592407
2023-01-07 08:40:38,460 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:38,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,460 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.430682182312012
2023-01-07 08:40:38,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01654631644487381
2023-01-07 08:40:38,461 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,462 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -4.430682182312012
2023-01-07 08:40:38,462 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:38,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,462 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.152726173400879
2023-01-07 08:40:38,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1445237100124359
2023-01-07 08:40:38,463 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,463 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.03626403957605362
2023-01-07 08:40:38,464 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:38,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,464 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.152726173400879
2023-01-07 08:40:38,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15308484435081482
2023-01-07 08:40:38,465 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,465 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 5.152726173400879
2023-01-07 08:40:38,466 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,466 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,466 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:38,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,466 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -205.94781494140625
2023-01-07 08:40:38,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2222909927368164
2023-01-07 08:40:38,467 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,467 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -323.046875
2023-01-07 08:40:38,468 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,468 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:38,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,468 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -205.94781494140625
2023-01-07 08:40:38,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7412282228469849
2023-01-07 08:40:38,470 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,470 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -205.94781494140625
2023-01-07 08:40:38,470 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,471 > [DEBUG] 0 :: 7.287662506103516
2023-01-07 08:40:38,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,475 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00909423828125
2023-01-07 08:40:38,475 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -276.8467712402344
2023-01-07 08:40:38,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,478 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.32441046833992004
2023-01-07 08:40:38,478 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,478 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00909423828125
2023-01-07 08:40:38,479 > [DEBUG] 0 :: before allreduce fusion buffer :: -305.67535400390625
2023-01-07 08:40:38,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,481 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.0021306276321411133
2023-01-07 08:40:38,482 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12122897803783417
2023-01-07 08:40:38,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.02957703173160553
2023-01-07 08:40:38,485 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.0021306276321411133
2023-01-07 08:40:38,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5553245544433594
2023-01-07 08:40:38,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,488 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.5584447383880615
2023-01-07 08:40:38,488 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.023748379200696945
2023-01-07 08:40:38,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,489 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.04705367609858513
2023-01-07 08:40:38,489 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,489 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.5584447383880615
2023-01-07 08:40:38,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3742312788963318
2023-01-07 08:40:38,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,491 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.490731716156006
2023-01-07 08:40:38,491 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14680969715118408
2023-01-07 08:40:38,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,492 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.3168901205062866
2023-01-07 08:40:38,492 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,493 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.490731716156006
2023-01-07 08:40:38,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5793853998184204
2023-01-07 08:40:38,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,494 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.630359649658203
2023-01-07 08:40:38,494 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,494 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20408356189727783
2023-01-07 08:40:38,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,495 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.031125038862228394
2023-01-07 08:40:38,495 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,496 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.630359649658203
2023-01-07 08:40:38,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0971492528915405
2023-01-07 08:40:38,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,497 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.2426910400390625
2023-01-07 08:40:38,497 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4788011312484741
2023-01-07 08:40:38,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,498 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.03403530269861221
2023-01-07 08:40:38,499 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,499 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.2426910400390625
2023-01-07 08:40:38,499 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6607984304428101
2023-01-07 08:40:38,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,500 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -11.682577133178711
2023-01-07 08:40:38,500 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6851823925971985
2023-01-07 08:40:38,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,502 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.038371019065380096
2023-01-07 08:40:38,502 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,502 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -11.682577133178711
2023-01-07 08:40:38,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2013038992881775
2023-01-07 08:40:38,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,503 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.454903602600098
2023-01-07 08:40:38,504 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08089663088321686
2023-01-07 08:40:38,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,505 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.10881946980953217
2023-01-07 08:40:38,505 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,505 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.454903602600098
2023-01-07 08:40:38,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6926891803741455
2023-01-07 08:40:38,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,507 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.863737106323242
2023-01-07 08:40:38,507 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,507 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.058854639530181885
2023-01-07 08:40:38,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.0265854150056839
2023-01-07 08:40:38,508 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.863737106323242
2023-01-07 08:40:38,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9718805551528931
2023-01-07 08:40:38,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,510 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 46.23332977294922
2023-01-07 08:40:38,510 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31429606676101685
2023-01-07 08:40:38,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.07471080869436264
2023-01-07 08:40:38,511 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 46.23332977294922
2023-01-07 08:40:38,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6021944284439087
2023-01-07 08:40:38,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,513 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -3.0395283699035645
2023-01-07 08:40:38,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5337131023406982
2023-01-07 08:40:38,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.057015229016542435
2023-01-07 08:40:38,514 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,514 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -3.0395283699035645
2023-01-07 08:40:38,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.317680299282074
2023-01-07 08:40:38,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,516 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 10.110532760620117
2023-01-07 08:40:38,516 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13979701697826385
2023-01-07 08:40:38,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.08389368653297424
2023-01-07 08:40:38,518 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 10.110532760620117
2023-01-07 08:40:38,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3058675527572632
2023-01-07 08:40:38,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -58.11057662963867
2023-01-07 08:40:38,520 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3022358417510986
2023-01-07 08:40:38,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.026187874376773834
2023-01-07 08:40:38,521 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -58.11057662963867
2023-01-07 08:40:38,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7967908382415771
2023-01-07 08:40:38,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.07536506652832
2023-01-07 08:40:38,523 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5152537822723389
2023-01-07 08:40:38,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.32756873965263367
2023-01-07 08:40:38,524 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.07536506652832
2023-01-07 08:40:38,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4241899251937866
2023-01-07 08:40:38,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,526 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 9.46795654296875
2023-01-07 08:40:38,526 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8278899192810059
2023-01-07 08:40:38,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.05620758980512619
2023-01-07 08:40:38,527 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 9.46795654296875
2023-01-07 08:40:38,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19255758821964264
2023-01-07 08:40:38,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,529 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 29.543895721435547
2023-01-07 08:40:38,529 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5747281312942505
2023-01-07 08:40:38,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.04846826195716858
2023-01-07 08:40:38,530 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 29.543895721435547
2023-01-07 08:40:38,531 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2881171703338623
2023-01-07 08:40:38,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,532 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.397003173828125
2023-01-07 08:40:38,532 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6322567462921143
2023-01-07 08:40:38,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,533 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.16664639115333557
2023-01-07 08:40:38,533 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,534 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.397003173828125
2023-01-07 08:40:38,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09587034583091736
2023-01-07 08:40:38,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,535 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.242734432220459
2023-01-07 08:40:38,535 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9478335380554199
2023-01-07 08:40:38,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,536 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.08232441544532776
2023-01-07 08:40:38,536 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,537 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 6.242734432220459
2023-01-07 08:40:38,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4242507219314575
2023-01-07 08:40:38,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,538 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -16.61918067932129
2023-01-07 08:40:38,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02409067004919052
2023-01-07 08:40:38,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,539 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.1402653455734253
2023-01-07 08:40:38,539 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,540 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -16.61918067932129
2023-01-07 08:40:38,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24856826663017273
2023-01-07 08:40:38,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,541 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.359907627105713
2023-01-07 08:40:38,541 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11262286454439163
2023-01-07 08:40:38,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.023343034088611603
2023-01-07 08:40:38,543 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,543 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.359907627105713
2023-01-07 08:40:38,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34315672516822815
2023-01-07 08:40:38,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.965061664581299
2023-01-07 08:40:38,544 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,545 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2150115966796875
2023-01-07 08:40:38,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,546 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.24387279152870178
2023-01-07 08:40:38,546 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,546 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.965061664581299
2023-01-07 08:40:38,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9129775762557983
2023-01-07 08:40:38,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -6.633338928222656
2023-01-07 08:40:38,547 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9563214778900146
2023-01-07 08:40:38,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.18173950910568237
2023-01-07 08:40:38,549 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -6.633338928222656
2023-01-07 08:40:38,549 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.991083145141602
2023-01-07 08:40:38,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.363061904907227
2023-01-07 08:40:38,551 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.149670124053955
2023-01-07 08:40:38,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.04130050539970398
2023-01-07 08:40:38,552 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.363061904907227
2023-01-07 08:40:38,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7589595317840576
2023-01-07 08:40:38,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,554 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 62.682586669921875
2023-01-07 08:40:38,554 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1022191047668457
2023-01-07 08:40:38,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.3545432686805725
2023-01-07 08:40:38,555 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 62.682586669921875
2023-01-07 08:40:38,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3750786781311035
2023-01-07 08:40:38,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,557 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 257.439697265625
2023-01-07 08:40:38,557 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4884701371192932
2023-01-07 08:40:38,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 257.439697265625
2023-01-07 08:40:38,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.272528648376465
2023-01-07 08:40:38,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,561 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 131.24349975585938
2023-01-07 08:40:38,561 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.060030460357666
2023-01-07 08:40:38,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.5465515851974487
2023-01-07 08:40:38,562 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 131.24349975585938
2023-01-07 08:40:38,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.011199474334717
2023-01-07 08:40:38,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,564 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 205.99008178710938
2023-01-07 08:40:38,564 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.99727725982666
2023-01-07 08:40:38,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.39521050453186035
2023-01-07 08:40:38,565 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 205.99008178710938
2023-01-07 08:40:38,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3521963953971863
2023-01-07 08:40:38,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 71.28477478027344
2023-01-07 08:40:38,567 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.266536712646484
2023-01-07 08:40:38,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.531899094581604
2023-01-07 08:40:38,568 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 71.28477478027344
2023-01-07 08:40:38,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.632871627807617
2023-01-07 08:40:38,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -92.34257507324219
2023-01-07 08:40:38,570 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.623016357421875
2023-01-07 08:40:38,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -92.34257507324219
2023-01-07 08:40:38,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5288424491882324
2023-01-07 08:40:38,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.69947814941406
2023-01-07 08:40:38,573 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.649988174438477
2023-01-07 08:40:38,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.69947814941406
2023-01-07 08:40:38,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.642263412475586
2023-01-07 08:40:38,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,575 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 15.570169448852539
2023-01-07 08:40:38,576 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0838189125061035
2023-01-07 08:40:38,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,577 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 15.570169448852539
2023-01-07 08:40:38,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01439809799194336
2023-01-07 08:40:38,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,578 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -36.22834396362305
2023-01-07 08:40:38,578 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.584187984466553
2023-01-07 08:40:38,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,579 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -36.22834396362305
2023-01-07 08:40:38,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.502098083496094
2023-01-07 08:40:38,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,581 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.019287109375
2023-01-07 08:40:38,581 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.645897388458252
2023-01-07 08:40:38,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,582 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.019287109375
2023-01-07 08:40:38,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.219561576843262
2023-01-07 08:40:38,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 20.783201217651367
2023-01-07 08:40:38,584 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.596078634262085
2023-01-07 08:40:38,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 20.783201217651367
2023-01-07 08:40:38,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.66518235206604
2023-01-07 08:40:38,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,586 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 296.4792175292969
2023-01-07 08:40:38,586 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.294731140136719
2023-01-07 08:40:38,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,587 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 296.4792175292969
2023-01-07 08:40:38,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.58213424682617
2023-01-07 08:40:38,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,589 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -4.244293212890625
2023-01-07 08:40:38,589 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.672934055328369
2023-01-07 08:40:38,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,590 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.08034026622772217
2023-01-07 08:40:38,590 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,591 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -4.244293212890625
2023-01-07 08:40:38,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.527751922607422
2023-01-07 08:40:38,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,592 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -2.9054007530212402
2023-01-07 08:40:38,592 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.599113941192627
2023-01-07 08:40:38,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,594 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -2.9054007530212402
2023-01-07 08:40:38,594 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.833312034606934
2023-01-07 08:40:38,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -133.86761474609375
2023-01-07 08:40:38,595 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.21154499053955
2023-01-07 08:40:38,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,596 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -133.86761474609375
2023-01-07 08:40:38,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.280867099761963
2023-01-07 08:40:38,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -6.570925712585449
2023-01-07 08:40:38,598 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.689051628112793
2023-01-07 08:40:38,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,599 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.336933970451355
2023-01-07 08:40:38,599 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,599 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -6.570925712585449
2023-01-07 08:40:38,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.098896026611328
2023-01-07 08:40:38,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,601 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 84.85845947265625
2023-01-07 08:40:38,601 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.5051469802856445
2023-01-07 08:40:38,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,602 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 84.85845947265625
2023-01-07 08:40:38,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0398049354553223
2023-01-07 08:40:38,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,603 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 29.054454803466797
2023-01-07 08:40:38,604 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.890431880950928
2023-01-07 08:40:38,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,605 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 29.054454803466797
2023-01-07 08:40:38,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.799367904663086
2023-01-07 08:40:38,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,606 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,606 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.43183708190918
2023-01-07 08:40:38,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.6084072589874268
2023-01-07 08:40:38,608 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.396621704101562
2023-01-07 08:40:38,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,609 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.479084014892578
2023-01-07 08:40:38,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,611 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.4613085985183716
2023-01-07 08:40:38,611 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 73.85194396972656
2023-01-07 08:40:38,611 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.815765380859375
2023-01-07 08:40:38,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,613 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.27177619934082
2023-01-07 08:40:38,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,614 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 73.85194396972656
2023-01-07 08:40:38,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.37277603149414
2023-01-07 08:40:38,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,616 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,616 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.68177795410156
2023-01-07 08:40:38,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,617 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.925743818283081
2023-01-07 08:40:38,617 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,617 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.67427635192871
2023-01-07 08:40:38,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.84953308105469
2023-01-07 08:40:38,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,620 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -3.5101006031036377
2023-01-07 08:40:38,620 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,620 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.563148498535156
2023-01-07 08:40:38,620 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.65827178955078
2023-01-07 08:40:38,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,622 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.873825073242188
2023-01-07 08:40:38,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,623 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 1.8958148956298828
2023-01-07 08:40:38,623 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.71177673339844
2023-01-07 08:40:38,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,625 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.962545871734619
2023-01-07 08:40:38,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,626 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,626 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.08209991455078
2023-01-07 08:40:38,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,628 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.446677207946777
2023-01-07 08:40:38,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 5.537753582000732
2023-01-07 08:40:38,630 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,630 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 113.82720947265625
2023-01-07 08:40:38,630 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,630 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,630 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.4246826171875
2023-01-07 08:40:38,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,632 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.88663101196289
2023-01-07 08:40:38,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,633 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 151.8192138671875
2023-01-07 08:40:38,633 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.284175872802734
2023-01-07 08:40:38,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,635 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.239294052124023
2023-01-07 08:40:38,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,636 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 151.8192138671875
2023-01-07 08:40:38,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 91.43553161621094
2023-01-07 08:40:38,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,638 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,638 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.401874542236328
2023-01-07 08:40:38,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,639 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -5.655794143676758
2023-01-07 08:40:38,639 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,639 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -134.61685180664062
2023-01-07 08:40:38,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,641 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -87.81773376464844
2023-01-07 08:40:38,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,642 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.21402359008789
2023-01-07 08:40:38,647 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:40:38,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,648 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 1531.56103515625
2023-01-07 08:40:38,648 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,650 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,651 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 29.054454803466797
2023-01-07 08:40:38,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,651 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 84.85845947265625
2023-01-07 08:40:38,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,652 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -6.570925712585449
2023-01-07 08:40:38,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,653 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -133.86761474609375
2023-01-07 08:40:38,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -2.9054007530212402
2023-01-07 08:40:38,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -4.244293212890625
2023-01-07 08:40:38,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 296.4792175292969
2023-01-07 08:40:38,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,656 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 20.783201217651367
2023-01-07 08:40:38,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,656 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.019287109375
2023-01-07 08:40:38,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,656 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -36.22834396362305
2023-01-07 08:40:38,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,657 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 15.570169448852539
2023-01-07 08:40:38,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,657 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.69947814941406
2023-01-07 08:40:38,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,657 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -92.34257507324219
2023-01-07 08:40:38,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 71.28477478027344
2023-01-07 08:40:38,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 205.99008178710938
2023-01-07 08:40:38,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 131.24349975585938
2023-01-07 08:40:38,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 257.439697265625
2023-01-07 08:40:38,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 62.682586669921875
2023-01-07 08:40:38,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.363061904907227
2023-01-07 08:40:38,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -6.633338928222656
2023-01-07 08:40:38,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.965061664581299
2023-01-07 08:40:38,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.359907627105713
2023-01-07 08:40:38,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -47.41374588012695
2023-01-07 08:40:38,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 2147.6875
2023-01-07 08:40:38,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.2781248092651367
2023-01-07 08:40:38,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 27.819150924682617
2023-01-07 08:40:38,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1.1592254638671875
2023-01-07 08:40:38,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1410.9326171875
2023-01-07 08:40:38,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 402.34234619140625
2023-01-07 08:40:38,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 680.8699340820312
2023-01-07 08:40:38,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 489.37518310546875
2023-01-07 08:40:38,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1048.9608154296875
2023-01-07 08:40:38,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 877.7903442382812
2023-01-07 08:40:38,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 1075.275146484375
2023-01-07 08:40:38,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.863737106323242
2023-01-07 08:40:38,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.454903602600098
2023-01-07 08:40:38,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -11.682577133178711
2023-01-07 08:40:38,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.2426910400390625
2023-01-07 08:40:38,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.754419326782227
2023-01-07 08:40:38,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.630359649658203
2023-01-07 08:40:38,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.490731716156006
2023-01-07 08:40:38,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.5584447383880615
2023-01-07 08:40:38,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.201767921447754
2023-01-07 08:40:38,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,672 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.0021306276321411133
2023-01-07 08:40:38,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,672 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00909423828125
2023-01-07 08:40:38,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 578.970703125
2023-01-07 08:40:38,673 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,673 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 1531.56103515625
2023-01-07 08:40:38,673 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,674 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 1.7156816720962524
2023-01-07 08:40:38,674 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,674 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 150.31954956054688
2023-01-07 08:40:38,675 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,675 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 1.7156816720962524
2023-01-07 08:40:38,676 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.8936614990234375
2023-01-07 08:40:38,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,676 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 178.6278076171875
2023-01-07 08:40:38,676 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,676 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 180.9525909423828
2023-01-07 08:40:38,678 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,678 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 178.6278076171875
2023-01-07 08:40:38,678 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:38,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,679 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.849328994750977
2023-01-07 08:40:38,680 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,680 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -5.655794143676758
2023-01-07 08:40:38,680 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:38,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,680 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -263.07757568359375
2023-01-07 08:40:38,681 > [DEBUG] 0 :: before allreduce fusion buffer :: -112.46794128417969
2023-01-07 08:40:38,682 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,682 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -263.07757568359375
2023-01-07 08:40:38,682 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,682 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.5606093406677246
2023-01-07 08:40:38,683 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,683 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 41.947105407714844
2023-01-07 08:40:38,683 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,683 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,683 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.10686492919922
2023-01-07 08:40:38,684 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,685 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.5606093406677246
2023-01-07 08:40:38,685 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,685 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.6922607421875
2023-01-07 08:40:38,686 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,687 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 41.947105407714844
2023-01-07 08:40:38,687 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,687 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -2.0263421535491943
2023-01-07 08:40:38,687 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,688 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 113.82720947265625
2023-01-07 08:40:38,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.73738098144531
2023-01-07 08:40:38,689 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,689 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -2.0263421535491943
2023-01-07 08:40:38,689 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,689 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.51524543762207
2023-01-07 08:40:38,690 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,691 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 151.8192138671875
2023-01-07 08:40:38,691 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,691 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 138.08401489257812
2023-01-07 08:40:38,692 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,692 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 5.537753582000732
2023-01-07 08:40:38,693 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,693 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,693 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.00786590576172
2023-01-07 08:40:38,694 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,695 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 113.82720947265625
2023-01-07 08:40:38,695 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,695 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.1385071277618408
2023-01-07 08:40:38,695 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,696 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -293.6089782714844
2023-01-07 08:40:38,697 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,697 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.1385071277618408
2023-01-07 08:40:38,697 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:38,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,697 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -172.95004272460938
2023-01-07 08:40:38,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.726104736328125
2023-01-07 08:40:38,699 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,699 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -172.95004272460938
2023-01-07 08:40:38,699 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,700 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,700 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.6994785070419312
2023-01-07 08:40:38,700 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,700 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 1.8958148956298828
2023-01-07 08:40:38,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.844324111938477
2023-01-07 08:40:38,701 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,702 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.6994785070419312
2023-01-07 08:40:38,702 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,702 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,702 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.113990783691406
2023-01-07 08:40:38,703 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,704 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 1.8958148956298828
2023-01-07 08:40:38,704 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:38,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,704 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.563148498535156
2023-01-07 08:40:38,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.83718490600586
2023-01-07 08:40:38,705 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,705 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -3.5101006031036377
2023-01-07 08:40:38,706 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:38,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.563148498535156
2023-01-07 08:40:38,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.201297760009766
2023-01-07 08:40:38,708 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,708 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -39.563148498535156
2023-01-07 08:40:38,708 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,708 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.801010131835938
2023-01-07 08:40:38,709 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,710 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 1.925743818283081
2023-01-07 08:40:38,710 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,710 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,710 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:38,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1.9419403076171875
2023-01-07 08:40:38,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.36298370361328
2023-01-07 08:40:38,711 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,712 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1.9419403076171875
2023-01-07 08:40:38,712 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,712 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:38,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,712 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.36698973178863525
2023-01-07 08:40:38,712 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -23.147716522216797
2023-01-07 08:40:38,713 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 73.85194396972656
2023-01-07 08:40:38,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.546966552734375
2023-01-07 08:40:38,714 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,714 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.36698973178863525
2023-01-07 08:40:38,714 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,715 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.894235610961914
2023-01-07 08:40:38,716 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,716 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -23.147716522216797
2023-01-07 08:40:38,717 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -9.872394561767578
2023-01-07 08:40:38,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,717 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 73.85194396972656
2023-01-07 08:40:38,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.581687927246094
2023-01-07 08:40:38,718 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,718 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -1.4613085985183716
2023-01-07 08:40:38,718 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,719 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,719 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.016921997070312
2023-01-07 08:40:38,720 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,720 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 73.85194396972656
2023-01-07 08:40:38,721 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,721 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.145448684692383
2023-01-07 08:40:38,722 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,722 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -0.6084072589874268
2023-01-07 08:40:38,722 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:38,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -37.07183074951172
2023-01-07 08:40:38,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.532222270965576
2023-01-07 08:40:38,724 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,724 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -37.07183074951172
2023-01-07 08:40:38,724 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,725 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.19035708904266357
2023-01-07 08:40:38,725 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 29.054454803466797
2023-01-07 08:40:38,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.74057960510254
2023-01-07 08:40:38,726 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,726 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.19035708904266357
2023-01-07 08:40:38,727 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,727 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 4.868085861206055
2023-01-07 08:40:38,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 29.054454803466797
2023-01-07 08:40:38,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.470183849334717
2023-01-07 08:40:38,728 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,728 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 29.054454803466797
2023-01-07 08:40:38,729 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,729 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -2.136030673980713
2023-01-07 08:40:38,729 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 84.85845947265625
2023-01-07 08:40:38,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.134400367736816
2023-01-07 08:40:38,730 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,731 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -2.136030673980713
2023-01-07 08:40:38,731 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,731 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -12.447158813476562
2023-01-07 08:40:38,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 84.85845947265625
2023-01-07 08:40:38,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.384727478027344
2023-01-07 08:40:38,732 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,733 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 84.85845947265625
2023-01-07 08:40:38,733 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:38,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -6.570925712585449
2023-01-07 08:40:38,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.320022583007812
2023-01-07 08:40:38,734 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,734 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.336933970451355
2023-01-07 08:40:38,734 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:38,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -6.570925712585449
2023-01-07 08:40:38,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.882915496826172
2023-01-07 08:40:38,736 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,736 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -6.570925712585449
2023-01-07 08:40:38,736 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,737 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.4383385181427002
2023-01-07 08:40:38,737 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -133.86761474609375
2023-01-07 08:40:38,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.404067993164062
2023-01-07 08:40:38,738 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,739 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.4383385181427002
2023-01-07 08:40:38,739 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -32.070369720458984
2023-01-07 08:40:38,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,739 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -133.86761474609375
2023-01-07 08:40:38,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.06797409057617
2023-01-07 08:40:38,740 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,741 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -133.86761474609375
2023-01-07 08:40:38,741 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,741 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0046656131744384766
2023-01-07 08:40:38,741 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -2.9054007530212402
2023-01-07 08:40:38,742 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.695318222045898
2023-01-07 08:40:38,743 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,743 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.0046656131744384766
2023-01-07 08:40:38,743 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,743 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.4443514347076416
2023-01-07 08:40:38,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -2.9054007530212402
2023-01-07 08:40:38,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.24028205871582
2023-01-07 08:40:38,745 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,745 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -2.9054007530212402
2023-01-07 08:40:38,745 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:38,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -4.244293212890625
2023-01-07 08:40:38,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.808226585388184
2023-01-07 08:40:38,746 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,747 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.08034026622772217
2023-01-07 08:40:38,747 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:38,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,747 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -4.244293212890625
2023-01-07 08:40:38,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1875624656677246
2023-01-07 08:40:38,749 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,749 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -4.244293212890625
2023-01-07 08:40:38,749 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,749 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.002408742904663086
2023-01-07 08:40:38,749 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,750 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 296.4792175292969
2023-01-07 08:40:38,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.04110336303711
2023-01-07 08:40:38,751 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,751 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.002408742904663086
2023-01-07 08:40:38,751 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.05154252052307129
2023-01-07 08:40:38,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,752 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 296.4792175292969
2023-01-07 08:40:38,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.777302265167236
2023-01-07 08:40:38,753 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,753 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 296.4792175292969
2023-01-07 08:40:38,753 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,754 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.6110907793045044
2023-01-07 08:40:38,754 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,754 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 20.783201217651367
2023-01-07 08:40:38,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3523526191711426
2023-01-07 08:40:38,755 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,755 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.6110907793045044
2023-01-07 08:40:38,755 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,755 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.814935684204102
2023-01-07 08:40:38,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,756 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 20.783201217651367
2023-01-07 08:40:38,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8832013607025146
2023-01-07 08:40:38,757 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,757 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 20.783201217651367
2023-01-07 08:40:38,757 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,757 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,757 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.5326357483863831
2023-01-07 08:40:38,758 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,758 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.019287109375
2023-01-07 08:40:38,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3827064037323
2023-01-07 08:40:38,759 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,759 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.5326357483863831
2023-01-07 08:40:38,759 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,760 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 64.07452392578125
2023-01-07 08:40:38,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,760 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.019287109375
2023-01-07 08:40:38,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.978967666625977
2023-01-07 08:40:38,761 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,761 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 29.019287109375
2023-01-07 08:40:38,762 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,762 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.06796726584434509
2023-01-07 08:40:38,762 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,762 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -36.22834396362305
2023-01-07 08:40:38,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.417601585388184
2023-01-07 08:40:38,764 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,764 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.06796726584434509
2023-01-07 08:40:38,764 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.62215805053711
2023-01-07 08:40:38,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,764 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -36.22834396362305
2023-01-07 08:40:38,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1877777576446533
2023-01-07 08:40:38,766 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,766 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -36.22834396362305
2023-01-07 08:40:38,766 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:38,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,767 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.17688405513763428
2023-01-07 08:40:38,767 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,767 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 15.570169448852539
2023-01-07 08:40:38,767 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.883176803588867
2023-01-07 08:40:38,768 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,768 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.17688405513763428
2023-01-07 08:40:38,769 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,769 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,769 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 7.18283748626709
2023-01-07 08:40:38,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,769 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 15.570169448852539
2023-01-07 08:40:38,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5745998620986938
2023-01-07 08:40:38,770 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,770 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 15.570169448852539
2023-01-07 08:40:38,771 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:38,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,771 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.366292804479599
2023-01-07 08:40:38,771 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,771 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.69947814941406
2023-01-07 08:40:38,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.609830856323242
2023-01-07 08:40:38,772 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,773 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.366292804479599
2023-01-07 08:40:38,773 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.088607788085938
2023-01-07 08:40:38,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.69947814941406
2023-01-07 08:40:38,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21120071411132812
2023-01-07 08:40:38,774 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,775 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -58.69947814941406
2023-01-07 08:40:38,775 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,775 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,775 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,775 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.2807629108428955
2023-01-07 08:40:38,775 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,776 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -92.34257507324219
2023-01-07 08:40:38,776 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1124110221862793
2023-01-07 08:40:38,777 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,777 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.2807629108428955
2023-01-07 08:40:38,777 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,777 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -19.012502670288086
2023-01-07 08:40:38,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,778 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -92.34257507324219
2023-01-07 08:40:38,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.973653793334961
2023-01-07 08:40:38,779 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,779 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -92.34257507324219
2023-01-07 08:40:38,779 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:38,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,780 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 71.28477478027344
2023-01-07 08:40:38,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6411728858947754
2023-01-07 08:40:38,781 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,781 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.531899094581604
2023-01-07 08:40:38,781 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,781 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,781 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:38,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,782 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 71.28477478027344
2023-01-07 08:40:38,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0596941709518433
2023-01-07 08:40:38,783 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,783 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 71.28477478027344
2023-01-07 08:40:38,783 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:38,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,784 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 205.99008178710938
2023-01-07 08:40:38,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7588582038879395
2023-01-07 08:40:38,785 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,785 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.39521050453186035
2023-01-07 08:40:38,785 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:38,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 205.99008178710938
2023-01-07 08:40:38,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08009153604507446
2023-01-07 08:40:38,786 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,787 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 205.99008178710938
2023-01-07 08:40:38,787 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,787 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,787 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:38,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 131.24349975585938
2023-01-07 08:40:38,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.596078872680664
2023-01-07 08:40:38,788 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,788 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.5465515851974487
2023-01-07 08:40:38,789 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:38,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 131.24349975585938
2023-01-07 08:40:38,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4347180128097534
2023-01-07 08:40:38,790 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,790 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 131.24349975585938
2023-01-07 08:40:38,791 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:38,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.3794415295124054
2023-01-07 08:40:38,791 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:40:38,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 257.439697265625
2023-01-07 08:40:38,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.792550265789032
2023-01-07 08:40:38,793 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,793 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.3794415295124054
2023-01-07 08:40:38,793 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,793 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,793 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 18.518516540527344
2023-01-07 08:40:38,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 257.439697265625
2023-01-07 08:40:38,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.484584808349609
2023-01-07 08:40:38,795 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,795 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 257.439697265625
2023-01-07 08:40:38,795 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,795 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,795 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:38,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,795 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 62.682586669921875
2023-01-07 08:40:38,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.345416307449341
2023-01-07 08:40:38,796 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,797 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -0.3545432686805725
2023-01-07 08:40:38,797 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,797 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:38,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,797 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 62.682586669921875
2023-01-07 08:40:38,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.038912177085876465
2023-01-07 08:40:38,799 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,799 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 62.682586669921875
2023-01-07 08:40:38,800 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,800 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,800 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:38,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,800 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.363061904907227
2023-01-07 08:40:38,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9794609546661377
2023-01-07 08:40:38,801 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,801 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.04130050539970398
2023-01-07 08:40:38,801 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:38,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,802 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -5.363061904907227
2023-01-07 08:40:38,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1907715797424316
2023-01-07 08:40:38,803 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,803 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -5.363061904907227
2023-01-07 08:40:38,803 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:38,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,804 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -6.633338928222656
2023-01-07 08:40:38,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8711766004562378
2023-01-07 08:40:38,805 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,805 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.18173950910568237
2023-01-07 08:40:38,805 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,805 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,805 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:38,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,806 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -6.633338928222656
2023-01-07 08:40:38,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1173943281173706
2023-01-07 08:40:38,807 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,807 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -6.633338928222656
2023-01-07 08:40:38,807 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,808 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:38,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,808 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.965061664581299
2023-01-07 08:40:38,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.045625925064087
2023-01-07 08:40:38,809 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,809 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.24387279152870178
2023-01-07 08:40:38,809 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:38,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -7.965061664581299
2023-01-07 08:40:38,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.473644495010376
2023-01-07 08:40:38,811 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,811 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -7.965061664581299
2023-01-07 08:40:38,811 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,811 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,812 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:38,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,812 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.359907627105713
2023-01-07 08:40:38,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.757584571838379
2023-01-07 08:40:38,813 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,813 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.023343034088611603
2023-01-07 08:40:38,813 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,813 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,813 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:38,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,814 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -5.359907627105713
2023-01-07 08:40:38,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6272534132003784
2023-01-07 08:40:38,815 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,815 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -5.359907627105713
2023-01-07 08:40:38,815 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,815 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,815 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:38,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,816 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -47.41374588012695
2023-01-07 08:40:38,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.62539005279541
2023-01-07 08:40:38,817 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,817 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.1402653455734253
2023-01-07 08:40:38,817 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,817 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,817 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:38,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,817 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -47.41374588012695
2023-01-07 08:40:38,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8873887658119202
2023-01-07 08:40:38,819 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,819 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -47.41374588012695
2023-01-07 08:40:38,819 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,819 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,819 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:38,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,819 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.2781248092651367
2023-01-07 08:40:38,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3542027473449707
2023-01-07 08:40:38,820 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,821 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.08232441544532776
2023-01-07 08:40:38,821 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,821 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,821 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:38,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,821 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.2781248092651367
2023-01-07 08:40:38,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3673939108848572
2023-01-07 08:40:38,822 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,823 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 1.2781248092651367
2023-01-07 08:40:38,823 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,823 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,823 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:38,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,823 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 27.819150924682617
2023-01-07 08:40:38,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6051801443099976
2023-01-07 08:40:38,824 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,824 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.16664639115333557
2023-01-07 08:40:38,825 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,825 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,825 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:38,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,825 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 27.819150924682617
2023-01-07 08:40:38,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.427690029144287
2023-01-07 08:40:38,826 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,826 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 27.819150924682617
2023-01-07 08:40:38,827 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,827 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,827 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:38,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,827 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1.1592254638671875
2023-01-07 08:40:38,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7625243663787842
2023-01-07 08:40:38,828 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,828 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.04846826195716858
2023-01-07 08:40:38,828 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,828 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:38,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1.1592254638671875
2023-01-07 08:40:38,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0482258796691895
2023-01-07 08:40:38,830 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,830 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1.1592254638671875
2023-01-07 08:40:38,830 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,830 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,831 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:38,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1410.9326171875
2023-01-07 08:40:38,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5536584854125977
2023-01-07 08:40:38,832 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,832 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.05620758980512619
2023-01-07 08:40:38,832 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,832 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,832 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:38,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,833 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1410.9326171875
2023-01-07 08:40:38,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04479268193244934
2023-01-07 08:40:38,834 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,834 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1410.9326171875
2023-01-07 08:40:38,834 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,834 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,834 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:38,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,835 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 402.34234619140625
2023-01-07 08:40:38,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11688217520713806
2023-01-07 08:40:38,836 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,836 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.32756873965263367
2023-01-07 08:40:38,836 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,836 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,836 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:38,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 402.34234619140625
2023-01-07 08:40:38,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07427166402339935
2023-01-07 08:40:38,838 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,838 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 402.34234619140625
2023-01-07 08:40:38,838 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,838 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,838 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:38,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,839 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 680.8699340820312
2023-01-07 08:40:38,839 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3580689430236816
2023-01-07 08:40:38,840 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,840 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.026187874376773834
2023-01-07 08:40:38,840 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,840 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,840 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:38,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,840 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 680.8699340820312
2023-01-07 08:40:38,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9859627485275269
2023-01-07 08:40:38,842 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,842 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 680.8699340820312
2023-01-07 08:40:38,842 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,842 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,842 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:38,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,842 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 489.37518310546875
2023-01-07 08:40:38,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17268094420433044
2023-01-07 08:40:38,843 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,844 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.08389368653297424
2023-01-07 08:40:38,844 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,844 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,844 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:38,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,844 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 489.37518310546875
2023-01-07 08:40:38,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9742192029953003
2023-01-07 08:40:38,845 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,846 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 489.37518310546875
2023-01-07 08:40:38,846 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,846 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:38,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,846 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1048.9608154296875
2023-01-07 08:40:38,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6081587672233582
2023-01-07 08:40:38,847 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,847 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.057015229016542435
2023-01-07 08:40:38,848 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,848 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,848 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:38,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1048.9608154296875
2023-01-07 08:40:38,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.668123245239258
2023-01-07 08:40:38,849 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,849 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1048.9608154296875
2023-01-07 08:40:38,850 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,850 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,850 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:38,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 864.80322265625
2023-01-07 08:40:38,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14322929084300995
2023-01-07 08:40:38,851 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,851 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.07471080869436264
2023-01-07 08:40:38,851 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,851 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,852 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:38,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 864.80322265625
2023-01-07 08:40:38,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9425058364868164
2023-01-07 08:40:38,853 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,853 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 864.80322265625
2023-01-07 08:40:38,853 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,853 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,854 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:38,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,854 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 88.06271362304688
2023-01-07 08:40:38,854 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5277983546257019
2023-01-07 08:40:38,855 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,855 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.0265854150056839
2023-01-07 08:40:38,856 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,856 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:38,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,856 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 88.06271362304688
2023-01-07 08:40:38,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0055606067180633545
2023-01-07 08:40:38,857 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,857 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 88.06271362304688
2023-01-07 08:40:38,858 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,858 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,858 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:38,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,858 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3208.67431640625
2023-01-07 08:40:38,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16346710920333862
2023-01-07 08:40:38,859 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,859 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.10881946980953217
2023-01-07 08:40:38,859 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,859 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,860 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:38,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3208.67431640625
2023-01-07 08:40:38,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36847418546676636
2023-01-07 08:40:38,861 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,861 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 3208.67431640625
2023-01-07 08:40:38,861 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,861 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:38,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,862 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 721.702392578125
2023-01-07 08:40:38,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.450790137052536
2023-01-07 08:40:38,863 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,863 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.038371019065380096
2023-01-07 08:40:38,863 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:38,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,863 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 721.702392578125
2023-01-07 08:40:38,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6171510815620422
2023-01-07 08:40:38,865 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,865 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 721.702392578125
2023-01-07 08:40:38,865 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:38,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,865 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -19.453420639038086
2023-01-07 08:40:38,866 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38111281394958496
2023-01-07 08:40:38,866 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,867 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.03403530269861221
2023-01-07 08:40:38,867 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,867 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,867 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:38,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,867 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -19.453420639038086
2023-01-07 08:40:38,867 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17946314811706543
2023-01-07 08:40:38,868 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,869 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -19.453420639038086
2023-01-07 08:40:38,869 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,869 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,869 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:38,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,869 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.630359649658203
2023-01-07 08:40:38,869 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4959442615509033
2023-01-07 08:40:38,870 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,870 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.031125038862228394
2023-01-07 08:40:38,871 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,871 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,871 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:38,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,871 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2.630359649658203
2023-01-07 08:40:38,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01703374832868576
2023-01-07 08:40:38,872 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,872 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2.630359649658203
2023-01-07 08:40:38,873 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,873 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,873 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:38,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,873 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.490731716156006
2023-01-07 08:40:38,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29143065214157104
2023-01-07 08:40:38,874 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,874 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.3168901205062866
2023-01-07 08:40:38,874 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,874 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,875 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:38,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,875 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.490731716156006
2023-01-07 08:40:38,875 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10486243665218353
2023-01-07 08:40:38,876 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,876 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -3.490731716156006
2023-01-07 08:40:38,876 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,876 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,877 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:38,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,877 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.5584447383880615
2023-01-07 08:40:38,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13352344930171967
2023-01-07 08:40:38,878 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,878 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.04705367609858513
2023-01-07 08:40:38,878 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,878 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,878 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:38,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,879 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2.5584447383880615
2023-01-07 08:40:38,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31110692024230957
2023-01-07 08:40:38,880 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,880 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -2.5584447383880615
2023-01-07 08:40:38,880 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,880 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,880 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:38,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,881 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.0021306276321411133
2023-01-07 08:40:38,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3086455464363098
2023-01-07 08:40:38,882 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,882 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.02957703173160553
2023-01-07 08:40:38,882 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,882 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,882 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:38,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,882 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.0021306276321411133
2023-01-07 08:40:38,883 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2516911029815674
2023-01-07 08:40:38,884 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,884 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -0.0021306276321411133
2023-01-07 08:40:38,884 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,884 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,884 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:38,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,885 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00909423828125
2023-01-07 08:40:38,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1417551040649414
2023-01-07 08:40:38,886 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,886 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.32441046833992004
2023-01-07 08:40:38,886 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,886 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,886 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:38,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,886 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.00909423828125
2023-01-07 08:40:38,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8658124208450317
2023-01-07 08:40:38,888 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:38,888 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.00909423828125
2023-01-07 08:40:38,888 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:38,888 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:38,889 > [DEBUG] 0 :: 7.362274169921875
2023-01-07 08:40:38,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,893 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00665283203125
2023-01-07 08:40:38,893 > [DEBUG] 0 :: before allreduce fusion buffer :: -312.97998046875
2023-01-07 08:40:38,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,896 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2773474454879761
2023-01-07 08:40:38,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,897 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00665283203125
2023-01-07 08:40:38,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -384.6149597167969
2023-01-07 08:40:38,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,900 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.602403163909912
2023-01-07 08:40:38,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16267088055610657
2023-01-07 08:40:38,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,903 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.00718173012137413
2023-01-07 08:40:38,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,903 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.602403163909912
2023-01-07 08:40:38,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.49448084831237793
2023-01-07 08:40:38,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,905 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.595742702484131
2023-01-07 08:40:38,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09463344514369965
2023-01-07 08:40:38,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,906 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.061116792261600494
2023-01-07 08:40:38,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,907 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.595742702484131
2023-01-07 08:40:38,907 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23823793232440948
2023-01-07 08:40:38,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -8.91431999206543
2023-01-07 08:40:38,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.348296195268631
2023-01-07 08:40:38,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2925698757171631
2023-01-07 08:40:38,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,910 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -8.91431999206543
2023-01-07 08:40:38,910 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0772697925567627
2023-01-07 08:40:38,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.4067330360412598
2023-01-07 08:40:38,911 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08000999689102173
2023-01-07 08:40:38,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,912 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.03322191536426544
2023-01-07 08:40:38,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,913 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.4067330360412598
2023-01-07 08:40:38,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7917683720588684
2023-01-07 08:40:38,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,914 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.481292486190796
2023-01-07 08:40:38,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3074890375137329
2023-01-07 08:40:38,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,915 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.004272576421499252
2023-01-07 08:40:38,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,916 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.481292486190796
2023-01-07 08:40:38,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6161351203918457
2023-01-07 08:40:38,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,917 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -10.47540283203125
2023-01-07 08:40:38,918 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44728267192840576
2023-01-07 08:40:38,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,919 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.2157728224992752
2023-01-07 08:40:38,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,919 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -10.47540283203125
2023-01-07 08:40:38,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.348524272441864
2023-01-07 08:40:38,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,920 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.391202926635742
2023-01-07 08:40:38,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9428129196166992
2023-01-07 08:40:38,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,922 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.18415948748588562
2023-01-07 08:40:38,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,922 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.391202926635742
2023-01-07 08:40:38,922 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29933440685272217
2023-01-07 08:40:38,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,924 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -11.313096046447754
2023-01-07 08:40:38,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5532909631729126
2023-01-07 08:40:38,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,925 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.0667085275053978
2023-01-07 08:40:38,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,925 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -11.313096046447754
2023-01-07 08:40:38,925 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8616405725479126
2023-01-07 08:40:38,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,927 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 16.5954647064209
2023-01-07 08:40:38,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4742652475833893
2023-01-07 08:40:38,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,928 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.1008845716714859
2023-01-07 08:40:38,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,928 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 16.5954647064209
2023-01-07 08:40:38,928 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05596761405467987
2023-01-07 08:40:38,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,930 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.290708541870117
2023-01-07 08:40:38,930 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8569252490997314
2023-01-07 08:40:38,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,931 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.051556311547756195
2023-01-07 08:40:38,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,931 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.290708541870117
2023-01-07 08:40:38,932 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4309964179992676
2023-01-07 08:40:38,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,933 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.0475125312805176
2023-01-07 08:40:38,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43335917592048645
2023-01-07 08:40:38,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,935 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.06235498934984207
2023-01-07 08:40:38,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,935 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.0475125312805176
2023-01-07 08:40:38,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16138328611850739
2023-01-07 08:40:38,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,936 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -23.323440551757812
2023-01-07 08:40:38,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43564748764038086
2023-01-07 08:40:38,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.0014659464359283447
2023-01-07 08:40:38,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -23.323440551757812
2023-01-07 08:40:38,938 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7338576316833496
2023-01-07 08:40:38,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,939 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -5.04494047164917
2023-01-07 08:40:38,940 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.026815414428711
2023-01-07 08:40:38,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,941 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.2073708176612854
2023-01-07 08:40:38,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,941 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -5.04494047164917
2023-01-07 08:40:38,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.612663984298706
2023-01-07 08:40:38,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,943 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.3558642864227295
2023-01-07 08:40:38,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5935683846473694
2023-01-07 08:40:38,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.06750774383544922
2023-01-07 08:40:38,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.3558642864227295
2023-01-07 08:40:38,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.183760166168213
2023-01-07 08:40:38,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 11.691553115844727
2023-01-07 08:40:38,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7073390483856201
2023-01-07 08:40:38,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,947 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.04806577414274216
2023-01-07 08:40:38,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,947 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 11.691553115844727
2023-01-07 08:40:38,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26017341017723083
2023-01-07 08:40:38,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.6766748428344727
2023-01-07 08:40:38,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9152400493621826
2023-01-07 08:40:38,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,950 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.16148623824119568
2023-01-07 08:40:38,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,950 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.6766748428344727
2023-01-07 08:40:38,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.891108751296997
2023-01-07 08:40:38,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,952 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -3.891234874725342
2023-01-07 08:40:38,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4142690598964691
2023-01-07 08:40:38,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,953 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.05190260708332062
2023-01-07 08:40:38,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,953 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -3.891234874725342
2023-01-07 08:40:38,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4842383861541748
2023-01-07 08:40:38,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,955 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 22.82958984375
2023-01-07 08:40:38,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2645286321640015
2023-01-07 08:40:38,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,956 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.06009422615170479
2023-01-07 08:40:38,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,956 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 22.82958984375
2023-01-07 08:40:38,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.324223279953003
2023-01-07 08:40:38,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,958 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.131904602050781
2023-01-07 08:40:38,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4569228887557983
2023-01-07 08:40:38,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,959 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.07073148339986801
2023-01-07 08:40:38,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,959 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.131904602050781
2023-01-07 08:40:38,960 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15334394574165344
2023-01-07 08:40:38,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,961 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -5.404110908508301
2023-01-07 08:40:38,961 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.057414472103118896
2023-01-07 08:40:38,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,962 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.05021880567073822
2023-01-07 08:40:38,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,962 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -5.404110908508301
2023-01-07 08:40:38,963 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9499940872192383
2023-01-07 08:40:38,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,964 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 46.13944625854492
2023-01-07 08:40:38,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12724554538726807
2023-01-07 08:40:38,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,965 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.015085890889167786
2023-01-07 08:40:38,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,965 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 46.13944625854492
2023-01-07 08:40:38,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4273430109024048
2023-01-07 08:40:38,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,967 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.652083396911621
2023-01-07 08:40:38,967 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3044233322143555
2023-01-07 08:40:38,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.14799924194812775
2023-01-07 08:40:38,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.652083396911621
2023-01-07 08:40:38,969 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.521996021270752
2023-01-07 08:40:38,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,970 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -12.420856475830078
2023-01-07 08:40:38,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9774184226989746
2023-01-07 08:40:38,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,971 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.08692355453968048
2023-01-07 08:40:38,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -12.420856475830078
2023-01-07 08:40:38,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.047466278076172
2023-01-07 08:40:38,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,973 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -44.005611419677734
2023-01-07 08:40:38,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8851058483123779
2023-01-07 08:40:38,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,975 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -44.005611419677734
2023-01-07 08:40:38,975 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.270748138427734
2023-01-07 08:40:38,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8070621490478516
2023-01-07 08:40:38,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.238080024719238
2023-01-07 08:40:38,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,977 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.2711206078529358
2023-01-07 08:40:38,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,978 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8070621490478516
2023-01-07 08:40:38,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2077484130859375
2023-01-07 08:40:38,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,979 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.877429962158203
2023-01-07 08:40:38,979 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0056767463684082
2023-01-07 08:40:38,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,980 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.14039796590805054
2023-01-07 08:40:38,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,981 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.877429962158203
2023-01-07 08:40:38,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.06815242767334
2023-01-07 08:40:38,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,982 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -5.648489952087402
2023-01-07 08:40:38,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3369405269622803
2023-01-07 08:40:38,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,983 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.35942065715789795
2023-01-07 08:40:38,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,984 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -5.648489952087402
2023-01-07 08:40:38,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3387699127197266
2023-01-07 08:40:38,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -124.45425415039062
2023-01-07 08:40:38,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.344123125076294
2023-01-07 08:40:38,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -124.45425415039062
2023-01-07 08:40:38,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.784768104553223
2023-01-07 08:40:38,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -48.94367980957031
2023-01-07 08:40:38,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7573153376579285
2023-01-07 08:40:38,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -48.94367980957031
2023-01-07 08:40:38,990 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.524841785430908
2023-01-07 08:40:38,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,991 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.9476237297058105
2023-01-07 08:40:38,991 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8224061727523804
2023-01-07 08:40:38,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,992 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.9476237297058105
2023-01-07 08:40:38,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1007459163665771
2023-01-07 08:40:38,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,993 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 44.13374710083008
2023-01-07 08:40:38,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.358372211456299
2023-01-07 08:40:38,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,995 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 44.13374710083008
2023-01-07 08:40:38,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.109928131103516
2023-01-07 08:40:38,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.212600708007812
2023-01-07 08:40:38,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7902022004127502
2023-01-07 08:40:38,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,997 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.212600708007812
2023-01-07 08:40:38,998 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4779099225997925
2023-01-07 08:40:38,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:38,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:38,999 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.882040977478027
2023-01-07 08:40:38,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.564049243927002
2023-01-07 08:40:39,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,000 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.882040977478027
2023-01-07 08:40:39,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35918182134628296
2023-01-07 08:40:39,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -54.84788513183594
2023-01-07 08:40:39,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.744805335998535
2023-01-07 08:40:39,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,003 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -54.84788513183594
2023-01-07 08:40:39,003 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.477294921875
2023-01-07 08:40:39,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,004 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -73.50697326660156
2023-01-07 08:40:39,004 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.631356716156006
2023-01-07 08:40:39,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,005 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.0649901032447815
2023-01-07 08:40:39,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,006 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -73.50697326660156
2023-01-07 08:40:39,006 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.912638664245605
2023-01-07 08:40:39,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,007 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 14.291179656982422
2023-01-07 08:40:39,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.944277048110962
2023-01-07 08:40:39,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 14.291179656982422
2023-01-07 08:40:39,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.208483695983887
2023-01-07 08:40:39,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,010 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -168.90402221679688
2023-01-07 08:40:39,010 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.036020278930664
2023-01-07 08:40:39,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,011 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -168.90402221679688
2023-01-07 08:40:39,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.856382369995117
2023-01-07 08:40:39,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,013 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -15.960615158081055
2023-01-07 08:40:39,013 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.500642776489258
2023-01-07 08:40:39,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,014 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 3.3972859382629395
2023-01-07 08:40:39,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,014 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -15.960615158081055
2023-01-07 08:40:39,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7683441042900085
2023-01-07 08:40:39,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,016 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.11937713623047
2023-01-07 08:40:39,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.246057152748108
2023-01-07 08:40:39,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.11937713623047
2023-01-07 08:40:39,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.606948852539062
2023-01-07 08:40:39,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,018 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -37.379608154296875
2023-01-07 08:40:39,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.200458526611328
2023-01-07 08:40:39,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,020 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -37.379608154296875
2023-01-07 08:40:39,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.79517364501953
2023-01-07 08:40:39,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,022 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.701350212097168
2023-01-07 08:40:39,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,023 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.09864926338195801
2023-01-07 08:40:39,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,023 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.541074752807617
2023-01-07 08:40:39,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,025 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.826774597167969
2023-01-07 08:40:39,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,026 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -2.076484203338623
2023-01-07 08:40:39,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,027 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -14.261747360229492
2023-01-07 08:40:39,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,027 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,027 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.99891662597656
2023-01-07 08:40:39,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,028 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.519311904907227
2023-01-07 08:40:39,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,030 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -14.261747360229492
2023-01-07 08:40:39,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.64529800415039
2023-01-07 08:40:39,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.596111297607422
2023-01-07 08:40:39,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,032 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.6356804370880127
2023-01-07 08:40:39,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,033 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.35744094848633
2023-01-07 08:40:39,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.811136245727539
2023-01-07 08:40:39,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,035 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -0.2265596091747284
2023-01-07 08:40:39,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,036 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 58.314369201660156
2023-01-07 08:40:39,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.984519958496094
2023-01-07 08:40:39,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,037 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.509110450744629
2023-01-07 08:40:39,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -34.037139892578125
2023-01-07 08:40:39,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.501001358032227
2023-01-07 08:40:39,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,040 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.590168952941895
2023-01-07 08:40:39,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,042 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.630922317504883
2023-01-07 08:40:39,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,043 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.248183250427246
2023-01-07 08:40:39,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,045 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 4.117694854736328
2023-01-07 08:40:39,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,045 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -16.21558952331543
2023-01-07 08:40:39,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,045 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,046 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.929017543792725
2023-01-07 08:40:39,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,047 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,048 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.502453804016113
2023-01-07 08:40:39,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,049 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.25224304199219
2023-01-07 08:40:39,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.353668212890625
2023-01-07 08:40:39,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,050 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.599292755126953
2023-01-07 08:40:39,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,051 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.25224304199219
2023-01-07 08:40:39,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,052 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -101.8344497680664
2023-01-07 08:40:39,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,053 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 152.97169494628906
2023-01-07 08:40:39,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,054 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 4.535260200500488
2023-01-07 08:40:39,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,055 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.72503662109375
2023-01-07 08:40:39,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,056 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -86.74891662597656
2023-01-07 08:40:39,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,057 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.64029598236084
2023-01-07 08:40:39,061 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:40:39,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,062 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1727.345947265625
2023-01-07 08:40:39,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,063 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,064 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -37.379608154296875
2023-01-07 08:40:39,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,066 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.11937713623047
2023-01-07 08:40:39,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,067 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -15.960615158081055
2023-01-07 08:40:39,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,068 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -168.90402221679688
2023-01-07 08:40:39,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 14.291179656982422
2023-01-07 08:40:39,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,069 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -73.50697326660156
2023-01-07 08:40:39,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,070 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -54.84788513183594
2023-01-07 08:40:39,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.882040977478027
2023-01-07 08:40:39,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.212600708007812
2023-01-07 08:40:39,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,072 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 44.13374710083008
2023-01-07 08:40:39,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,072 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.9476237297058105
2023-01-07 08:40:39,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,073 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -48.94367980957031
2023-01-07 08:40:39,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,073 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -124.45425415039062
2023-01-07 08:40:39,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,074 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -5.648489952087402
2023-01-07 08:40:39,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,074 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.877429962158203
2023-01-07 08:40:39,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,075 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8070621490478516
2023-01-07 08:40:39,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,075 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -44.005611419677734
2023-01-07 08:40:39,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,076 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -12.420856475830078
2023-01-07 08:40:39,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,076 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -12.652083396911621
2023-01-07 08:40:39,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,076 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 46.13944625854492
2023-01-07 08:40:39,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,077 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -5.404110908508301
2023-01-07 08:40:39,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,077 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.131904602050781
2023-01-07 08:40:39,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,077 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 22.82958984375
2023-01-07 08:40:39,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -1555.8619384765625
2023-01-07 08:40:39,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,080 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -3.891234874725342
2023-01-07 08:40:39,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,080 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.6766748428344727
2023-01-07 08:40:39,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,080 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 11.691553115844727
2023-01-07 08:40:39,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,081 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.3558642864227295
2023-01-07 08:40:39,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,081 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -5.04494047164917
2023-01-07 08:40:39,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,081 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -23.323440551757812
2023-01-07 08:40:39,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,082 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.0475125312805176
2023-01-07 08:40:39,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.290708541870117
2023-01-07 08:40:39,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 16.5954647064209
2023-01-07 08:40:39,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.218494415283203
2023-01-07 08:40:39,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,084 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -11.313096046447754
2023-01-07 08:40:39,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,084 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -9.391202926635742
2023-01-07 08:40:39,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,085 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -10.47540283203125
2023-01-07 08:40:39,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,085 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 145.56936645507812
2023-01-07 08:40:39,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.8372802734375
2023-01-07 08:40:39,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,086 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 175.240966796875
2023-01-07 08:40:39,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,087 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 5426.1689453125
2023-01-07 08:40:39,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,087 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 594.5594482421875
2023-01-07 08:40:39,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 6335.69140625
2023-01-07 08:40:39,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,088 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.602403163909912
2023-01-07 08:40:39,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,089 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00665283203125
2023-01-07 08:40:39,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 697.5307006835938
2023-01-07 08:40:39,089 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,090 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -1727.345947265625
2023-01-07 08:40:39,090 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,090 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 0.30650806427001953
2023-01-07 08:40:39,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,090 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.472503662109375
2023-01-07 08:40:39,092 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,092 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.30650806427001953
2023-01-07 08:40:39,092 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.8936614990234375
2023-01-07 08:40:39,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,092 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 80.48590850830078
2023-01-07 08:40:39,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,093 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 95.93463134765625
2023-01-07 08:40:39,094 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,094 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 80.48590850830078
2023-01-07 08:40:39,095 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,095 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,095 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:39,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,095 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,095 > [DEBUG] 0 :: before allreduce fusion buffer :: -265.3178405761719
2023-01-07 08:40:39,096 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,096 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 4.535260200500488
2023-01-07 08:40:39,096 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,097 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:39,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,097 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -507.6257629394531
2023-01-07 08:40:39,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -187.2849578857422
2023-01-07 08:40:39,098 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,098 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -507.6257629394531
2023-01-07 08:40:39,098 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,098 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,099 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,099 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 2.7608706951141357
2023-01-07 08:40:39,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,099 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -86.61201477050781
2023-01-07 08:40:39,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,100 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -100.139404296875
2023-01-07 08:40:39,101 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,101 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 2.7608706951141357
2023-01-07 08:40:39,101 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,102 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 47.87093734741211
2023-01-07 08:40:39,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.56065368652344
2023-01-07 08:40:39,103 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,103 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -86.61201477050781
2023-01-07 08:40:39,103 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,104 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.34713125228881836
2023-01-07 08:40:39,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,104 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -16.21558952331543
2023-01-07 08:40:39,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.419069766998291
2023-01-07 08:40:39,105 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,105 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -0.34713125228881836
2023-01-07 08:40:39,105 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,106 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,106 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.93624877929688
2023-01-07 08:40:39,107 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,107 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -48.25224304199219
2023-01-07 08:40:39,107 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,108 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.282623291015625
2023-01-07 08:40:39,109 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,109 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.117694854736328
2023-01-07 08:40:39,109 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,109 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.870630264282227
2023-01-07 08:40:39,111 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,111 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -16.21558952331543
2023-01-07 08:40:39,111 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,111 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.39181792736053467
2023-01-07 08:40:39,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,112 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 250.27752685546875
2023-01-07 08:40:39,113 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,113 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.39181792736053467
2023-01-07 08:40:39,113 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,113 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,113 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,113 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 443.96380615234375
2023-01-07 08:40:39,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,114 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 122.86822509765625
2023-01-07 08:40:39,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.28894805908203
2023-01-07 08:40:39,115 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,115 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 443.96380615234375
2023-01-07 08:40:39,116 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,116 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 2.8642795085906982
2023-01-07 08:40:39,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,116 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -34.037139892578125
2023-01-07 08:40:39,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 106.88409423828125
2023-01-07 08:40:39,118 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,118 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 2.8642795085906982
2023-01-07 08:40:39,118 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,118 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 676.808349609375
2023-01-07 08:40:39,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 743.5458984375
2023-01-07 08:40:39,120 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,120 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -34.037139892578125
2023-01-07 08:40:39,120 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,120 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,120 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:39,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,120 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 58.314369201660156
2023-01-07 08:40:39,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.378463745117188
2023-01-07 08:40:39,122 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,122 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -0.2265596091747284
2023-01-07 08:40:39,122 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,122 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:39,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,122 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 58.314369201660156
2023-01-07 08:40:39,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,123 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 37327.68359375
2023-01-07 08:40:39,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 36423.609375
2023-01-07 08:40:39,124 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,124 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -235.7998046875
2023-01-07 08:40:39,124 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,124 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,125 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 292461.6875
2023-01-07 08:40:39,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 249813.1875
2023-01-07 08:40:39,126 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,126 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.6356804370880127
2023-01-07 08:40:39,126 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,126 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 541065.375
2023-01-07 08:40:39,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 248608.078125
2023-01-07 08:40:39,128 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,128 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 541065.375
2023-01-07 08:40:39,128 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,129 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 44317.39453125
2023-01-07 08:40:39,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,129 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 459999.125
2023-01-07 08:40:39,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,129 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1447467.625
2023-01-07 08:40:39,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 10765510.0
2023-01-07 08:40:39,131 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,131 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 44317.39453125
2023-01-07 08:40:39,131 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,131 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 18812560.0
2023-01-07 08:40:39,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 742612.875
2023-01-07 08:40:39,133 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,133 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 459999.125
2023-01-07 08:40:39,133 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -9.872394561767578
2023-01-07 08:40:39,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,133 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1447467.625
2023-01-07 08:40:39,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 1447627.5
2023-01-07 08:40:39,135 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,135 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -2.076484203338623
2023-01-07 08:40:39,135 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,135 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 18812560.0
2023-01-07 08:40:39,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 2901508.75
2023-01-07 08:40:39,137 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,137 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 1447467.625
2023-01-07 08:40:39,137 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,137 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 18812560.0
2023-01-07 08:40:39,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 2907287296.0
2023-01-07 08:40:39,138 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,139 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -0.09864926338195801
2023-01-07 08:40:39,139 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,139 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 18812560.0
2023-01-07 08:40:39,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 9361149.0
2023-01-07 08:40:39,141 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,141 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 18812560.0
2023-01-07 08:40:39,141 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,141 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 3465.40625
2023-01-07 08:40:39,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,142 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 32225754.0
2023-01-07 08:40:39,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 10764728.0
2023-01-07 08:40:39,143 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,143 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 3465.40625
2023-01-07 08:40:39,143 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 4.868085861206055
2023-01-07 08:40:39,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,144 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 32225754.0
2023-01-07 08:40:39,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 371960119296.0
2023-01-07 08:40:39,145 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,145 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 32225754.0
2023-01-07 08:40:39,145 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,145 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 5740958.5
2023-01-07 08:40:39,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 37701376.0
2023-01-07 08:40:39,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 381703880704.0
2023-01-07 08:40:39,147 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,147 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 5740958.5
2023-01-07 08:40:39,147 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,148 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,148 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -12.447158813476562
2023-01-07 08:40:39,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,148 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 37701376.0
2023-01-07 08:40:39,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 448720.53125
2023-01-07 08:40:39,149 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,149 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 37701376.0
2023-01-07 08:40:39,149 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:39,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,150 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 45786368.0
2023-01-07 08:40:39,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 22949826.0
2023-01-07 08:40:39,151 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,151 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 3.3972859382629395
2023-01-07 08:40:39,151 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:39,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,152 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 45786368.0
2023-01-07 08:40:39,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 22836538.0
2023-01-07 08:40:39,153 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,153 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 45786368.0
2023-01-07 08:40:39,153 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,154 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 55869.0
2023-01-07 08:40:39,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,154 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 136732256.0
2023-01-07 08:40:39,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 228222566400.0
2023-01-07 08:40:39,155 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,155 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 55869.0
2023-01-07 08:40:39,155 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -32.070369720458984
2023-01-07 08:40:39,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,156 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 136732256.0
2023-01-07 08:40:39,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 91337024.0
2023-01-07 08:40:39,157 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,157 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 136732256.0
2023-01-07 08:40:39,157 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,158 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 112151.0
2023-01-07 08:40:39,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,158 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 274775808.0
2023-01-07 08:40:39,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 3650957082624.0
2023-01-07 08:40:39,159 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,159 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 112151.0
2023-01-07 08:40:39,159 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,160 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.4443514347076416
2023-01-07 08:40:39,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,160 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 274775808.0
2023-01-07 08:40:39,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 182323616.0
2023-01-07 08:40:39,161 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,161 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 274775808.0
2023-01-07 08:40:39,161 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,162 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:39,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,162 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1093974912.0
2023-01-07 08:40:39,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 364639040.0
2023-01-07 08:40:39,163 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,163 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -0.0649901032447815
2023-01-07 08:40:39,163 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,164 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:39,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,164 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1093974912.0
2023-01-07 08:40:39,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 729335872.0
2023-01-07 08:40:39,165 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,165 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 1093974912.0
2023-01-07 08:40:39,165 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,166 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 898000.0
2023-01-07 08:40:39,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,166 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 2195727360.0
2023-01-07 08:40:39,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 733083264.0
2023-01-07 08:40:39,167 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,168 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 898000.0
2023-01-07 08:40:39,168 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.05154252052307129
2023-01-07 08:40:39,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,168 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 2195727360.0
2023-01-07 08:40:39,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.233020916943553e+18
2023-01-07 08:40:39,169 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,170 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 2195727360.0
2023-01-07 08:40:39,170 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,170 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 3592000.0
2023-01-07 08:40:39,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,171 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5884791808.0
2023-01-07 08:40:39,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 12994231296.0
2023-01-07 08:40:39,172 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,172 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 3592000.0
2023-01-07 08:40:39,172 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.814935684204102
2023-01-07 08:40:39,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5884791808.0
2023-01-07 08:40:39,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 2959566336.0
2023-01-07 08:40:39,174 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,174 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 5884791808.0
2023-01-07 08:40:39,174 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 2981406208.0
2023-01-07 08:40:39,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 14775953408.0
2023-01-07 08:40:39,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2708436565941824e+21
2023-01-07 08:40:39,176 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,177 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 2981406208.0
2023-01-07 08:40:39,177 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 64.07452392578125
2023-01-07 08:40:39,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,177 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 14775953408.0
2023-01-07 08:40:39,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 11838268416.0
2023-01-07 08:40:39,178 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,179 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 14775953408.0
2023-01-07 08:40:39,179 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,179 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -897810.125
2023-01-07 08:40:39,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,180 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 23517552640.0
2023-01-07 08:40:39,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5214865504796383e+23
2023-01-07 08:40:39,181 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,181 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -897810.125
2023-01-07 08:40:39,181 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.62215805053711
2023-01-07 08:40:39,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,182 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 23517552640.0
2023-01-07 08:40:39,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 23487170560.0
2023-01-07 08:40:39,183 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,183 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 23517552640.0
2023-01-07 08:40:39,183 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,184 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 55704064.0
2023-01-07 08:40:39,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,184 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 140634161152.0
2023-01-07 08:40:39,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 46933704704.0
2023-01-07 08:40:39,186 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,186 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 55704064.0
2023-01-07 08:40:39,186 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 7.18283748626709
2023-01-07 08:40:39,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,186 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 140634161152.0
2023-01-07 08:40:39,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 93756178432.0
2023-01-07 08:40:39,188 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,188 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 140634161152.0
2023-01-07 08:40:39,188 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,188 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 47706877952.0
2023-01-07 08:40:39,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,189 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 231305068544.0
2023-01-07 08:40:39,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 93003874304.0
2023-01-07 08:40:39,190 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,190 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47706877952.0
2023-01-07 08:40:39,190 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,190 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,190 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.088607788085938
2023-01-07 08:40:39,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,191 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 231305068544.0
2023-01-07 08:40:39,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 186008076288.0
2023-01-07 08:40:39,192 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,192 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 231305068544.0
2023-01-07 08:40:39,192 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,193 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 507707031552.0
2023-01-07 08:40:39,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,193 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 236322308096.0
2023-01-07 08:40:39,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 371984465920.0
2023-01-07 08:40:39,194 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,194 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 507707031552.0
2023-01-07 08:40:39,195 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -19.012502670288086
2023-01-07 08:40:39,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,195 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 236322308096.0
2023-01-07 08:40:39,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 372044922880.0
2023-01-07 08:40:39,196 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,197 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 236322308096.0
2023-01-07 08:40:39,197 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,197 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:39,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,197 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1488303423488.0
2023-01-07 08:40:39,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 744089452544.0
2023-01-07 08:40:39,198 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,198 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.35942065715789795
2023-01-07 08:40:39,199 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:39,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,199 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1488303423488.0
2023-01-07 08:40:39,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 744213970944.0
2023-01-07 08:40:39,200 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,201 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 1488303423488.0
2023-01-07 08:40:39,201 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:39,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,201 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2595742023680.0
2023-01-07 08:40:39,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 1106772492288.0
2023-01-07 08:40:39,202 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,202 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.14039796590805054
2023-01-07 08:40:39,203 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,203 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:39,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,203 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2595742023680.0
2023-01-07 08:40:39,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 1488969531392.0
2023-01-07 08:40:39,204 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,204 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2595742023680.0
2023-01-07 08:40:39,204 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:39,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,205 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5958124175360.0
2023-01-07 08:40:39,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 2977939062784.0
2023-01-07 08:40:39,206 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,206 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.2711206078529358
2023-01-07 08:40:39,206 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:39,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,207 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5958124175360.0
2023-01-07 08:40:39,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 2980185636864.0
2023-01-07 08:40:39,208 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,208 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 5958124175360.0
2023-01-07 08:40:39,208 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,209 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 8123559444480.0
2023-01-07 08:40:39,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,209 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 9757511385088.0
2023-01-07 08:40:39,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 5634600881618944.0
2023-01-07 08:40:39,211 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,211 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 8123559444480.0
2023-01-07 08:40:39,211 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 18.518516540527344
2023-01-07 08:40:39,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 9757511385088.0
2023-01-07 08:40:39,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 11920696410112.0
2023-01-07 08:40:39,213 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,213 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 4624567381262336.0
2023-01-07 08:40:39,213 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,213 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:39,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,213 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -226885086740480.0
2023-01-07 08:40:39,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 11920721575936.0
2023-01-07 08:40:39,215 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,215 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 6.791311810101248e+16
2023-01-07 08:40:39,215 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:39,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,215 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -226885086740480.0
2023-01-07 08:40:39,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 23841443151872.0
2023-01-07 08:40:39,217 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,217 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -226885086740480.0
2023-01-07 08:40:39,217 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:39,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 5758780734177280.0
2023-01-07 08:40:39,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 47682886303744.0
2023-01-07 08:40:39,218 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,219 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 1.1269225385558016e+16
2023-01-07 08:40:39,219 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:39,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,219 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 5758780734177280.0
2023-01-07 08:40:39,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 47746245459968.0
2023-01-07 08:40:39,221 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,221 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 5758780734177280.0
2023-01-07 08:40:39,221 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:39,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,221 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 399376996368384.0
2023-01-07 08:40:39,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 95492490919936.0
2023-01-07 08:40:39,222 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,223 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 8123559444480.0
2023-01-07 08:40:39,223 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:39,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,223 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 399376996368384.0
2023-01-07 08:40:39,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 190756778147840.0
2023-01-07 08:40:39,224 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,225 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 399376996368384.0
2023-01-07 08:40:39,225 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:39,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,225 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 521724776939520.0
2023-01-07 08:40:39,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 228218372096.0
2023-01-07 08:40:39,226 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,226 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 2120254092214272.0
2023-01-07 08:40:39,227 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:39,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,227 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 521724776939520.0
2023-01-07 08:40:39,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.294805447888391e+19
2023-01-07 08:40:39,228 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,229 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 521724776939520.0
2023-01-07 08:40:39,229 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:39,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1919863534649344.0
2023-01-07 08:40:39,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.159271396362158e+18
2023-01-07 08:40:39,230 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,230 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 2975693537280.0
2023-01-07 08:40:39,231 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:39,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,231 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1919863534649344.0
2023-01-07 08:40:39,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 765966413725696.0
2023-01-07 08:40:39,232 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,233 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 1919863534649344.0
2023-01-07 08:40:39,233 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:39,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.3892328343207936e+16
2023-01-07 08:40:39,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.510438838125359e+22
2023-01-07 08:40:39,234 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,235 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 264016303751168.0
2023-01-07 08:40:39,235 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:39,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,235 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 1.3892328343207936e+16
2023-01-07 08:40:39,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 3063867802386432.0
2023-01-07 08:40:39,236 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,237 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 1.3892328343207936e+16
2023-01-07 08:40:39,237 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:39,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.0569723389411328e+16
2023-01-07 08:40:39,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 6124084345700352.0
2023-01-07 08:40:39,238 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,239 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 6931410422071296.0
2023-01-07 08:40:39,239 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:39,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,239 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.0569723389411328e+16
2023-01-07 08:40:39,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 6160160997244928.0
2023-01-07 08:40:39,240 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,241 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 1.0569723389411328e+16
2023-01-07 08:40:39,241 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:39,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.3485862301990912e+16
2023-01-07 08:40:39,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.032081974485058e+19
2023-01-07 08:40:39,243 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,243 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 2332007254196224.0
2023-01-07 08:40:39,243 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:39,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 1.3485862301990912e+16
2023-01-07 08:40:39,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -1184493731840.0
2023-01-07 08:40:39,245 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,245 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 1.3485862301990912e+16
2023-01-07 08:40:39,245 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:39,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.421729257291776e+16
2023-01-07 08:40:39,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.1679715039686164e+18
2023-01-07 08:40:39,247 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,247 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 866172497035264.0
2023-01-07 08:40:39,247 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:39,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.421729257291776e+16
2023-01-07 08:40:39,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.92789515157504e+16
2023-01-07 08:40:39,249 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,249 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 7.421729257291776e+16
2023-01-07 08:40:39,249 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:39,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 9.754810468060365e+16
2023-01-07 08:40:39,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 1303925632.0
2023-01-07 08:40:39,250 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,251 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 432578502852608.0
2023-01-07 08:40:39,251 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:39,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,251 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 9.754810468060365e+16
2023-01-07 08:40:39,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.75077835276288e+16
2023-01-07 08:40:39,252 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,253 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 9.754810468060365e+16
2023-01-07 08:40:39,253 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:39,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,253 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 5.85125247527682e+17
2023-01-07 08:40:39,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.950155670552576e+17
2023-01-07 08:40:39,254 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,254 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 743906213888.0
2023-01-07 08:40:39,254 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:39,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 5.85125247527682e+17
2023-01-07 08:40:39,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.900311341105152e+17
2023-01-07 08:40:39,256 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,256 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 5.85125247527682e+17
2023-01-07 08:40:39,256 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:39,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1.560165355811242e+18
2023-01-07 08:40:39,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.800622682210304e+17
2023-01-07 08:40:39,258 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,258 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 91389773742080.0
2023-01-07 08:40:39,258 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:39,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,259 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1.560165355811242e+18
2023-01-07 08:40:39,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.800622682210304e+17
2023-01-07 08:40:39,260 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,260 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 1.560165355811242e+18
2023-01-07 08:40:39,260 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:39,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.680379656640135e+18
2023-01-07 08:40:39,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5601245364420608e+18
2023-01-07 08:40:39,262 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,262 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 74127218049024.0
2023-01-07 08:40:39,262 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:39,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.680379656640135e+18
2023-01-07 08:40:39,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0398671484402644e+22
2023-01-07 08:40:39,264 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,264 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 4.680379656640135e+18
2023-01-07 08:40:39,264 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:39,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,265 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 3.1165239274892165e+18
2023-01-07 08:40:39,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 133948841984.0
2023-01-07 08:40:39,266 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,266 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 371952844800.0
2023-01-07 08:40:39,266 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:39,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,266 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 3.1165239274892165e+18
2023-01-07 08:40:39,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.116510458471776e+18
2023-01-07 08:40:39,268 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,268 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 3.1165239274892165e+18
2023-01-07 08:40:39,268 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:39,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,269 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1.2466048430956872e+19
2023-01-07 08:40:39,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.233021466699366e+18
2023-01-07 08:40:39,270 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,270 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 3434988437504.0
2023-01-07 08:40:39,270 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:39,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,270 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1.2466048430956872e+19
2023-01-07 08:40:39,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.233020916943553e+18
2023-01-07 08:40:39,272 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,272 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 1.2466048430956872e+19
2023-01-07 08:40:39,272 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:39,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,272 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 3.739813319824271e+19
2023-01-07 08:40:39,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2466041833887105e+19
2023-01-07 08:40:39,274 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,275 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -6305241104384.0
2023-01-07 08:40:39,275 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:39,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,275 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 3.739813319824271e+19
2023-01-07 08:40:39,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.493208366777421e+19
2023-01-07 08:40:39,277 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,277 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 3.739813319824271e+19
2023-01-07 08:40:39,277 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:39,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,278 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.986417173359493e+19
2023-01-07 08:40:39,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.986416733554842e+19
2023-01-07 08:40:39,279 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,279 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 3911763886080.0
2023-01-07 08:40:39,279 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:39,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,280 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.986417173359493e+19
2023-01-07 08:40:39,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 12994258944.0
2023-01-07 08:40:39,281 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,281 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 4.986417173359493e+19
2023-01-07 08:40:39,281 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:39,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,281 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 2.991850304015696e+20
2023-01-07 08:40:39,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.972833467109684e+19
2023-01-07 08:40:39,282 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,283 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93003907072.0
2023-01-07 08:40:39,283 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:39,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,283 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 2.991850304015696e+20
2023-01-07 08:40:39,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9945666934219368e+20
2023-01-07 08:40:39,284 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,285 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 2.991850304015696e+20
2023-01-07 08:40:39,285 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:39,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,285 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 9.73791283391028e+24
2023-01-07 08:40:39,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9891333868438736e+20
2023-01-07 08:40:39,286 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,286 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 238424408064.0
2023-01-07 08:40:39,287 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:39,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,287 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 9.73791283391028e+24
2023-01-07 08:40:39,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 2804946504253440.0
2023-01-07 08:40:39,288 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,288 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 9.73791283391028e+24
2023-01-07 08:40:39,289 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:39,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,289 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.332915390917831e+24
2023-01-07 08:40:39,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.978294217497977e+20
2023-01-07 08:40:39,290 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,290 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -1.4617652525698007e+24
2023-01-07 08:40:39,290 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,291 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:39,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,291 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 6.332915390917831e+24
2023-01-07 08:40:39,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5956588434995953e+21
2023-01-07 08:40:39,292 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,292 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 6.332915390917831e+24
2023-01-07 08:40:39,292 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:39,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,293 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.2234924063645844e+24
2023-01-07 08:40:39,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1913176869991906e+21
2023-01-07 08:40:39,294 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,294 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 2.4343784807674213e+24
2023-01-07 08:40:39,294 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:39,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,295 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.2234924063645844e+24
2023-01-07 08:40:39,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1117919988791755e+21
2023-01-07 08:40:39,296 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,296 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 1.2234924063645844e+24
2023-01-07 08:40:39,296 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:39,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,297 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 8.073512979994541e+23
2023-01-07 08:40:39,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2708436565941824e+21
2023-01-07 08:40:39,298 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,298 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -1.827206565712251e+23
2023-01-07 08:40:39,298 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:39,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,299 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 8.073512979994541e+23
2023-01-07 08:40:39,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2765270747996762e+22
2023-01-07 08:40:39,300 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,300 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 8.073512979994541e+23
2023-01-07 08:40:39,300 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:39,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,301 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.243977152555014e+23
2023-01-07 08:40:39,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5530541495993525e+22
2023-01-07 08:40:39,302 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,302 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -9.136032828561254e+22
2023-01-07 08:40:39,302 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:39,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,302 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 6.243977152555014e+23
2023-01-07 08:40:39,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.106090735160158e+22
2023-01-07 08:40:39,304 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,304 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 6.243977152555014e+23
2023-01-07 08:40:39,304 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:39,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,305 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4.0848761910078285e+23
2023-01-07 08:40:39,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0212181470320316e+23
2023-01-07 08:40:39,306 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,306 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 4.0848725881281266e+23
2023-01-07 08:40:39,306 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:39,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,306 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4.0848761910078285e+23
2023-01-07 08:40:39,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.021221659839741e+23
2023-01-07 08:40:39,308 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,308 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 4.0848761910078285e+23
2023-01-07 08:40:39,308 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,309 > [DEBUG] 0 :: 7.21522855758667
2023-01-07 08:40:39,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,312 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:40:39,312 > [DEBUG] 0 :: before allreduce fusion buffer :: -314.1319274902344
2023-01-07 08:40:39,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,315 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.259221613407135
2023-01-07 08:40:39,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,316 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:40:39,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -215.39984130859375
2023-01-07 08:40:39,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,320 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.315006256103516
2023-01-07 08:40:39,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.027685947716236115
2023-01-07 08:40:39,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,321 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.001276991330087185
2023-01-07 08:40:39,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,321 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.315006256103516
2023-01-07 08:40:39,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34593886137008667
2023-01-07 08:40:39,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,323 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 15.173887252807617
2023-01-07 08:40:39,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3216548264026642
2023-01-07 08:40:39,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,324 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.0035372860729694366
2023-01-07 08:40:39,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,324 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 15.173887252807617
2023-01-07 08:40:39,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4700819253921509
2023-01-07 08:40:39,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,326 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.035327434539795
2023-01-07 08:40:39,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2340736985206604
2023-01-07 08:40:39,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,327 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.21467237174510956
2023-01-07 08:40:39,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,327 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.035327434539795
2023-01-07 08:40:39,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0514767169952393
2023-01-07 08:40:39,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,329 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.7784652709960938
2023-01-07 08:40:39,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3260541558265686
2023-01-07 08:40:39,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,330 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.00023891963064670563
2023-01-07 08:40:39,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,330 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.7784652709960938
2023-01-07 08:40:39,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.078857421875
2023-01-07 08:40:39,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,332 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.715571403503418
2023-01-07 08:40:39,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31773900985717773
2023-01-07 08:40:39,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,333 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.007308892905712128
2023-01-07 08:40:39,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,333 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.715571403503418
2023-01-07 08:40:39,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7316818833351135
2023-01-07 08:40:39,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,335 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.5360915660858154
2023-01-07 08:40:39,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6747918128967285
2023-01-07 08:40:39,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,336 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.14022257924079895
2023-01-07 08:40:39,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,337 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.5360915660858154
2023-01-07 08:40:39,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4327373504638672
2023-01-07 08:40:39,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,338 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.19304656982422
2023-01-07 08:40:39,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04571196436882019
2023-01-07 08:40:39,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,339 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.041563138365745544
2023-01-07 08:40:39,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,340 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.19304656982422
2023-01-07 08:40:39,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.107733130455017
2023-01-07 08:40:39,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,341 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.942434310913086
2023-01-07 08:40:39,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39240214228630066
2023-01-07 08:40:39,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,342 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.07343257963657379
2023-01-07 08:40:39,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,343 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.942434310913086
2023-01-07 08:40:39,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7273725271224976
2023-01-07 08:40:39,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,344 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 49.871707916259766
2023-01-07 08:40:39,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7593934535980225
2023-01-07 08:40:39,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,345 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.02173428423702717
2023-01-07 08:40:39,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,346 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 49.871707916259766
2023-01-07 08:40:39,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15483951568603516
2023-01-07 08:40:39,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,347 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.347164154052734
2023-01-07 08:40:39,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7987687587738037
2023-01-07 08:40:39,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.12608911097049713
2023-01-07 08:40:39,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,349 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.347164154052734
2023-01-07 08:40:39,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.977524757385254
2023-01-07 08:40:39,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.9066076278686523
2023-01-07 08:40:39,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23911318182945251
2023-01-07 08:40:39,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.028301436454057693
2023-01-07 08:40:39,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.9066076278686523
2023-01-07 08:40:39,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.992889165878296
2023-01-07 08:40:39,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 16.10890007019043
2023-01-07 08:40:39,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23670172691345215
2023-01-07 08:40:39,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.012874804437160492
2023-01-07 08:40:39,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 16.10890007019043
2023-01-07 08:40:39,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44135764241218567
2023-01-07 08:40:39,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -8.613632202148438
2023-01-07 08:40:39,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.498004674911499
2023-01-07 08:40:39,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,358 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.03653404116630554
2023-01-07 08:40:39,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,358 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -8.613632202148438
2023-01-07 08:40:39,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.526473343372345
2023-01-07 08:40:39,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 7.957001686096191
2023-01-07 08:40:39,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0213472843170166
2023-01-07 08:40:39,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.07950164377689362
2023-01-07 08:40:39,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 7.957001686096191
2023-01-07 08:40:39,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8944830894470215
2023-01-07 08:40:39,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 5.821473121643066
2023-01-07 08:40:39,363 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29748857021331787
2023-01-07 08:40:39,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.04653240740299225
2023-01-07 08:40:39,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,364 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 5.821473121643066
2023-01-07 08:40:39,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.43190336227417
2023-01-07 08:40:39,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,366 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -3.3521666526794434
2023-01-07 08:40:39,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0405884981155396
2023-01-07 08:40:39,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.0701301097869873
2023-01-07 08:40:39,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -3.3521666526794434
2023-01-07 08:40:39,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6229138374328613
2023-01-07 08:40:39,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 21.993391036987305
2023-01-07 08:40:39,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2803758680820465
2023-01-07 08:40:39,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.12472353875637054
2023-01-07 08:40:39,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 21.993391036987305
2023-01-07 08:40:39,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8703069686889648
2023-01-07 08:40:39,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.99542498588562
2023-01-07 08:40:39,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5729455947875977
2023-01-07 08:40:39,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.10482916235923767
2023-01-07 08:40:39,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,374 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.99542498588562
2023-01-07 08:40:39,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3113080859184265
2023-01-07 08:40:39,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 19.753643035888672
2023-01-07 08:40:39,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6422914266586304
2023-01-07 08:40:39,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.012139618396759033
2023-01-07 08:40:39,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 19.753643035888672
2023-01-07 08:40:39,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6660849452018738
2023-01-07 08:40:39,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.829530715942383
2023-01-07 08:40:39,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9311624765396118
2023-01-07 08:40:39,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.021216168999671936
2023-01-07 08:40:39,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.829530715942383
2023-01-07 08:40:39,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8913869857788086
2023-01-07 08:40:39,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,381 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.728317260742188
2023-01-07 08:40:39,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7171732187271118
2023-01-07 08:40:39,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.01752983033657074
2023-01-07 08:40:39,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 27.728317260742188
2023-01-07 08:40:39,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.3611602783203125
2023-01-07 08:40:39,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,384 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 3.090630531311035
2023-01-07 08:40:39,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2636730670928955
2023-01-07 08:40:39,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,385 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.12776143848896027
2023-01-07 08:40:39,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,386 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 3.090630531311035
2023-01-07 08:40:39,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9134643077850342
2023-01-07 08:40:39,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,387 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 22.65734100341797
2023-01-07 08:40:39,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5014969110488892
2023-01-07 08:40:39,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,388 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.16490048170089722
2023-01-07 08:40:39,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,389 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 22.65734100341797
2023-01-07 08:40:39,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.606217384338379
2023-01-07 08:40:39,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 30.645263671875
2023-01-07 08:40:39,391 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.90665864944458
2023-01-07 08:40:39,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,392 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 30.645263671875
2023-01-07 08:40:39,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.554668426513672
2023-01-07 08:40:39,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 63057065672704.0
2023-01-07 08:40:39,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 46205115564032.0
2023-01-07 08:40:39,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,394 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 3108705650343936.0
2023-01-07 08:40:39,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 63057065672704.0
2023-01-07 08:40:39,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 3125558028271616.0
2023-01-07 08:40:39,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,396 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4054974187175936.0
2023-01-07 08:40:39,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 4021995012358144.0
2023-01-07 08:40:39,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 3059020663357440.0
2023-01-07 08:40:39,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,398 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4054974187175936.0
2023-01-07 08:40:39,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 3092000106610688.0
2023-01-07 08:40:39,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,399 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1588998178865152.0
2023-01-07 08:40:39,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 6004278380462080.0
2023-01-07 08:40:39,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,400 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 1.6669588414529536e+16
2023-01-07 08:40:39,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,401 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1588998178865152.0
2023-01-07 08:40:39,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2254307273408512e+16
2023-01-07 08:40:39,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.936177297247437e+16
2023-01-07 08:40:39,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 3275085972504576.0
2023-01-07 08:40:39,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.936177297247437e+16
2023-01-07 08:40:39,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.608668646309888e+16
2023-01-07 08:40:39,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.219296227203482e+16
2023-01-07 08:40:39,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.191029758938317e+16
2023-01-07 08:40:39,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,406 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.219296227203482e+16
2023-01-07 08:40:39,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 282667937431552.0
2023-01-07 08:40:39,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,407 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.856216473758597e+17
2023-01-07 08:40:39,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.062776131551232e+16
2023-01-07 08:40:39,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,409 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.856216473758597e+17
2023-01-07 08:40:39,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.949938860603474e+17
2023-01-07 08:40:39,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,410 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.748423774052024e+17
2023-01-07 08:40:39,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 42775261216768.0
2023-01-07 08:40:39,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,411 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.748423774052024e+17
2023-01-07 08:40:39,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7479953081145754e+17
2023-01-07 08:40:39,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.491683279427338e+17
2023-01-07 08:40:39,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.485550753323418e+17
2023-01-07 08:40:39,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,414 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.491683279427338e+17
2023-01-07 08:40:39,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 613376359137280.0
2023-01-07 08:40:39,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,415 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2479409088021135e+18
2023-01-07 08:40:39,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3748473438740152e+18
2023-01-07 08:40:39,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,416 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2479409088021135e+18
2023-01-07 08:40:39,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.873093702367052e+18
2023-01-07 08:40:39,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,418 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.688366287252619e+18
2023-01-07 08:40:39,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.688366287252619e+18
2023-01-07 08:40:39,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,419 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.688366287252619e+18
2023-01-07 08:40:39,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 52192870400.0
2023-01-07 08:40:39,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,420 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1432565774963704e+19
2023-01-07 08:40:39,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1432472316475343e+19
2023-01-07 08:40:39,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,422 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 97706915659776.0
2023-01-07 08:40:39,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,422 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1432565774963704e+19
2023-01-07 08:40:39,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 190982096158720.0
2023-01-07 08:40:39,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 6.4512142715757e+19
2023-01-07 08:40:39,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1491243392509673e+19
2023-01-07 08:40:39,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,425 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 6.4512142715757e+19
2023-01-07 08:40:39,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.302089492520082e+19
2023-01-07 08:40:39,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,426 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.57971728194386e+20
2023-01-07 08:40:39,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.59649735700387e+19
2023-01-07 08:40:39,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,427 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.57971728194386e+20
2023-01-07 08:40:39,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.720067898087194e+20
2023-01-07 08:40:39,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,429 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 7.738503573775843e+18
2023-01-07 08:40:39,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.579634873547358e+18
2023-01-07 08:40:39,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,430 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 3.414343540370949e+20
2023-01-07 08:40:39,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,430 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 7.738503573775843e+18
2023-01-07 08:40:39,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.465931570415036e+20
2023-01-07 08:40:39,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,432 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.4537077254543125e+21
2023-01-07 08:40:39,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.802606353169085e+20
2023-01-07 08:40:39,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,433 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.4537077254543125e+21
2023-01-07 08:40:39,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.573447090137404e+21
2023-01-07 08:40:39,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,435 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.292396748264759e+21
2023-01-07 08:40:39,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.145966157276593e+21
2023-01-07 08:40:39,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,436 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.292396748264759e+21
2023-01-07 08:40:39,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.146430309513189e+21
2023-01-07 08:40:39,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.292159183384415e+21
2023-01-07 08:40:39,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,439 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.5362904228867604e+19
2023-01-07 08:40:39,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,439 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2562451138778132e+22
2023-01-07 08:40:39,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,440 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2562741620954098e+22
2023-01-07 08:40:39,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,442 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 3.500794357342976e+22
2023-01-07 08:40:39,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.882456953821845e+21
2023-01-07 08:40:39,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5125483241908195e+22
2023-01-07 08:40:39,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.02489939071796e+22
2023-01-07 08:40:39,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.882456953821845e+21
2023-01-07 08:40:39,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 1532055636672512.0
2023-01-07 08:40:39,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,447 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0049840214552492e+23
2023-01-07 08:40:39,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,448 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 7.24783878691113e+22
2023-01-07 08:40:39,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,448 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0099837154372017e+23
2023-01-07 08:40:39,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,450 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.0199674308744034e+23
2023-01-07 08:40:39,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,451 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 4157447811891200.0
2023-01-07 08:40:39,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,451 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -1094918428688384.0
2023-01-07 08:40:39,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 3062529651638272.0
2023-01-07 08:40:39,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,453 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.894534613065269e+18
2023-01-07 08:40:39,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,454 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 1.2130618219647992e+19
2023-01-07 08:40:39,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,454 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2130618219647992e+19
2023-01-07 08:40:39,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,456 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4809762449428316e+18
2023-01-07 08:40:39,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,457 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.68359254346637e+19
2023-01-07 08:40:39,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,458 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.367178050058322e+19
2023-01-07 08:40:39,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,460 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.362546035270248e+20
2023-01-07 08:40:39,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,460 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -2.8911002943439766e+19
2023-01-07 08:40:39,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,461 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,461 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0734360498163155e+20
2023-01-07 08:40:39,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,462 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3037430099246947e+20
2023-01-07 08:40:39,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,464 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.3990294319687858e+21
2023-01-07 08:40:39,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8283006761728528e+20
2023-01-07 08:40:39,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,465 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.5053922911692993e+20
2023-01-07 08:40:39,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,466 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.3990294319687858e+21
2023-01-07 08:40:39,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,467 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1161992939827563e+21
2023-01-07 08:40:39,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,468 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8981134168618917e+21
2023-01-07 08:40:39,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,469 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.411247210374103e+17
2023-01-07 08:40:39,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,470 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7994438112326095e+21
2023-01-07 08:40:39,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,471 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.598887622465219e+21
2023-01-07 08:40:39,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,472 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0839852022284943e+18
2023-01-07 08:40:39,477 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:40:39,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,478 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 1.2898118017765868e+18
2023-01-07 08:40:39,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,479 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,479 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,480 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.292396748264759e+21
2023-01-07 08:40:39,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,481 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.4537077254543125e+21
2023-01-07 08:40:39,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,482 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 7.738503573775843e+18
2023-01-07 08:40:39,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,482 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.57971728194386e+20
2023-01-07 08:40:39,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,483 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 6.4512142715757e+19
2023-01-07 08:40:39,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,484 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1432565774963704e+19
2023-01-07 08:40:39,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,485 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.688366287252619e+18
2023-01-07 08:40:39,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,486 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2479409088021135e+18
2023-01-07 08:40:39,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,486 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.491683279427338e+17
2023-01-07 08:40:39,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,487 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.748423774052024e+17
2023-01-07 08:40:39,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,487 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.856216473758597e+17
2023-01-07 08:40:39,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.219296227203482e+16
2023-01-07 08:40:39,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.936177297247437e+16
2023-01-07 08:40:39,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1588998178865152.0
2023-01-07 08:40:39,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4054974187175936.0
2023-01-07 08:40:39,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 63057065672704.0
2023-01-07 08:40:39,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 8128.58837890625
2023-01-07 08:40:39,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 2528.9775390625
2023-01-07 08:40:39,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 6.6566925048828125
2023-01-07 08:40:39,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -48.76078796386719
2023-01-07 08:40:39,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -30.822715759277344
2023-01-07 08:40:39,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -43.2907829284668
2023-01-07 08:40:39,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,492 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2.656646728515625
2023-01-07 08:40:39,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2898118017765868e+18
2023-01-07 08:40:39,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 21.993391036987305
2023-01-07 08:40:39,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -3.3521666526794434
2023-01-07 08:40:39,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 5.821473121643066
2023-01-07 08:40:39,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 7.957001686096191
2023-01-07 08:40:39,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -8.613632202148438
2023-01-07 08:40:39,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 16.10890007019043
2023-01-07 08:40:39,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.9066076278686523
2023-01-07 08:40:39,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,497 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.347164154052734
2023-01-07 08:40:39,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,498 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 49.871707916259766
2023-01-07 08:40:39,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 95.86241149902344
2023-01-07 08:40:39,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,499 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.942434310913086
2023-01-07 08:40:39,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,499 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.19304656982422
2023-01-07 08:40:39,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,500 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.5360915660858154
2023-01-07 08:40:39,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,500 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.715571403503418
2023-01-07 08:40:39,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.8644962310791
2023-01-07 08:40:39,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,501 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.7784652709960938
2023-01-07 08:40:39,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,502 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.035327434539795
2023-01-07 08:40:39,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,502 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 15.173887252807617
2023-01-07 08:40:39,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 2589.65673828125
2023-01-07 08:40:39,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,503 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.315006256103516
2023-01-07 08:40:39,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,503 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:40:39,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 545.3154907226562
2023-01-07 08:40:39,504 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,505 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 1.2898118017765868e+18
2023-01-07 08:40:39,505 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,505 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 0.8436784744262695
2023-01-07 08:40:39,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,505 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -182.79940795898438
2023-01-07 08:40:39,507 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,507 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.8436784744262695
2023-01-07 08:40:39,507 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.8936614990234375
2023-01-07 08:40:39,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,507 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -79.00205993652344
2023-01-07 08:40:39,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,508 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.57410430908203
2023-01-07 08:40:39,509 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,510 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -79.00205993652344
2023-01-07 08:40:39,510 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,510 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:39,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,510 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.650609970092773
2023-01-07 08:40:39,511 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,511 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 2.411247210374103e+17
2023-01-07 08:40:39,511 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 3.18812894821167
2023-01-07 08:40:39,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,512 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.3298287526494756e+22
2023-01-07 08:40:39,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.3775634765625
2023-01-07 08:40:39,513 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,513 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 1.3298287526494756e+22
2023-01-07 08:40:39,513 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,513 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,514 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 53.144126892089844
2023-01-07 08:40:39,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,514 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -88.13404083251953
2023-01-07 08:40:39,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,515 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,515 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.978263854980469
2023-01-07 08:40:39,516 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,516 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 53.144126892089844
2023-01-07 08:40:39,516 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,517 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.244407653808594
2023-01-07 08:40:39,518 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,518 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -88.13404083251953
2023-01-07 08:40:39,518 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,519 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 436.0135498046875
2023-01-07 08:40:39,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,519 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -2.8911002943439766e+19
2023-01-07 08:40:39,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 475.4151306152344
2023-01-07 08:40:39,520 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,520 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 436.0135498046875
2023-01-07 08:40:39,520 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,521 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 1160.103271484375
2023-01-07 08:40:39,522 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,522 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1.3990294319687858e+21
2023-01-07 08:40:39,522 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,523 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 421.09954833984375
2023-01-07 08:40:39,524 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,524 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.362546035270248e+20
2023-01-07 08:40:39,524 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,525 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 2494.351318359375
2023-01-07 08:40:39,526 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,526 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -2.8911002943439766e+19
2023-01-07 08:40:39,526 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,527 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 911.3736572265625
2023-01-07 08:40:39,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,527 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 4724.68017578125
2023-01-07 08:40:39,528 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,528 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 911.3736572265625
2023-01-07 08:40:39,528 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.379749298095703
2023-01-07 08:40:39,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,529 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 2.68359254346637e+19
2023-01-07 08:40:39,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,529 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.35870361328125
2023-01-07 08:40:39,531 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,531 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 2.68359254346637e+19
2023-01-07 08:40:39,531 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,531 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,531 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 924.1024780273438
2023-01-07 08:40:39,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,532 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 1.2130618219647992e+19
2023-01-07 08:40:39,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 9197.4130859375
2023-01-07 08:40:39,533 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,533 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 924.1024780273438
2023-01-07 08:40:39,533 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,534 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.64407348632812
2023-01-07 08:40:39,535 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,535 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 1.2130618219647992e+19
2023-01-07 08:40:39,535 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:39,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,536 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -1094918428688384.0
2023-01-07 08:40:39,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.991276741027832
2023-01-07 08:40:39,537 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,537 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 4157447811891200.0
2023-01-07 08:40:39,537 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -2.900054931640625
2023-01-07 08:40:39,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,537 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -1094918428688384.0
2023-01-07 08:40:39,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,538 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.57323455810547
2023-01-07 08:40:39,539 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,539 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -1094918428688384.0
2023-01-07 08:40:39,539 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,540 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.94773864746094
2023-01-07 08:40:39,541 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,541 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 7.24783878691113e+22
2023-01-07 08:40:39,541 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.983715057373047
2023-01-07 08:40:39,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,542 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.317581102560996e+23
2023-01-07 08:40:39,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.683174133300781
2023-01-07 08:40:39,543 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,543 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 6.317581102560996e+23
2023-01-07 08:40:39,543 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:40:39,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,544 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1.7088842391967773
2023-01-07 08:40:39,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,544 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -26.69051742553711
2023-01-07 08:40:39,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.882456953821845e+21
2023-01-07 08:40:39,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.161348342895508
2023-01-07 08:40:39,545 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,546 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.7088842391967773
2023-01-07 08:40:39,546 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.8211555480957
2023-01-07 08:40:39,547 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,548 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -26.69051742553711
2023-01-07 08:40:39,548 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -9.872394561767578
2023-01-07 08:40:39,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -9.882456953821845e+21
2023-01-07 08:40:39,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.376789093017578
2023-01-07 08:40:39,549 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,549 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 3.500794357342976e+22
2023-01-07 08:40:39,550 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.16038513183594
2023-01-07 08:40:39,551 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,552 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -9.882456953821845e+21
2023-01-07 08:40:39,552 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,552 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.61878204345703
2023-01-07 08:40:39,553 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,553 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 1.5362904228867604e+19
2023-01-07 08:40:39,553 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 29.14023208618164
2023-01-07 08:40:39,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,554 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.165099082736673e+22
2023-01-07 08:40:39,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.565412998199463
2023-01-07 08:40:39,555 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,555 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 8.165099082736673e+22
2023-01-07 08:40:39,556 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.38044342398643494
2023-01-07 08:40:39,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.292396748264759e+21
2023-01-07 08:40:39,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.814578056335449
2023-01-07 08:40:39,557 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,558 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.38044342398643494
2023-01-07 08:40:39,558 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 4.868085861206055
2023-01-07 08:40:39,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 6.292396748264759e+21
2023-01-07 08:40:39,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.937469482421875
2023-01-07 08:40:39,559 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,560 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 6.292396748264759e+21
2023-01-07 08:40:39,560 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1.3044395446777344
2023-01-07 08:40:39,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.4537077254543125e+21
2023-01-07 08:40:39,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.886884689331055
2023-01-07 08:40:39,562 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,562 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -1.3044395446777344
2023-01-07 08:40:39,562 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -12.447158813476562
2023-01-07 08:40:39,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2.4537077254543125e+21
2023-01-07 08:40:39,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.86746597290039
2023-01-07 08:40:39,563 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,564 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2.4537077254543125e+21
2023-01-07 08:40:39,564 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:39,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 7.738503573775843e+18
2023-01-07 08:40:39,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.103118419647217
2023-01-07 08:40:39,565 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,565 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 3.414343540370949e+20
2023-01-07 08:40:39,565 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,566 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.229381561279297
2023-01-07 08:40:39,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 7.738503573775843e+18
2023-01-07 08:40:39,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.072235107421875
2023-01-07 08:40:39,567 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,567 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 7.738503573775843e+18
2023-01-07 08:40:39,568 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,568 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.5904830694198608
2023-01-07 08:40:39,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,568 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.57971728194386e+20
2023-01-07 08:40:39,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.105262279510498
2023-01-07 08:40:39,569 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,570 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.5904830694198608
2023-01-07 08:40:39,570 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,570 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,570 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -32.070369720458984
2023-01-07 08:40:39,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.57971728194386e+20
2023-01-07 08:40:39,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.494112014770508
2023-01-07 08:40:39,571 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,572 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 2.57971728194386e+20
2023-01-07 08:40:39,572 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.6703977584838867
2023-01-07 08:40:39,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,573 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 6.4512142715757e+19
2023-01-07 08:40:39,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.171429634094238
2023-01-07 08:40:39,574 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,574 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.6703977584838867
2023-01-07 08:40:39,574 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,574 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,574 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.4443514347076416
2023-01-07 08:40:39,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,574 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 6.4512142715757e+19
2023-01-07 08:40:39,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0828857421875
2023-01-07 08:40:39,576 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,576 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 6.4512142715757e+19
2023-01-07 08:40:39,576 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,576 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,576 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:39,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,577 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1432565774963704e+19
2023-01-07 08:40:39,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6224627494812012
2023-01-07 08:40:39,578 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,578 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 97706915659776.0
2023-01-07 08:40:39,578 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -1.3776965141296387
2023-01-07 08:40:39,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,578 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1432565774963704e+19
2023-01-07 08:40:39,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.947137832641602
2023-01-07 08:40:39,580 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,580 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 1.1432565774963704e+19
2023-01-07 08:40:39,580 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,580 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.5503236055374146
2023-01-07 08:40:39,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,581 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.688366287252619e+18
2023-01-07 08:40:39,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.791954040527344
2023-01-07 08:40:39,582 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,582 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.5503236055374146
2023-01-07 08:40:39,582 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.05154252052307129
2023-01-07 08:40:39,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.688366287252619e+18
2023-01-07 08:40:39,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.332880020141602
2023-01-07 08:40:39,584 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,584 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 5.688366287252619e+18
2023-01-07 08:40:39,584 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,584 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,584 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.5601742267608643
2023-01-07 08:40:39,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2479409088021135e+18
2023-01-07 08:40:39,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0417561531066895
2023-01-07 08:40:39,586 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,586 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.5601742267608643
2023-01-07 08:40:39,586 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,586 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 5.814935684204102
2023-01-07 08:40:39,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,587 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 4.2479409088021135e+18
2023-01-07 08:40:39,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.35573959350586
2023-01-07 08:40:39,588 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,588 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 4.2479409088021135e+18
2023-01-07 08:40:39,588 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,589 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2502504587173462
2023-01-07 08:40:39,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,589 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.491683279427338e+17
2023-01-07 08:40:39,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8307647705078125
2023-01-07 08:40:39,590 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,590 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.2502504587173462
2023-01-07 08:40:39,590 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 64.07452392578125
2023-01-07 08:40:39,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,591 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.491683279427338e+17
2023-01-07 08:40:39,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.42087173461914
2023-01-07 08:40:39,592 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,592 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 7.491683279427338e+17
2023-01-07 08:40:39,592 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,593 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.3069208860397339
2023-01-07 08:40:39,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,593 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.748423774052024e+17
2023-01-07 08:40:39,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.345331192016602
2023-01-07 08:40:39,594 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,595 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.3069208860397339
2023-01-07 08:40:39,595 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -17.62215805053711
2023-01-07 08:40:39,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,595 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.748423774052024e+17
2023-01-07 08:40:39,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2384704351425171
2023-01-07 08:40:39,597 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,597 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 3.748423774052024e+17
2023-01-07 08:40:39,597 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:40:39,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.31576108932495117
2023-01-07 08:40:39,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,598 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.856216473758597e+17
2023-01-07 08:40:39,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.262504577636719
2023-01-07 08:40:39,599 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,599 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.31576108932495117
2023-01-07 08:40:39,599 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 7.18283748626709
2023-01-07 08:40:39,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,600 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 2.856216473758597e+17
2023-01-07 08:40:39,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34771907329559326
2023-01-07 08:40:39,601 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,601 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 2.856216473758597e+17
2023-01-07 08:40:39,602 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,602 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,602 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:40:39,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,602 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.9240702390670776
2023-01-07 08:40:39,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.219296227203482e+16
2023-01-07 08:40:39,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.758993148803711
2023-01-07 08:40:39,603 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,604 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.9240702390670776
2023-01-07 08:40:39,604 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,604 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,604 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.088607788085938
2023-01-07 08:40:39,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.219296227203482e+16
2023-01-07 08:40:39,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.4674787521362305
2023-01-07 08:40:39,606 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,606 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 5.219296227203482e+16
2023-01-07 08:40:39,606 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,606 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,606 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.22358714044094086
2023-01-07 08:40:39,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,607 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.936177297247437e+16
2023-01-07 08:40:39,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5326308012008667
2023-01-07 08:40:39,608 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,608 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.22358714044094086
2023-01-07 08:40:39,608 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,608 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -19.012502670288086
2023-01-07 08:40:39,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 2.936177297247437e+16
2023-01-07 08:40:39,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.247707843780518
2023-01-07 08:40:39,610 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,610 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 2.936177297247437e+16
2023-01-07 08:40:39,610 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,610 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,610 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:39,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1588998178865152.0
2023-01-07 08:40:39,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.162960946559906
2023-01-07 08:40:39,612 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,612 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 1.6669588414529536e+16
2023-01-07 08:40:39,612 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.06409454345703
2023-01-07 08:40:39,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1588998178865152.0
2023-01-07 08:40:39,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5270872116088867
2023-01-07 08:40:39,614 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,614 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 1588998178865152.0
2023-01-07 08:40:39,614 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,614 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:39,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4054974187175936.0
2023-01-07 08:40:39,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.685779571533203
2023-01-07 08:40:39,615 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,616 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 3059020663357440.0
2023-01-07 08:40:39,616 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 28.751176834106445
2023-01-07 08:40:39,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4054974187175936.0
2023-01-07 08:40:39,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3922781944274902
2023-01-07 08:40:39,617 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,617 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 4054974187175936.0
2023-01-07 08:40:39,617 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:39,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 63057065672704.0
2023-01-07 08:40:39,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2342910766601562
2023-01-07 08:40:39,619 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,619 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 3108705650343936.0
2023-01-07 08:40:39,619 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,620 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 42.98280334472656
2023-01-07 08:40:39,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 63057065672704.0
2023-01-07 08:40:39,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8902865052223206
2023-01-07 08:40:39,621 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,621 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 63057065672704.0
2023-01-07 08:40:39,621 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:40:39,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.09377890825271606
2023-01-07 08:40:39,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 8128.58837890625
2023-01-07 08:40:39,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.133568525314331
2023-01-07 08:40:39,623 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,623 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.09377890825271606
2023-01-07 08:40:39,624 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,624 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 18.518516540527344
2023-01-07 08:40:39,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,624 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 8128.58837890625
2023-01-07 08:40:39,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.618234157562256
2023-01-07 08:40:39,625 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,625 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 8128.58837890625
2023-01-07 08:40:39,626 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,626 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:39,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,626 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 2528.9775390625
2023-01-07 08:40:39,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7856571674346924
2023-01-07 08:40:39,627 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,627 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.16490048170089722
2023-01-07 08:40:39,627 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 18.28196907043457
2023-01-07 08:40:39,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 2528.9775390625
2023-01-07 08:40:39,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7900657653808594
2023-01-07 08:40:39,629 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,629 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 2528.9775390625
2023-01-07 08:40:39,629 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,629 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:39,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 6.6566925048828125
2023-01-07 08:40:39,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.574718475341797
2023-01-07 08:40:39,631 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,631 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.12776143848896027
2023-01-07 08:40:39,631 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 11.178434371948242
2023-01-07 08:40:39,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,632 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 6.6566925048828125
2023-01-07 08:40:39,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8312656283378601
2023-01-07 08:40:39,633 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,633 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 6.6566925048828125
2023-01-07 08:40:39,633 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:39,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -48.76078796386719
2023-01-07 08:40:39,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.109004020690918
2023-01-07 08:40:39,635 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,635 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.01752983033657074
2023-01-07 08:40:39,635 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,635 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -47.159332275390625
2023-01-07 08:40:39,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,636 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -48.76078796386719
2023-01-07 08:40:39,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.016238301992416382
2023-01-07 08:40:39,637 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,637 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -48.76078796386719
2023-01-07 08:40:39,637 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:39,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -30.822715759277344
2023-01-07 08:40:39,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5570151805877686
2023-01-07 08:40:39,639 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,639 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.021216168999671936
2023-01-07 08:40:39,639 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,639 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,639 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 21.993061065673828
2023-01-07 08:40:39,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -30.822715759277344
2023-01-07 08:40:39,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.812598705291748
2023-01-07 08:40:39,641 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,641 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -30.822715759277344
2023-01-07 08:40:39,641 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:39,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -43.2907829284668
2023-01-07 08:40:39,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.482941150665283
2023-01-07 08:40:39,642 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,643 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.012139618396759033
2023-01-07 08:40:39,643 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -72.74202728271484
2023-01-07 08:40:39,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -43.2907829284668
2023-01-07 08:40:39,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3805670738220215
2023-01-07 08:40:39,645 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,645 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -43.2907829284668
2023-01-07 08:40:39,645 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:39,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.818389892578125
2023-01-07 08:40:39,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13568487763404846
2023-01-07 08:40:39,646 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,647 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.10482916235923767
2023-01-07 08:40:39,647 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 40.97120666503906
2023-01-07 08:40:39,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 2.818389892578125
2023-01-07 08:40:39,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7698309421539307
2023-01-07 08:40:39,648 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,649 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 2.818389892578125
2023-01-07 08:40:39,649 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:39,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,649 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 8.679044723510742
2023-01-07 08:40:39,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7933192253112793
2023-01-07 08:40:39,650 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,650 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.12472353875637054
2023-01-07 08:40:39,651 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -2.0987744331359863
2023-01-07 08:40:39,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,651 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 8.679044723510742
2023-01-07 08:40:39,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3617643713951111
2023-01-07 08:40:39,652 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,652 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 8.679044723510742
2023-01-07 08:40:39,653 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:39,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 8.356270790100098
2023-01-07 08:40:39,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.968113422393799
2023-01-07 08:40:39,654 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,654 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0701301097869873
2023-01-07 08:40:39,654 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 76.03522491455078
2023-01-07 08:40:39,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 8.356270790100098
2023-01-07 08:40:39,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16874688863754272
2023-01-07 08:40:39,656 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,656 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 8.356270790100098
2023-01-07 08:40:39,656 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,657 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:39,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,657 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -6.0002360343933105
2023-01-07 08:40:39,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.669260263442993
2023-01-07 08:40:39,658 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,658 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.04653240740299225
2023-01-07 08:40:39,658 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -8.081640243530273
2023-01-07 08:40:39,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -6.0002360343933105
2023-01-07 08:40:39,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.03765606880188
2023-01-07 08:40:39,660 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,660 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -6.0002360343933105
2023-01-07 08:40:39,660 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:39,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -479.9512023925781
2023-01-07 08:40:39,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6947462558746338
2023-01-07 08:40:39,662 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,662 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.07950164377689362
2023-01-07 08:40:39,662 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -38.36809539794922
2023-01-07 08:40:39,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -479.9512023925781
2023-01-07 08:40:39,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4011928141117096
2023-01-07 08:40:39,664 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,664 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -479.9512023925781
2023-01-07 08:40:39,664 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,664 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,664 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:39,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 116.81686401367188
2023-01-07 08:40:39,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41442692279815674
2023-01-07 08:40:39,666 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,666 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.03653404116630554
2023-01-07 08:40:39,666 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 7.209291458129883
2023-01-07 08:40:39,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,666 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 116.81686401367188
2023-01-07 08:40:39,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8905916213989258
2023-01-07 08:40:39,668 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,668 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 116.81686401367188
2023-01-07 08:40:39,668 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:39,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,668 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 713.839111328125
2023-01-07 08:40:39,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2583569288253784
2023-01-07 08:40:39,669 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,670 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.012874804437160492
2023-01-07 08:40:39,670 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -11.830982208251953
2023-01-07 08:40:39,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 713.839111328125
2023-01-07 08:40:39,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3062387704849243
2023-01-07 08:40:39,671 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,672 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 713.839111328125
2023-01-07 08:40:39,672 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:39,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,672 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2305.520263671875
2023-01-07 08:40:39,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4575468599796295
2023-01-07 08:40:39,673 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,673 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.028301436454057693
2023-01-07 08:40:39,674 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 17.139808654785156
2023-01-07 08:40:39,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,674 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2305.520263671875
2023-01-07 08:40:39,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.634890079498291
2023-01-07 08:40:39,675 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,675 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 2305.520263671875
2023-01-07 08:40:39,676 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:39,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 8102.9375
2023-01-07 08:40:39,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.127448558807373
2023-01-07 08:40:39,677 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,677 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.12608911097049713
2023-01-07 08:40:39,677 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -14.549468994140625
2023-01-07 08:40:39,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,678 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 8102.9375
2023-01-07 08:40:39,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4168689250946045
2023-01-07 08:40:39,679 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,679 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 8102.9375
2023-01-07 08:40:39,679 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,679 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:39,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8006.7841796875
2023-01-07 08:40:39,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22064167261123657
2023-01-07 08:40:39,681 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,681 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.02173428423702717
2023-01-07 08:40:39,681 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 0.8757271766662598
2023-01-07 08:40:39,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,682 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8006.7841796875
2023-01-07 08:40:39,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.041361689567566
2023-01-07 08:40:39,683 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,683 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 8006.7841796875
2023-01-07 08:40:39,683 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,683 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:39,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,684 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.942434310913086
2023-01-07 08:40:39,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5359563827514648
2023-01-07 08:40:39,685 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,685 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.07343257963657379
2023-01-07 08:40:39,685 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 20.952308654785156
2023-01-07 08:40:39,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,685 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.942434310913086
2023-01-07 08:40:39,686 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006799372844398022
2023-01-07 08:40:39,687 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,687 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 9.942434310913086
2023-01-07 08:40:39,687 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:39,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,688 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.19304656982422
2023-01-07 08:40:39,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6790239214897156
2023-01-07 08:40:39,689 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,689 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.041563138365745544
2023-01-07 08:40:39,689 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.29376792907715
2023-01-07 08:40:39,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,689 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.19304656982422
2023-01-07 08:40:39,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1402793824672699
2023-01-07 08:40:39,691 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,691 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 22.19304656982422
2023-01-07 08:40:39,691 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:39,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,691 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.5360915660858154
2023-01-07 08:40:39,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8238500356674194
2023-01-07 08:40:39,692 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,693 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.14022257924079895
2023-01-07 08:40:39,693 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,693 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 71.40876770019531
2023-01-07 08:40:39,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,693 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.5360915660858154
2023-01-07 08:40:39,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2543647587299347
2023-01-07 08:40:39,694 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,695 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -0.5360915660858154
2023-01-07 08:40:39,695 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:39,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,695 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.715571403503418
2023-01-07 08:40:39,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15054143965244293
2023-01-07 08:40:39,696 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,696 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.007308892905712128
2023-01-07 08:40:39,697 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.941645622253418
2023-01-07 08:40:39,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,697 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 6.715571403503418
2023-01-07 08:40:39,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.034812621772289276
2023-01-07 08:40:39,698 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,698 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 6.715571403503418
2023-01-07 08:40:39,698 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:39,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,699 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.7784652709960938
2023-01-07 08:40:39,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9691779613494873
2023-01-07 08:40:39,700 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,700 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.00023891963064670563
2023-01-07 08:40:39,700 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -46.92707824707031
2023-01-07 08:40:39,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,701 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.7784652709960938
2023-01-07 08:40:39,701 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1382165551185608
2023-01-07 08:40:39,702 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,702 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 3.7784652709960938
2023-01-07 08:40:39,702 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:39,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,703 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.035327434539795
2023-01-07 08:40:39,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4496666491031647
2023-01-07 08:40:39,704 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,704 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.21467237174510956
2023-01-07 08:40:39,704 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 80.2401123046875
2023-01-07 08:40:39,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,705 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.035327434539795
2023-01-07 08:40:39,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15277224779129028
2023-01-07 08:40:39,706 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,706 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -1.035327434539795
2023-01-07 08:40:39,706 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:39,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,707 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 15.173887252807617
2023-01-07 08:40:39,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.051072221249341965
2023-01-07 08:40:39,708 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,708 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.0035372860729694366
2023-01-07 08:40:39,708 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -9.652278900146484
2023-01-07 08:40:39,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,709 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 15.173887252807617
2023-01-07 08:40:39,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07490041106939316
2023-01-07 08:40:39,710 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,710 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 15.173887252807617
2023-01-07 08:40:39,710 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,710 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,710 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:39,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,710 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.315006256103516
2023-01-07 08:40:39,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3606817424297333
2023-01-07 08:40:39,712 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,712 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.001276991330087185
2023-01-07 08:40:39,712 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,712 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -10.83056640625
2023-01-07 08:40:39,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,712 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.315006256103516
2023-01-07 08:40:39,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3002343475818634
2023-01-07 08:40:39,714 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,714 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 10.315006256103516
2023-01-07 08:40:39,714 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,714 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:39,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,714 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:40:39,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2714672088623047
2023-01-07 08:40:39,715 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,716 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.259221613407135
2023-01-07 08:40:39,716 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,716 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 0.7732844352722168
2023-01-07 08:40:39,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,716 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027069091796875
2023-01-07 08:40:39,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7493903040885925
2023-01-07 08:40:39,718 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:40:39,718 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -0.027069091796875
2023-01-07 08:40:39,718 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:40:39,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:40:39,719 > [DEBUG] 0 :: 7.118379592895508
2023-01-07 08:40:39,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,723 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00439453125
2023-01-07 08:40:39,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -340.56982421875
2023-01-07 08:40:39,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,726 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2648301422595978
2023-01-07 08:40:39,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,727 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00439453125
2023-01-07 08:40:39,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -329.2349853515625
2023-01-07 08:40:39,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,729 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.221203327178955
2023-01-07 08:40:39,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24046677350997925
2023-01-07 08:40:39,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,730 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.05812041088938713
2023-01-07 08:40:39,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,731 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.221203327178955
2023-01-07 08:40:39,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3927052617073059
2023-01-07 08:40:39,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,733 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.8453168869018555
2023-01-07 08:40:39,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.014258029870688915
2023-01-07 08:40:39,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,734 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.003018490970134735
2023-01-07 08:40:39,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,735 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.8453168869018555
2023-01-07 08:40:39,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17194347083568573
2023-01-07 08:40:39,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,737 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.4573345184326172
2023-01-07 08:40:39,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.243528127670288
2023-01-07 08:40:39,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,738 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.3163248896598816
2023-01-07 08:40:39,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,739 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.4573345184326172
2023-01-07 08:40:39,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4509561359882355
2023-01-07 08:40:39,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,740 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.955558776855469
2023-01-07 08:40:39,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4575098156929016
2023-01-07 08:40:39,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,741 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.02691333368420601
2023-01-07 08:40:39,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,742 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 5.955558776855469
2023-01-07 08:40:39,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7968490123748779
2023-01-07 08:40:39,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,743 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 16.357330322265625
2023-01-07 08:40:39,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3171011507511139
2023-01-07 08:40:39,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,744 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.06988537311553955
2023-01-07 08:40:39,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,746 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 16.357330322265625
2023-01-07 08:40:39,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0003570728003978729
2023-01-07 08:40:39,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,747 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.41281652450561523
2023-01-07 08:40:39,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22388167679309845
2023-01-07 08:40:39,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,748 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.059110257774591446
2023-01-07 08:40:39,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,749 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -0.41281652450561523
2023-01-07 08:40:39,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.295527696609497
2023-01-07 08:40:39,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,750 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.84256362915039
2023-01-07 08:40:39,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17863669991493225
2023-01-07 08:40:39,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,751 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.3361719846725464
2023-01-07 08:40:39,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,752 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.84256362915039
2023-01-07 08:40:39,752 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1944239139556885
2023-01-07 08:40:39,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,753 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 5.256791114807129
2023-01-07 08:40:39,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07297219336032867
2023-01-07 08:40:39,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,754 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.04989618808031082
2023-01-07 08:40:39,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,755 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 5.256791114807129
2023-01-07 08:40:39,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4407852590084076
2023-01-07 08:40:39,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,756 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 19.963626861572266
2023-01-07 08:40:39,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5199959874153137
2023-01-07 08:40:39,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,758 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.06515178084373474
2023-01-07 08:40:39,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,758 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 19.963626861572266
2023-01-07 08:40:39,758 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25974011421203613
2023-01-07 08:40:39,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,759 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1.0568947792053223
2023-01-07 08:40:39,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.727104902267456
2023-01-07 08:40:39,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.13436849415302277
2023-01-07 08:40:39,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,761 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1.0568947792053223
2023-01-07 08:40:39,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8995273113250732
2023-01-07 08:40:39,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,762 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -6.913036346435547
2023-01-07 08:40:39,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01568092405796051
2023-01-07 08:40:39,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.03933443874120712
2023-01-07 08:40:39,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -6.913036346435547
2023-01-07 08:40:39,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8920177221298218
2023-01-07 08:40:39,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 23.357147216796875
2023-01-07 08:40:39,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8941690325737
2023-01-07 08:40:39,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.14227136969566345
2023-01-07 08:40:39,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 23.357147216796875
2023-01-07 08:40:39,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.072584390640259
2023-01-07 08:40:39,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,769 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 10.610356330871582
2023-01-07 08:40:39,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3479841947555542
2023-01-07 08:40:39,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.021325893700122833
2023-01-07 08:40:39,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 10.610356330871582
2023-01-07 08:40:39,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39484673738479614
2023-01-07 08:40:39,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,772 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 5.006856441497803
2023-01-07 08:40:39,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1810372769832611
2023-01-07 08:40:39,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.02825673669576645
2023-01-07 08:40:39,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 5.006856441497803
2023-01-07 08:40:39,774 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5507938861846924
2023-01-07 08:40:39,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,775 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -3.813962936401367
2023-01-07 08:40:39,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0861830711364746
2023-01-07 08:40:39,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,776 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.09994271397590637
2023-01-07 08:40:39,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,777 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -3.813962936401367
2023-01-07 08:40:39,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.038933515548706
2023-01-07 08:40:39,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,778 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 17.658763885498047
2023-01-07 08:40:39,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8245487213134766
2023-01-07 08:40:39,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,779 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.08492642641067505
2023-01-07 08:40:39,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,780 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 17.658763885498047
2023-01-07 08:40:39,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3858729600906372
2023-01-07 08:40:39,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,781 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 4.421507835388184
2023-01-07 08:40:39,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35700511932373047
2023-01-07 08:40:39,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,782 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.07243204861879349
2023-01-07 08:40:39,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,783 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 4.421507835388184
2023-01-07 08:40:39,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.234548807144165
2023-01-07 08:40:39,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,784 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -12.00986385345459
2023-01-07 08:40:39,784 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6368775367736816
2023-01-07 08:40:39,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.02325253188610077
2023-01-07 08:40:39,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -12.00986385345459
2023-01-07 08:40:39,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.284599244594574
2023-01-07 08:40:39,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -15.361692428588867
2023-01-07 08:40:39,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.369182825088501
2023-01-07 08:40:39,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1383688598871231
2023-01-07 08:40:39,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -15.361692428588867
2023-01-07 08:40:39,789 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7337173223495483
2023-01-07 08:40:39,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.923587799072266
2023-01-07 08:40:39,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0562642812728882
2023-01-07 08:40:39,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.05265404284000397
2023-01-07 08:40:39,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 13.923587799072266
2023-01-07 08:40:39,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2061309814453125
2023-01-07 08:40:39,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 81.0716323852539
2023-01-07 08:40:39,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6766197681427002
2023-01-07 08:40:39,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.009742707014083862
2023-01-07 08:40:39,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,795 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 81.0716323852539
2023-01-07 08:40:39,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.302847385406494
2023-01-07 08:40:39,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,796 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 13.09776782989502
2023-01-07 08:40:39,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.597418785095215
2023-01-07 08:40:39,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.3782764673233032
2023-01-07 08:40:39,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 13.09776782989502
2023-01-07 08:40:39,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2111692875623703
2023-01-07 08:40:39,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,799 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 8.552759170532227
2023-01-07 08:40:39,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8573288917541504
2023-01-07 08:40:39,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,801 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.12859484553337097
2023-01-07 08:40:39,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,801 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 8.552759170532227
2023-01-07 08:40:39,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.389519691467285
2023-01-07 08:40:39,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,803 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 11.614604949951172
2023-01-07 08:40:39,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.681910991668701
2023-01-07 08:40:39,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,804 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 11.614604949951172
2023-01-07 08:40:39,804 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.765473484992981
2023-01-07 08:40:39,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,806 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -21.946426391601562
2023-01-07 08:40:39,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.024054184556007385
2023-01-07 08:40:39,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,807 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.3426869809627533
2023-01-07 08:40:39,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,807 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -21.946426391601562
2023-01-07 08:40:39,807 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.123934745788574
2023-01-07 08:40:39,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,809 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.102788925170898
2023-01-07 08:40:39,809 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8751723766326904
2023-01-07 08:40:39,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.3583797216415405
2023-01-07 08:40:39,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.102788925170898
2023-01-07 08:40:39,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42285341024398804
2023-01-07 08:40:39,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,812 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 12.328341484069824
2023-01-07 08:40:39,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.587126612663269
2023-01-07 08:40:39,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,813 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.08019605278968811
2023-01-07 08:40:39,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,813 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 12.328341484069824
2023-01-07 08:40:39,813 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27661412954330444
2023-01-07 08:40:39,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,815 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -205.05374145507812
2023-01-07 08:40:39,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.1577558517456055
2023-01-07 08:40:39,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,816 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -205.05374145507812
2023-01-07 08:40:39,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.091140747070312
2023-01-07 08:40:39,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,817 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 14.692167282104492
2023-01-07 08:40:39,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.22990882396698
2023-01-07 08:40:39,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,819 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 14.692167282104492
2023-01-07 08:40:39,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9954865574836731
2023-01-07 08:40:39,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,820 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 27.45343017578125
2023-01-07 08:40:39,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3980477452278137
2023-01-07 08:40:39,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,821 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 27.45343017578125
2023-01-07 08:40:39,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.686459541320801
2023-01-07 08:40:39,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,823 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -142.49623107910156
2023-01-07 08:40:39,823 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.597003936767578
2023-01-07 08:40:39,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,824 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -142.49623107910156
2023-01-07 08:40:39,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.478731632232666
2023-01-07 08:40:39,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,825 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.343097686767578
2023-01-07 08:40:39,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3734164237976074
2023-01-07 08:40:39,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,827 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.343097686767578
2023-01-07 08:40:39,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9113190174102783
2023-01-07 08:40:39,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -14.306085586547852
2023-01-07 08:40:39,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9802740812301636
2023-01-07 08:40:39,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,829 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -14.306085586547852
2023-01-07 08:40:39,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.20385217666626
2023-01-07 08:40:39,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,831 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -43.66647720336914
2023-01-07 08:40:39,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.383725643157959
2023-01-07 08:40:39,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,832 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -43.66647720336914
2023-01-07 08:40:39,832 > [DEBUG] 0 :: before allreduce fusion buffer :: -65.5128173828125
2023-01-07 08:40:39,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,833 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 23.54395866394043
2023-01-07 08:40:39,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1444738507270813
2023-01-07 08:40:39,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,835 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.6333916187286377
2023-01-07 08:40:39,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,835 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 23.54395866394043
2023-01-07 08:40:39,835 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.688724517822266
2023-01-07 08:40:39,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,837 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 11.090704917907715
2023-01-07 08:40:39,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.293579578399658
2023-01-07 08:40:39,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,838 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 11.090704917907715
2023-01-07 08:40:39,838 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.252196311950684
2023-01-07 08:40:39,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,839 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 199.5096435546875
2023-01-07 08:40:39,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.657630920410156
2023-01-07 08:40:39,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,841 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 199.5096435546875
2023-01-07 08:40:39,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.39655685424805
2023-01-07 08:40:39,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 18.77381706237793
2023-01-07 08:40:39,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9803581237792969
2023-01-07 08:40:39,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,843 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.7153968811035156
2023-01-07 08:40:39,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,844 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 18.77381706237793
2023-01-07 08:40:39,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.70537567138672
2023-01-07 08:40:39,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,845 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -16.19219207763672
2023-01-07 08:40:39,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.824510097503662
2023-01-07 08:40:39,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,846 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -16.19219207763672
2023-01-07 08:40:39,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.434065818786621
2023-01-07 08:40:39,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,848 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 27.025150299072266
2023-01-07 08:40:39,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5980277061462402
2023-01-07 08:40:39,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,849 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 27.025150299072266
2023-01-07 08:40:39,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.305924415588379
2023-01-07 08:40:39,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,850 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.021291732788086
2023-01-07 08:40:39,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,852 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.5445460081100464
2023-01-07 08:40:39,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,852 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -139.16500854492188
2023-01-07 08:40:39,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,853 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,854 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.13089370727539
2023-01-07 08:40:39,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,855 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.0705839395523071
2023-01-07 08:40:39,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,855 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -114.42776489257812
2023-01-07 08:40:39,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,855 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.650497436523438
2023-01-07 08:40:39,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,857 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.80919647216797
2023-01-07 08:40:39,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,858 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -114.42776489257812
2023-01-07 08:40:39,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.63766860961914
2023-01-07 08:40:39,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,860 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,860 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.398120880126953
2023-01-07 08:40:39,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,861 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.2224657535552979
2023-01-07 08:40:39,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,861 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,861 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.58761978149414
2023-01-07 08:40:39,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,863 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,863 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.315691947937012
2023-01-07 08:40:39,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,864 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.1222842931747437
2023-01-07 08:40:39,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,864 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 23.176719665527344
2023-01-07 08:40:39,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.849916458129883
2023-01-07 08:40:39,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,866 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.065739870071411
2023-01-07 08:40:39,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,867 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -50.04143524169922
2023-01-07 08:40:39,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,867 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.098108291625977
2023-01-07 08:40:39,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,869 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.196167945861816
2023-01-07 08:40:39,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,870 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -262.01568603515625
2023-01-07 08:40:39,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.511646270751953
2023-01-07 08:40:39,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,872 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.300266265869141
2023-01-07 08:40:39,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,873 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 3.9391722679138184
2023-01-07 08:40:39,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,874 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -46.8707389831543
2023-01-07 08:40:39,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,874 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,874 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.897930145263672
2023-01-07 08:40:39,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,876 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.928122520446777
2023-01-07 08:40:39,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,877 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -8.382683753967285
2023-01-07 08:40:39,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1210498809814453
2023-01-07 08:40:39,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,879 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 6.3393402099609375
2023-01-07 08:40:39,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.61743927001953
2023-01-07 08:40:39,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,880 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -8.382683753967285
2023-01-07 08:40:39,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,880 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -312.76727294921875
2023-01-07 08:40:39,880 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.605598449707031
2023-01-07 08:40:39,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,882 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 166.36183166503906
2023-01-07 08:40:39,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.16580200195312
2023-01-07 08:40:39,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,883 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 1.9666414260864258
2023-01-07 08:40:39,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,883 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 166.36183166503906
2023-01-07 08:40:39,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 118.2828369140625
2023-01-07 08:40:39,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,885 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 166.36183166503906
2023-01-07 08:40:39,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.754323959350586
2023-01-07 08:40:39,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:40:39,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:40:39,886 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 166.36183166503906
2023-01-07 08:40:39,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.27851867675781
