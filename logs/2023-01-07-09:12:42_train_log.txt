2023-01-07 09:12:49,520 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:12:49,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:49,558 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:49,558 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:49,558 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 09:12:49,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,420 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,420 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,420 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 09:12:50,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,422 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,422 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,422 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 09:12:50,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,423 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,424 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,424 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 09:12:50,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,425 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,425 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,425 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 09:12:50,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,462 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,462 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,462 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 09:12:50,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,463 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,463 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,463 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 09:12:50,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,465 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,465 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,465 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 09:12:50,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,466 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,466 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,466 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 09:12:50,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,467 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,467 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,467 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 09:12:50,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,468 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,468 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,468 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 09:12:50,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,469 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,469 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,469 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 09:12:50,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,470 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,470 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,470 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 09:12:50,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,471 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,471 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,471 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 09:12:50,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,473 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,473 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,473 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 09:12:50,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,474 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,474 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,474 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 09:12:50,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,475 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,475 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,475 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 09:12:50,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,476 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,476 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,476 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 09:12:50,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,477 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,477 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,477 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 09:12:50,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,478 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,478 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,478 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 09:12:50,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,480 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,480 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,480 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 09:12:50,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,481 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,481 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,481 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 09:12:50,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,482 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,482 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,482 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 09:12:50,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,483 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,483 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,483 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 09:12:50,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,484 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,484 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,485 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 09:12:50,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,486 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,486 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,486 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 09:12:50,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,487 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,487 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,487 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 09:12:50,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,488 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,488 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,488 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 09:12:50,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,489 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,489 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,489 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 09:12:50,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,490 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,491 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,491 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 09:12:50,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,492 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,492 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,492 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 09:12:50,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,493 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,493 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,493 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 09:12:50,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,494 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,494 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,494 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 09:12:50,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,495 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,495 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,496 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 09:12:50,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,497 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,497 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,497 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 09:12:50,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,498 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,498 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,498 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 09:12:50,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,499 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,499 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,499 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 09:12:50,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,500 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,500 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,500 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 09:12:50,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,501 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,501 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,501 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 09:12:50,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,502 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,502 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,502 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 09:12:50,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,504 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,504 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,504 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 09:12:50,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,505 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,505 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,505 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 09:12:50,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,506 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,506 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,506 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 09:12:50,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,507 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,507 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,507 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 09:12:50,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,508 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,508 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,508 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 09:12:50,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,509 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,509 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,509 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 09:12:50,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,510 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,511 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,511 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 09:12:50,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,512 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,512 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,512 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 09:12:50,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,513 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,513 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,513 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 09:12:50,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,514 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,514 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,514 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 09:12:50,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,515 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,515 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,515 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 09:12:50,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,517 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,517 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,517 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 09:12:50,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,518 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,518 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,518 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 09:12:50,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,519 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,519 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,519 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 09:12:50,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,520 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,520 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,520 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 09:12:50,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,521 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,521 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,521 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 09:12:50,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,522 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,522 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,522 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 09:12:50,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,523 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,524 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,524 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 09:12:50,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,525 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,525 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,525 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 09:12:50,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,526 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,526 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,526 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 09:12:50,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,527 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,527 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,527 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 09:12:50,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,528 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,528 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,528 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 09:12:50,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,529 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,530 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,530 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 09:12:50,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,531 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,531 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,531 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 09:12:50,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,532 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,532 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,532 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 09:12:50,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,533 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,533 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,533 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 09:12:50,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,534 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,534 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,534 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 09:12:50,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,535 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,535 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,535 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 09:12:50,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,536 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,536 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,536 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 09:12:50,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,537 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,537 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,537 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 09:12:50,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,539 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,539 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,539 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 09:12:50,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,540 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,540 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,540 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 09:12:50,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,541 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,541 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,541 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 09:12:50,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,542 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,542 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,542 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 09:12:50,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,543 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,543 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,543 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 09:12:50,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,544 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,544 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,544 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 09:12:50,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,545 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,545 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,546 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 09:12:50,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,546 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,546 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,547 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 09:12:50,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,548 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,548 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,548 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 09:12:50,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,549 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,549 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,549 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 09:12:50,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,550 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,550 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,550 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 09:12:50,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,551 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,551 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,551 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 09:12:50,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,552 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,552 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,552 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 09:12:50,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,553 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,553 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,553 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 09:12:50,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,554 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,554 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,554 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 09:12:50,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,555 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,555 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,555 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 09:12:50,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,557 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,557 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,557 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 09:12:50,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,558 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,558 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,558 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 09:12:50,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,559 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,559 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,559 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 09:12:50,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,560 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,560 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,560 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 09:12:50,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,561 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,561 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,562 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 09:12:50,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,562 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,563 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,563 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 09:12:50,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,564 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,564 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,564 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 09:12:50,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,565 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,565 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,565 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 09:12:50,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,566 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,566 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,566 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 09:12:50,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,567 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,567 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,567 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 09:12:50,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,568 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,568 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,568 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 09:12:50,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,569 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,570 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,570 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 09:12:50,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,571 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,571 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,571 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 09:12:50,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,572 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,572 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,572 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 09:12:50,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,573 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,573 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,573 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 09:12:50,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,574 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,574 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,574 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 09:12:50,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,575 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,575 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,575 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 09:12:50,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,576 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,576 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,576 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 09:12:50,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,577 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,578 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,578 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 09:12:50,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,578 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,579 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,579 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 09:12:50,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:50,580 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:50,580 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:12:50,580 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 09:12:50,582 > [DEBUG] 0 :: 6.936882495880127
2023-01-07 09:12:50,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,587 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,587 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.01416015625
2023-01-07 09:12:50,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -363.46710205078125
2023-01-07 09:12:50,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,590 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,590 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.19848322868347168
2023-01-07 09:12:50,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,591 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -209.6824951171875
2023-01-07 09:12:50,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.68798828125
2023-01-07 09:12:50,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,603 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,603 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 10.185383796691895
2023-01-07 09:12:50,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12410006672143936
2023-01-07 09:12:50,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,604 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.022788457572460175
2023-01-07 09:12:50,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -150.82752990722656
2023-01-07 09:12:50,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6547566652297974
2023-01-07 09:12:50,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,607 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,607 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 13.847755432128906
2023-01-07 09:12:50,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.38406240940093994
2023-01-07 09:12:50,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,608 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,608 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.03287770599126816
2023-01-07 09:12:50,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,609 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -347.4720153808594
2023-01-07 09:12:50,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16282087564468384
2023-01-07 09:12:50,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,611 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,611 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 19.27171516418457
2023-01-07 09:12:50,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27647557854652405
2023-01-07 09:12:50,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,612 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: -0.03118997998535633
2023-01-07 09:12:50,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -389.052978515625
2023-01-07 09:12:50,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21463248133659363
2023-01-07 09:12:50,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,614 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 8.809120178222656
2023-01-07 09:12:50,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19714058935642242
2023-01-07 09:12:50,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,616 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,616 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.03305942937731743
2023-01-07 09:12:50,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,616 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -199.5203094482422
2023-01-07 09:12:50,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1799091398715973
2023-01-07 09:12:50,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,618 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,618 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 17.299869537353516
2023-01-07 09:12:50,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5424841642379761
2023-01-07 09:12:50,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,619 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.008462101221084595
2023-01-07 09:12:50,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -356.04345703125
2023-01-07 09:12:50,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2038213610649109
2023-01-07 09:12:50,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,622 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 11.385332107543945
2023-01-07 09:12:50,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.462552547454834
2023-01-07 09:12:50,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,623 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.08224057406187057
2023-01-07 09:12:50,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -494.21466064453125
2023-01-07 09:12:50,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.027780532836914
2023-01-07 09:12:50,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,625 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 21.782573699951172
2023-01-07 09:12:50,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7979216575622559
2023-01-07 09:12:50,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,627 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,627 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.10600954294204712
2023-01-07 09:12:50,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,627 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -495.9526062011719
2023-01-07 09:12:50,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29023265838623047
2023-01-07 09:12:50,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,629 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,629 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.70423412322998
2023-01-07 09:12:50,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4764256477355957
2023-01-07 09:12:50,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,630 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,630 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.004640698432922363
2023-01-07 09:12:50,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,630 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -263.2518310546875
2023-01-07 09:12:50,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9653587341308594
2023-01-07 09:12:50,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,633 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,633 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 24.243955612182617
2023-01-07 09:12:50,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.113598346710205
2023-01-07 09:12:50,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,634 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,634 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.019426435232162476
2023-01-07 09:12:50,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,635 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -357.3763122558594
2023-01-07 09:12:50,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6861143112182617
2023-01-07 09:12:50,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,637 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,637 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 4.1229987144470215
2023-01-07 09:12:50,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3234968185424805
2023-01-07 09:12:50,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,638 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.14353293180465698
2023-01-07 09:12:50,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,638 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -376.096435546875
2023-01-07 09:12:50,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.978726387023926
2023-01-07 09:12:50,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,640 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 8.81285285949707
2023-01-07 09:12:50,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7156896591186523
2023-01-07 09:12:50,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,642 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.07619789987802505
2023-01-07 09:12:50,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -260.3354187011719
2023-01-07 09:12:50,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3033628463745117
2023-01-07 09:12:50,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,645 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -2.4184279441833496
2023-01-07 09:12:50,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6061081886291504
2023-01-07 09:12:50,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,646 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,646 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.058733291923999786
2023-01-07 09:12:50,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,646 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -265.9368591308594
2023-01-07 09:12:50,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8806869983673096
2023-01-07 09:12:50,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,648 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,648 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 5.3343658447265625
2023-01-07 09:12:50,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5164321660995483
2023-01-07 09:12:50,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,649 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,649 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.10279341787099838
2023-01-07 09:12:50,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -358.620361328125
2023-01-07 09:12:50,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8634095191955566
2023-01-07 09:12:50,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,652 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,652 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 2.7255430221557617
2023-01-07 09:12:50,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1804413795471191
2023-01-07 09:12:50,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,653 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.027316279709339142
2023-01-07 09:12:50,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -259.66168212890625
2023-01-07 09:12:50,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4502439498901367
2023-01-07 09:12:50,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,655 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -35.54156494140625
2023-01-07 09:12:50,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.364910364151001
2023-01-07 09:12:50,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,656 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.018169716000556946
2023-01-07 09:12:50,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,657 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -288.7059326171875
2023-01-07 09:12:50,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0911651849746704
2023-01-07 09:12:50,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,658 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -7.7638702392578125
2023-01-07 09:12:50,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44168317317962646
2023-01-07 09:12:50,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,660 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.20651952922344208
2023-01-07 09:12:50,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -363.7181701660156
2023-01-07 09:12:50,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.946803092956543
2023-01-07 09:12:50,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,662 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 7.051214694976807
2023-01-07 09:12:50,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4238431453704834
2023-01-07 09:12:50,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,663 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.006555825471878052
2023-01-07 09:12:50,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -246.2533721923828
2023-01-07 09:12:50,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2122764587402344
2023-01-07 09:12:50,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,665 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -11.39385986328125
2023-01-07 09:12:50,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.683286428451538
2023-01-07 09:12:50,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,667 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.1497497707605362
2023-01-07 09:12:50,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -261.20489501953125
2023-01-07 09:12:50,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9902768135070801
2023-01-07 09:12:50,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,669 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,669 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -13.137297630310059
2023-01-07 09:12:50,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31247439980506897
2023-01-07 09:12:50,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,670 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.021084576845169067
2023-01-07 09:12:50,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -367.3478698730469
2023-01-07 09:12:50,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2497987747192383
2023-01-07 09:12:50,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,672 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,672 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 14.619169235229492
2023-01-07 09:12:50,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.543786883354187
2023-01-07 09:12:50,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,673 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,674 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.024784743785858154
2023-01-07 09:12:50,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,674 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -221.75909423828125
2023-01-07 09:12:50,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9092140793800354
2023-01-07 09:12:50,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,676 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,676 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 38.81537628173828
2023-01-07 09:12:50,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8044477701187134
2023-01-07 09:12:50,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,677 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,677 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.21598270535469055
2023-01-07 09:12:50,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,677 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -183.213623046875
2023-01-07 09:12:50,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3542850017547607
2023-01-07 09:12:50,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,679 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,679 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 20.52444839477539
2023-01-07 09:12:50,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09529104083776474
2023-01-07 09:12:50,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,681 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,681 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.13549667596817017
2023-01-07 09:12:50,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,682 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -300.4294128417969
2023-01-07 09:12:50,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6985805630683899
2023-01-07 09:12:50,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,684 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,684 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 8.385639190673828
2023-01-07 09:12:50,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.68256014585495
2023-01-07 09:12:50,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,685 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,685 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.029996812343597412
2023-01-07 09:12:50,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,685 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -178.34564208984375
2023-01-07 09:12:50,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2770862579345703
2023-01-07 09:12:50,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,687 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,687 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -1.3994779586791992
2023-01-07 09:12:50,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13668760657310486
2023-01-07 09:12:50,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,688 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -183.7489471435547
2023-01-07 09:12:50,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3858728408813477
2023-01-07 09:12:50,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,690 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,690 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5.384981155395508
2023-01-07 09:12:50,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3584849834442139
2023-01-07 09:12:50,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,691 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,691 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.7577539682388306
2023-01-07 09:12:50,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,692 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -275.2427673339844
2023-01-07 09:12:50,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.346949815750122
2023-01-07 09:12:50,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,694 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,694 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 20.653545379638672
2023-01-07 09:12:50,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.53885555267334
2023-01-07 09:12:50,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,695 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,695 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.7396843433380127
2023-01-07 09:12:50,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,695 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -91.31132507324219
2023-01-07 09:12:50,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.234832763671875
2023-01-07 09:12:50,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,697 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,697 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.8191404342651367
2023-01-07 09:12:50,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.677722930908203
2023-01-07 09:12:50,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,698 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.2454897165298462
2023-01-07 09:12:50,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -132.64193725585938
2023-01-07 09:12:50,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.489837646484375
2023-01-07 09:12:50,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,701 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,701 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -2.1504554748535156
2023-01-07 09:12:50,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5506772994995117
2023-01-07 09:12:50,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -130.57992553710938
2023-01-07 09:12:50,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.755359649658203
2023-01-07 09:12:50,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,704 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,704 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 8.496732711791992
2023-01-07 09:12:50,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.099723815917969
2023-01-07 09:12:50,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,705 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -171.9005584716797
2023-01-07 09:12:50,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.442095756530762
2023-01-07 09:12:50,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,707 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -11.906904220581055
2023-01-07 09:12:50,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.47031569480896
2023-01-07 09:12:50,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -142.6618194580078
2023-01-07 09:12:50,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.062273025512695
2023-01-07 09:12:50,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,710 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,710 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.2477035522461
2023-01-07 09:12:50,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.124357223510742
2023-01-07 09:12:50,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,711 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -237.5204315185547
2023-01-07 09:12:50,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.35592269897461
2023-01-07 09:12:50,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,713 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -11.242696762084961
2023-01-07 09:12:50,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5434677600860596
2023-01-07 09:12:50,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -185.89926147460938
2023-01-07 09:12:50,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.026287078857422
2023-01-07 09:12:50,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,716 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,716 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -22.658926010131836
2023-01-07 09:12:50,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23149889707565308
2023-01-07 09:12:50,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,717 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -130.69178771972656
2023-01-07 09:12:50,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1069893836975098
2023-01-07 09:12:50,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,718 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,718 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -66.40402221679688
2023-01-07 09:12:50,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.312227249145508
2023-01-07 09:12:50,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,720 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -180.5952606201172
2023-01-07 09:12:50,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.13062286376953
2023-01-07 09:12:50,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,721 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 14.60361385345459
2023-01-07 09:12:50,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3169503211975098
2023-01-07 09:12:50,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,722 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,722 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.9173623323440552
2023-01-07 09:12:50,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -155.65074157714844
2023-01-07 09:12:50,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.754909038543701
2023-01-07 09:12:50,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,724 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,724 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -35.68107986450195
2023-01-07 09:12:50,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.320110023021698
2023-01-07 09:12:50,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,726 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -153.9542236328125
2023-01-07 09:12:50,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.0931010246276855
2023-01-07 09:12:50,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,727 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 14.2589111328125
2023-01-07 09:12:50,727 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5419931411743164
2023-01-07 09:12:50,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,728 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -104.38949584960938
2023-01-07 09:12:50,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.03078842163086
2023-01-07 09:12:50,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,730 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,730 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -45.01694107055664
2023-01-07 09:12:50,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.321986198425293
2023-01-07 09:12:50,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,731 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.8313518762588501
2023-01-07 09:12:50,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -224.037841796875
2023-01-07 09:12:50,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.6372480392456055
2023-01-07 09:12:50,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,733 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -44.885520935058594
2023-01-07 09:12:50,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.771183967590332
2023-01-07 09:12:50,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -116.38424682617188
2023-01-07 09:12:50,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.678857803344727
2023-01-07 09:12:50,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,736 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,736 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -34.40399169921875
2023-01-07 09:12:50,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.731229305267334
2023-01-07 09:12:50,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -171.48867797851562
2023-01-07 09:12:50,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.20467185974121
2023-01-07 09:12:50,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,739 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,739 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 155.15292358398438
2023-01-07 09:12:50,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.114692687988281
2023-01-07 09:12:50,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,740 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.4009820818901062
2023-01-07 09:12:50,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,741 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2.754779815673828
2023-01-07 09:12:50,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.61004638671875
2023-01-07 09:12:50,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -569.0397338867188
2023-01-07 09:12:50,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.918627381324768
2023-01-07 09:12:50,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,744 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,744 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8807677030563354
2023-01-07 09:12:50,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,744 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -62.59282684326172
2023-01-07 09:12:50,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -760.7621459960938
2023-01-07 09:12:50,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.285476684570312
2023-01-07 09:12:50,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,747 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -861.0968627929688
2023-01-07 09:12:50,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.309452056884766
2023-01-07 09:12:50,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,748 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -621.8184204101562
2023-01-07 09:12:50,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.969831466674805
2023-01-07 09:12:50,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,750 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,750 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 10.317375183105469
2023-01-07 09:12:50,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.355056762695312
2023-01-07 09:12:50,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,752 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,752 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.7499808073043823
2023-01-07 09:12:50,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,753 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -90.39552307128906
2023-01-07 09:12:50,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.34085750579834
2023-01-07 09:12:50,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,755 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -541.9728393554688
2023-01-07 09:12:50,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.399709224700928
2023-01-07 09:12:50,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,756 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,756 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -0.576158881187439
2023-01-07 09:12:50,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,756 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,756 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -83.76191711425781
2023-01-07 09:12:50,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.36642074584961
2023-01-07 09:12:50,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,758 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -721.8775634765625
2023-01-07 09:12:50,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.76392364501953
2023-01-07 09:12:50,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,759 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,759 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -7.281508445739746
2023-01-07 09:12:50,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,760 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -996.1065063476562
2023-01-07 09:12:50,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.09211540222168
2023-01-07 09:12:50,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,761 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -857.2203369140625
2023-01-07 09:12:50,761 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.57688045501709
2023-01-07 09:12:50,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,762 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,762 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 59.987693786621094
2023-01-07 09:12:50,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.41576385498047
2023-01-07 09:12:50,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,764 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -993.2752685546875
2023-01-07 09:12:50,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.67945098876953
2023-01-07 09:12:50,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,765 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,765 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.860105037689209
2023-01-07 09:12:50,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,765 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,765 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -72.96048736572266
2023-01-07 09:12:50,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,766 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1182.6610107421875
2023-01-07 09:12:50,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,766 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1137.7342529296875
2023-01-07 09:12:50,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.121971130371094
2023-01-07 09:12:50,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,768 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1290.990234375
2023-01-07 09:12:50,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19385409355163574
2023-01-07 09:12:50,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,769 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,769 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 276.3520812988281
2023-01-07 09:12:50,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 114.36829376220703
2023-01-07 09:12:50,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,770 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1323.5159912109375
2023-01-07 09:12:50,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -83.08406829833984
2023-01-07 09:12:50,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,772 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -514.5498046875
2023-01-07 09:12:50,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,772 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1170.801025390625
2023-01-07 09:12:50,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 161.76861572265625
2023-01-07 09:12:50,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,774 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,774 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -493.1292419433594
2023-01-07 09:12:50,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2421016693115234
2023-01-07 09:12:50,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,775 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,775 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -1.9869775772094727
2023-01-07 09:12:50,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,775 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -684.9237670898438
2023-01-07 09:12:50,776 > [DEBUG] 0 :: before allreduce fusion buffer :: -188.83538818359375
2023-01-07 09:12:50,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,778 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1166.3756103515625
2023-01-07 09:12:50,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.33362579345703
2023-01-07 09:12:50,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,779 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1537.17138671875
2023-01-07 09:12:50,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.299910068511963
2023-01-07 09:12:50,781 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:12:50,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,781 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:50,781 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -3641.267333984375
2023-01-07 09:12:50,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,781 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1852.11669921875
2023-01-07 09:12:50,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1162.744140625
2023-01-07 09:12:50,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -766.7471923828125
2023-01-07 09:12:50,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -549.75634765625
2023-01-07 09:12:50,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -706.8529052734375
2023-01-07 09:12:50,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -764.38232421875
2023-01-07 09:12:50,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -706.9214477539062
2023-01-07 09:12:50,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,784 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -568.271484375
2023-01-07 09:12:50,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,784 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -870.0454711914062
2023-01-07 09:12:50,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,784 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -691.20947265625
2023-01-07 09:12:50,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,784 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -690.6727294921875
2023-01-07 09:12:50,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -945.2762451171875
2023-01-07 09:12:50,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -732.872314453125
2023-01-07 09:12:50,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -675.6996459960938
2023-01-07 09:12:50,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -832.7787475585938
2023-01-07 09:12:50,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -996.576171875
2023-01-07 09:12:50,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -673.0076904296875
2023-01-07 09:12:50,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1006.8482666015625
2023-01-07 09:12:50,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -877.1198120117188
2023-01-07 09:12:50,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -982.0556640625
2023-01-07 09:12:50,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -931.8392333984375
2023-01-07 09:12:50,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -869.965087890625
2023-01-07 09:12:50,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1034.593994140625
2023-01-07 09:12:50,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1014.4542846679688
2023-01-07 09:12:50,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -996.2841796875
2023-01-07 09:12:50,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -3910.95751953125
2023-01-07 09:12:50,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1064.840087890625
2023-01-07 09:12:50,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1008.7743530273438
2023-01-07 09:12:50,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1021.7000122070312
2023-01-07 09:12:50,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1079.2314453125
2023-01-07 09:12:50,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1010.6254272460938
2023-01-07 09:12:50,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -1009.8676147460938
2023-01-07 09:12:50,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1087.197265625
2023-01-07 09:12:50,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,793 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1034.27001953125
2023-01-07 09:12:50,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,793 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1113.8701171875
2023-01-07 09:12:50,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.593942642211914
2023-01-07 09:12:50,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1093.422607421875
2023-01-07 09:12:50,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1017.6414794921875
2023-01-07 09:12:50,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,795 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1013.88818359375
2023-01-07 09:12:50,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1080.710693359375
2023-01-07 09:12:50,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.133419036865234
2023-01-07 09:12:50,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1028.8382568359375
2023-01-07 09:12:50,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -907.66845703125
2023-01-07 09:12:50,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,797 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1014.4554443359375
2023-01-07 09:12:50,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.21492385864258
2023-01-07 09:12:50,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -981.4595947265625
2023-01-07 09:12:50,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:50,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:50,799 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -374.379638671875
2023-01-07 09:12:50,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 724.2587890625
2023-01-07 09:12:51,648 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -905.4503173828125 param sum :: 113.3508071899414
2023-01-07 09:12:51,648 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:12:51,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,648 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,648 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 0.9531664848327637
2023-01-07 09:12:51,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1967.033447265625
2023-01-07 09:12:51,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -165.72390747070312
2023-01-07 09:12:51,651 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 1.1921911239624023 param sum :: 64.20001220703125
2023-01-07 09:12:51,651 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -11.420797348022461
2023-01-07 09:12:51,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,651 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,651 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -63.69385528564453
2023-01-07 09:12:51,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,651 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1995.052001953125
2023-01-07 09:12:51,652 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.45381164550781
2023-01-07 09:12:51,653 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 84.13268280029297 param sum :: -23.220640182495117
2023-01-07 09:12:51,653 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.807575225830078
2023-01-07 09:12:51,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,654 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1989.5556640625
2023-01-07 09:12:51,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -163.00181579589844
2023-01-07 09:12:51,655 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 8.163412094116211 param sum :: 63.40000915527344
2023-01-07 09:12:51,655 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 5.807575225830078
2023-01-07 09:12:51,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,656 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1857.0509033203125
2023-01-07 09:12:51,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.40617370605469
2023-01-07 09:12:51,657 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -1771.4793701171875 param sum :: 318.345947265625
2023-01-07 09:12:51,657 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,657 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,657 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:12:51,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,658 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.8478026390075684
2023-01-07 09:12:51,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,658 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 54.71397399902344
2023-01-07 09:12:51,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,658 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1430.529296875
2023-01-07 09:12:51,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.95132446289062
2023-01-07 09:12:51,660 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.8282155990600586 param sum :: 63.39976119995117
2023-01-07 09:12:51,660 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -20.24698829650879
2023-01-07 09:12:51,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,661 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1434.964111328125
2023-01-07 09:12:51,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.570837020874023
2023-01-07 09:12:51,662 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -6.510662078857422 param sum :: 3.9754385948181152
2023-01-07 09:12:51,662 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:12:51,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,662 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,662 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1551046371459961
2023-01-07 09:12:51,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -667.6712036132812
2023-01-07 09:12:51,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.926795959472656
2023-01-07 09:12:51,665 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 1.01810884475708 param sum :: 254.7999725341797
2023-01-07 09:12:51,665 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.245198249816895
2023-01-07 09:12:51,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,665 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -452.93475341796875
2023-01-07 09:12:51,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.21994018554688
2023-01-07 09:12:51,666 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1243.469970703125 param sum :: 149.66030883789062
2023-01-07 09:12:51,666 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,667 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.245198249816895
2023-01-07 09:12:51,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,667 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -560.9091796875
2023-01-07 09:12:51,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.96356201171875
2023-01-07 09:12:51,668 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -6.226737976074219 param sum :: 254.8000946044922
2023-01-07 09:12:51,668 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.245198249816895
2023-01-07 09:12:51,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,668 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -674.7845458984375
2023-01-07 09:12:51,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -172.27777099609375
2023-01-07 09:12:51,670 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -629.893798828125 param sum :: 95.7799301147461
2023-01-07 09:12:51,670 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:12:51,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,670 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,670 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.1258082389831543
2023-01-07 09:12:51,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,671 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -692.9989013671875
2023-01-07 09:12:51,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 159.95196533203125
2023-01-07 09:12:51,672 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 1.9200701713562012 param sum :: 61.4000129699707
2023-01-07 09:12:51,672 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.245198249816895
2023-01-07 09:12:51,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,673 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -807.375244140625
2023-01-07 09:12:51,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1217.048583984375
2023-01-07 09:12:51,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -113.30938720703125
2023-01-07 09:12:51,675 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -829.2487182617188 param sum :: 113.75157165527344
2023-01-07 09:12:51,675 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,675 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,675 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:12:51,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,675 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,675 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0830346941947937
2023-01-07 09:12:51,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,676 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -582.2359619140625
2023-01-07 09:12:51,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.158493041992188
2023-01-07 09:12:51,677 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.916886806488037 param sum :: 65.79995727539062
2023-01-07 09:12:51,677 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -20.24698829650879
2023-01-07 09:12:51,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,678 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1410.7906494140625
2023-01-07 09:12:51,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.27312088012695
2023-01-07 09:12:51,679 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -541.8278198242188 param sum :: 94.126953125
2023-01-07 09:12:51,679 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,679 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,679 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.959938049316406
2023-01-07 09:12:51,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,680 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -778.0882568359375
2023-01-07 09:12:51,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.013134002685547
2023-01-07 09:12:51,681 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -66.1328125 param sum :: 265.99993896484375
2023-01-07 09:12:51,681 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.959938049316406
2023-01-07 09:12:51,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,681 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -760.443359375
2023-01-07 09:12:51,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,682 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1450.0675048828125
2023-01-07 09:12:51,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.39982986450195
2023-01-07 09:12:51,683 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -758.8660278320312 param sum :: 203.97998046875
2023-01-07 09:12:51,683 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -20.24698829650879
2023-01-07 09:12:51,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,684 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1429.9349365234375
2023-01-07 09:12:51,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.809974670410156
2023-01-07 09:12:51,685 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -8.694780349731445 param sum :: 66.39998626708984
2023-01-07 09:12:51,685 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -20.24698829650879
2023-01-07 09:12:51,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,686 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1435.414794921875
2023-01-07 09:12:51,686 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.727245330810547
2023-01-07 09:12:51,687 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -1375.188232421875 param sum :: 194.7434844970703
2023-01-07 09:12:51,688 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,688 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:12:51,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,688 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,688 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.2933084964752197
2023-01-07 09:12:51,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,688 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,688 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -18.17919921875
2023-01-07 09:12:51,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1264.8714599609375
2023-01-07 09:12:51,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.08391571044922
2023-01-07 09:12:51,690 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.8184518814086914 param sum :: 64.19989013671875
2023-01-07 09:12:51,691 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -13.248717308044434
2023-01-07 09:12:51,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1178.7877197265625
2023-01-07 09:12:51,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.68475914001465
2023-01-07 09:12:51,692 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -62.16999053955078 param sum :: 35.93439483642578
2023-01-07 09:12:51,692 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -30.226551055908203
2023-01-07 09:12:51,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1239.970947265625
2023-01-07 09:12:51,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8879776000976562
2023-01-07 09:12:51,694 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -54.65104675292969 param sum :: 268.20001220703125
2023-01-07 09:12:51,694 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -13.248717308044434
2023-01-07 09:12:51,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1203.8912353515625
2023-01-07 09:12:51,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.87548065185547
2023-01-07 09:12:51,696 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1198.2630615234375 param sum :: 335.14385986328125
2023-01-07 09:12:51,696 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,696 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -13.248717308044434
2023-01-07 09:12:51,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1193.601806640625
2023-01-07 09:12:51,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 83.08040618896484
2023-01-07 09:12:51,698 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -29.772855758666992 param sum :: 134.60006713867188
2023-01-07 09:12:51,698 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,698 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -13.248717308044434
2023-01-07 09:12:51,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1179.26904296875
2023-01-07 09:12:51,699 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.944405555725098
2023-01-07 09:12:51,700 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -1204.8179931640625 param sum :: 378.1451721191406
2023-01-07 09:12:51,700 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,700 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,700 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.24475812911987305
2023-01-07 09:12:51,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -746.091552734375
2023-01-07 09:12:51,701 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.593870162963867
2023-01-07 09:12:51,702 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.7985626459121704 param sum :: 129.00006103515625
2023-01-07 09:12:51,702 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 6.051527976989746
2023-01-07 09:12:51,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -753.59423828125
2023-01-07 09:12:51,703 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.665654182434082
2023-01-07 09:12:51,705 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -758.916015625 param sum :: 304.6231689453125
2023-01-07 09:12:51,705 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:12:51,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,705 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.17741179466247559
2023-01-07 09:12:51,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -498.3814697265625
2023-01-07 09:12:51,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2886636257171631
2023-01-07 09:12:51,707 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.4168201982975006 param sum :: 510.79949951171875
2023-01-07 09:12:51,707 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 29.013198852539062
2023-01-07 09:12:51,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -499.9428405761719
2023-01-07 09:12:51,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.481410980224609
2023-01-07 09:12:51,709 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -516.8341674804688 param sum :: 241.32803344726562
2023-01-07 09:12:51,709 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 46.412017822265625
2023-01-07 09:12:51,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -672.3565063476562
2023-01-07 09:12:51,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7944138050079346
2023-01-07 09:12:51,711 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -113.42372131347656 param sum :: 553.6002197265625
2023-01-07 09:12:51,711 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 46.412017822265625
2023-01-07 09:12:51,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,711 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -682.496826171875
2023-01-07 09:12:51,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.395207405090332
2023-01-07 09:12:51,713 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -674.9417114257812 param sum :: 227.05531311035156
2023-01-07 09:12:51,713 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,713 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.5056204795837402
2023-01-07 09:12:51,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,714 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -732.2503051757812
2023-01-07 09:12:51,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.542341232299805
2023-01-07 09:12:51,716 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.40017449855804443 param sum :: 127.00003814697266
2023-01-07 09:12:51,716 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,716 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 19.682836532592773
2023-01-07 09:12:51,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,716 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -767.3521728515625
2023-01-07 09:12:51,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.4881591796875
2023-01-07 09:12:51,718 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -801.005859375 param sum :: 324.7969970703125
2023-01-07 09:12:51,718 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,718 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,718 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.08709043264389038
2023-01-07 09:12:51,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,719 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -703.5665283203125
2023-01-07 09:12:51,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.312088012695312
2023-01-07 09:12:51,720 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.14240041375160217 param sum :: 127.39988708496094
2023-01-07 09:12:51,720 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -22.445621490478516
2023-01-07 09:12:51,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -708.105712890625
2023-01-07 09:12:51,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.812430381774902
2023-01-07 09:12:51,722 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -701.3925170898438 param sum :: 352.4848327636719
2023-01-07 09:12:51,722 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 39.271324157714844
2023-01-07 09:12:51,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -567.8668212890625
2023-01-07 09:12:51,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.990120887756348
2023-01-07 09:12:51,724 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -92.5348892211914 param sum :: 551.9996337890625
2023-01-07 09:12:51,724 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 39.271324157714844
2023-01-07 09:12:51,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -574.1320190429688
2023-01-07 09:12:51,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8327957391738892
2023-01-07 09:12:51,726 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -584.2918090820312 param sum :: 391.298583984375
2023-01-07 09:12:51,726 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,727 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.34261780977249146
2023-01-07 09:12:51,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -864.815185546875
2023-01-07 09:12:51,727 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.67194366455078
2023-01-07 09:12:51,729 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.5427570939064026 param sum :: 127.60030364990234
2023-01-07 09:12:51,729 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -39.95487594604492
2023-01-07 09:12:51,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,730 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -854.379150390625
2023-01-07 09:12:51,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.08721923828125
2023-01-07 09:12:51,732 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -833.23046875 param sum :: 476.7754211425781
2023-01-07 09:12:51,732 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,732 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,732 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.261584997177124
2023-01-07 09:12:51,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -689.6654663085938
2023-01-07 09:12:51,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6403450965881348
2023-01-07 09:12:51,734 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.6661479473114014 param sum :: 129.99981689453125
2023-01-07 09:12:51,734 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,734 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 4.547539234161377
2023-01-07 09:12:51,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -690.2152099609375
2023-01-07 09:12:51,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.803987503051758
2023-01-07 09:12:51,736 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -685.5570068359375 param sum :: 416.5409240722656
2023-01-07 09:12:51,736 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,737 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:12:51,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,737 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.7463423013687134
2023-01-07 09:12:51,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -691.8917236328125
2023-01-07 09:12:51,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.392388343811035
2023-01-07 09:12:51,739 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -1.1961755752563477 param sum :: 514.59912109375
2023-01-07 09:12:51,739 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -43.55385208129883
2023-01-07 09:12:51,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,739 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -691.0653686523438
2023-01-07 09:12:51,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8298182487487793
2023-01-07 09:12:51,741 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -689.64990234375 param sum :: 454.1266784667969
2023-01-07 09:12:51,741 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,741 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.4967186152935028
2023-01-07 09:12:51,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -941.941162109375
2023-01-07 09:12:51,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.712332725524902
2023-01-07 09:12:51,743 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.07446551322937012 param sum :: 127.99998474121094
2023-01-07 09:12:51,744 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 13.422676086425781
2023-01-07 09:12:51,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -948.9193725585938
2023-01-07 09:12:51,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4521381855010986
2023-01-07 09:12:51,746 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -965.1119995117188 param sum :: 914.5188598632812
2023-01-07 09:12:51,746 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,746 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,746 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:12:51,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,746 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.4822627604007721
2023-01-07 09:12:51,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -731.8408203125
2023-01-07 09:12:51,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.612063884735107
2023-01-07 09:12:51,748 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 1.0637658834457397 param sum :: 124.80010986328125
2023-01-07 09:12:51,748 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,748 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,748 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -27.334836959838867
2023-01-07 09:12:51,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,749 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -729.8831176757812
2023-01-07 09:12:51,749 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3201425075531006
2023-01-07 09:12:51,750 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -726.6316528320312 param sum :: 463.88055419921875
2023-01-07 09:12:51,750 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,750 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,750 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:12:51,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,751 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,751 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.8066310882568359
2023-01-07 09:12:51,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,751 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -684.089111328125
2023-01-07 09:12:51,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.614425182342529
2023-01-07 09:12:51,753 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 1.4001693725585938 param sum :: 504.19989013671875
2023-01-07 09:12:51,753 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -9.325854301452637
2023-01-07 09:12:51,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -687.8514404296875
2023-01-07 09:12:51,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9240789413452148
2023-01-07 09:12:51,755 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -685.7656860351562 param sum :: 422.9533996582031
2023-01-07 09:12:51,755 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,755 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,755 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:12:51,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,755 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 1.0045086145401
2023-01-07 09:12:51,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,756 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1182.8841552734375
2023-01-07 09:12:51,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.3702392578125
2023-01-07 09:12:51,757 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 1.0566486120224 param sum :: 256.2006530761719
2023-01-07 09:12:51,758 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 47.43771743774414
2023-01-07 09:12:51,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1161.40966796875
2023-01-07 09:12:51,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8890457153320312
2023-01-07 09:12:51,760 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1147.4163818359375 param sum :: -23289.6171875
2023-01-07 09:12:51,760 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,760 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 40.90364074707031
2023-01-07 09:12:51,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -880.8237915039062
2023-01-07 09:12:51,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.062976837158203
2023-01-07 09:12:51,762 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -45.993431091308594 param sum :: 290.59197998046875
2023-01-07 09:12:51,762 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 40.90364074707031
2023-01-07 09:12:51,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,762 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -880.3387451171875
2023-01-07 09:12:51,762 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2877663373947144
2023-01-07 09:12:51,764 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -878.11376953125 param sum :: 551.1388549804688
2023-01-07 09:12:51,764 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -10.693031311035156
2023-01-07 09:12:51,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -253.4803466796875
2023-01-07 09:12:51,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.195050001144409
2023-01-07 09:12:51,766 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -186.89010620117188 param sum :: 1178.000732421875
2023-01-07 09:12:51,766 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -10.693031311035156
2023-01-07 09:12:51,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -243.44244384765625
2023-01-07 09:12:51,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2001304626464844
2023-01-07 09:12:51,768 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -244.96861267089844 param sum :: -31915.09375
2023-01-07 09:12:51,768 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 56.957740783691406
2023-01-07 09:12:51,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,768 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -760.628173828125
2023-01-07 09:12:51,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.394955635070801
2023-01-07 09:12:51,769 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -188.6892852783203 param sum :: 1182.39794921875
2023-01-07 09:12:51,770 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 56.957740783691406
2023-01-07 09:12:51,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -761.293701171875
2023-01-07 09:12:51,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.6688222885131836
2023-01-07 09:12:51,772 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -756.8475341796875 param sum :: -22656.869140625
2023-01-07 09:12:51,772 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,772 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,772 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:12:51,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,772 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:12:51,772 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.0006958246231079102
2023-01-07 09:12:51,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -304.2923583984375
2023-01-07 09:12:51,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.652256011962891
2023-01-07 09:12:51,774 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.27696990966796875 param sum :: 258.7986145019531
2023-01-07 09:12:51,774 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,775 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.58400821685791
2023-01-07 09:12:51,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,775 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -294.58514404296875
2023-01-07 09:12:51,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2601269483566284
2023-01-07 09:12:51,777 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -290.12567138671875 param sum :: -56048.1328125
2023-01-07 09:12:51,777 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,777 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.7613086700439453
2023-01-07 09:12:51,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,777 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -849.8388671875
2023-01-07 09:12:51,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.724967002868652
2023-01-07 09:12:51,778 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -49.36039352416992 param sum :: 296.7996826171875
2023-01-07 09:12:51,778 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.7613086700439453
2023-01-07 09:12:51,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,779 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -852.4280395507812
2023-01-07 09:12:51,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3932678997516632
2023-01-07 09:12:51,781 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -848.429931640625 param sum :: -11713.2646484375
2023-01-07 09:12:51,781 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,781 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,781 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -105.52589416503906
2023-01-07 09:12:51,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,781 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -944.2899780273438
2023-01-07 09:12:51,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3704099655151367
2023-01-07 09:12:51,782 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -191.1612548828125 param sum :: 1196.7982177734375
2023-01-07 09:12:51,782 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -105.52589416503906
2023-01-07 09:12:51,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,783 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -939.9090576171875
2023-01-07 09:12:51,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5849456787109375
2023-01-07 09:12:51,785 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -935.3724365234375 param sum :: 629.6111450195312
2023-01-07 09:12:51,785 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.11709880828857422
2023-01-07 09:12:51,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -907.527587890625
2023-01-07 09:12:51,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.197462558746338
2023-01-07 09:12:51,787 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -49.21269989013672 param sum :: 298.7995910644531
2023-01-07 09:12:51,787 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,787 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,787 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.11709880828857422
2023-01-07 09:12:51,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -908.0955810546875
2023-01-07 09:12:51,787 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.181668996810913
2023-01-07 09:12:51,789 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -904.0663452148438 param sum :: 787.4805297851562
2023-01-07 09:12:51,789 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -19.77853012084961
2023-01-07 09:12:51,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,789 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1045.6658935546875
2023-01-07 09:12:51,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0505900382995605
2023-01-07 09:12:51,791 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -49.47703552246094 param sum :: 301.1997375488281
2023-01-07 09:12:51,791 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -19.77853012084961
2023-01-07 09:12:51,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1046.8660888671875
2023-01-07 09:12:51,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2065870761871338
2023-01-07 09:12:51,793 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1045.593505859375 param sum :: 1023.38818359375
2023-01-07 09:12:51,793 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,793 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,793 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -0.4403038024902344
2023-01-07 09:12:51,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1013.864013671875
2023-01-07 09:12:51,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.316977858543396
2023-01-07 09:12:51,795 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -192.51446533203125 param sum :: 1205.9996337890625
2023-01-07 09:12:51,795 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,795 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,795 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -0.4403038024902344
2023-01-07 09:12:51,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,795 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1011.1742553710938
2023-01-07 09:12:51,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3298355042934418
2023-01-07 09:12:51,797 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -1009.1710205078125 param sum :: 1053.4224853515625
2023-01-07 09:12:51,797 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,798 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 24.86676025390625
2023-01-07 09:12:51,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2921.875
2023-01-07 09:12:51,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.333061933517456
2023-01-07 09:12:51,799 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -48.733619689941406 param sum :: 300.7996826171875
2023-01-07 09:12:51,799 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 24.86676025390625
2023-01-07 09:12:51,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,800 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2921.7333984375
2023-01-07 09:12:51,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.156674861907959
2023-01-07 09:12:51,801 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -2919.310546875 param sum :: 1195.530029296875
2023-01-07 09:12:51,802 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -3.1993484497070312
2023-01-07 09:12:51,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,802 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1078.531005859375
2023-01-07 09:12:51,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5450608730316162
2023-01-07 09:12:51,803 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -48.122886657714844 param sum :: 300.99969482421875
2023-01-07 09:12:51,803 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,803 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -3.1993484497070312
2023-01-07 09:12:51,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,804 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1079.6240234375
2023-01-07 09:12:51,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5364331007003784
2023-01-07 09:12:51,806 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1077.605712890625 param sum :: 1103.8092041015625
2023-01-07 09:12:51,806 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -57.34138107299805
2023-01-07 09:12:51,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,806 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -960.2584838867188
2023-01-07 09:12:51,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.263474941253662
2023-01-07 09:12:51,807 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -193.21588134765625 param sum :: 1211.5987548828125
2023-01-07 09:12:51,807 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,808 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -57.34138107299805
2023-01-07 09:12:51,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,808 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -963.895751953125
2023-01-07 09:12:51,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19804541766643524
2023-01-07 09:12:51,810 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -965.6785888671875 param sum :: 725.5042724609375
2023-01-07 09:12:51,810 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,810 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,810 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.35668182373047
2023-01-07 09:12:51,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1663.041259765625
2023-01-07 09:12:51,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19211050868034363
2023-01-07 09:12:51,811 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -48.20848846435547 param sum :: 302.00006103515625
2023-01-07 09:12:51,812 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,812 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,812 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.35668182373047
2023-01-07 09:12:51,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,812 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1659.693115234375
2023-01-07 09:12:51,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1627633571624756
2023-01-07 09:12:51,814 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1658.8662109375 param sum :: 1541.852294921875
2023-01-07 09:12:51,814 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,814 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,814 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.45500946044922
2023-01-07 09:12:51,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,814 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2184.7900390625
2023-01-07 09:12:51,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23331159353256226
2023-01-07 09:12:51,815 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -48.240962982177734 param sum :: 302.99981689453125
2023-01-07 09:12:51,816 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,816 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,816 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 23.45500946044922
2023-01-07 09:12:51,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,816 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2185.8466796875
2023-01-07 09:12:51,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07658315449953079
2023-01-07 09:12:51,818 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2185.756591796875 param sum :: -1884.8714599609375
2023-01-07 09:12:51,818 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,818 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,818 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -10.569559097290039
2023-01-07 09:12:51,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,818 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -496.169921875
2023-01-07 09:12:51,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0478253364562988
2023-01-07 09:12:51,820 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -193.94635009765625 param sum :: 1218.01171875
2023-01-07 09:12:51,820 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,820 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -10.569559097290039
2023-01-07 09:12:51,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,820 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -497.1214599609375
2023-01-07 09:12:51,820 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8047528266906738
2023-01-07 09:12:51,822 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -499.7704772949219 param sum :: -19852.67578125
2023-01-07 09:12:51,822 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,822 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,822 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -24.68990135192871
2023-01-07 09:12:51,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,823 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -478.95269775390625
2023-01-07 09:12:51,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.40159010887146
2023-01-07 09:12:51,824 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -48.3376579284668 param sum :: 304.7976989746094
2023-01-07 09:12:51,824 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,824 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,824 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -24.68990135192871
2023-01-07 09:12:51,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,825 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -476.70355224609375
2023-01-07 09:12:51,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0315693616867065
2023-01-07 09:12:51,826 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -474.34295654296875 param sum :: -8091.7255859375
2023-01-07 09:12:51,826 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,826 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,827 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -15.866579055786133
2023-01-07 09:12:51,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,827 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -600.37890625
2023-01-07 09:12:51,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04080499708652496
2023-01-07 09:12:51,828 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -48.54998779296875 param sum :: 305.19970703125
2023-01-07 09:12:51,828 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,828 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,828 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -15.866579055786133
2023-01-07 09:12:51,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -599.566650390625
2023-01-07 09:12:51,829 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006052978336811066
2023-01-07 09:12:51,830 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -600.824951171875 param sum :: -22563.58203125
2023-01-07 09:12:51,830 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,831 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,831 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 30.19340705871582
2023-01-07 09:12:51,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,831 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.021240234375
2023-01-07 09:12:51,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8099309802055359
2023-01-07 09:12:51,832 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -194.14266967773438 param sum :: 1220.9986572265625
2023-01-07 09:12:51,832 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,832 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,832 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 30.19340705871582
2023-01-07 09:12:51,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,833 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 17.73699951171875
2023-01-07 09:12:51,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1812375783920288
2023-01-07 09:12:51,834 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 19.63690185546875 param sum :: -49252.640625
2023-01-07 09:12:51,835 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,835 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,835 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 34.705142974853516
2023-01-07 09:12:51,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,835 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -4112.2802734375
2023-01-07 09:12:51,835 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.448355197906494
2023-01-07 09:12:51,836 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -99.41993713378906 param sum :: 611.1992797851562
2023-01-07 09:12:51,836 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,836 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,837 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 34.705142974853516
2023-01-07 09:12:51,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,837 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -4112.54833984375
2023-01-07 09:12:51,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36026298999786377
2023-01-07 09:12:51,839 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -4110.904296875 param sum :: -41658.5859375
2023-01-07 09:12:51,839 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,839 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,839 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 14.189874649047852
2023-01-07 09:12:51,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2465.4404296875
2023-01-07 09:12:51,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31402313709259033
2023-01-07 09:12:51,841 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -98.56181335449219 param sum :: 610.998046875
2023-01-07 09:12:51,841 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,841 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,841 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 14.189874649047852
2023-01-07 09:12:51,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -2465.540283203125
2023-01-07 09:12:51,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22654429078102112
2023-01-07 09:12:51,843 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -2466.11474609375 param sum :: 1754.400146484375
2023-01-07 09:12:51,843 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,843 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,843 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 45.17615509033203
2023-01-07 09:12:51,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2591.01513671875
2023-01-07 09:12:51,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6652020215988159
2023-01-07 09:12:51,845 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -406.0920715332031 param sum :: 2451.39794921875
2023-01-07 09:12:51,845 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,845 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,845 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 45.17615509033203
2023-01-07 09:12:51,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,845 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2592.06103515625
2023-01-07 09:12:51,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18883976340293884
2023-01-07 09:12:51,847 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2591.72705078125 param sum :: -121804.7890625
2023-01-07 09:12:51,847 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,847 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,847 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 92.07999420166016
2023-01-07 09:12:51,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 98.709228515625
2023-01-07 09:12:51,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.41011762619018555
2023-01-07 09:12:51,849 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -407.1494140625 param sum :: 2451.7978515625
2023-01-07 09:12:51,849 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,849 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,849 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 92.07999420166016
2023-01-07 09:12:51,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,849 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 98.0831298828125
2023-01-07 09:12:51,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17145851254463196
2023-01-07 09:12:51,851 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 98.2882080078125 param sum :: -25809.98046875
2023-01-07 09:12:51,851 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,851 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,851 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -24.49972915649414
2023-01-07 09:12:51,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,851 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11453.328125
2023-01-07 09:12:51,851 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4137461185455322
2023-01-07 09:12:51,853 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -99.22276306152344 param sum :: 612.5994262695312
2023-01-07 09:12:51,853 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,853 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,853 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -24.49972915649414
2023-01-07 09:12:51,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,853 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11451.935546875
2023-01-07 09:12:51,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2373013198375702
2023-01-07 09:12:51,855 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -11450.5625 param sum :: 1850.377685546875
2023-01-07 09:12:51,855 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,855 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,855 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -40.928104400634766
2023-01-07 09:12:51,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,855 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1184.28662109375
2023-01-07 09:12:51,856 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26579174399375916
2023-01-07 09:12:51,857 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -99.09544372558594 param sum :: 612.7991333007812
2023-01-07 09:12:51,857 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,857 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,857 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -40.928104400634766
2023-01-07 09:12:51,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,857 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1183.590576171875
2023-01-07 09:12:51,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3463895916938782
2023-01-07 09:12:51,859 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1183.621337890625 param sum :: -32953.3046875
2023-01-07 09:12:51,859 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,859 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,859 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.787105560302734
2023-01-07 09:12:51,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7484.8388671875
2023-01-07 09:12:51,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23559682071208954
2023-01-07 09:12:51,861 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -408.3912353515625 param sum :: 2452.998046875
2023-01-07 09:12:51,861 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,861 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.787105560302734
2023-01-07 09:12:51,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,861 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7484.9091796875
2023-01-07 09:12:51,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25373896956443787
2023-01-07 09:12:51,863 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 7485.29833984375 param sum :: -101829.7421875
2023-01-07 09:12:51,863 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.825760841369629
2023-01-07 09:12:51,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,864 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -15425.564453125
2023-01-07 09:12:51,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03663231432437897
2023-01-07 09:12:51,865 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -98.61163330078125 param sum :: 612.9993896484375
2023-01-07 09:12:51,865 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.825760841369629
2023-01-07 09:12:51,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,865 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -15425.5341796875
2023-01-07 09:12:51,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.015144824981689453
2023-01-07 09:12:51,867 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -15424.91015625 param sum :: -11786.4873046875
2023-01-07 09:12:51,867 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,867 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,868 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -14.30004596710205
2023-01-07 09:12:51,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,868 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -6075.99365234375
2023-01-07 09:12:51,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12508335709571838
2023-01-07 09:12:51,869 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -98.73078918457031 param sum :: 612.9992065429688
2023-01-07 09:12:51,869 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,869 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,869 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -14.30004596710205
2023-01-07 09:12:51,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,870 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -6075.8359375
2023-01-07 09:12:51,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04008064046502113
2023-01-07 09:12:51,871 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -6076.0419921875 param sum :: 1544.148681640625
2023-01-07 09:12:51,872 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,872 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,872 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 6.102652072906494
2023-01-07 09:12:51,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,872 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17434.6328125
2023-01-07 09:12:51,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.931800127029419
2023-01-07 09:12:51,873 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -408.7363586425781 param sum :: 2452.19677734375
2023-01-07 09:12:51,873 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,873 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,873 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 6.102652072906494
2023-01-07 09:12:51,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,874 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 17438.017578125
2023-01-07 09:12:51,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4013155400753021
2023-01-07 09:12:51,876 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 17438.505859375 param sum :: -149623.6875
2023-01-07 09:12:51,876 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:51,876 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:51,877 > [DEBUG] 0 :: 10.828207969665527
2023-01-07 09:12:51,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,880 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0674438625574112
2023-01-07 09:12:51,881 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 654.26708984375
2023-01-07 09:12:51,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,884 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 4.311567306518555
2023-01-07 09:12:51,884 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,885 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -69.48681640625
2023-01-07 09:12:51,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -192.4520263671875
2023-01-07 09:12:51,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,888 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 49.21356964111328
2023-01-07 09:12:51,888 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0005995461251586676
2023-01-07 09:12:51,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,889 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.12809759378433228
2023-01-07 09:12:51,889 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,890 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 27.83643341064453
2023-01-07 09:12:51,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08729544281959534
2023-01-07 09:12:51,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,892 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 195.0169677734375
2023-01-07 09:12:51,892 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.668932084925473e-07
2023-01-07 09:12:51,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,893 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.12669235467910767
2023-01-07 09:12:51,893 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,893 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 194.91001892089844
2023-01-07 09:12:51,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1185070276260376
2023-01-07 09:12:51,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,895 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.858248233795166
2023-01-07 09:12:51,895 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.033675044775009155
2023-01-07 09:12:51,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,897 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 2.440868377685547
2023-01-07 09:12:51,897 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,897 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 6.644648551940918
2023-01-07 09:12:51,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6827564239501953
2023-01-07 09:12:51,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,899 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.017428398132324
2023-01-07 09:12:51,899 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.001453096978366375
2023-01-07 09:12:51,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,900 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.07692977786064148
2023-01-07 09:12:51,900 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,901 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.4527511596679688
2023-01-07 09:12:51,901 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8260288238525391
2023-01-07 09:12:51,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,902 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 477.35882568359375
2023-01-07 09:12:51,903 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.018069902434945107
2023-01-07 09:12:51,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,904 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.05785125494003296
2023-01-07 09:12:51,904 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,904 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 479.5771484375
2023-01-07 09:12:51,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.038064271211624146
2023-01-07 09:12:51,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,906 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -94.70535278320312
2023-01-07 09:12:51,906 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.017009325325489044
2023-01-07 09:12:51,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,907 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.288581609725952
2023-01-07 09:12:51,908 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -92.04334259033203
2023-01-07 09:12:51,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.074615240097046
2023-01-07 09:12:51,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,910 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.158374786376953
2023-01-07 09:12:51,910 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004361878614872694
2023-01-07 09:12:51,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 2.307518482208252
2023-01-07 09:12:51,911 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.131385803222656
2023-01-07 09:12:51,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3403091430664062
2023-01-07 09:12:51,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,913 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -20.56124496459961
2023-01-07 09:12:51,913 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,914 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00826733373105526
2023-01-07 09:12:51,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,915 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.09171181917190552
2023-01-07 09:12:51,915 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,915 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -18.418853759765625
2023-01-07 09:12:51,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23578768968582153
2023-01-07 09:12:51,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,917 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 992.23486328125
2023-01-07 09:12:51,917 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,917 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0006263248506002128
2023-01-07 09:12:51,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,918 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.28112316131591797
2023-01-07 09:12:51,919 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,919 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 991.9510498046875
2023-01-07 09:12:51,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0736159086227417
2023-01-07 09:12:51,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,921 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -4.756248474121094
2023-01-07 09:12:51,921 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,921 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.002139955759048462
2023-01-07 09:12:51,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,922 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.293107807636261
2023-01-07 09:12:51,923 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,923 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -4.752201557159424
2023-01-07 09:12:51,923 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2765052318572998
2023-01-07 09:12:51,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,925 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.1742833852767944
2023-01-07 09:12:51,925 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.009665319696068764
2023-01-07 09:12:51,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.3386702835559845
2023-01-07 09:12:51,926 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,926 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.3502318859100342
2023-01-07 09:12:51,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30383801460266113
2023-01-07 09:12:51,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,928 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 154.5357666015625
2023-01-07 09:12:51,929 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007601540070027113
2023-01-07 09:12:51,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,930 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -2.407951831817627
2023-01-07 09:12:51,930 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,930 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 154.41148376464844
2023-01-07 09:12:51,930 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3835079669952393
2023-01-07 09:12:51,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,932 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.051426410675049
2023-01-07 09:12:51,932 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005623163655400276
2023-01-07 09:12:51,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,933 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.4191989302635193
2023-01-07 09:12:51,934 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,935 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 3.4984021186828613
2023-01-07 09:12:51,935 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6800221800804138
2023-01-07 09:12:51,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,937 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -31.87274932861328
2023-01-07 09:12:51,937 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.000337185338139534
2023-01-07 09:12:51,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.010007856413722038
2023-01-07 09:12:51,938 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -34.831817626953125
2023-01-07 09:12:51,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.020544299855828285
2023-01-07 09:12:51,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,940 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 8.086244583129883
2023-01-07 09:12:51,941 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0002524508163332939
2023-01-07 09:12:51,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,942 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.05724608153104782
2023-01-07 09:12:51,942 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,942 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 5.070331573486328
2023-01-07 09:12:51,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.060253046452999115
2023-01-07 09:12:51,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,944 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.2919864654541016
2023-01-07 09:12:51,944 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002921789651736617
2023-01-07 09:12:51,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.11493045091629028
2023-01-07 09:12:51,946 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -5.188227653503418
2023-01-07 09:12:51,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12479105591773987
2023-01-07 09:12:51,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.258990287780762
2023-01-07 09:12:51,948 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,948 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.009976457804441452
2023-01-07 09:12:51,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.034727469086647034
2023-01-07 09:12:51,949 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,949 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.257757186889648
2023-01-07 09:12:51,950 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014895707368850708
2023-01-07 09:12:51,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,951 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -16.314939498901367
2023-01-07 09:12:51,952 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0038748488295823336
2023-01-07 09:12:51,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,953 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.07641531527042389
2023-01-07 09:12:51,953 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,953 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -16.278793334960938
2023-01-07 09:12:51,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08269651234149933
2023-01-07 09:12:51,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,955 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 47.45090866088867
2023-01-07 09:12:51,955 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.008565317839384079
2023-01-07 09:12:51,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,956 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.2927776277065277
2023-01-07 09:12:51,957 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,957 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 47.328704833984375
2023-01-07 09:12:51,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21799801290035248
2023-01-07 09:12:51,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,959 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -46.26226806640625
2023-01-07 09:12:51,959 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03391600027680397
2023-01-07 09:12:51,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,960 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.08581168949604034
2023-01-07 09:12:51,960 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,961 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -46.66585159301758
2023-01-07 09:12:51,961 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.048318445682525635
2023-01-07 09:12:51,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,963 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 11.16631031036377
2023-01-07 09:12:51,963 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.023434951901435852
2023-01-07 09:12:51,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,964 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.17413823306560516
2023-01-07 09:12:51,964 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,964 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 10.58237361907959
2023-01-07 09:12:51,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21790921688079834
2023-01-07 09:12:51,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,966 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -112.33280944824219
2023-01-07 09:12:51,966 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.015565183945000172
2023-01-07 09:12:51,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.7383546829223633
2023-01-07 09:12:51,968 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -112.84577941894531
2023-01-07 09:12:51,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8005897998809814
2023-01-07 09:12:51,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,970 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -148.62103271484375
2023-01-07 09:12:51,970 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.00665658712387085
2023-01-07 09:12:51,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -2.713381290435791
2023-01-07 09:12:51,972 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -148.93682861328125
2023-01-07 09:12:51,972 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.711801767349243
2023-01-07 09:12:51,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,974 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -8.224165916442871
2023-01-07 09:12:51,974 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02522929012775421
2023-01-07 09:12:51,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -8.279385566711426
2023-01-07 09:12:51,976 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07230416685342789
2023-01-07 09:12:51,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,977 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.155004501342773
2023-01-07 09:12:51,977 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,977 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0848236232995987
2023-01-07 09:12:51,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,978 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -6.99301815032959
2023-01-07 09:12:51,979 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,979 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.5689239501953125
2023-01-07 09:12:51,979 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.839397430419922
2023-01-07 09:12:51,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,981 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 160.19967651367188
2023-01-07 09:12:51,981 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.025918135419487953
2023-01-07 09:12:51,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,982 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -1.668550968170166
2023-01-07 09:12:51,982 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,983 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 160.46656799316406
2023-01-07 09:12:51,983 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6372458934783936
2023-01-07 09:12:51,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 201.2044677734375
2023-01-07 09:12:51,985 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,985 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012405723333358765
2023-01-07 09:12:51,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 1.0517382621765137
2023-01-07 09:12:51,986 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 204.0872344970703
2023-01-07 09:12:51,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8332149982452393
2023-01-07 09:12:51,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 53.90888977050781
2023-01-07 09:12:51,989 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,989 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004250082187354565
2023-01-07 09:12:51,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 59.16148376464844
2023-01-07 09:12:51,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.059766143560409546
2023-01-07 09:12:51,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 315.6029052734375
2023-01-07 09:12:51,992 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06482266634702682
2023-01-07 09:12:51,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 319.09307861328125
2023-01-07 09:12:51,993 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17812101542949677
2023-01-07 09:12:51,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,995 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -33.03346252441406
2023-01-07 09:12:51,995 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.034723129123449326
2023-01-07 09:12:51,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -24.63837432861328
2023-01-07 09:12:51,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3430124819278717
2023-01-07 09:12:51,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,998 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 61.18376922607422
2023-01-07 09:12:51,998 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:51,998 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11993277072906494
2023-01-07 09:12:51,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:51,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:51,999 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 66.2073974609375
2023-01-07 09:12:51,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13355664908885956
2023-01-07 09:12:52,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,001 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 25.888065338134766
2023-01-07 09:12:52,001 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,001 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30014318227767944
2023-01-07 09:12:52,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 17.88008689880371
2023-01-07 09:12:52,002 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11774323880672455
2023-01-07 09:12:52,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,004 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -58.834815979003906
2023-01-07 09:12:52,004 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5940854549407959
2023-01-07 09:12:52,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,005 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -54.85588073730469
2023-01-07 09:12:52,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3465043902397156
2023-01-07 09:12:52,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,007 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 3.396686553955078
2023-01-07 09:12:52,007 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,007 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16133105754852295
2023-01-07 09:12:52,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,008 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 3.3183517456054688
2023-01-07 09:12:52,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19889160990715027
2023-01-07 09:12:52,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,010 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 7.441701889038086
2023-01-07 09:12:52,010 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5656570196151733
2023-01-07 09:12:52,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,011 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.2819225788116455
2023-01-07 09:12:52,011 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,012 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -8.271994590759277
2023-01-07 09:12:52,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5274205207824707
2023-01-07 09:12:52,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,014 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -12.329330444335938
2023-01-07 09:12:52,014 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.036816120147705
2023-01-07 09:12:52,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,015 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 9.297557830810547
2023-01-07 09:12:52,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9418401122093201
2023-01-07 09:12:52,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 18.99081039428711
2023-01-07 09:12:52,017 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13809502124786377
2023-01-07 09:12:52,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,018 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 36.5141487121582
2023-01-07 09:12:52,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04467064142227173
2023-01-07 09:12:52,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,020 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 52.33000946044922
2023-01-07 09:12:52,020 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0685932636260986
2023-01-07 09:12:52,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,021 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.5199003219604492
2023-01-07 09:12:52,021 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,022 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 83.68650817871094
2023-01-07 09:12:52,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3933684825897217
2023-01-07 09:12:52,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,024 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -204.490966796875
2023-01-07 09:12:52,024 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3064699172973633
2023-01-07 09:12:52,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,025 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -201.6421356201172
2023-01-07 09:12:52,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.076986789703369
2023-01-07 09:12:52,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,027 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 122.75447845458984
2023-01-07 09:12:52,027 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8406276702880859
2023-01-07 09:12:52,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,028 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 126.82331848144531
2023-01-07 09:12:52,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8952610492706299
2023-01-07 09:12:52,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,030 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -75.96282958984375
2023-01-07 09:12:52,030 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7778525352478027
2023-01-07 09:12:52,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,031 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.6585818529129028
2023-01-07 09:12:52,031 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,031 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -68.168212890625
2023-01-07 09:12:52,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.554426908493042
2023-01-07 09:12:52,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,033 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -63.20577621459961
2023-01-07 09:12:52,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2697510719299316
2023-01-07 09:12:52,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,035 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.5175758600234985
2023-01-07 09:12:52,035 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,035 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 177.59591674804688
2023-01-07 09:12:52,035 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,036 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -51.39040756225586
2023-01-07 09:12:52,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.613197326660156
2023-01-07 09:12:52,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,038 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -51.389644622802734
2023-01-07 09:12:52,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7997446060180664
2023-01-07 09:12:52,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 178.79800415039062
2023-01-07 09:12:52,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.40768432617188
2023-01-07 09:12:52,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,041 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 179.45257568359375
2023-01-07 09:12:52,041 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.213417053222656
2023-01-07 09:12:52,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,042 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.3493072986602783
2023-01-07 09:12:52,042 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,042 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 229.56137084960938
2023-01-07 09:12:52,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.117414474487305
2023-01-07 09:12:52,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,044 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 331.7276611328125
2023-01-07 09:12:52,045 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.808198928833008
2023-01-07 09:12:52,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,046 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 2.587212562561035
2023-01-07 09:12:52,046 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,046 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -34.921417236328125
2023-01-07 09:12:52,046 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3310413360595703
2023-01-07 09:12:52,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,048 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 424.9887390136719
2023-01-07 09:12:52,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.33253288269043
2023-01-07 09:12:52,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,050 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 44.27909469604492
2023-01-07 09:12:52,050 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,050 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -32.45809555053711
2023-01-07 09:12:52,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.47760772705078
2023-01-07 09:12:52,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,052 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 502.9392395019531
2023-01-07 09:12:52,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.496448516845703
2023-01-07 09:12:52,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,053 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 175.24923706054688
2023-01-07 09:12:52,054 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.459716796875
2023-01-07 09:12:52,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,055 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 591.835693359375
2023-01-07 09:12:52,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9925482273101807
2023-01-07 09:12:52,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,056 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.7409117221832275
2023-01-07 09:12:52,057 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,057 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 129.10052490234375
2023-01-07 09:12:52,057 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,057 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 679.7474975585938
2023-01-07 09:12:52,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,057 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 18.69156265258789
2023-01-07 09:12:52,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.55500793457031
2023-01-07 09:12:52,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,060 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 704.7234497070312
2023-01-07 09:12:52,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.617652893066406
2023-01-07 09:12:52,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,061 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 264.82965087890625
2023-01-07 09:12:52,061 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 133.20001220703125
2023-01-07 09:12:52,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,063 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 763.8330688476562
2023-01-07 09:12:52,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4649889469146729
2023-01-07 09:12:52,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,064 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 624.6620483398438
2023-01-07 09:12:52,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,064 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 13.35928726196289
2023-01-07 09:12:52,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 134.14813232421875
2023-01-07 09:12:52,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,066 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -83.45816040039062
2023-01-07 09:12:52,066 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.541045188903809
2023-01-07 09:12:52,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,068 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 0.5183775424957275
2023-01-07 09:12:52,068 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,068 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 33.98683166503906
2023-01-07 09:12:52,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.26224899291992
2023-01-07 09:12:52,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,070 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 423.8963623046875
2023-01-07 09:12:52,070 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8174171447753906
2023-01-07 09:12:52,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,071 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 773.932373046875
2023-01-07 09:12:52,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.93048095703125
2023-01-07 09:12:52,076 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:12:52,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,076 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -138.71786499023438
2023-01-07 09:12:52,077 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,077 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 962.891357421875
2023-01-07 09:12:52,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 24.763675689697266
2023-01-07 09:12:52,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,079 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 131.7098846435547
2023-01-07 09:12:52,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,079 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -166.29856872558594
2023-01-07 09:12:52,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 108.91446685791016
2023-01-07 09:12:52,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,081 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 53.91150665283203
2023-01-07 09:12:52,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -35.934608459472656
2023-01-07 09:12:52,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -48.62321853637695
2023-01-07 09:12:52,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,083 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -16.61248016357422
2023-01-07 09:12:52,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,083 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -74.64170837402344
2023-01-07 09:12:52,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 38.63533020019531
2023-01-07 09:12:52,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 72.85488891601562
2023-01-07 09:12:52,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,084 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -12.318882942199707
2023-01-07 09:12:52,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,085 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 326.847412109375
2023-01-07 09:12:52,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,085 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 41.42500686645508
2023-01-07 09:12:52,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,085 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 206.31204223632812
2023-01-07 09:12:52,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 160.62661743164062
2023-01-07 09:12:52,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -2.648606777191162
2023-01-07 09:12:52,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -8.144524574279785
2023-01-07 09:12:52,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,087 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -149.29530334472656
2023-01-07 09:12:52,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,087 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -112.7868881225586
2023-01-07 09:12:52,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,087 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 10.335736274719238
2023-01-07 09:12:52,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -46.478851318359375
2023-01-07 09:12:52,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 47.29228973388672
2023-01-07 09:12:52,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -16.158191680908203
2023-01-07 09:12:52,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 105.642333984375
2023-01-07 09:12:52,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.225308418273926
2023-01-07 09:12:52,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -5.243588447570801
2023-01-07 09:12:52,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 5.1348371505737305
2023-01-07 09:12:52,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -34.68950271606445
2023-01-07 09:12:52,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 3.4996519088745117
2023-01-07 09:12:52,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 154.81663513183594
2023-01-07 09:12:52,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.7397267818450928
2023-01-07 09:12:52,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,093 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -4.36691951751709
2023-01-07 09:12:52,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,094 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 991.8681640625
2023-01-07 09:12:52,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 1105.0399169921875
2023-01-07 09:12:52,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,095 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -20.72557258605957
2023-01-07 09:12:52,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,095 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 16.623844146728516
2023-01-07 09:12:52,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,096 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -93.42477416992188
2023-01-07 09:12:52,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,096 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 480.14019775390625
2023-01-07 09:12:52,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 849.32666015625
2023-01-07 09:12:52,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,097 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3.324721336364746
2023-01-07 09:12:52,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,098 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 6.660318851470947
2023-01-07 09:12:52,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,098 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 194.9047088623047
2023-01-07 09:12:52,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 282.4395446777344
2023-01-07 09:12:52,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,099 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 28.99240493774414
2023-01-07 09:12:52,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,099 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 286.2449951171875
2023-01-07 09:12:52,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -365.9056396484375
2023-01-07 09:12:52,939 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -0.0013978638453409076 param sum :: 190.36534118652344
2023-01-07 09:12:52,939 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,939 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,939 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.20001220703125
2023-01-07 09:12:52,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,940 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -8.377982139587402
2023-01-07 09:12:52,940 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,940 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1010.1624755859375
2023-01-07 09:12:52,940 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.79568099975586
2023-01-07 09:12:52,942 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -8.273677825927734 param sum :: 65.1376724243164
2023-01-07 09:12:52,942 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,942 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,943 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -23.220640182495117
2023-01-07 09:12:52,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,943 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 146.1337432861328
2023-01-07 09:12:52,943 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,943 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1008.0866088867188
2023-01-07 09:12:52,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 142.1343994140625
2023-01-07 09:12:52,945 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 131.88470458984375 param sum :: -46.274925231933594
2023-01-07 09:12:52,945 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,945 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,945 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 318.345947265625
2023-01-07 09:12:52,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,945 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1004.624267578125
2023-01-07 09:12:52,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.826307773590088
2023-01-07 09:12:52,947 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 28.523818969726562 param sum :: 61.47555923461914
2023-01-07 09:12:52,947 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,947 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,947 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 318.345947265625
2023-01-07 09:12:52,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,947 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1007.063720703125
2023-01-07 09:12:52,947 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.81876277923584
2023-01-07 09:12:52,949 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 1009.0120849609375 param sum :: 455.1439208984375
2023-01-07 09:12:52,949 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,949 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,949 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 63.39976119995117
2023-01-07 09:12:52,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,949 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.7675487995147705
2023-01-07 09:12:52,949 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,949 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 66.33189392089844
2023-01-07 09:12:52,950 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,950 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 855.006103515625
2023-01-07 09:12:52,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 110.23439025878906
2023-01-07 09:12:52,952 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 2.4503893852233887 param sum :: 62.27489471435547
2023-01-07 09:12:52,952 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,952 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,952 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 194.7434844970703
2023-01-07 09:12:52,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,952 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 860.649169921875
2023-01-07 09:12:52,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.23111343383789
2023-01-07 09:12:52,953 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -26.89204978942871 param sum :: 9.67458438873291
2023-01-07 09:12:52,953 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,953 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,954 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.7999725341797
2023-01-07 09:12:52,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,954 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -10.403841018676758
2023-01-07 09:12:52,954 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,954 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 337.76513671875
2023-01-07 09:12:52,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.08576202392578
2023-01-07 09:12:52,956 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -9.056427001953125 param sum :: 256.68341064453125
2023-01-07 09:12:52,956 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,956 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,956 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 113.75157165527344
2023-01-07 09:12:52,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,956 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 321.31011962890625
2023-01-07 09:12:52,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.443172454833984
2023-01-07 09:12:52,957 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1073.2655029296875 param sum :: 109.84890747070312
2023-01-07 09:12:52,958 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,958 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,958 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 113.75157165527344
2023-01-07 09:12:52,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,958 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 356.30401611328125
2023-01-07 09:12:52,958 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.837407112121582
2023-01-07 09:12:52,959 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 32.66095733642578 param sum :: 252.15994262695312
2023-01-07 09:12:52,959 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,959 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,959 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 113.75157165527344
2023-01-07 09:12:52,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,960 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 339.5054626464844
2023-01-07 09:12:52,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.979376316070557
2023-01-07 09:12:52,961 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 376.00927734375 param sum :: 86.02381896972656
2023-01-07 09:12:52,961 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,961 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,961 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.4000129699707
2023-01-07 09:12:52,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,961 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.5835392475128174
2023-01-07 09:12:52,962 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,962 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 334.1123962402344
2023-01-07 09:12:52,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 130.15017700195312
2023-01-07 09:12:52,963 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -2.9731736183166504 param sum :: 61.364017486572266
2023-01-07 09:12:52,963 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,964 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,964 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 113.75157165527344
2023-01-07 09:12:52,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,964 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 353.88653564453125
2023-01-07 09:12:52,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,964 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 139.03611755371094
2023-01-07 09:12:52,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39652740955352783
2023-01-07 09:12:52,966 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 345.533203125 param sum :: 139.19224548339844
2023-01-07 09:12:52,966 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,966 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,966 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 65.79995727539062
2023-01-07 09:12:52,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.7966647148132324
2023-01-07 09:12:52,966 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,967 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 174.89427185058594
2023-01-07 09:12:52,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.279237747192383
2023-01-07 09:12:52,968 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -3.0320827960968018 param sum :: 66.64739990234375
2023-01-07 09:12:52,968 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,969 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,969 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 194.7434844970703
2023-01-07 09:12:52,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 844.6175537109375
2023-01-07 09:12:52,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.7157182693481445
2023-01-07 09:12:52,970 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 136.88937377929688 param sum :: 122.30560302734375
2023-01-07 09:12:52,970 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,970 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,970 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 203.97998046875
2023-01-07 09:12:52,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,971 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 106.22055053710938
2023-01-07 09:12:52,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.83039093017578
2023-01-07 09:12:52,972 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 26.174571990966797 param sum :: 269.62664794921875
2023-01-07 09:12:52,972 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,972 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,972 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 203.97998046875
2023-01-07 09:12:52,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 114.75569152832031
2023-01-07 09:12:52,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,973 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 837.4502563476562
2023-01-07 09:12:52,973 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.65880012512207
2023-01-07 09:12:52,974 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 110.92618560791016 param sum :: 317.18017578125
2023-01-07 09:12:52,974 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,974 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,975 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 194.7434844970703
2023-01-07 09:12:52,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,975 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 813.9906616210938
2023-01-07 09:12:52,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.32416534423828
2023-01-07 09:12:52,976 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 8.83224105834961 param sum :: 66.52763366699219
2023-01-07 09:12:52,976 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,976 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,976 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 194.7434844970703
2023-01-07 09:12:52,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,976 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 792.8019409179688
2023-01-07 09:12:52,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.284379959106445
2023-01-07 09:12:52,978 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 798.7559814453125 param sum :: 199.84332275390625
2023-01-07 09:12:52,978 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,978 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,978 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.19989013671875
2023-01-07 09:12:52,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,979 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -2.316746950149536
2023-01-07 09:12:52,979 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,979 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 77.75527954101562
2023-01-07 09:12:52,979 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,979 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 211.43063354492188
2023-01-07 09:12:52,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 135.6357879638672
2023-01-07 09:12:52,981 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -3.3787248134613037 param sum :: 64.7784423828125
2023-01-07 09:12:52,981 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,981 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,981 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 378.1451721191406
2023-01-07 09:12:52,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,982 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 139.90699768066406
2023-01-07 09:12:52,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.286491870880127
2023-01-07 09:12:52,983 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 100.77005004882812 param sum :: 16.95813751220703
2023-01-07 09:12:52,983 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,983 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,983 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 335.14385986328125
2023-01-07 09:12:52,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 124.44354248046875
2023-01-07 09:12:52,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.377077102661133
2023-01-07 09:12:52,985 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 10.665574073791504 param sum :: 273.02618408203125
2023-01-07 09:12:52,985 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,985 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,985 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 378.1451721191406
2023-01-07 09:12:52,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,985 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 140.88406372070312
2023-01-07 09:12:52,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.078632354736328
2023-01-07 09:12:52,987 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 131.16085815429688 param sum :: 532.1267700195312
2023-01-07 09:12:52,987 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,987 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,987 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 378.1451721191406
2023-01-07 09:12:52,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,987 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 146.68380737304688
2023-01-07 09:12:52,987 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.97500228881836
2023-01-07 09:12:52,988 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 1.7782245874404907 param sum :: 138.61135864257812
2023-01-07 09:12:52,988 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,988 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,989 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 378.1451721191406
2023-01-07 09:12:52,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,989 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 163.32589721679688
2023-01-07 09:12:52,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.815288782119751
2023-01-07 09:12:52,990 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 163.90753173828125 param sum :: 564.7163696289062
2023-01-07 09:12:52,991 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,991 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,991 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 129.00006103515625
2023-01-07 09:12:52,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,991 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -1.4367642402648926
2023-01-07 09:12:52,991 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,991 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -0.6869630813598633
2023-01-07 09:12:52,991 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.5257720947265625
2023-01-07 09:12:52,993 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -1.948256015777588 param sum :: 130.4813232421875
2023-01-07 09:12:52,993 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,993 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,993 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 304.6231689453125
2023-01-07 09:12:52,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,993 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 2.0612926483154297
2023-01-07 09:12:52,993 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.052133560180664
2023-01-07 09:12:52,996 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -12.527360916137695 param sum :: 523.105712890625
2023-01-07 09:12:52,996 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,996 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,996 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 510.79949951171875
2023-01-07 09:12:52,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7280659079551697
2023-01-07 09:12:52,996 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:52,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 57.60125732421875
2023-01-07 09:12:52,997 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.32670783996582
2023-01-07 09:12:52,998 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 1.284277319908142 param sum :: 510.10357666015625
2023-01-07 09:12:52,998 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:52,998 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:52,998 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 241.32803344726562
2023-01-07 09:12:52,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:52,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:52,999 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 53.53631591796875
2023-01-07 09:12:52,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8822884559631348
2023-01-07 09:12:53,000 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 44.50483703613281 param sum :: 345.66143798828125
2023-01-07 09:12:53,000 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,000 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,000 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 227.05531311035156
2023-01-07 09:12:53,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,000 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 69.64764404296875
2023-01-07 09:12:53,001 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.884208679199219
2023-01-07 09:12:53,002 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 14.302359580993652 param sum :: 573.3621826171875
2023-01-07 09:12:53,002 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,002 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,002 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 227.05531311035156
2023-01-07 09:12:53,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,002 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 61.689422607421875
2023-01-07 09:12:53,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.676937103271484
2023-01-07 09:12:53,004 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 66.28953552246094 param sum :: 258.7024841308594
2023-01-07 09:12:53,004 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,004 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,004 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.00003814697266
2023-01-07 09:12:53,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,004 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -2.167415142059326
2023-01-07 09:12:53,004 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,005 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 77.512451171875
2023-01-07 09:12:53,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.282556533813477
2023-01-07 09:12:53,006 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.5951664447784424 param sum :: 127.59780883789062
2023-01-07 09:12:53,006 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,006 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,006 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 324.7969970703125
2023-01-07 09:12:53,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,007 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 65.72203063964844
2023-01-07 09:12:53,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.025520324707031
2023-01-07 09:12:53,008 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 66.85149383544922 param sum :: 197.9300994873047
2023-01-07 09:12:53,008 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,008 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,009 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 127.39988708496094
2023-01-07 09:12:53,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.6689074039459229
2023-01-07 09:12:53,009 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -43.194740295410156
2023-01-07 09:12:53,009 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.513155937194824
2023-01-07 09:12:53,011 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 1.27145254611969 param sum :: 128.03244018554688
2023-01-07 09:12:53,011 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,011 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,011 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 352.4848327636719
2023-01-07 09:12:53,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,011 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -38.86940002441406
2023-01-07 09:12:53,011 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.039985179901123
2023-01-07 09:12:53,013 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -36.18332290649414 param sum :: 443.7290344238281
2023-01-07 09:12:53,013 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,013 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,013 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 391.298583984375
2023-01-07 09:12:53,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,013 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -42.59030532836914
2023-01-07 09:12:53,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8825349807739258
2023-01-07 09:12:53,014 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 4.031422138214111 param sum :: 575.298828125
2023-01-07 09:12:53,014 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,015 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 391.298583984375
2023-01-07 09:12:53,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,015 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -30.709714889526367
2023-01-07 09:12:53,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1849216222763062
2023-01-07 09:12:53,017 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -33.8690185546875 param sum :: 601.124755859375
2023-01-07 09:12:53,017 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,017 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 127.60030364990234
2023-01-07 09:12:53,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.8422929644584656
2023-01-07 09:12:53,017 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -9.884246826171875
2023-01-07 09:12:53,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.70811128616333
2023-01-07 09:12:53,019 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -1.165252685546875 param sum :: 127.94561767578125
2023-01-07 09:12:53,019 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 476.7754211425781
2023-01-07 09:12:53,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,019 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -6.132743835449219
2023-01-07 09:12:53,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7085399627685547
2023-01-07 09:12:53,021 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -4.796787261962891 param sum :: 716.2728881835938
2023-01-07 09:12:53,021 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 129.99981689453125
2023-01-07 09:12:53,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,022 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.12488055229187012
2023-01-07 09:12:53,022 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,022 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -71.57965087890625
2023-01-07 09:12:53,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.409114360809326
2023-01-07 09:12:53,023 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.10401919484138489 param sum :: 130.6510467529297
2023-01-07 09:12:53,023 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,023 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,024 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 416.5409240722656
2023-01-07 09:12:53,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,024 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -73.40934753417969
2023-01-07 09:12:53,024 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.631312370300293
2023-01-07 09:12:53,025 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -86.60760498046875 param sum :: 770.6195068359375
2023-01-07 09:12:53,025 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 514.59912109375
2023-01-07 09:12:53,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,026 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -1.7295844554901123
2023-01-07 09:12:53,026 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,026 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 40.6715202331543
2023-01-07 09:12:53,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5829349160194397
2023-01-07 09:12:53,028 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.7818980813026428 param sum :: 516.930908203125
2023-01-07 09:12:53,028 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,028 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,028 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 454.1266784667969
2023-01-07 09:12:53,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,028 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 41.45464324951172
2023-01-07 09:12:53,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.39867401123047
2023-01-07 09:12:53,030 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 39.25639724731445 param sum :: 694.16796875
2023-01-07 09:12:53,030 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,030 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 127.99998474121094
2023-01-07 09:12:53,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,030 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.03227311372756958
2023-01-07 09:12:53,030 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,031 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 78.3927001953125
2023-01-07 09:12:53,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.564573287963867
2023-01-07 09:12:53,032 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.10053795576095581 param sum :: 128.4334716796875
2023-01-07 09:12:53,032 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,032 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 914.5188598632812
2023-01-07 09:12:53,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,033 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 78.90071868896484
2023-01-07 09:12:53,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.143241882324219
2023-01-07 09:12:53,035 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 79.06243896484375 param sum :: 1299.8602294921875
2023-01-07 09:12:53,035 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,035 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,035 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 124.80010986328125
2023-01-07 09:12:53,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,035 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.10967960953712463
2023-01-07 09:12:53,035 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,035 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -11.01822280883789
2023-01-07 09:12:53,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.011206150054932
2023-01-07 09:12:53,037 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.9790631532669067 param sum :: 122.9132080078125
2023-01-07 09:12:53,037 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 463.88055419921875
2023-01-07 09:12:53,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -20.486080169677734
2023-01-07 09:12:53,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8062953352928162
2023-01-07 09:12:53,039 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -23.243589401245117 param sum :: 835.6869506835938
2023-01-07 09:12:53,039 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,039 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 504.19989013671875
2023-01-07 09:12:53,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,040 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.39064592123031616
2023-01-07 09:12:53,040 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,040 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 192.25955200195312
2023-01-07 09:12:53,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.138591766357422
2023-01-07 09:12:53,041 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.4468264579772949 param sum :: 500.6766357421875
2023-01-07 09:12:53,041 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,041 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,042 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 422.9533996582031
2023-01-07 09:12:53,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,042 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 172.25221252441406
2023-01-07 09:12:53,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.866660118103027
2023-01-07 09:12:53,043 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 166.3690948486328 param sum :: 311.65869140625
2023-01-07 09:12:53,044 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,044 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,044 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.2006530761719
2023-01-07 09:12:53,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,044 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -6.315506458282471
2023-01-07 09:12:53,044 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,044 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -486.9691467285156
2023-01-07 09:12:53,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.851644039154053
2023-01-07 09:12:53,046 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -4.492920875549316 param sum :: 257.6792907714844
2023-01-07 09:12:53,046 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,046 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,046 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -23289.6171875
2023-01-07 09:12:53,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,046 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -481.8019714355469
2023-01-07 09:12:53,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.248507976531982
2023-01-07 09:12:53,048 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -477.88580322265625 param sum :: -39317.8671875
2023-01-07 09:12:53,048 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,048 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,048 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 551.1388549804688
2023-01-07 09:12:53,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,048 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 206.31204223632812
2023-01-07 09:12:53,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.148941993713379
2023-01-07 09:12:53,050 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 3.716017723083496 param sum :: 307.33203125
2023-01-07 09:12:53,050 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,050 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 551.1388549804688
2023-01-07 09:12:53,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,050 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 210.8736572265625
2023-01-07 09:12:53,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7191791534423828
2023-01-07 09:12:53,052 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 213.137939453125 param sum :: 375.604736328125
2023-01-07 09:12:53,052 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,052 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,052 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31915.09375
2023-01-07 09:12:53,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,052 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 160.83261108398438
2023-01-07 09:12:53,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.5589447021484375
2023-01-07 09:12:53,054 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -1.2131054401397705 param sum :: 1276.4940185546875
2023-01-07 09:12:53,054 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,054 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,054 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31915.09375
2023-01-07 09:12:53,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,054 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 161.5320587158203
2023-01-07 09:12:53,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1768698692321777
2023-01-07 09:12:53,056 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 161.0564727783203 param sum :: -54000.2890625
2023-01-07 09:12:53,056 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,056 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22656.869140625
2023-01-07 09:12:53,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,056 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -2.9554688930511475
2023-01-07 09:12:53,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0633301734924316
2023-01-07 09:12:53,057 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -6.317020416259766 param sum :: 1292.687744140625
2023-01-07 09:12:53,057 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,058 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,058 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22656.869140625
2023-01-07 09:12:53,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,058 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 0.9324497580528259
2023-01-07 09:12:53,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4673331379890442
2023-01-07 09:12:53,060 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 2.4655659198760986 param sum :: -37895.5625
2023-01-07 09:12:53,060 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,060 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,060 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 258.7986145019531
2023-01-07 09:12:53,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,060 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 4.568593978881836
2023-01-07 09:12:53,060 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:12:53,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,060 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -8.157090187072754
2023-01-07 09:12:53,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.321205735206604
2023-01-07 09:12:53,062 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 3.8373186588287354 param sum :: 253.8910369873047
2023-01-07 09:12:53,062 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,062 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,062 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -56048.1328125
2023-01-07 09:12:53,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,062 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -3.1436409950256348
2023-01-07 09:12:53,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9422690868377686
2023-01-07 09:12:53,064 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -2.2461390495300293 param sum :: -93591.546875
2023-01-07 09:12:53,064 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,064 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,064 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11713.2646484375
2023-01-07 09:12:53,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,065 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 2.4684348106384277
2023-01-07 09:12:53,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.833949089050293
2023-01-07 09:12:53,066 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -2.361501932144165 param sum :: 323.5942687988281
2023-01-07 09:12:53,066 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,066 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11713.2646484375
2023-01-07 09:12:53,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,066 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 30.423322677612305
2023-01-07 09:12:53,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.196099042892456
2023-01-07 09:12:53,068 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 32.808013916015625 param sum :: -19706.00390625
2023-01-07 09:12:53,068 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,068 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,068 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 629.6111450195312
2023-01-07 09:12:53,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,068 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -37.082637786865234
2023-01-07 09:12:53,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8906354904174805
2023-01-07 09:12:53,070 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.28171777725219727 param sum :: 1308.80322265625
2023-01-07 09:12:53,070 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,070 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 629.6111450195312
2023-01-07 09:12:53,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,070 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -36.95746994018555
2023-01-07 09:12:53,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.365311622619629
2023-01-07 09:12:53,072 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -36.67476272583008 param sum :: 1012.1820678710938
2023-01-07 09:12:53,072 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,072 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 787.4805297851562
2023-01-07 09:12:53,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,072 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 120.50586700439453
2023-01-07 09:12:53,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05817319452762604
2023-01-07 09:12:53,074 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.16704148054122925 param sum :: 326.24591064453125
2023-01-07 09:12:53,074 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 787.4805297851562
2023-01-07 09:12:53,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,074 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 120.58494567871094
2023-01-07 09:12:53,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2127656787633896
2023-01-07 09:12:53,076 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 120.61197662353516 param sum :: 1242.0123291015625
2023-01-07 09:12:53,076 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,076 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,076 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1023.38818359375
2023-01-07 09:12:53,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,076 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 22.063016891479492
2023-01-07 09:12:53,076 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9194185733795166
2023-01-07 09:12:53,078 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.3520800471305847 param sum :: 330.5401611328125
2023-01-07 09:12:53,078 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,078 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1023.38818359375
2023-01-07 09:12:53,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,078 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 18.12703514099121
2023-01-07 09:12:53,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.282526969909668
2023-01-07 09:12:53,080 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 14.317459106445312 param sum :: 1597.624755859375
2023-01-07 09:12:53,080 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1053.4224853515625
2023-01-07 09:12:53,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,080 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 61.525108337402344
2023-01-07 09:12:53,080 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6357927322387695
2023-01-07 09:12:53,081 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.40087175369262695 param sum :: 1327.030029296875
2023-01-07 09:12:53,082 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1053.4224853515625
2023-01-07 09:12:53,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,082 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 66.2542724609375
2023-01-07 09:12:53,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.655457496643066
2023-01-07 09:12:53,084 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 67.30145263671875 param sum :: 1698.5218505859375
2023-01-07 09:12:53,084 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,084 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,084 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1195.530029296875
2023-01-07 09:12:53,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,084 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 615.4574584960938
2023-01-07 09:12:53,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4551369547843933
2023-01-07 09:12:53,085 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.08914905786514282 param sum :: 329.90380859375
2023-01-07 09:12:53,086 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,086 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,086 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1195.530029296875
2023-01-07 09:12:53,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 615.3660888671875
2023-01-07 09:12:53,086 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06363269686698914
2023-01-07 09:12:53,088 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 615.662353515625 param sum :: 1845.7841796875
2023-01-07 09:12:53,088 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,088 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1103.8092041015625
2023-01-07 09:12:53,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 89.37416076660156
2023-01-07 09:12:53,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1667596101760864
2023-01-07 09:12:53,089 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.038121312856674194 param sum :: 331.26666259765625
2023-01-07 09:12:53,089 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,089 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1103.8092041015625
2023-01-07 09:12:53,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,090 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 89.07246398925781
2023-01-07 09:12:53,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06023436784744263
2023-01-07 09:12:53,091 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 88.46835327148438 param sum :: 1760.8192138671875
2023-01-07 09:12:53,092 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 725.5042724609375
2023-01-07 09:12:53,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 58.96052551269531
2023-01-07 09:12:53,092 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1337368488311768
2023-01-07 09:12:53,093 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -3.03669810295105 param sum :: 1333.680908203125
2023-01-07 09:12:53,093 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,093 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,094 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 725.5042724609375
2023-01-07 09:12:53,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,094 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 57.979408264160156
2023-01-07 09:12:53,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.417531967163086
2023-01-07 09:12:53,095 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 55.90461730957031 param sum :: 832.803466796875
2023-01-07 09:12:53,096 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1541.852294921875
2023-01-07 09:12:53,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,096 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 298.0441589355469
2023-01-07 09:12:53,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3498276472091675
2023-01-07 09:12:53,097 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -3.052786111831665 param sum :: 332.7917175292969
2023-01-07 09:12:53,097 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,097 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,097 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1541.852294921875
2023-01-07 09:12:53,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 298.0188293457031
2023-01-07 09:12:53,098 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03478092700242996
2023-01-07 09:12:53,099 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 297.9601745605469 param sum :: 2073.80810546875
2023-01-07 09:12:53,099 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,100 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1884.8714599609375
2023-01-07 09:12:53,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -938.1261596679688
2023-01-07 09:12:53,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2747822999954224
2023-01-07 09:12:53,101 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -3.111870765686035 param sum :: 334.2900085449219
2023-01-07 09:12:53,101 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1884.8714599609375
2023-01-07 09:12:53,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -938.3491821289062
2023-01-07 09:12:53,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7164729833602905
2023-01-07 09:12:53,103 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -938.0166015625 param sum :: -3481.74951171875
2023-01-07 09:12:53,103 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19852.67578125
2023-01-07 09:12:53,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,104 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.958215713500977
2023-01-07 09:12:53,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3633735179901123
2023-01-07 09:12:53,105 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -2.966405153274536 param sum :: 1343.12060546875
2023-01-07 09:12:53,105 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,105 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19852.67578125
2023-01-07 09:12:53,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.469728469848633
2023-01-07 09:12:53,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.371731281280518
2023-01-07 09:12:53,107 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -12.142900466918945 param sum :: -33195.875
2023-01-07 09:12:53,107 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -8091.7255859375
2023-01-07 09:12:53,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 678.5592041015625
2023-01-07 09:12:53,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0206170082092285
2023-01-07 09:12:53,109 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -2.9525396823883057 param sum :: 336.67816162109375
2023-01-07 09:12:53,109 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -8091.7255859375
2023-01-07 09:12:53,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 679.2685546875
2023-01-07 09:12:53,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8735593557357788
2023-01-07 09:12:53,111 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 679.4520874023438 param sum :: -14667.45703125
2023-01-07 09:12:53,111 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22563.58203125
2023-01-07 09:12:53,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.6840243339538574
2023-01-07 09:12:53,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8031237125396729
2023-01-07 09:12:53,113 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.5488399267196655 param sum :: 336.0178527832031
2023-01-07 09:12:53,113 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,113 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,113 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22563.58203125
2023-01-07 09:12:53,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,113 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 0.4701526165008545
2023-01-07 09:12:53,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27095916867256165
2023-01-07 09:12:53,115 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -0.9279670715332031 param sum :: -37660.15625
2023-01-07 09:12:53,115 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,115 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,115 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49252.640625
2023-01-07 09:12:53,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,115 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -4.284348011016846
2023-01-07 09:12:53,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.269071578979492
2023-01-07 09:12:53,117 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.25236833095550537 param sum :: 1346.077880859375
2023-01-07 09:12:53,117 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,117 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,117 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49252.640625
2023-01-07 09:12:53,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,117 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1.0937474966049194
2023-01-07 09:12:53,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16714750230312347
2023-01-07 09:12:53,119 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 0.9494338631629944 param sum :: -82309.78125
2023-01-07 09:12:53,119 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41658.5859375
2023-01-07 09:12:53,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,119 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1594.0450439453125
2023-01-07 09:12:53,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03785339742898941
2023-01-07 09:12:53,121 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.00296628475189209 param sum :: 670.9805908203125
2023-01-07 09:12:53,121 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41658.5859375
2023-01-07 09:12:53,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,121 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 1594.193115234375
2023-01-07 09:12:53,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07123295217752457
2023-01-07 09:12:53,123 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 1594.1776123046875 param sum :: -70066.796875
2023-01-07 09:12:53,123 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1754.400146484375
2023-01-07 09:12:53,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,123 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 551.3052978515625
2023-01-07 09:12:53,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8843669891357422
2023-01-07 09:12:53,125 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 5.007722938898951e-05 param sum :: 677.334228515625
2023-01-07 09:12:53,125 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1754.400146484375
2023-01-07 09:12:53,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,125 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 550.8914794921875
2023-01-07 09:12:53,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45982152223587036
2023-01-07 09:12:53,127 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 551.4715576171875 param sum :: 2290.123291015625
2023-01-07 09:12:53,127 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121804.7890625
2023-01-07 09:12:53,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,127 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -222.5357666015625
2023-01-07 09:12:53,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.013103723526001
2023-01-07 09:12:53,129 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 4.785856246948242 param sum :: 2711.15380859375
2023-01-07 09:12:53,129 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121804.7890625
2023-01-07 09:12:53,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,129 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -222.7347412109375
2023-01-07 09:12:53,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.093478262424469
2023-01-07 09:12:53,131 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -222.2564697265625 param sum :: -205269.921875
2023-01-07 09:12:53,131 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25809.98046875
2023-01-07 09:12:53,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,131 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -9.555061340332031
2023-01-07 09:12:53,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.434755563735962
2023-01-07 09:12:53,132 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.939389228820801 param sum :: 2712.094970703125
2023-01-07 09:12:53,132 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25809.98046875
2023-01-07 09:12:53,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,133 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -12.02164077758789
2023-01-07 09:12:53,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5273120403289795
2023-01-07 09:12:53,135 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -11.241130828857422 param sum :: -43116.3359375
2023-01-07 09:12:53,135 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1850.377685546875
2023-01-07 09:12:53,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,135 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 5008.66796875
2023-01-07 09:12:53,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3893166184425354
2023-01-07 09:12:53,136 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.5383161902427673 param sum :: 674.59423828125
2023-01-07 09:12:53,136 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1850.377685546875
2023-01-07 09:12:53,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,137 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 5008.72509765625
2023-01-07 09:12:53,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014094758778810501
2023-01-07 09:12:53,138 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 5008.7392578125 param sum :: 1534.0406494140625
2023-01-07 09:12:53,139 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -32953.3046875
2023-01-07 09:12:53,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,139 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1158.631103515625
2023-01-07 09:12:53,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3310910761356354
2023-01-07 09:12:53,140 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.451383113861084 param sum :: 675.1919555664062
2023-01-07 09:12:53,140 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -32953.3046875
2023-01-07 09:12:53,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,141 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1159.0419921875
2023-01-07 09:12:53,141 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03584190458059311
2023-01-07 09:12:53,142 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1158.611083984375 param sum :: -56327.3671875
2023-01-07 09:12:53,142 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101829.7421875
2023-01-07 09:12:53,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,143 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 6.711701393127441
2023-01-07 09:12:53,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2979063987731934
2023-01-07 09:12:53,144 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 4.204160690307617 param sum :: 2716.3037109375
2023-01-07 09:12:53,144 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101829.7421875
2023-01-07 09:12:53,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,145 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 3.6050219535827637
2023-01-07 09:12:53,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40351182222366333
2023-01-07 09:12:53,146 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 3.248762845993042 param sum :: -170075.1875
2023-01-07 09:12:53,146 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11786.4873046875
2023-01-07 09:12:53,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,147 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6919.013671875
2023-01-07 09:12:53,147 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00861908309161663
2023-01-07 09:12:53,148 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.021560072898864746 param sum :: 676.64794921875
2023-01-07 09:12:53,148 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,148 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,148 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11786.4873046875
2023-01-07 09:12:53,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,149 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6918.93017578125
2023-01-07 09:12:53,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005269408691674471
2023-01-07 09:12:53,150 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 6918.890625 param sum :: -20250.28125
2023-01-07 09:12:53,150 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1544.148681640625
2023-01-07 09:12:53,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,151 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3294.27490234375
2023-01-07 09:12:53,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.110203817486763
2023-01-07 09:12:53,152 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -21.505443572998047 param sum :: 675.4019775390625
2023-01-07 09:12:53,152 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1544.148681640625
2023-01-07 09:12:53,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,152 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3293.9921875
2023-01-07 09:12:53,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.46132755279541
2023-01-07 09:12:53,154 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 3295.2509765625 param sum :: 1233.00146484375
2023-01-07 09:12:53,154 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149623.6875
2023-01-07 09:12:53,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,155 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -1805.7705078125
2023-01-07 09:12:53,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:53,156 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 29.572635650634766 param sum :: 2649.650390625
2023-01-07 09:12:53,156 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149623.6875
2023-01-07 09:12:53,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,156 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -1805.7705078125
2023-01-07 09:12:53,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9802322387695312e-08
2023-01-07 09:12:53,158 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -1805.7705078125 param sum :: -251249.03125
2023-01-07 09:12:53,158 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:53,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:53,159 > [DEBUG] 0 :: 110.52632904052734
2023-01-07 09:12:53,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,163 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.010009780526161194
2023-01-07 09:12:53,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -398.3279113769531
2023-01-07 09:12:53,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,166 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 38.54563903808594
2023-01-07 09:12:53,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,166 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -341.21270751953125
2023-01-07 09:12:53,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.80359649658203
2023-01-07 09:12:53,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,170 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -198.56871032714844
2023-01-07 09:12:53,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.000880167877767235
2023-01-07 09:12:53,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,172 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.5285014510154724
2023-01-07 09:12:53,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,172 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -294.8158874511719
2023-01-07 09:12:53,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.428980588912964
2023-01-07 09:12:53,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,174 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 0.8542652130126953
2023-01-07 09:12:53,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.653258656617254e-05
2023-01-07 09:12:53,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,176 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 3.965322494506836
2023-01-07 09:12:53,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,176 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -95.91935729980469
2023-01-07 09:12:53,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.971369743347168
2023-01-07 09:12:53,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,178 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.37978318333625793
2023-01-07 09:12:53,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.009976068511605263
2023-01-07 09:12:53,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,179 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 38.431861877441406
2023-01-07 09:12:53,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,179 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -93.17687225341797
2023-01-07 09:12:53,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.331050872802734
2023-01-07 09:12:53,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,181 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 77.14518737792969
2023-01-07 09:12:53,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00326229864731431
2023-01-07 09:12:53,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,183 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.17860135436058044
2023-01-07 09:12:53,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,183 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -8.138423919677734
2023-01-07 09:12:53,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2114346027374268
2023-01-07 09:12:53,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,185 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1098.9229736328125
2023-01-07 09:12:53,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.035951294004917145
2023-01-07 09:12:53,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,186 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 1.801396369934082
2023-01-07 09:12:53,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,186 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1076.9111328125
2023-01-07 09:12:53,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7729380130767822
2023-01-07 09:12:53,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,188 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -367.77294921875
2023-01-07 09:12:53,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.028378907591104507
2023-01-07 09:12:53,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,190 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 25.598466873168945
2023-01-07 09:12:53,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,190 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -340.4886474609375
2023-01-07 09:12:53,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.53568649291992
2023-01-07 09:12:53,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,192 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 41.837547302246094
2023-01-07 09:12:53,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.008121689781546593
2023-01-07 09:12:53,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,193 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 31.23666763305664
2023-01-07 09:12:53,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,193 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 94.68570709228516
2023-01-07 09:12:53,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.73088836669922
2023-01-07 09:12:53,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,195 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 16.35153579711914
2023-01-07 09:12:53,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.029509104788303375
2023-01-07 09:12:53,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,197 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.8041561841964722
2023-01-07 09:12:53,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,197 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 36.98954391479492
2023-01-07 09:12:53,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9331188201904297
2023-01-07 09:12:53,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,200 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 526.685546875
2023-01-07 09:12:53,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0021526999771595
2023-01-07 09:12:53,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,201 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.09829843044281006
2023-01-07 09:12:53,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,201 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 530.196533203125
2023-01-07 09:12:53,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.144645094871521
2023-01-07 09:12:53,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,203 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 8.631600379943848
2023-01-07 09:12:53,204 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.034387826919555664
2023-01-07 09:12:53,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,205 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.020017504692077637
2023-01-07 09:12:53,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,205 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 12.295761108398438
2023-01-07 09:12:53,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05557881295681
2023-01-07 09:12:53,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,207 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.3465285301208496
2023-01-07 09:12:53,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.006399139761924744
2023-01-07 09:12:53,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,208 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.14466983079910278
2023-01-07 09:12:53,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,209 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.3572757244110107
2023-01-07 09:12:53,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.016591250896453857
2023-01-07 09:12:53,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,211 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 11.157648086547852
2023-01-07 09:12:53,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005034861154854298
2023-01-07 09:12:53,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 1.886765956878662
2023-01-07 09:12:53,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 11.413986206054688
2023-01-07 09:12:53,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8588025569915771
2023-01-07 09:12:53,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,214 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 16.89697265625
2023-01-07 09:12:53,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004758371971547604
2023-01-07 09:12:53,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,215 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.049202337861061096
2023-01-07 09:12:53,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,216 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 19.000547409057617
2023-01-07 09:12:53,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3831765949726105
2023-01-07 09:12:53,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,218 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 30.185741424560547
2023-01-07 09:12:53,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.010386064648628235
2023-01-07 09:12:53,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,219 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.1315358281135559
2023-01-07 09:12:53,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,219 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 32.59516906738281
2023-01-07 09:12:53,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09592320024967194
2023-01-07 09:12:53,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,221 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -30.373863220214844
2023-01-07 09:12:53,221 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.013570638373494148
2023-01-07 09:12:53,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,222 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.6315292119979858
2023-01-07 09:12:53,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,223 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -27.936084747314453
2023-01-07 09:12:53,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6433231830596924
2023-01-07 09:12:53,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,225 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -68.00990295410156
2023-01-07 09:12:53,225 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006576895713806152
2023-01-07 09:12:53,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,226 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 1.8089663982391357
2023-01-07 09:12:53,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,226 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -64.60031127929688
2023-01-07 09:12:53,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8465818166732788
2023-01-07 09:12:53,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 166.53402709960938
2023-01-07 09:12:53,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.023495454341173172
2023-01-07 09:12:53,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.2759975790977478
2023-01-07 09:12:53,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 173.53172302246094
2023-01-07 09:12:53,230 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4087585210800171
2023-01-07 09:12:53,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,231 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -356.9502258300781
2023-01-07 09:12:53,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002071575028821826
2023-01-07 09:12:53,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.45422685146331787
2023-01-07 09:12:53,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,233 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -350.6626281738281
2023-01-07 09:12:53,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43456023931503296
2023-01-07 09:12:53,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,235 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -194.29217529296875
2023-01-07 09:12:53,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006810978055000305
2023-01-07 09:12:53,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,236 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -1.856996774673462
2023-01-07 09:12:53,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,236 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -207.60337829589844
2023-01-07 09:12:53,237 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1919984817504883
2023-01-07 09:12:53,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,238 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -95.93952941894531
2023-01-07 09:12:53,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0050665587186813354
2023-01-07 09:12:53,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.5139782428741455
2023-01-07 09:12:53,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -111.26448059082031
2023-01-07 09:12:53,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5977052450180054
2023-01-07 09:12:53,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 75.2538833618164
2023-01-07 09:12:53,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.010724574327468872
2023-01-07 09:12:53,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.27548420429229736
2023-01-07 09:12:53,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,243 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 60.77586364746094
2023-01-07 09:12:53,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30471158027648926
2023-01-07 09:12:53,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 115.48853302001953
2023-01-07 09:12:53,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.020064298063516617
2023-01-07 09:12:53,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -2.823498010635376
2023-01-07 09:12:53,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 114.95161437988281
2023-01-07 09:12:53,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.898958683013916
2023-01-07 09:12:53,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1144.271728515625
2023-01-07 09:12:53,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.008876688778400421
2023-01-07 09:12:53,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 12.129549026489258
2023-01-07 09:12:53,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,251 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1143.9620361328125
2023-01-07 09:12:53,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.18687629699707
2023-01-07 09:12:53,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -22.44582748413086
2023-01-07 09:12:53,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07359454780817032
2023-01-07 09:12:53,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,254 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -25.74966812133789
2023-01-07 09:12:53,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3434247672557831
2023-01-07 09:12:53,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 4.56780481338501
2023-01-07 09:12:53,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01936165988445282
2023-01-07 09:12:53,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,256 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.5380605459213257
2023-01-07 09:12:53,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -0.07108402252197266
2023-01-07 09:12:53,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4250810146331787
2023-01-07 09:12:53,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,259 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 13.339897155761719
2023-01-07 09:12:53,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.010123901069164276
2023-01-07 09:12:53,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -1.4032390117645264
2023-01-07 09:12:53,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 18.201217651367188
2023-01-07 09:12:53,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4117697477340698
2023-01-07 09:12:53,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -179.00753784179688
2023-01-07 09:12:53,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.029647700488567352
2023-01-07 09:12:53,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.9603052139282227
2023-01-07 09:12:53,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -175.56475830078125
2023-01-07 09:12:53,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4454641342163086
2023-01-07 09:12:53,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,265 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -90.67086791992188
2023-01-07 09:12:53,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06816624850034714
2023-01-07 09:12:53,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -123.03768920898438
2023-01-07 09:12:53,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08855517953634262
2023-01-07 09:12:53,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,268 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -268.1606140136719
2023-01-07 09:12:53,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34096604585647583
2023-01-07 09:12:53,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -339.45709228515625
2023-01-07 09:12:53,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04942270740866661
2023-01-07 09:12:53,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 6.441675186157227
2023-01-07 09:12:53,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1406402289867401
2023-01-07 09:12:53,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,272 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.9073276519775391
2023-01-07 09:12:53,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4130178987979889
2023-01-07 09:12:53,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,274 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 12.417070388793945
2023-01-07 09:12:53,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23052626848220825
2023-01-07 09:12:53,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 25.423940658569336
2023-01-07 09:12:53,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3121441602706909
2023-01-07 09:12:53,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 20.7528076171875
2023-01-07 09:12:53,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01969609037041664
2023-01-07 09:12:53,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 39.694091796875
2023-01-07 09:12:53,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16321387887001038
2023-01-07 09:12:53,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -9.517278671264648
2023-01-07 09:12:53,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1061744689941406
2023-01-07 09:12:53,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -83.74005126953125
2023-01-07 09:12:53,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5543534755706787
2023-01-07 09:12:53,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -27.83754539489746
2023-01-07 09:12:53,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21282780170440674
2023-01-07 09:12:53,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 63.770774841308594
2023-01-07 09:12:53,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7764862775802612
2023-01-07 09:12:53,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 118.23674011230469
2023-01-07 09:12:53,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.610806941986084
2023-01-07 09:12:53,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 3.757632255554199
2023-01-07 09:12:53,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -161.81982421875
2023-01-07 09:12:53,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.783380031585693
2023-01-07 09:12:53,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 82.22470092773438
2023-01-07 09:12:53,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7117555141448975
2023-01-07 09:12:53,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 175.75572204589844
2023-01-07 09:12:53,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.37139892578125
2023-01-07 09:12:53,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,291 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -118.15724182128906
2023-01-07 09:12:53,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6262353658676147
2023-01-07 09:12:53,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2.135051727294922
2023-01-07 09:12:53,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4408072233200073
2023-01-07 09:12:53,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -188.58981323242188
2023-01-07 09:12:53,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.75357437133789
2023-01-07 09:12:53,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.9274692535400391
2023-01-07 09:12:53,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -115.33399963378906
2023-01-07 09:12:53,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7669484615325928
2023-01-07 09:12:53,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 171.3068389892578
2023-01-07 09:12:53,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.881475448608398
2023-01-07 09:12:53,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 183.99525451660156
2023-01-07 09:12:53,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.438111782073975
2023-01-07 09:12:53,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,300 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -80.28346252441406
2023-01-07 09:12:53,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.8408284187316895
2023-01-07 09:12:53,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -31.195999145507812
2023-01-07 09:12:53,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.558815002441406
2023-01-07 09:12:53,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,302 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 927.6202392578125
2023-01-07 09:12:53,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0770494937896729
2023-01-07 09:12:53,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -3.683239698410034
2023-01-07 09:12:53,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1118.9000244140625
2023-01-07 09:12:53,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.659773826599121
2023-01-07 09:12:53,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3405.5673828125
2023-01-07 09:12:53,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.793575286865234
2023-01-07 09:12:53,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,307 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.129054307937622
2023-01-07 09:12:53,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,307 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -10.28329849243164
2023-01-07 09:12:53,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3865.4150390625
2023-01-07 09:12:53,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.510801315307617
2023-01-07 09:12:53,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3971.596435546875
2023-01-07 09:12:53,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.64577865600586
2023-01-07 09:12:53,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 679.3712158203125
2023-01-07 09:12:53,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.294806480407715
2023-01-07 09:12:53,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,312 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -183.20779418945312
2023-01-07 09:12:53,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.19565200805664
2023-01-07 09:12:53,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,314 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.2829577922821045
2023-01-07 09:12:53,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,314 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -30.892520904541016
2023-01-07 09:12:53,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.52935266494751
2023-01-07 09:12:53,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,316 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1118.7728271484375
2023-01-07 09:12:53,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.85078430175781
2023-01-07 09:12:53,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.430100917816162
2023-01-07 09:12:53,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -57.340450286865234
2023-01-07 09:12:53,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.88971710205078
2023-01-07 09:12:53,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,319 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1234.75732421875
2023-01-07 09:12:53,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.607304573059082
2023-01-07 09:12:53,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,320 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 40.225284576416016
2023-01-07 09:12:53,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,321 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4184.50390625
2023-01-07 09:12:53,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.402353286743164
2023-01-07 09:12:53,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,322 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1399.7130126953125
2023-01-07 09:12:53,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.71891784667969
2023-01-07 09:12:53,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,323 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 177.50653076171875
2023-01-07 09:12:53,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.644848823547363
2023-01-07 09:12:53,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,325 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1557.9998779296875
2023-01-07 09:12:53,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -105.92677307128906
2023-01-07 09:12:53,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,327 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -8.843107223510742
2023-01-07 09:12:53,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,327 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 153.96115112304688
2023-01-07 09:12:53,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,327 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1705.2342529296875
2023-01-07 09:12:53,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4200.1044921875
2023-01-07 09:12:53,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.191549301147461
2023-01-07 09:12:53,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,329 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1711.043701171875
2023-01-07 09:12:53,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -111.0364990234375
2023-01-07 09:12:53,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,331 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -286.39996337890625
2023-01-07 09:12:53,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -174.86984252929688
2023-01-07 09:12:53,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,332 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1565.867431640625
2023-01-07 09:12:53,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.633045673370361
2023-01-07 09:12:53,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,333 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -303.3385925292969
2023-01-07 09:12:53,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4203.40380859375
2023-01-07 09:12:53,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -110.99960327148438
2023-01-07 09:12:53,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,335 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 550.0185546875
2023-01-07 09:12:53,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.843719482421875
2023-01-07 09:12:53,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,337 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 3.745189666748047
2023-01-07 09:12:53,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,337 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 382.58026123046875
2023-01-07 09:12:53,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 363.17205810546875
2023-01-07 09:12:53,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,339 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 348.20648193359375
2023-01-07 09:12:53,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.201099395751953
2023-01-07 09:12:53,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 348.20648193359375
2023-01-07 09:12:53,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.23591613769531
2023-01-07 09:12:53,342 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:12:53,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,342 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 65.20989227294922
2023-01-07 09:12:53,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,343 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 703.7662353515625
2023-01-07 09:12:53,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,343 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4199.8818359375
2023-01-07 09:12:53,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,343 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 162.69061279296875
2023-01-07 09:12:53,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,344 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 277.63433837890625
2023-01-07 09:12:53,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,344 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 47.28555679321289
2023-01-07 09:12:53,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,344 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 121.31468200683594
2023-01-07 09:12:53,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,345 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 502.25543212890625
2023-01-07 09:12:53,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,345 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -531.7010498046875
2023-01-07 09:12:53,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,345 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 283.3225402832031
2023-01-07 09:12:53,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,345 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -281.558349609375
2023-01-07 09:12:53,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,346 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -99.96007537841797
2023-01-07 09:12:53,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,346 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 93.25267791748047
2023-01-07 09:12:53,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,346 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -6.3795166015625
2023-01-07 09:12:53,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -417.0616760253906
2023-01-07 09:12:53,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -171.8075714111328
2023-01-07 09:12:53,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -142.53515625
2023-01-07 09:12:53,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 13.820457458496094
2023-01-07 09:12:53,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,348 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.201457977294922
2023-01-07 09:12:53,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,348 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -30.34601593017578
2023-01-07 09:12:53,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,348 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1143.458740234375
2023-01-07 09:12:53,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 114.42069244384766
2023-01-07 09:12:53,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 59.77935791015625
2023-01-07 09:12:53,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -123.0898666381836
2023-01-07 09:12:53,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -206.7643585205078
2023-01-07 09:12:53,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -349.0291442871094
2023-01-07 09:12:53,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 1097.4886474609375
2023-01-07 09:12:53,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 175.75863647460938
2023-01-07 09:12:53,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -64.67485046386719
2023-01-07 09:12:53,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -27.67643165588379
2023-01-07 09:12:53,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 33.09369659423828
2023-01-07 09:12:53,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 19.045574188232422
2023-01-07 09:12:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 11.554973602294922
2023-01-07 09:12:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.344388484954834
2023-01-07 09:12:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,354 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.087348937988281
2023-01-07 09:12:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,355 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 530.235107421875
2023-01-07 09:12:53,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 625.2554931640625
2023-01-07 09:12:53,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,356 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 56.823028564453125
2023-01-07 09:12:53,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,356 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 100.0583724975586
2023-01-07 09:12:53,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,357 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -339.5872802734375
2023-01-07 09:12:53,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,357 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1069.325439453125
2023-01-07 09:12:53,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -1162.3736572265625
2023-01-07 09:12:53,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,358 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.8266754150390625
2023-01-07 09:12:53,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,358 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -93.2935562133789
2023-01-07 09:12:53,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,359 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -95.90679931640625
2023-01-07 09:12:53,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.85002517700195
2023-01-07 09:12:53,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,359 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -296.5124206542969
2023-01-07 09:12:53,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:53,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:53,360 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -339.20794677734375
2023-01-07 09:12:53,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 207.11294555664062
2023-01-07 09:12:54,202 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -0.023085448890924454 param sum :: 249.95010375976562
2023-01-07 09:12:54,202 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 65.1376724243164
2023-01-07 09:12:54,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,202 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 15.38049602508545
2023-01-07 09:12:54,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 680.9632568359375
2023-01-07 09:12:54,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.20283508300781
2023-01-07 09:12:54,204 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 7.460563659667969 param sum :: 65.51138305664062
2023-01-07 09:12:54,204 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -46.274925231933594
2023-01-07 09:12:54,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -316.49127197265625
2023-01-07 09:12:54,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 631.7598876953125
2023-01-07 09:12:54,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -322.14886474609375
2023-01-07 09:12:54,207 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -314.4241638183594 param sum :: -44.4479866027832
2023-01-07 09:12:54,207 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,207 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,207 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 455.1439208984375
2023-01-07 09:12:54,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 633.4508056640625
2023-01-07 09:12:54,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.296669006347656
2023-01-07 09:12:54,209 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -12.103053092956543 param sum :: 60.676414489746094
2023-01-07 09:12:54,209 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 455.1439208984375
2023-01-07 09:12:54,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 425.26092529296875
2023-01-07 09:12:54,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.494438648223877
2023-01-07 09:12:54,211 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 407.564453125 param sum :: 522.6602172851562
2023-01-07 09:12:54,211 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 62.27489471435547
2023-01-07 09:12:54,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,211 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.13315653800964355
2023-01-07 09:12:54,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 316.32855224609375
2023-01-07 09:12:54,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1419.7305908203125
2023-01-07 09:12:54,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 303.3492431640625
2023-01-07 09:12:54,214 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 15.139928817749023 param sum :: 61.766239166259766
2023-01-07 09:12:54,214 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,214 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,214 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 199.84332275390625
2023-01-07 09:12:54,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,214 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1439.51611328125
2023-01-07 09:12:54,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.381662368774414
2023-01-07 09:12:54,215 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 144.8750457763672 param sum :: -10.483442306518555
2023-01-07 09:12:54,216 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,216 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,216 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.68341064453125
2023-01-07 09:12:54,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,216 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 0.011232376098632812
2023-01-07 09:12:54,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,216 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 317.82330322265625
2023-01-07 09:12:54,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 132.55560302734375
2023-01-07 09:12:54,218 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 6.631641387939453 param sum :: 257.1244201660156
2023-01-07 09:12:54,218 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,218 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,218 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 139.19224548339844
2023-01-07 09:12:54,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,219 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 336.9283447265625
2023-01-07 09:12:54,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.368194580078125
2023-01-07 09:12:54,220 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -448.95306396484375 param sum :: 109.71334838867188
2023-01-07 09:12:54,220 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 139.19224548339844
2023-01-07 09:12:54,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,220 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 774.5181884765625
2023-01-07 09:12:54,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.499061584472656
2023-01-07 09:12:54,221 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -98.51454162597656 param sum :: 254.97840881347656
2023-01-07 09:12:54,221 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 139.19224548339844
2023-01-07 09:12:54,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,222 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 441.15008544921875
2023-01-07 09:12:54,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.40801239013672
2023-01-07 09:12:54,223 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 208.97994995117188 param sum :: 62.65809631347656
2023-01-07 09:12:54,223 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.364017486572266
2023-01-07 09:12:54,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,224 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -5.484407424926758
2023-01-07 09:12:54,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,224 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 690.4498901367188
2023-01-07 09:12:54,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.40390396118164
2023-01-07 09:12:54,226 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 15.09149169921875 param sum :: 61.38249206542969
2023-01-07 09:12:54,226 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,226 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,226 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 139.19224548339844
2023-01-07 09:12:54,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,226 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 694.710693359375
2023-01-07 09:12:54,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,226 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3331.9111328125
2023-01-07 09:12:54,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.085676193237305
2023-01-07 09:12:54,228 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 665.7355346679688 param sum :: 61.71021270751953
2023-01-07 09:12:54,228 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 66.64739990234375
2023-01-07 09:12:54,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,229 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.630732536315918
2023-01-07 09:12:54,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,229 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 256.8065185546875
2023-01-07 09:12:54,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.52651596069336
2023-01-07 09:12:54,231 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 21.266298294067383 param sum :: 66.53099822998047
2023-01-07 09:12:54,231 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 199.84332275390625
2023-01-07 09:12:54,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,231 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1458.5455322265625
2023-01-07 09:12:54,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.13125991821289
2023-01-07 09:12:54,232 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 146.96502685546875 param sum :: 138.7475128173828
2023-01-07 09:12:54,232 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 317.18017578125
2023-01-07 09:12:54,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,233 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 138.82228088378906
2023-01-07 09:12:54,233 > [DEBUG] 0 :: before allreduce fusion buffer :: -70.40055847167969
2023-01-07 09:12:54,234 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -13.749580383300781 param sum :: 272.353271484375
2023-01-07 09:12:54,234 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 317.18017578125
2023-01-07 09:12:54,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -108.74581909179688
2023-01-07 09:12:54,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1612.90283203125
2023-01-07 09:12:54,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.813690185546875
2023-01-07 09:12:54,237 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -125.14451599121094 param sum :: 420.38372802734375
2023-01-07 09:12:54,237 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 199.84332275390625
2023-01-07 09:12:54,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,237 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1526.6649169921875
2023-01-07 09:12:54,237 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.27163314819336
2023-01-07 09:12:54,238 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 12.188356399536133 param sum :: 65.36997985839844
2023-01-07 09:12:54,238 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 199.84332275390625
2023-01-07 09:12:54,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,239 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1631.5791015625
2023-01-07 09:12:54,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.682896614074707
2023-01-07 09:12:54,240 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1559.28759765625 param sum :: 187.2625732421875
2023-01-07 09:12:54,240 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.7784423828125
2023-01-07 09:12:54,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,242 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 3.821507215499878
2023-01-07 09:12:54,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,242 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -72.22947692871094
2023-01-07 09:12:54,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,242 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 2398.59521484375
2023-01-07 09:12:54,242 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.18963623046875
2023-01-07 09:12:54,244 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 22.023784637451172 param sum :: 64.36270904541016
2023-01-07 09:12:54,244 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 564.7163696289062
2023-01-07 09:12:54,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,245 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3331.296875
2023-01-07 09:12:54,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.944204330444336
2023-01-07 09:12:54,246 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -205.75241088867188 param sum :: 9.909540176391602
2023-01-07 09:12:54,246 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 532.1267700195312
2023-01-07 09:12:54,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,246 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 2275.27294921875
2023-01-07 09:12:54,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.221864223480225
2023-01-07 09:12:54,248 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 7.964616298675537 param sum :: 275.0821228027344
2023-01-07 09:12:54,248 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 564.7163696289062
2023-01-07 09:12:54,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,248 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3301.77294921875
2023-01-07 09:12:54,249 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.846281051635742
2023-01-07 09:12:54,250 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 2175.01611328125 param sum :: 566.8568115234375
2023-01-07 09:12:54,250 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 564.7163696289062
2023-01-07 09:12:54,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,250 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3316.64892578125
2023-01-07 09:12:54,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.35753631591797
2023-01-07 09:12:54,251 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 9.766161918640137 param sum :: 140.09725952148438
2023-01-07 09:12:54,252 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 564.7163696289062
2023-01-07 09:12:54,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,252 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3297.542724609375
2023-01-07 09:12:54,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.738494873046875
2023-01-07 09:12:54,254 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 3285.554443359375 param sum :: 551.4695434570312
2023-01-07 09:12:54,254 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 130.4813232421875
2023-01-07 09:12:54,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,254 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.612883448600769
2023-01-07 09:12:54,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,254 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 285.7607727050781
2023-01-07 09:12:54,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.032272338867188
2023-01-07 09:12:54,256 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -10.42438793182373 param sum :: 131.55776977539062
2023-01-07 09:12:54,256 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 523.105712890625
2023-01-07 09:12:54,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,256 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 219.65924072265625
2023-01-07 09:12:54,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.28302764892578
2023-01-07 09:12:54,258 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 206.33995056152344 param sum :: 603.243408203125
2023-01-07 09:12:54,258 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 510.10357666015625
2023-01-07 09:12:54,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,258 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -17.443485260009766
2023-01-07 09:12:54,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,259 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 50.73142623901367
2023-01-07 09:12:54,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.164695739746094
2023-01-07 09:12:54,260 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -16.21381378173828 param sum :: 509.56842041015625
2023-01-07 09:12:54,260 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 345.66143798828125
2023-01-07 09:12:54,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,261 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -73.39071655273438
2023-01-07 09:12:54,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.335857391357422
2023-01-07 09:12:54,262 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -85.41899108886719 param sum :: 391.04998779296875
2023-01-07 09:12:54,262 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 258.7024841308594
2023-01-07 09:12:54,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,263 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 259.16748046875
2023-01-07 09:12:54,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.621822357177734
2023-01-07 09:12:54,264 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 34.13795471191406 param sum :: 582.4049072265625
2023-01-07 09:12:54,264 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 258.7024841308594
2023-01-07 09:12:54,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,264 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 404.2740478515625
2023-01-07 09:12:54,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.759039402008057
2023-01-07 09:12:54,266 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 432.8565673828125 param sum :: 435.0699768066406
2023-01-07 09:12:54,266 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.59780883789062
2023-01-07 09:12:54,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,266 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.5730428695678711
2023-01-07 09:12:54,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,267 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 180.61712646484375
2023-01-07 09:12:54,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.618398666381836
2023-01-07 09:12:54,268 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 6.789447784423828 param sum :: 127.24115753173828
2023-01-07 09:12:54,268 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 197.9300994873047
2023-01-07 09:12:54,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,269 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 238.40472412109375
2023-01-07 09:12:54,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.06589126586914
2023-01-07 09:12:54,270 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 185.59878540039062 param sum :: 891.2916870117188
2023-01-07 09:12:54,270 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.03244018554688
2023-01-07 09:12:54,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -3.5132100582122803
2023-01-07 09:12:54,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 438.2337646484375
2023-01-07 09:12:54,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.189455032348633
2023-01-07 09:12:54,273 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -44.756805419921875 param sum :: 130.08084106445312
2023-01-07 09:12:54,273 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,273 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 443.7290344238281
2023-01-07 09:12:54,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,273 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 497.309814453125
2023-01-07 09:12:54,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.734370231628418
2023-01-07 09:12:54,275 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 597.30908203125 param sum :: 666.19287109375
2023-01-07 09:12:54,275 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 601.124755859375
2023-01-07 09:12:54,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -535.0673217773438
2023-01-07 09:12:54,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.38603973388672
2023-01-07 09:12:54,276 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 26.406038284301758 param sum :: 588.2977294921875
2023-01-07 09:12:54,276 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 601.124755859375
2023-01-07 09:12:54,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,277 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -450.7373962402344
2023-01-07 09:12:54,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3357245922088623
2023-01-07 09:12:54,278 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -388.96728515625 param sum :: 665.2001342773438
2023-01-07 09:12:54,279 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 127.94561767578125
2023-01-07 09:12:54,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -4.476517200469971
2023-01-07 09:12:54,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 267.47613525390625
2023-01-07 09:12:54,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.968888282775879
2023-01-07 09:12:54,281 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -4.475905418395996 param sum :: 128.99636840820312
2023-01-07 09:12:54,281 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 716.2728881835938
2023-01-07 09:12:54,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,281 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 311.141357421875
2023-01-07 09:12:54,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.509951114654541
2023-01-07 09:12:54,283 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 297.0449523925781 param sum :: 849.1844482421875
2023-01-07 09:12:54,283 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 130.6510467529297
2023-01-07 09:12:54,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.35958194732666016
2023-01-07 09:12:54,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,284 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -288.27093505859375
2023-01-07 09:12:54,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.238068580627441
2023-01-07 09:12:54,285 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -12.98338794708252 param sum :: 130.16050720214844
2023-01-07 09:12:54,285 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 770.6195068359375
2023-01-07 09:12:54,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -285.62738037109375
2023-01-07 09:12:54,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.73399543762207
2023-01-07 09:12:54,287 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -265.59613037109375 param sum :: 1035.0040283203125
2023-01-07 09:12:54,287 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 516.930908203125
2023-01-07 09:12:54,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -1.9894979000091553
2023-01-07 09:12:54,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -107.83204650878906
2023-01-07 09:12:54,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3536962270736694
2023-01-07 09:12:54,289 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 27.809799194335938 param sum :: 516.09619140625
2023-01-07 09:12:54,290 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 694.16796875
2023-01-07 09:12:54,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,290 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -111.11868286132812
2023-01-07 09:12:54,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.470821380615234
2023-01-07 09:12:54,292 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -151.63070678710938 param sum :: 894.716552734375
2023-01-07 09:12:54,292 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.4334716796875
2023-01-07 09:12:54,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.32759320735931396
2023-01-07 09:12:54,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 63.1706428527832
2023-01-07 09:12:54,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.874610424041748
2023-01-07 09:12:54,294 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -9.710162162780762 param sum :: 128.46072387695312
2023-01-07 09:12:54,294 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1299.8602294921875
2023-01-07 09:12:54,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 75.41181945800781
2023-01-07 09:12:54,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3284146785736084
2023-01-07 09:12:54,296 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 86.60955810546875 param sum :: 1566.7215576171875
2023-01-07 09:12:54,296 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 122.9132080078125
2023-01-07 09:12:54,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 1.501497745513916
2023-01-07 09:12:54,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -21.2081241607666
2023-01-07 09:12:54,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.228507041931152
2023-01-07 09:12:54,299 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 1.1483497619628906 param sum :: 120.98361206054688
2023-01-07 09:12:54,299 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 835.6869506835938
2023-01-07 09:12:54,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 12.888409614562988
2023-01-07 09:12:54,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.32799243927002
2023-01-07 09:12:54,301 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 67.50763702392578 param sum :: 1064.3114013671875
2023-01-07 09:12:54,301 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 500.6766357421875
2023-01-07 09:12:54,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.6295790672302246
2023-01-07 09:12:54,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,301 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -340.74542236328125
2023-01-07 09:12:54,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.692586898803711
2023-01-07 09:12:54,303 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 10.498991012573242 param sum :: 502.12744140625
2023-01-07 09:12:54,303 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,303 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 311.65869140625
2023-01-07 09:12:54,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,303 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -360.99774169921875
2023-01-07 09:12:54,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.201618671417236
2023-01-07 09:12:54,305 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -375.73443603515625 param sum :: 712.916748046875
2023-01-07 09:12:54,305 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 257.6792907714844
2023-01-07 09:12:54,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 1.131732702255249
2023-01-07 09:12:54,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 482.7002258300781
2023-01-07 09:12:54,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.405245304107666
2023-01-07 09:12:54,307 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -37.427284240722656 param sum :: 258.9994201660156
2023-01-07 09:12:54,308 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -39317.8671875
2023-01-07 09:12:54,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,308 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 503.0878601074219
2023-01-07 09:12:54,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.079469680786133
2023-01-07 09:12:54,310 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 513.77880859375 param sum :: -50461.1640625
2023-01-07 09:12:54,310 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,310 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,310 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 375.604736328125
2023-01-07 09:12:54,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,310 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -142.53515625
2023-01-07 09:12:54,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.632044792175293
2023-01-07 09:12:54,311 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -4.526259422302246 param sum :: 320.28143310546875
2023-01-07 09:12:54,311 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 375.604736328125
2023-01-07 09:12:54,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,312 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -147.67367553710938
2023-01-07 09:12:54,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4528303146362305
2023-01-07 09:12:54,313 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -135.77923583984375 param sum :: 518.505859375
2023-01-07 09:12:54,313 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -54000.2890625
2023-01-07 09:12:54,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,314 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.39354133605957
2023-01-07 09:12:54,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.763051986694336
2023-01-07 09:12:54,315 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 7.231476783752441 param sum :: 1337.34033203125
2023-01-07 09:12:54,315 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,315 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -54000.2890625
2023-01-07 09:12:54,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,316 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 20.31077003479004
2023-01-07 09:12:54,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0050182342529297
2023-01-07 09:12:54,317 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 31.100831985473633 param sum :: -71073.6171875
2023-01-07 09:12:54,317 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,317 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -37895.5625
2023-01-07 09:12:54,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,318 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.38885498046875
2023-01-07 09:12:54,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7118617296218872
2023-01-07 09:12:54,319 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -3.6837539672851562 param sum :: 1370.9185791015625
2023-01-07 09:12:54,319 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -37895.5625
2023-01-07 09:12:54,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,319 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -9.801532745361328
2023-01-07 09:12:54,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7621628046035767
2023-01-07 09:12:54,321 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -11.362650871276855 param sum :: -49663.359375
2023-01-07 09:12:54,321 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 253.8910369873047
2023-01-07 09:12:54,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,321 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 6.145379066467285
2023-01-07 09:12:54,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,322 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -30.039844512939453
2023-01-07 09:12:54,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.851663589477539
2023-01-07 09:12:54,323 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 39.534706115722656 param sum :: 234.67689514160156
2023-01-07 09:12:54,323 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -93591.546875
2023-01-07 09:12:54,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,324 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -26.60133171081543
2023-01-07 09:12:54,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.87278413772583
2023-01-07 09:12:54,325 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -20.490142822265625 param sum :: -122591.2265625
2023-01-07 09:12:54,326 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19706.00390625
2023-01-07 09:12:54,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,326 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -16.554094314575195
2023-01-07 09:12:54,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.637369155883789
2023-01-07 09:12:54,327 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 13.464561462402344 param sum :: 334.69927978515625
2023-01-07 09:12:54,327 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,328 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19706.00390625
2023-01-07 09:12:54,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -17.636871337890625
2023-01-07 09:12:54,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.380358695983887
2023-01-07 09:12:54,329 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -14.328780174255371 param sum :: -25807.74609375
2023-01-07 09:12:54,329 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,330 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,330 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1012.1820678710938
2023-01-07 09:12:54,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 29.871204376220703
2023-01-07 09:12:54,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.952426910400391
2023-01-07 09:12:54,331 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -3.1786646842956543 param sum :: 1389.271240234375
2023-01-07 09:12:54,331 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1012.1820678710938
2023-01-07 09:12:54,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,332 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 29.64935874938965
2023-01-07 09:12:54,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.237576425075531
2023-01-07 09:12:54,333 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 28.827369689941406 param sum :: 1200.822021484375
2023-01-07 09:12:54,333 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1242.0123291015625
2023-01-07 09:12:54,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,334 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -15.917597770690918
2023-01-07 09:12:54,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.411208838224411
2023-01-07 09:12:54,335 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.7974549531936646 param sum :: 343.2051696777344
2023-01-07 09:12:54,335 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1242.0123291015625
2023-01-07 09:12:54,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,335 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -14.159205436706543
2023-01-07 09:12:54,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.028226792812347412
2023-01-07 09:12:54,337 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -13.38968563079834 param sum :: 1651.47509765625
2023-01-07 09:12:54,337 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,337 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,337 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1597.624755859375
2023-01-07 09:12:54,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,338 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.6564879417419434
2023-01-07 09:12:54,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2862071990966797
2023-01-07 09:12:54,339 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.9999758005142212 param sum :: 349.8155822753906
2023-01-07 09:12:54,339 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,339 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1597.624755859375
2023-01-07 09:12:54,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,339 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -10.35287857055664
2023-01-07 09:12:54,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4758470058441162
2023-01-07 09:12:54,341 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -13.879558563232422 param sum :: 2009.8336181640625
2023-01-07 09:12:54,341 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,341 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,341 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1698.5218505859375
2023-01-07 09:12:54,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,341 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -15.147738456726074
2023-01-07 09:12:54,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.758659362792969
2023-01-07 09:12:54,343 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -15.331530570983887 param sum :: 1420.94873046875
2023-01-07 09:12:54,343 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1698.5218505859375
2023-01-07 09:12:54,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,343 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -21.969146728515625
2023-01-07 09:12:54,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1723103523254395
2023-01-07 09:12:54,345 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -22.88296890258789 param sum :: 2259.3759765625
2023-01-07 09:12:54,345 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,345 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1845.7841796875
2023-01-07 09:12:54,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 116.31983184814453
2023-01-07 09:12:54,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004087138921022415
2023-01-07 09:12:54,347 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 4.9936957359313965 param sum :: 346.50372314453125
2023-01-07 09:12:54,347 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1845.7841796875
2023-01-07 09:12:54,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 116.40573120117188
2023-01-07 09:12:54,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08394116163253784
2023-01-07 09:12:54,349 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 117.56953430175781 param sum :: 2413.02587890625
2023-01-07 09:12:54,349 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1760.8192138671875
2023-01-07 09:12:54,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 20.396686553955078
2023-01-07 09:12:54,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.120088577270508
2023-01-07 09:12:54,350 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 4.534478664398193 param sum :: 348.7162170410156
2023-01-07 09:12:54,350 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1760.8192138671875
2023-01-07 09:12:54,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,351 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 13.316741943359375
2023-01-07 09:12:54,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.322277545928955
2023-01-07 09:12:54,353 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 13.110062599182129 param sum :: 2186.31591796875
2023-01-07 09:12:54,353 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 832.803466796875
2023-01-07 09:12:54,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 80.42073059082031
2023-01-07 09:12:54,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.735705614089966
2023-01-07 09:12:54,354 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 5.260565757751465 param sum :: 1421.681640625
2023-01-07 09:12:54,354 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 832.803466796875
2023-01-07 09:12:54,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 77.18512725830078
2023-01-07 09:12:54,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3271899223327637
2023-01-07 09:12:54,357 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 72.99421691894531 param sum :: 2258.6396484375
2023-01-07 09:12:54,357 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2073.80810546875
2023-01-07 09:12:54,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -519.2818603515625
2023-01-07 09:12:54,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2801066040992737
2023-01-07 09:12:54,358 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 3.0495872497558594 param sum :: 352.737060546875
2023-01-07 09:12:54,358 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2073.80810546875
2023-01-07 09:12:54,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,359 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -518.7716064453125
2023-01-07 09:12:54,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15544039011001587
2023-01-07 09:12:54,360 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -518.3811645507812 param sum :: 3273.265625
2023-01-07 09:12:54,361 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3481.74951171875
2023-01-07 09:12:54,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1207.6712646484375
2023-01-07 09:12:54,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.030853271484375
2023-01-07 09:12:54,362 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 2.4162561893463135 param sum :: 354.9400634765625
2023-01-07 09:12:54,362 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3481.74951171875
2023-01-07 09:12:54,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 1204.7205810546875
2023-01-07 09:12:54,363 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6106019020080566
2023-01-07 09:12:54,364 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 1203.788330078125 param sum :: -3653.725341796875
2023-01-07 09:12:54,364 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -33195.875
2023-01-07 09:12:54,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.26820182800293
2023-01-07 09:12:54,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.754692554473877
2023-01-07 09:12:54,366 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 2.0431265830993652 param sum :: 1436.2138671875
2023-01-07 09:12:54,366 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -33195.875
2023-01-07 09:12:54,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,366 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -2.728120803833008
2023-01-07 09:12:54,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.148833274841309
2023-01-07 09:12:54,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -4.84030818939209 param sum :: -43505.3671875
2023-01-07 09:12:54,368 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -14667.45703125
2023-01-07 09:12:54,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,369 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -333.1737365722656
2023-01-07 09:12:54,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4801895022392273
2023-01-07 09:12:54,370 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 2.024372100830078 param sum :: 358.34783935546875
2023-01-07 09:12:54,370 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -14667.45703125
2023-01-07 09:12:54,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -333.62554931640625
2023-01-07 09:12:54,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32334423065185547
2023-01-07 09:12:54,372 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -334.15936279296875 param sum :: -19471.1875
2023-01-07 09:12:54,372 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37660.15625
2023-01-07 09:12:54,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.372636079788208
2023-01-07 09:12:54,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3601540327072144
2023-01-07 09:12:54,374 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.13902539014816284 param sum :: 358.8280029296875
2023-01-07 09:12:54,374 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37660.15625
2023-01-07 09:12:54,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,374 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.5648493766784668
2023-01-07 09:12:54,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1363019943237305
2023-01-07 09:12:54,376 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -2.291431188583374 param sum :: -49311.16796875
2023-01-07 09:12:54,376 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,376 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82309.78125
2023-01-07 09:12:54,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,376 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.224308967590332
2023-01-07 09:12:54,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.42770767211914
2023-01-07 09:12:54,378 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 3.6120758056640625 param sum :: 1431.8746337890625
2023-01-07 09:12:54,378 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82309.78125
2023-01-07 09:12:54,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,378 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 21.012226104736328
2023-01-07 09:12:54,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7003560066223145
2023-01-07 09:12:54,380 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 21.204147338867188 param sum :: -107946.453125
2023-01-07 09:12:54,380 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -70066.796875
2023-01-07 09:12:54,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,380 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 304.08453369140625
2023-01-07 09:12:54,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0728650763630867
2023-01-07 09:12:54,382 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 3.625892162322998 param sum :: 706.8477783203125
2023-01-07 09:12:54,382 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -70066.796875
2023-01-07 09:12:54,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,382 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 303.9713439941406
2023-01-07 09:12:54,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1682383418083191
2023-01-07 09:12:54,384 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 303.8932189941406 param sum :: -91957.8515625
2023-01-07 09:12:54,384 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 2290.123291015625
2023-01-07 09:12:54,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -338.17919921875
2023-01-07 09:12:54,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39028143882751465
2023-01-07 09:12:54,386 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 15.938416481018066 param sum :: 711.95947265625
2023-01-07 09:12:54,386 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 2290.123291015625
2023-01-07 09:12:54,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -337.2657470703125
2023-01-07 09:12:54,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8274648189544678
2023-01-07 09:12:54,388 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -337.2261962890625 param sum :: 4145.32763671875
2023-01-07 09:12:54,388 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -205269.921875
2023-01-07 09:12:54,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1234.248291015625
2023-01-07 09:12:54,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33091679215431213
2023-01-07 09:12:54,390 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 55.11377716064453 param sum :: 2863.00830078125
2023-01-07 09:12:54,390 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -205269.921875
2023-01-07 09:12:54,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1233.57177734375
2023-01-07 09:12:54,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5585660934448242
2023-01-07 09:12:54,392 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1233.546875 param sum :: -267814.9375
2023-01-07 09:12:54,392 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -43116.3359375
2023-01-07 09:12:54,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,392 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 118.4920654296875
2023-01-07 09:12:54,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2382731437683105
2023-01-07 09:12:54,393 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 52.894290924072266 param sum :: 2865.566162109375
2023-01-07 09:12:54,393 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -43116.3359375
2023-01-07 09:12:54,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 118.83636474609375
2023-01-07 09:12:54,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5105614066123962
2023-01-07 09:12:54,395 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 112.5008773803711 param sum :: -56533.7734375
2023-01-07 09:12:54,396 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1534.0406494140625
2023-01-07 09:12:54,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1341.7625732421875
2023-01-07 09:12:54,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1802211999893188
2023-01-07 09:12:54,397 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 5.819151878356934 param sum :: 714.9794311523438
2023-01-07 09:12:54,397 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1534.0406494140625
2023-01-07 09:12:54,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,398 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1341.444091796875
2023-01-07 09:12:54,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6533619165420532
2023-01-07 09:12:54,399 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -1341.875732421875 param sum :: 3843.963623046875
2023-01-07 09:12:54,399 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -56327.3671875
2023-01-07 09:12:54,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,400 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3417.95556640625
2023-01-07 09:12:54,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8326318264007568
2023-01-07 09:12:54,401 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -87.15350341796875 param sum :: 747.0411376953125
2023-01-07 09:12:54,401 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -56327.3671875
2023-01-07 09:12:54,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,402 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 3418.779296875
2023-01-07 09:12:54,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8671186566352844
2023-01-07 09:12:54,403 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 3418.6474609375 param sum :: -73451.9765625
2023-01-07 09:12:54,403 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -170075.1875
2023-01-07 09:12:54,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,404 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -93.09178924560547
2023-01-07 09:12:54,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35113656520843506
2023-01-07 09:12:54,405 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -54.34227752685547 param sum :: 2912.540283203125
2023-01-07 09:12:54,405 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -170075.1875
2023-01-07 09:12:54,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,406 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -94.1972427368164
2023-01-07 09:12:54,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.283902645111084
2023-01-07 09:12:54,408 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -94.11544036865234 param sum :: -222785.046875
2023-01-07 09:12:54,408 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -20250.28125
2023-01-07 09:12:54,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,408 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1086.282958984375
2023-01-07 09:12:54,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0011631371453404427
2023-01-07 09:12:54,409 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -92.81101989746094 param sum :: 749.9560546875
2023-01-07 09:12:54,409 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -20250.28125
2023-01-07 09:12:54,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,410 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1086.2747802734375
2023-01-07 09:12:54,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0015678791096433997
2023-01-07 09:12:54,411 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 1086.2630615234375 param sum :: -26678.986328125
2023-01-07 09:12:54,412 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1233.00146484375
2023-01-07 09:12:54,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,412 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4118.6689453125
2023-01-07 09:12:54,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2053566575050354
2023-01-07 09:12:54,413 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -96.776123046875 param sum :: 748.806640625
2023-01-07 09:12:54,413 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1233.00146484375
2023-01-07 09:12:54,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,414 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4119.3466796875
2023-01-07 09:12:54,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3738934099674225
2023-01-07 09:12:54,415 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -4119.52734375 param sum :: 4495.5888671875
2023-01-07 09:12:54,415 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -251249.03125
2023-01-07 09:12:54,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,416 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 6508.216796875
2023-01-07 09:12:54,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:54,417 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -516.522216796875 param sum :: 2923.802734375
2023-01-07 09:12:54,417 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -251249.03125
2023-01-07 09:12:54,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,418 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 6508.216796875
2023-01-07 09:12:54,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4901161193847656e-08
2023-01-07 09:12:54,420 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 6508.216796875 param sum :: -329490.46875
2023-01-07 09:12:54,420 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:54,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:54,421 > [DEBUG] 0 :: 186.8134307861328
2023-01-07 09:12:54,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,424 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00927734375
2023-01-07 09:12:54,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -499.84814453125
2023-01-07 09:12:54,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,427 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 64.5765380859375
2023-01-07 09:12:54,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,428 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -538.85986328125
2023-01-07 09:12:54,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 547.5545654296875
2023-01-07 09:12:54,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.8942286968231201
2023-01-07 09:12:54,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0036152652464807034
2023-01-07 09:12:54,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,433 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.16751591861248016
2023-01-07 09:12:54,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,434 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -109.20927429199219
2023-01-07 09:12:54,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24294619262218475
2023-01-07 09:12:54,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,436 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 14.871176719665527
2023-01-07 09:12:54,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0003755368525162339
2023-01-07 09:12:54,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,437 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -1.3360792398452759
2023-01-07 09:12:54,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,437 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 14.833239555358887
2023-01-07 09:12:54,438 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3359436988830566
2023-01-07 09:12:54,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,439 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.07787656784057617
2023-01-07 09:12:54,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0015528722433373332
2023-01-07 09:12:54,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,441 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 53.82777404785156
2023-01-07 09:12:54,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,441 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 47.353023529052734
2023-01-07 09:12:54,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.73683166503906
2023-01-07 09:12:54,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -96.91944885253906
2023-01-07 09:12:54,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0045221527107059956
2023-01-07 09:12:54,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 1.6660895347595215
2023-01-07 09:12:54,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -90.78326416015625
2023-01-07 09:12:54,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4432086944580078
2023-01-07 09:12:54,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,447 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -4033.69140625
2023-01-07 09:12:54,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10607520490884781
2023-01-07 09:12:54,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.34324541687965393
2023-01-07 09:12:54,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -4025.9033203125
2023-01-07 09:12:54,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4184815287590027
2023-01-07 09:12:54,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -467.1195373535156
2023-01-07 09:12:54,451 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0963917151093483
2023-01-07 09:12:54,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,452 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 32.78185272216797
2023-01-07 09:12:54,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,452 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -435.0393371582031
2023-01-07 09:12:54,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.21269607543945
2023-01-07 09:12:54,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,454 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 79.07496643066406
2023-01-07 09:12:54,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002532169222831726
2023-01-07 09:12:54,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 44.177818298339844
2023-01-07 09:12:54,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 107.70579528808594
2023-01-07 09:12:54,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.346160888671875
2023-01-07 09:12:54,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -120.06135559082031
2023-01-07 09:12:54,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04870499670505524
2023-01-07 09:12:54,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 1.8982200622558594
2023-01-07 09:12:54,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -93.77775573730469
2023-01-07 09:12:54,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.489645779132843
2023-01-07 09:12:54,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -513.4061279296875
2023-01-07 09:12:54,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005469406023621559
2023-01-07 09:12:54,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,462 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 3.852285861968994
2023-01-07 09:12:54,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,463 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -485.3885192871094
2023-01-07 09:12:54,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8747217655181885
2023-01-07 09:12:54,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,465 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.33486104011535645
2023-01-07 09:12:54,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10639970004558563
2023-01-07 09:12:54,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 3.4810988903045654
2023-01-07 09:12:54,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,466 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 32.24437713623047
2023-01-07 09:12:54,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.056858539581299
2023-01-07 09:12:54,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.0712823867797852
2023-01-07 09:12:54,468 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.024398162961006165
2023-01-07 09:12:54,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,469 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -2.260910749435425
2023-01-07 09:12:54,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,470 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 32.66877365112305
2023-01-07 09:12:54,470 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.253983497619629
2023-01-07 09:12:54,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,472 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -111.85975646972656
2023-01-07 09:12:54,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04323098808526993
2023-01-07 09:12:54,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,473 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 28.140026092529297
2023-01-07 09:12:54,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -80.41336059570312
2023-01-07 09:12:54,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.126771926879883
2023-01-07 09:12:54,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -122.07969665527344
2023-01-07 09:12:54,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.009667813777923584
2023-01-07 09:12:54,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 3.472590923309326
2023-01-07 09:12:54,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -62.41543197631836
2023-01-07 09:12:54,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3236472606658936
2023-01-07 09:12:54,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 196.655517578125
2023-01-07 09:12:54,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.017477504909038544
2023-01-07 09:12:54,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -1.4018577337265015
2023-01-07 09:12:54,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 260.2118835449219
2023-01-07 09:12:54,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4539490938186646
2023-01-07 09:12:54,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,482 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 70.3731689453125
2023-01-07 09:12:54,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.028980623930692673
2023-01-07 09:12:54,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -2.5024664402008057
2023-01-07 09:12:54,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 71.96098327636719
2023-01-07 09:12:54,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6356210708618164
2023-01-07 09:12:54,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,486 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 61.5806884765625
2023-01-07 09:12:54,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13358427584171295
2023-01-07 09:12:54,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,487 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 1.3454540967941284
2023-01-07 09:12:54,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,487 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 59.168060302734375
2023-01-07 09:12:54,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7437111139297485
2023-01-07 09:12:54,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -28.738109588623047
2023-01-07 09:12:54,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.057498469948768616
2023-01-07 09:12:54,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.667134702205658
2023-01-07 09:12:54,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -31.767751693725586
2023-01-07 09:12:54,491 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.787578821182251
2023-01-07 09:12:54,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 495.6690368652344
2023-01-07 09:12:54,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08075405657291412
2023-01-07 09:12:54,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -1.882574200630188
2023-01-07 09:12:54,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 496.5319519042969
2023-01-07 09:12:54,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8979558944702148
2023-01-07 09:12:54,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,496 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -101.72738647460938
2023-01-07 09:12:54,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14399726688861847
2023-01-07 09:12:54,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.7481069564819336
2023-01-07 09:12:54,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -93.8013916015625
2023-01-07 09:12:54,498 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8677792549133301
2023-01-07 09:12:54,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 309.39501953125
2023-01-07 09:12:54,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06022844463586807
2023-01-07 09:12:54,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.9314332008361816
2023-01-07 09:12:54,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 318.9575500488281
2023-01-07 09:12:54,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6240358352661133
2023-01-07 09:12:54,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -144.83766174316406
2023-01-07 09:12:54,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030303239822387695
2023-01-07 09:12:54,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.03933918476104736
2023-01-07 09:12:54,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,505 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -143.6405029296875
2023-01-07 09:12:54,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4177374839782715
2023-01-07 09:12:54,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,507 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -665.8458251953125
2023-01-07 09:12:54,507 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08137711137533188
2023-01-07 09:12:54,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -18.822885513305664
2023-01-07 09:12:54,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,508 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -664.84521484375
2023-01-07 09:12:54,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.932716369628906
2023-01-07 09:12:54,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,510 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1701.50927734375
2023-01-07 09:12:54,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0222223661839962
2023-01-07 09:12:54,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,511 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -7.207398414611816
2023-01-07 09:12:54,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,512 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1693.796630859375
2023-01-07 09:12:54,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.227268695831299
2023-01-07 09:12:54,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -7.820667266845703
2023-01-07 09:12:54,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04584154859185219
2023-01-07 09:12:54,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,515 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -24.203969955444336
2023-01-07 09:12:54,515 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.123905897140503
2023-01-07 09:12:54,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 20.139141082763672
2023-01-07 09:12:54,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43462908267974854
2023-01-07 09:12:54,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -7.120481967926025
2023-01-07 09:12:54,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 2.491522789001465
2023-01-07 09:12:54,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.703137397766113
2023-01-07 09:12:54,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 8.122790336608887
2023-01-07 09:12:54,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.013572558760643005
2023-01-07 09:12:54,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 2.1409220695495605
2023-01-07 09:12:54,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,522 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -10.391281127929688
2023-01-07 09:12:54,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1868510246276855
2023-01-07 09:12:54,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -43.332645416259766
2023-01-07 09:12:54,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13566717505455017
2023-01-07 09:12:54,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,525 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -1.4389615058898926
2023-01-07 09:12:54,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,525 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 156.6136016845703
2023-01-07 09:12:54,525 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01466381549835205
2023-01-07 09:12:54,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 141.38177490234375
2023-01-07 09:12:54,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05514845624566078
2023-01-07 09:12:54,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,528 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 136.167236328125
2023-01-07 09:12:54,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14397436380386353
2023-01-07 09:12:54,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -481.0185852050781
2023-01-07 09:12:54,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0849745124578476
2023-01-07 09:12:54,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,531 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -512.4625244140625
2023-01-07 09:12:54,531 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3197440505027771
2023-01-07 09:12:54,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,533 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 33.151397705078125
2023-01-07 09:12:54,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07705381512641907
2023-01-07 09:12:54,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 39.48680114746094
2023-01-07 09:12:54,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4297815263271332
2023-01-07 09:12:54,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,535 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -16.088603973388672
2023-01-07 09:12:54,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13853943347930908
2023-01-07 09:12:54,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,537 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 6.314988136291504
2023-01-07 09:12:54,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.016341501846909523
2023-01-07 09:12:54,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,538 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 33.37176513671875
2023-01-07 09:12:54,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4428667426109314
2023-01-07 09:12:54,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 128.0504150390625
2023-01-07 09:12:54,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1112201064825058
2023-01-07 09:12:54,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 5.826591491699219
2023-01-07 09:12:54,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6642670035362244
2023-01-07 09:12:54,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,542 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -14.154857635498047
2023-01-07 09:12:54,542 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7096002101898193
2023-01-07 09:12:54,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -34.242149353027344
2023-01-07 09:12:54,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4685461223125458
2023-01-07 09:12:54,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,545 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 36.74528884887695
2023-01-07 09:12:54,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16618934273719788
2023-01-07 09:12:54,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 112.91571807861328
2023-01-07 09:12:54,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005234807729721069
2023-01-07 09:12:54,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -3.1999897956848145
2023-01-07 09:12:54,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,548 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 128.38009643554688
2023-01-07 09:12:54,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.509570360183716
2023-01-07 09:12:54,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -73.0283203125
2023-01-07 09:12:54,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.254068374633789
2023-01-07 09:12:54,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -50.982112884521484
2023-01-07 09:12:54,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.463102340698242
2023-01-07 09:12:54,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 120.58827209472656
2023-01-07 09:12:54,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.839580535888672
2023-01-07 09:12:54,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,554 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 193.886962890625
2023-01-07 09:12:54,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3734569549560547
2023-01-07 09:12:54,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 128.94818115234375
2023-01-07 09:12:54,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8816758990287781
2023-01-07 09:12:54,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,557 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 9.37112045288086
2023-01-07 09:12:54,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,557 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 218.41920471191406
2023-01-07 09:12:54,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.474832057952881
2023-01-07 09:12:54,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,559 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1060.090576171875
2023-01-07 09:12:54,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8929475545883179
2023-01-07 09:12:54,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1051.734619140625
2023-01-07 09:12:54,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.221279263496399
2023-01-07 09:12:54,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 163.64971923828125
2023-01-07 09:12:54,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0681782960891724
2023-01-07 09:12:54,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -179.24317932128906
2023-01-07 09:12:54,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1902918815612793
2023-01-07 09:12:54,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 826.701171875
2023-01-07 09:12:54,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09834444522857666
2023-01-07 09:12:54,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 2.269120216369629
2023-01-07 09:12:54,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 488.8068542480469
2023-01-07 09:12:54,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1013550758361816
2023-01-07 09:12:54,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,568 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1150.471435546875
2023-01-07 09:12:54,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8861726522445679
2023-01-07 09:12:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,569 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 11.468168258666992
2023-01-07 09:12:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,569 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -7.2166547775268555
2023-01-07 09:12:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1355.7015380859375
2023-01-07 09:12:54,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.831464767456055
2023-01-07 09:12:54,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 992.211181640625
2023-01-07 09:12:54,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.26331329345703
2023-01-07 09:12:54,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,573 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1050.3336181640625
2023-01-07 09:12:54,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.04447364807129
2023-01-07 09:12:54,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,574 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -85.9482421875
2023-01-07 09:12:54,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.237131118774414
2023-01-07 09:12:54,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,576 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 9.117216110229492
2023-01-07 09:12:54,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,576 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -444.94256591796875
2023-01-07 09:12:54,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.98078918457031
2023-01-07 09:12:54,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,578 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -50.573143005371094
2023-01-07 09:12:54,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.379131317138672
2023-01-07 09:12:54,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,579 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -13.618518829345703
2023-01-07 09:12:54,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,579 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -99.53193664550781
2023-01-07 09:12:54,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.57225799560547
2023-01-07 09:12:54,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,581 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -400.84814453125
2023-01-07 09:12:54,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.766263961791992
2023-01-07 09:12:54,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,582 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -139.4794158935547
2023-01-07 09:12:54,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 666.0444946289062
2023-01-07 09:12:54,583 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.283714294433594
2023-01-07 09:12:54,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,584 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -803.8721923828125
2023-01-07 09:12:54,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.524909973144531
2023-01-07 09:12:54,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,586 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -169.3376922607422
2023-01-07 09:12:54,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.752876281738281
2023-01-07 09:12:54,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,587 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1236.0244140625
2023-01-07 09:12:54,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.84077453613281
2023-01-07 09:12:54,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -5.508793830871582
2023-01-07 09:12:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 82.03114318847656
2023-01-07 09:12:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,589 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1685.262939453125
2023-01-07 09:12:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,590 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 633.5726318359375
2023-01-07 09:12:54,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 71.60276794433594
2023-01-07 09:12:54,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,591 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1701.9619140625
2023-01-07 09:12:54,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.74148178100586
2023-01-07 09:12:54,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,593 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 260.4507751464844
2023-01-07 09:12:54,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 191.414306640625
2023-01-07 09:12:54,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,594 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1809.3131103515625
2023-01-07 09:12:54,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.351100206375122
2023-01-07 09:12:54,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,595 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -225.61370849609375
2023-01-07 09:12:54,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,596 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 644.2713623046875
2023-01-07 09:12:54,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.84852600097656
2023-01-07 09:12:54,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,598 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -303.1095886230469
2023-01-07 09:12:54,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -78.97399139404297
2023-01-07 09:12:54,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,599 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.3323354721069336
2023-01-07 09:12:54,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,599 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -740.0906982421875
2023-01-07 09:12:54,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -130.13909912109375
2023-01-07 09:12:54,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,601 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1143.29443359375
2023-01-07 09:12:54,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.91918182373047
2023-01-07 09:12:54,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,602 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1727.162109375
2023-01-07 09:12:54,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9381120204925537
2023-01-07 09:12:54,607 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:12:54,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,607 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 566.4117431640625
2023-01-07 09:12:54,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,608 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2375.06103515625
2023-01-07 09:12:54,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,609 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 650.3519287109375
2023-01-07 09:12:54,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 457.2521667480469
2023-01-07 09:12:54,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1511.51953125
2023-01-07 09:12:54,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 480.57940673828125
2023-01-07 09:12:54,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 622.949951171875
2023-01-07 09:12:54,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 148.7017059326172
2023-01-07 09:12:54,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,613 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 208.63136291503906
2023-01-07 09:12:54,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,614 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 15.992469787597656
2023-01-07 09:12:54,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,614 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -134.90269470214844
2023-01-07 09:12:54,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 159.42440795898438
2023-01-07 09:12:54,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 191.18580627441406
2023-01-07 09:12:54,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,616 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 164.79708862304688
2023-01-07 09:12:54,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -550.974853515625
2023-01-07 09:12:54,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 54.54338073730469
2023-01-07 09:12:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 384.4329833984375
2023-01-07 09:12:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.679369926452637
2023-01-07 09:12:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 3.532097816467285
2023-01-07 09:12:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -41.64529037475586
2023-01-07 09:12:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1695.327392578125
2023-01-07 09:12:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -664.6311645507812
2023-01-07 09:12:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -143.31332397460938
2023-01-07 09:12:54,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 327.7308044433594
2023-01-07 09:12:54,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -93.66227722167969
2023-01-07 09:12:54,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 498.4455871582031
2023-01-07 09:12:54,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 4226.1240234375
2023-01-07 09:12:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -35.550270080566406
2023-01-07 09:12:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 59.23615646362305
2023-01-07 09:12:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 71.96482849121094
2023-01-07 09:12:54,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 287.1551208496094
2023-01-07 09:12:54,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -62.490726470947266
2023-01-07 09:12:54,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -55.240840911865234
2023-01-07 09:12:54,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,624 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 58.25726318359375
2023-01-07 09:12:54,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 31.79943084716797
2023-01-07 09:12:54,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -476.8513488769531
2023-01-07 09:12:54,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -443.824951171875
2023-01-07 09:12:54,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -88.52853393554688
2023-01-07 09:12:54,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 107.99298858642578
2023-01-07 09:12:54,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,626 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -436.2401428222656
2023-01-07 09:12:54,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,627 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3997.29931640625
2023-01-07 09:12:54,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -4619.896484375
2023-01-07 09:12:54,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,628 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -93.34614562988281
2023-01-07 09:12:54,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,628 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 47.355682373046875
2023-01-07 09:12:54,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,628 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 14.832266807556152
2023-01-07 09:12:54,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -534.7667236328125
2023-01-07 09:12:54,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,629 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -111.25848388671875
2023-01-07 09:12:54,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:54,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:54,630 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -614.607177734375
2023-01-07 09:12:54,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.83161735534668
2023-01-07 09:12:55,469 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -0.063262939453125 param sum :: 298.834716796875
2023-01-07 09:12:55,469 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 65.51138305664062
2023-01-07 09:12:55,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,470 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 8.476816177368164
2023-01-07 09:12:55,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,470 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2417.27734375
2023-01-07 09:12:55,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.208324432373047
2023-01-07 09:12:55,472 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -22.298633575439453 param sum :: 65.86074829101562
2023-01-07 09:12:55,472 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -44.4479866027832
2023-01-07 09:12:55,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,472 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -138.696533203125
2023-01-07 09:12:55,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,473 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2310.103515625
2023-01-07 09:12:55,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -140.04705810546875
2023-01-07 09:12:55,474 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -58.65904235839844 param sum :: -37.83891296386719
2023-01-07 09:12:55,475 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,475 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,475 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 522.6602172851562
2023-01-07 09:12:55,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,475 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2314.249755859375
2023-01-07 09:12:55,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.437842845916748
2023-01-07 09:12:55,476 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -36.17855453491211 param sum :: 60.60076904296875
2023-01-07 09:12:55,476 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,477 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 522.6602172851562
2023-01-07 09:12:55,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,477 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2231.406494140625
2023-01-07 09:12:55,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.62350845336914
2023-01-07 09:12:55,478 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -2211.396728515625 param sum :: 609.6046142578125
2023-01-07 09:12:55,478 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,479 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 61.766239166259766
2023-01-07 09:12:55,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,479 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 8.639261245727539
2023-01-07 09:12:55,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,479 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 28.937225341796875
2023-01-07 09:12:55,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,479 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2247.05224609375
2023-01-07 09:12:55,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.30746078491211
2023-01-07 09:12:55,481 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -39.54007339477539 param sum :: 62.121429443359375
2023-01-07 09:12:55,481 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 187.2625732421875
2023-01-07 09:12:55,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,482 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2350.123779296875
2023-01-07 09:12:55,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.142717361450195
2023-01-07 09:12:55,483 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 508.8700256347656 param sum :: -47.89112091064453
2023-01-07 09:12:55,483 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 257.1244201660156
2023-01-07 09:12:55,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,483 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -4.484635353088379
2023-01-07 09:12:55,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,484 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -415.38427734375
2023-01-07 09:12:55,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3162126541137695
2023-01-07 09:12:55,485 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -59.12016296386719 param sum :: 257.91448974609375
2023-01-07 09:12:55,486 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 61.71021270751953
2023-01-07 09:12:55,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,486 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -671.7537841796875
2023-01-07 09:12:55,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.94864273071289
2023-01-07 09:12:55,487 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -532.845703125 param sum :: 105.31574249267578
2023-01-07 09:12:55,487 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 61.71021270751953
2023-01-07 09:12:55,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,488 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -837.4744262695312
2023-01-07 09:12:55,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.417999267578125
2023-01-07 09:12:55,489 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -68.82534790039062 param sum :: 259.12005615234375
2023-01-07 09:12:55,489 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 61.71021270751953
2023-01-07 09:12:55,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,489 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -731.980224609375
2023-01-07 09:12:55,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.90070915222168
2023-01-07 09:12:55,491 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -309.5667419433594 param sum :: 52.111228942871094
2023-01-07 09:12:55,491 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.38249206542969
2023-01-07 09:12:55,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,491 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 8.149155616760254
2023-01-07 09:12:55,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,492 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -725.16455078125
2023-01-07 09:12:55,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.803531646728516
2023-01-07 09:12:55,494 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 3.454975128173828 param sum :: 61.252052307128906
2023-01-07 09:12:55,494 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 61.71021270751953
2023-01-07 09:12:55,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,494 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -809.6307373046875
2023-01-07 09:12:55,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,495 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -208.4297637939453
2023-01-07 09:12:55,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.58380889892578
2023-01-07 09:12:55,496 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -768.3822631835938 param sum :: 31.519947052001953
2023-01-07 09:12:55,496 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 66.53099822998047
2023-01-07 09:12:55,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,497 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.05644083023071289
2023-01-07 09:12:55,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,497 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -575.15478515625
2023-01-07 09:12:55,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.2103271484375
2023-01-07 09:12:55,499 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 15.005656242370605 param sum :: 65.82707977294922
2023-01-07 09:12:55,499 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 187.2625732421875
2023-01-07 09:12:55,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,499 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2434.9130859375
2023-01-07 09:12:55,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.833431243896484
2023-01-07 09:12:55,501 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -451.9859619140625 param sum :: 163.98773193359375
2023-01-07 09:12:55,501 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 420.38372802734375
2023-01-07 09:12:55,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,501 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -468.453369140625
2023-01-07 09:12:55,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.666824340820312
2023-01-07 09:12:55,503 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -23.306903839111328 param sum :: 275.0848388671875
2023-01-07 09:12:55,503 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 420.38372802734375
2023-01-07 09:12:55,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,503 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -433.0254821777344
2023-01-07 09:12:55,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,503 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2260.58984375
2023-01-07 09:12:55,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.593345642089844
2023-01-07 09:12:55,505 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -445.2513427734375 param sum :: 514.5616455078125
2023-01-07 09:12:55,505 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 187.2625732421875
2023-01-07 09:12:55,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,506 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2231.156005859375
2023-01-07 09:12:55,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.726666450500488
2023-01-07 09:12:55,507 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 4.543843746185303 param sum :: 64.50940704345703
2023-01-07 09:12:55,507 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 187.2625732421875
2023-01-07 09:12:55,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,507 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2325.32275390625
2023-01-07 09:12:55,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.010923385620117
2023-01-07 09:12:55,509 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -2292.82568359375 param sum :: 220.32901000976562
2023-01-07 09:12:55,509 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.36270904541016
2023-01-07 09:12:55,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,509 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 17.012067794799805
2023-01-07 09:12:55,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,510 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -97.28379821777344
2023-01-07 09:12:55,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,510 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1319.5089111328125
2023-01-07 09:12:55,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -81.72318267822266
2023-01-07 09:12:55,512 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 3.7544679641723633 param sum :: 63.68058776855469
2023-01-07 09:12:55,512 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 551.4695434570312
2023-01-07 09:12:55,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,512 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -184.67630004882812
2023-01-07 09:12:55,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.546613693237305
2023-01-07 09:12:55,513 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 418.0400390625 param sum :: -14.35962200164795
2023-01-07 09:12:55,514 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 566.8568115234375
2023-01-07 09:12:55,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,514 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 1378.43310546875
2023-01-07 09:12:55,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.178070068359375
2023-01-07 09:12:55,515 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -17.682296752929688 param sum :: 277.5535888671875
2023-01-07 09:12:55,516 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 551.4695434570312
2023-01-07 09:12:55,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,516 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -32.63072967529297
2023-01-07 09:12:55,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.527923583984375
2023-01-07 09:12:55,517 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 1204.462158203125 param sum :: 559.3900146484375
2023-01-07 09:12:55,517 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 551.4695434570312
2023-01-07 09:12:55,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,518 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 30.27921485900879
2023-01-07 09:12:55,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.26982879638672
2023-01-07 09:12:55,519 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -6.456034183502197 param sum :: 141.3893585205078
2023-01-07 09:12:55,519 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 551.4695434570312
2023-01-07 09:12:55,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,519 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 177.76084899902344
2023-01-07 09:12:55,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.542003631591797
2023-01-07 09:12:55,521 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 236.0447235107422 param sum :: 397.58868408203125
2023-01-07 09:12:55,521 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 131.55776977539062
2023-01-07 09:12:55,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,522 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -4.530287265777588
2023-01-07 09:12:55,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,522 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 316.24017333984375
2023-01-07 09:12:55,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.591306686401367
2023-01-07 09:12:55,523 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -43.761356353759766 param sum :: 132.87896728515625
2023-01-07 09:12:55,523 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 603.243408203125
2023-01-07 09:12:55,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 427.611083984375
2023-01-07 09:12:55,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.503530979156494
2023-01-07 09:12:55,525 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 356.53289794921875 param sum :: 619.82275390625
2023-01-07 09:12:55,525 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 509.56842041015625
2023-01-07 09:12:55,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 5.630372524261475
2023-01-07 09:12:55,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 451.2242431640625
2023-01-07 09:12:55,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.580204010009766
2023-01-07 09:12:55,528 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.8301982879638672 param sum :: 509.6986389160156
2023-01-07 09:12:55,528 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,528 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 391.04998779296875
2023-01-07 09:12:55,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,528 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 382.5921630859375
2023-01-07 09:12:55,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.911013603210449
2023-01-07 09:12:55,530 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 538.595947265625 param sum :: 390.9471435546875
2023-01-07 09:12:55,530 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 435.0699768066406
2023-01-07 09:12:55,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 318.99786376953125
2023-01-07 09:12:55,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3283771276473999
2023-01-07 09:12:55,531 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -7.859121322631836 param sum :: 587.641357421875
2023-01-07 09:12:55,531 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 435.0699768066406
2023-01-07 09:12:55,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 344.3038330078125
2023-01-07 09:12:55,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.27667808532715
2023-01-07 09:12:55,533 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 376.6170654296875 param sum :: 679.0567626953125
2023-01-07 09:12:55,533 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,534 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.24115753173828
2023-01-07 09:12:55,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.23411846160888672
2023-01-07 09:12:55,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 489.5141296386719
2023-01-07 09:12:55,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9576404094696045
2023-01-07 09:12:55,536 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 4.057262420654297 param sum :: 127.1537857055664
2023-01-07 09:12:55,536 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 891.2916870117188
2023-01-07 09:12:55,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,536 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 549.9523315429688
2023-01-07 09:12:55,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.013974189758301
2023-01-07 09:12:55,538 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 575.1444702148438 param sum :: 1623.142822265625
2023-01-07 09:12:55,538 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 130.08084106445312
2023-01-07 09:12:55,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,538 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -6.022998809814453
2023-01-07 09:12:55,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,539 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 160.2209014892578
2023-01-07 09:12:55,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.801372528076172
2023-01-07 09:12:55,540 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -30.37470817565918 param sum :: 132.2764892578125
2023-01-07 09:12:55,540 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 666.19287109375
2023-01-07 09:12:55,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 166.64369201660156
2023-01-07 09:12:55,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.353550434112549
2023-01-07 09:12:55,542 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 167.53912353515625 param sum :: 949.8642578125
2023-01-07 09:12:55,542 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 665.2001342773438
2023-01-07 09:12:55,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,543 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 192.97299194335938
2023-01-07 09:12:55,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.1775383949279785
2023-01-07 09:12:55,544 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 26.32976722717285 param sum :: 595.7491455078125
2023-01-07 09:12:55,544 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,544 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,544 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 665.2001342773438
2023-01-07 09:12:55,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 183.58041381835938
2023-01-07 09:12:55,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.377371788024902
2023-01-07 09:12:55,546 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 208.67343139648438 param sum :: 701.846435546875
2023-01-07 09:12:55,546 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.99636840820312
2023-01-07 09:12:55,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -3.3319902420043945
2023-01-07 09:12:55,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,547 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -38.121334075927734
2023-01-07 09:12:55,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0467534065246582
2023-01-07 09:12:55,548 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 16.707855224609375 param sum :: 130.32025146484375
2023-01-07 09:12:55,548 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,549 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 849.1844482421875
2023-01-07 09:12:55,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,549 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -44.372337341308594
2023-01-07 09:12:55,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.809662818908691
2023-01-07 09:12:55,550 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -10.538375854492188 param sum :: 1046.4140625
2023-01-07 09:12:55,550 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,551 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 130.16050720214844
2023-01-07 09:12:55,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 5.603691577911377
2023-01-07 09:12:55,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -162.16302490234375
2023-01-07 09:12:55,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.857646942138672
2023-01-07 09:12:55,553 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 10.119401931762695 param sum :: 130.1958770751953
2023-01-07 09:12:55,553 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 1035.0040283203125
2023-01-07 09:12:55,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -141.80126953125
2023-01-07 09:12:55,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.244629383087158
2023-01-07 09:12:55,555 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -140.01022338867188 param sum :: 1228.911376953125
2023-01-07 09:12:55,555 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,555 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,555 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 516.09619140625
2023-01-07 09:12:55,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 3.6472697257995605
2023-01-07 09:12:55,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 143.2791748046875
2023-01-07 09:12:55,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.403748512268066
2023-01-07 09:12:55,557 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -14.446908950805664 param sum :: 517.8522338867188
2023-01-07 09:12:55,557 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 894.716552734375
2023-01-07 09:12:55,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 145.666748046875
2023-01-07 09:12:55,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.20210266113281
2023-01-07 09:12:55,559 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 126.71488952636719 param sum :: 1035.976806640625
2023-01-07 09:12:55,559 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.46072387695312
2023-01-07 09:12:55,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -1.1521754264831543
2023-01-07 09:12:55,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 145.15895080566406
2023-01-07 09:12:55,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1048524379730225
2023-01-07 09:12:55,562 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -7.692392349243164 param sum :: 129.05946350097656
2023-01-07 09:12:55,562 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1566.7215576171875
2023-01-07 09:12:55,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 145.21734619140625
2023-01-07 09:12:55,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3560287952423096
2023-01-07 09:12:55,564 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 153.41751098632812 param sum :: 1831.792724609375
2023-01-07 09:12:55,564 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 120.98361206054688
2023-01-07 09:12:55,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 1.8291478157043457
2023-01-07 09:12:55,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 143.65435791015625
2023-01-07 09:12:55,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.919299125671387
2023-01-07 09:12:55,566 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.3829212188720703 param sum :: 119.65865325927734
2023-01-07 09:12:55,566 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1064.3114013671875
2023-01-07 09:12:55,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,567 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 149.2178955078125
2023-01-07 09:12:55,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.419157981872559
2023-01-07 09:12:55,568 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 123.86543273925781 param sum :: 1148.9156494140625
2023-01-07 09:12:55,569 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 502.12744140625
2023-01-07 09:12:55,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,569 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 6.041601181030273
2023-01-07 09:12:55,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -598.1176147460938
2023-01-07 09:12:55,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.08475112915039
2023-01-07 09:12:55,571 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -14.066201210021973 param sum :: 509.0491638183594
2023-01-07 09:12:55,571 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 712.916748046875
2023-01-07 09:12:55,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -400.6487731933594
2023-01-07 09:12:55,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.95962905883789
2023-01-07 09:12:55,573 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -429.7220153808594 param sum :: 1215.13037109375
2023-01-07 09:12:55,573 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 258.9994201660156
2023-01-07 09:12:55,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 7.803080081939697
2023-01-07 09:12:55,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 317.4891052246094
2023-01-07 09:12:55,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.468012809753418
2023-01-07 09:12:55,575 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 48.36752700805664 param sum :: 258.2674560546875
2023-01-07 09:12:55,575 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -50461.1640625
2023-01-07 09:12:55,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 328.3154602050781
2023-01-07 09:12:55,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.957902193069458
2023-01-07 09:12:55,577 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 322.5143737792969 param sum :: -59672.63671875
2023-01-07 09:12:55,577 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 518.505859375
2023-01-07 09:12:55,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 384.4329833984375
2023-01-07 09:12:55,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.113317966461182
2023-01-07 09:12:55,579 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -19.04669189453125 param sum :: 329.0807800292969
2023-01-07 09:12:55,579 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,579 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,579 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 518.505859375
2023-01-07 09:12:55,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,579 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 396.8992004394531
2023-01-07 09:12:55,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5950417518615723
2023-01-07 09:12:55,581 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 408.3661804199219 param sum :: 475.92413330078125
2023-01-07 09:12:55,581 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,581 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -71073.6171875
2023-01-07 09:12:55,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,582 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.724799156188965
2023-01-07 09:12:55,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.62113094329834
2023-01-07 09:12:55,583 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -18.164369583129883 param sum :: 1377.8468017578125
2023-01-07 09:12:55,583 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -71073.6171875
2023-01-07 09:12:55,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,583 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 8.929886817932129
2023-01-07 09:12:55,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.235574722290039
2023-01-07 09:12:55,585 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 4.351247787475586 param sum :: -85224.796875
2023-01-07 09:12:55,585 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49663.359375
2023-01-07 09:12:55,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,585 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 3.2917232513427734
2023-01-07 09:12:55,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.226524353027344
2023-01-07 09:12:55,587 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -23.54183578491211 param sum :: 1424.684326171875
2023-01-07 09:12:55,587 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49663.359375
2023-01-07 09:12:55,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 8.518880844116211
2023-01-07 09:12:55,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.820268154144287
2023-01-07 09:12:55,589 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 10.483367919921875 param sum :: -59387.515625
2023-01-07 09:12:55,589 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,589 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,589 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 234.67689514160156
2023-01-07 09:12:55,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -8.195889472961426
2023-01-07 09:12:55,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -41.339962005615234
2023-01-07 09:12:55,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.77759838104248
2023-01-07 09:12:55,591 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -8.739423751831055 param sum :: 231.90716552734375
2023-01-07 09:12:55,591 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,591 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -122591.2265625
2023-01-07 09:12:55,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -43.06211471557617
2023-01-07 09:12:55,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.855560064315796
2023-01-07 09:12:55,593 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -45.498207092285156 param sum :: -146370.265625
2023-01-07 09:12:55,593 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25807.74609375
2023-01-07 09:12:55,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,594 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2.137878894805908
2023-01-07 09:12:55,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42797788977622986
2023-01-07 09:12:55,595 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -15.850489616394043 param sum :: 342.0841979980469
2023-01-07 09:12:55,595 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25807.74609375
2023-01-07 09:12:55,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.0378022193908691
2023-01-07 09:12:55,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5951430797576904
2023-01-07 09:12:55,597 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -3.6312637329101562 param sum :: -30830.1640625
2023-01-07 09:12:55,597 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1200.822021484375
2023-01-07 09:12:55,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,598 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -6.2548627853393555
2023-01-07 09:12:55,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.019185543060303
2023-01-07 09:12:55,599 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -17.793413162231445 param sum :: 1451.051513671875
2023-01-07 09:12:55,599 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1200.822021484375
2023-01-07 09:12:55,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -8.925493240356445
2023-01-07 09:12:55,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4090574383735657
2023-01-07 09:12:55,601 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -9.153101921081543 param sum :: 1393.116455078125
2023-01-07 09:12:55,601 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1651.47509765625
2023-01-07 09:12:55,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 7.366672039031982
2023-01-07 09:12:55,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2711381912231445
2023-01-07 09:12:55,603 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.7296452522277832 param sum :: 353.04669189453125
2023-01-07 09:12:55,603 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1651.47509765625
2023-01-07 09:12:55,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 9.1867036819458
2023-01-07 09:12:55,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.057499468326568604
2023-01-07 09:12:55,605 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 9.445504188537598 param sum :: 1909.970458984375
2023-01-07 09:12:55,605 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2009.8336181640625
2023-01-07 09:12:55,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 23.53314781188965
2023-01-07 09:12:55,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08733686804771423
2023-01-07 09:12:55,607 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.7569320201873779 param sum :: 361.0688781738281
2023-01-07 09:12:55,607 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,607 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,607 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2009.8336181640625
2023-01-07 09:12:55,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,607 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 26.572906494140625
2023-01-07 09:12:55,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5699338912963867
2023-01-07 09:12:55,609 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 25.748577117919922 param sum :: 2249.454833984375
2023-01-07 09:12:55,609 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2259.3759765625
2023-01-07 09:12:55,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 11.267037391662598
2023-01-07 09:12:55,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6804593801498413
2023-01-07 09:12:55,611 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 7.086733818054199 param sum :: 1485.2086181640625
2023-01-07 09:12:55,611 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2259.3759765625
2023-01-07 09:12:55,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 9.664064407348633
2023-01-07 09:12:55,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4154592752456665
2023-01-07 09:12:55,613 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 9.115556716918945 param sum :: 2701.028564453125
2023-01-07 09:12:55,613 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,613 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2413.02587890625
2023-01-07 09:12:55,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -507.0010986328125
2023-01-07 09:12:55,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0159447193145752
2023-01-07 09:12:55,614 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -1.7403857707977295 param sum :: 357.2828369140625
2023-01-07 09:12:55,615 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2413.02587890625
2023-01-07 09:12:55,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -507.09820556640625
2023-01-07 09:12:55,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37461432814598083
2023-01-07 09:12:55,617 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -506.79022216796875 param sum :: 2886.699462890625
2023-01-07 09:12:55,617 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,617 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2186.31591796875
2023-01-07 09:12:55,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -22.92327880859375
2023-01-07 09:12:55,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.076756477355957
2023-01-07 09:12:55,618 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0989689826965332 param sum :: 360.71636962890625
2023-01-07 09:12:55,618 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,619 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2186.31591796875
2023-01-07 09:12:55,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -24.39503288269043
2023-01-07 09:12:55,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9599452018737793
2023-01-07 09:12:55,621 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -24.94512367248535 param sum :: 2925.96142578125
2023-01-07 09:12:55,621 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,621 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2258.6396484375
2023-01-07 09:12:55,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,621 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -78.02835845947266
2023-01-07 09:12:55,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7537301778793335
2023-01-07 09:12:55,622 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -1.1437392234802246 param sum :: 1491.359619140625
2023-01-07 09:12:55,622 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,622 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2258.6396484375
2023-01-07 09:12:55,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -75.18803405761719
2023-01-07 09:12:55,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.100030779838562
2023-01-07 09:12:55,624 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -77.1361312866211 param sum :: 3857.93603515625
2023-01-07 09:12:55,625 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 3273.265625
2023-01-07 09:12:55,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1216.11328125
2023-01-07 09:12:55,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.604494571685791
2023-01-07 09:12:55,626 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -2.5076117515563965 param sum :: 368.474609375
2023-01-07 09:12:55,626 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 3273.265625
2023-01-07 09:12:55,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,627 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1215.789306640625
2023-01-07 09:12:55,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0780484676361084
2023-01-07 09:12:55,628 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1216.5687255859375 param sum :: 4784.8251953125
2023-01-07 09:12:55,629 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,629 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3653.725341796875
2023-01-07 09:12:55,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 707.602783203125
2023-01-07 09:12:55,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5027763843536377
2023-01-07 09:12:55,630 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 35.171546936035156 param sum :: 358.7554931640625
2023-01-07 09:12:55,630 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3653.725341796875
2023-01-07 09:12:55,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 705.7632446289062
2023-01-07 09:12:55,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.979731321334839
2023-01-07 09:12:55,632 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 704.396484375 param sum :: -3489.111083984375
2023-01-07 09:12:55,632 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -43505.3671875
2023-01-07 09:12:55,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 64.05123138427734
2023-01-07 09:12:55,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.502213478088379
2023-01-07 09:12:55,634 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 63.172767639160156 param sum :: 1482.262939453125
2023-01-07 09:12:55,634 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -43505.3671875
2023-01-07 09:12:55,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 51.39176559448242
2023-01-07 09:12:55,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7383347153663635
2023-01-07 09:12:55,636 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 49.595340728759766 param sum :: -51981.7421875
2023-01-07 09:12:55,636 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,636 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -19471.1875
2023-01-07 09:12:55,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 116.78280639648438
2023-01-07 09:12:55,637 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.925692081451416
2023-01-07 09:12:55,638 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 34.34840393066406 param sum :: 362.582763671875
2023-01-07 09:12:55,638 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -19471.1875
2023-01-07 09:12:55,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 117.33792114257812
2023-01-07 09:12:55,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1182870864868164
2023-01-07 09:12:55,640 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 117.61032104492188 param sum :: -23757.47265625
2023-01-07 09:12:55,640 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49311.16796875
2023-01-07 09:12:55,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 58.25349426269531
2023-01-07 09:12:55,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4847557544708252
2023-01-07 09:12:55,642 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 6.242486953735352 param sum :: 370.4804992675781
2023-01-07 09:12:55,642 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49311.16796875
2023-01-07 09:12:55,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 55.12531280517578
2023-01-07 09:12:55,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5728162527084351
2023-01-07 09:12:55,644 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 54.52899932861328 param sum :: -58861.49609375
2023-01-07 09:12:55,644 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107946.453125
2023-01-07 09:12:55,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,645 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 32.34326171875
2023-01-07 09:12:55,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.258666753768921
2023-01-07 09:12:55,646 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 35.40790557861328 param sum :: 1479.61572265625
2023-01-07 09:12:55,646 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107946.453125
2023-01-07 09:12:55,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,647 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 22.46608543395996
2023-01-07 09:12:55,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0621337890625
2023-01-07 09:12:55,648 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 29.26189422607422 param sum :: -128960.359375
2023-01-07 09:12:55,648 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -91957.8515625
2023-01-07 09:12:55,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,649 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -955.9517822265625
2023-01-07 09:12:55,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2969483435153961
2023-01-07 09:12:55,650 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 23.394912719726562 param sum :: 723.7913818359375
2023-01-07 09:12:55,650 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,650 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,650 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -91957.8515625
2023-01-07 09:12:55,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,650 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -955.7739868164062
2023-01-07 09:12:55,651 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04358099400997162
2023-01-07 09:12:55,652 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -955.8671264648438 param sum :: -110126.7578125
2023-01-07 09:12:55,652 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 4145.32763671875
2023-01-07 09:12:55,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,653 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1596.0203857421875
2023-01-07 09:12:55,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0928975343704224
2023-01-07 09:12:55,654 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -7.28875893400982e-05 param sum :: 740.32421875
2023-01-07 09:12:55,654 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 4145.32763671875
2023-01-07 09:12:55,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,654 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1595.600341796875
2023-01-07 09:12:55,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22334611415863037
2023-01-07 09:12:55,656 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -1595.7103271484375 param sum :: 6936.57958984375
2023-01-07 09:12:55,656 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -267814.9375
2023-01-07 09:12:55,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,657 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 375.1737060546875
2023-01-07 09:12:55,657 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6522184610366821
2023-01-07 09:12:55,658 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 64.35569763183594 param sum :: 2942.51708984375
2023-01-07 09:12:55,658 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -267814.9375
2023-01-07 09:12:55,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,658 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 372.91796875
2023-01-07 09:12:55,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.048095464706421
2023-01-07 09:12:55,660 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 373.313232421875 param sum :: -318763.75
2023-01-07 09:12:55,660 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -56533.7734375
2023-01-07 09:12:55,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,660 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 54.65371322631836
2023-01-07 09:12:55,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.232281684875488
2023-01-07 09:12:55,662 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 64.8293228149414 param sum :: 2946.271728515625
2023-01-07 09:12:55,662 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -56533.7734375
2023-01-07 09:12:55,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,662 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 55.350242614746094
2023-01-07 09:12:55,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12226994335651398
2023-01-07 09:12:55,664 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 54.94420623779297 param sum :: -67654.390625
2023-01-07 09:12:55,664 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,664 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,664 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3843.963623046875
2023-01-07 09:12:55,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,664 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7332.6953125
2023-01-07 09:12:55,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30633848905563354
2023-01-07 09:12:55,666 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 5.817355155944824 param sum :: 741.0269165039062
2023-01-07 09:12:55,666 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3843.963623046875
2023-01-07 09:12:55,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7333.28466796875
2023-01-07 09:12:55,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06273005902767181
2023-01-07 09:12:55,668 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -7332.47216796875 param sum :: 7084.216796875
2023-01-07 09:12:55,668 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -73451.9765625
2023-01-07 09:12:55,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2815.5576171875
2023-01-07 09:12:55,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07837620377540588
2023-01-07 09:12:55,670 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 5.604129314422607 param sum :: 801.282958984375
2023-01-07 09:12:55,670 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -73451.9765625
2023-01-07 09:12:55,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2815.0791015625
2023-01-07 09:12:55,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0799601078033447
2023-01-07 09:12:55,672 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 2814.84619140625 param sum :: -87389.9375
2023-01-07 09:12:55,672 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222785.046875
2023-01-07 09:12:55,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,672 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 47.307247161865234
2023-01-07 09:12:55,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2473592907190323
2023-01-07 09:12:55,673 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 73.436767578125 param sum :: 3022.5888671875
2023-01-07 09:12:55,673 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222785.046875
2023-01-07 09:12:55,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 47.60074234008789
2023-01-07 09:12:55,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12542074918746948
2023-01-07 09:12:55,676 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 47.665016174316406 param sum :: -266008.8125
2023-01-07 09:12:55,676 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -26678.986328125
2023-01-07 09:12:55,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4147.93017578125
2023-01-07 09:12:55,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00176813337020576
2023-01-07 09:12:55,677 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -1.3736536502838135 param sum :: 810.3399047851562
2023-01-07 09:12:55,677 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -26678.986328125
2023-01-07 09:12:55,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,678 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4147.94580078125
2023-01-07 09:12:55,678 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005637310445308685
2023-01-07 09:12:55,679 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -4147.970703125 param sum :: -31646.26171875
2023-01-07 09:12:55,680 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4495.5888671875
2023-01-07 09:12:55,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,680 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -10950.41015625
2023-01-07 09:12:55,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18425580859184265
2023-01-07 09:12:55,681 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -107.48135375976562 param sum :: 826.8978271484375
2023-01-07 09:12:55,681 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4495.5888671875
2023-01-07 09:12:55,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,682 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -10950.5947265625
2023-01-07 09:12:55,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18666774034500122
2023-01-07 09:12:55,683 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -10950.79296875 param sum :: 8643.541015625
2023-01-07 09:12:55,683 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -329490.46875
2023-01-07 09:12:55,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,684 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 5268.3173828125
2023-01-07 09:12:55,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:55,685 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -570.1431884765625 param sum :: 3230.90673828125
2023-01-07 09:12:55,685 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -329490.46875
2023-01-07 09:12:55,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,686 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 5268.3173828125
2023-01-07 09:12:55,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:12:55,688 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 5268.3173828125 param sum :: -393814.03125
2023-01-07 09:12:55,688 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:12:55,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:12:55,689 > [DEBUG] 0 :: 512.0951538085938
2023-01-07 09:12:55,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,694 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0034179389476776123
2023-01-07 09:12:55,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -613.8399658203125
2023-01-07 09:12:55,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,697 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 149.50289916992188
2023-01-07 09:12:55,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,697 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -380.97216796875
2023-01-07 09:12:55,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -400.4799499511719
2023-01-07 09:12:55,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,702 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 39.924468994140625
2023-01-07 09:12:55,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0013107890263199806
2023-01-07 09:12:55,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,704 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.7038980722427368
2023-01-07 09:12:55,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,704 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -103.2222900390625
2023-01-07 09:12:55,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5847012400627136
2023-01-07 09:12:55,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,706 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -10.725237846374512
2023-01-07 09:12:55,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0002602349268272519
2023-01-07 09:12:55,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,707 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.8114553689956665
2023-01-07 09:12:55,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,708 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -10.716837882995605
2023-01-07 09:12:55,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8130811452865601
2023-01-07 09:12:55,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,710 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.3709776699542999
2023-01-07 09:12:55,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005189803428947926
2023-01-07 09:12:55,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,711 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 129.58897399902344
2023-01-07 09:12:55,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,711 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.14795982837677
2023-01-07 09:12:55,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 130.0386962890625
2023-01-07 09:12:55,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,713 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -74.70458984375
2023-01-07 09:12:55,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.010982304811477661
2023-01-07 09:12:55,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,714 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.4586237072944641
2023-01-07 09:12:55,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,715 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -28.056304931640625
2023-01-07 09:12:55,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0071696043014526
2023-01-07 09:12:55,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,717 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2404.4990234375
2023-01-07 09:12:55,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32113489508628845
2023-01-07 09:12:55,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,718 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 1.4416184425354004
2023-01-07 09:12:55,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,718 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2498.52294921875
2023-01-07 09:12:55,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1761643886566162
2023-01-07 09:12:55,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,720 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -991.282958984375
2023-01-07 09:12:55,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03113306127488613
2023-01-07 09:12:55,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,721 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 92.88497161865234
2023-01-07 09:12:55,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,722 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -946.9085083007812
2023-01-07 09:12:55,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 93.32379150390625
2023-01-07 09:12:55,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,724 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -82.18165588378906
2023-01-07 09:12:55,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0051474981009960175
2023-01-07 09:12:55,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,725 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 112.90414428710938
2023-01-07 09:12:55,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,725 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 14.001502990722656
2023-01-07 09:12:55,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 113.62845611572266
2023-01-07 09:12:55,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,727 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -411.50274658203125
2023-01-07 09:12:55,727 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0686454325914383
2023-01-07 09:12:55,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,728 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 1.2507126331329346
2023-01-07 09:12:55,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,729 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -328.36053466796875
2023-01-07 09:12:55,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.00909423828125
2023-01-07 09:12:55,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,731 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 898.513427734375
2023-01-07 09:12:55,731 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0013488046824932098
2023-01-07 09:12:55,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,732 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.5906433463096619
2023-01-07 09:12:55,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,732 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 897.978271484375
2023-01-07 09:12:55,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9324324131011963
2023-01-07 09:12:55,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,734 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.121142864227295
2023-01-07 09:12:55,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11933289468288422
2023-01-07 09:12:55,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 2.2085766792297363
2023-01-07 09:12:55,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,736 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -6.204700946807861
2023-01-07 09:12:55,736 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5685608386993408
2023-01-07 09:12:55,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.9108164310455322
2023-01-07 09:12:55,738 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.016565978527069092
2023-01-07 09:12:55,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 5.6864728927612305
2023-01-07 09:12:55,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.132612705230713
2023-01-07 09:12:55,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.740350723266602
2023-01-07 09:12:55,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -27.686614990234375
2023-01-07 09:12:55,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.008372358977794647
2023-01-07 09:12:55,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -3.0002381801605225
2023-01-07 09:12:55,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -19.922306060791016
2023-01-07 09:12:55,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1414430141448975
2023-01-07 09:12:55,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 28.442495346069336
2023-01-07 09:12:55,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26326072216033936
2023-01-07 09:12:55,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -3.3646492958068848
2023-01-07 09:12:55,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 33.12601089477539
2023-01-07 09:12:55,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.661180257797241
2023-01-07 09:12:55,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -36.814476013183594
2023-01-07 09:12:55,749 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.007223622873425484
2023-01-07 09:12:55,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.18939626216888428
2023-01-07 09:12:55,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -38.619239807128906
2023-01-07 09:12:55,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26283907890319824
2023-01-07 09:12:55,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 66.13768005371094
2023-01-07 09:12:55,752 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.023702282458543777
2023-01-07 09:12:55,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.15968704223632812
2023-01-07 09:12:55,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,754 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 69.2581558227539
2023-01-07 09:12:55,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16255739331245422
2023-01-07 09:12:55,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 84.33969116210938
2023-01-07 09:12:55,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.017897842451930046
2023-01-07 09:12:55,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,757 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.7879027128219604
2023-01-07 09:12:55,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,757 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 83.81147766113281
2023-01-07 09:12:55,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7448970079421997
2023-01-07 09:12:55,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,759 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 115.42483520507812
2023-01-07 09:12:55,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004120718687772751
2023-01-07 09:12:55,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.5743067860603333
2023-01-07 09:12:55,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 116.83992004394531
2023-01-07 09:12:55,761 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8638750910758972
2023-01-07 09:12:55,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,762 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -28.066600799560547
2023-01-07 09:12:55,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.041168034076690674
2023-01-07 09:12:55,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 1.947379469871521
2023-01-07 09:12:55,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 170.99911499023438
2023-01-07 09:12:55,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.108157157897949
2023-01-07 09:12:55,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,766 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 48.24357604980469
2023-01-07 09:12:55,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08109356462955475
2023-01-07 09:12:55,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -4.528435230255127
2023-01-07 09:12:55,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 248.97994995117188
2023-01-07 09:12:55,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.666559219360352
2023-01-07 09:12:55,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,769 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1011.431884765625
2023-01-07 09:12:55,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05730493366718292
2023-01-07 09:12:55,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -1.0617623329162598
2023-01-07 09:12:55,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,771 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -835.642822265625
2023-01-07 09:12:55,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9876899123191833
2023-01-07 09:12:55,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 11.810396194458008
2023-01-07 09:12:55,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005696769803762436
2023-01-07 09:12:55,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,774 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.16812539100646973
2023-01-07 09:12:55,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,774 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 8.442619323730469
2023-01-07 09:12:55,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2021780014038086
2023-01-07 09:12:55,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,776 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 160.86978149414062
2023-01-07 09:12:55,776 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.028500236570835114
2023-01-07 09:12:55,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,777 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 10.303415298461914
2023-01-07 09:12:55,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,778 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 157.80783081054688
2023-01-07 09:12:55,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.357730865478516
2023-01-07 09:12:55,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,780 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1118.4527587890625
2023-01-07 09:12:55,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02306395210325718
2023-01-07 09:12:55,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,781 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -4.6417460441589355
2023-01-07 09:12:55,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,781 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1108.1025390625
2023-01-07 09:12:55,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.635915756225586
2023-01-07 09:12:55,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,783 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 4.728725433349609
2023-01-07 09:12:55,784 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030114732682704926
2023-01-07 09:12:55,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -2.2283034324645996
2023-01-07 09:12:55,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9482538104057312
2023-01-07 09:12:55,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,786 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.057492256164551
2023-01-07 09:12:55,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2437719851732254
2023-01-07 09:12:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,787 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -8.052099227905273
2023-01-07 09:12:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -11.843612670898438
2023-01-07 09:12:55,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.324803352355957
2023-01-07 09:12:55,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,790 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 9.533795356750488
2023-01-07 09:12:55,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.001072760671377182
2023-01-07 09:12:55,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -1.056497573852539
2023-01-07 09:12:55,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,791 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.4275741577148438
2023-01-07 09:12:55,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0414366722106934
2023-01-07 09:12:55,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,793 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 130.09915161132812
2023-01-07 09:12:55,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1614820808172226
2023-01-07 09:12:55,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 4.315373420715332
2023-01-07 09:12:55,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 128.68069458007812
2023-01-07 09:12:55,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.741178512573242
2023-01-07 09:12:55,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,796 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -190.27784729003906
2023-01-07 09:12:55,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.053964875638484955
2023-01-07 09:12:55,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -188.1295623779297
2023-01-07 09:12:55,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04374830424785614
2023-01-07 09:12:55,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,799 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 148.7210693359375
2023-01-07 09:12:55,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24763141572475433
2023-01-07 09:12:55,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,801 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 192.53956604003906
2023-01-07 09:12:55,801 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12218677997589111
2023-01-07 09:12:55,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,802 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -2.8189167976379395
2023-01-07 09:12:55,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30390793085098267
2023-01-07 09:12:55,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,803 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 46.68451690673828
2023-01-07 09:12:55,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25381988286972046
2023-01-07 09:12:55,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,805 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 19.246925354003906
2023-01-07 09:12:55,805 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0018624477088451385
2023-01-07 09:12:55,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,806 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 32.915924072265625
2023-01-07 09:12:55,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08641043305397034
2023-01-07 09:12:55,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,807 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -13.472685813903809
2023-01-07 09:12:55,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012998312711715698
2023-01-07 09:12:55,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,809 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -120.3095932006836
2023-01-07 09:12:55,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42964887619018555
2023-01-07 09:12:55,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,810 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -121.55210876464844
2023-01-07 09:12:55,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7730698585510254
2023-01-07 09:12:55,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,811 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -77.55825805664062
2023-01-07 09:12:55,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8822799921035767
2023-01-07 09:12:55,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,813 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 97.78269958496094
2023-01-07 09:12:55,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.302623987197876
2023-01-07 09:12:55,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,814 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -21.460540771484375
2023-01-07 09:12:55,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1043100655078888
2023-01-07 09:12:55,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,816 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -13.166581153869629
2023-01-07 09:12:55,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7067513465881348
2023-01-07 09:12:55,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,817 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 12.26420783996582
2023-01-07 09:12:55,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,817 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -169.5719757080078
2023-01-07 09:12:55,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.809179306030273
2023-01-07 09:12:55,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,819 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 107.50956726074219
2023-01-07 09:12:55,819 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.324356555938721
2023-01-07 09:12:55,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,821 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 149.22618103027344
2023-01-07 09:12:55,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.675363540649414
2023-01-07 09:12:55,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,822 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -33.31002426147461
2023-01-07 09:12:55,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7214701175689697
2023-01-07 09:12:55,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,823 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -125.32850646972656
2023-01-07 09:12:55,823 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5054330825805664
2023-01-07 09:12:55,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,825 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -53.871971130371094
2023-01-07 09:12:55,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.020631790161133
2023-01-07 09:12:55,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,826 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -14.13093090057373
2023-01-07 09:12:55,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,826 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -363.7919921875
2023-01-07 09:12:55,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.907167434692383
2023-01-07 09:12:55,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 162.43057250976562
2023-01-07 09:12:55,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7998356819152832
2023-01-07 09:12:55,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,829 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 169.92376708984375
2023-01-07 09:12:55,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.969989061355591
2023-01-07 09:12:55,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,831 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 229.8794708251953
2023-01-07 09:12:55,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.653365135192871
2023-01-07 09:12:55,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,832 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 121.47803497314453
2023-01-07 09:12:55,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.6056389808654785
2023-01-07 09:12:55,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,834 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -348.20587158203125
2023-01-07 09:12:55,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.3454389572143555
2023-01-07 09:12:55,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,835 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -12.939634323120117
2023-01-07 09:12:55,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,835 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -455.3147277832031
2023-01-07 09:12:55,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.50704193115234
2023-01-07 09:12:55,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,837 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1208.8995361328125
2023-01-07 09:12:55,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9263088703155518
2023-01-07 09:12:55,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,838 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 4.8921051025390625
2023-01-07 09:12:55,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,839 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 269.1146240234375
2023-01-07 09:12:55,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,839 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1737.76953125
2023-01-07 09:12:55,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 167.90896606445312
2023-01-07 09:12:55,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,841 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1596.228515625
2023-01-07 09:12:55,841 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.222323417663574
2023-01-07 09:12:55,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -90.96014404296875
2023-01-07 09:12:55,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.573579788208008
2023-01-07 09:12:55,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,843 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 13.962414741516113
2023-01-07 09:12:55,844 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.35204315185547
2023-01-07 09:12:55,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,845 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -2.3669962882995605
2023-01-07 09:12:55,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,845 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -141.40040588378906
2023-01-07 09:12:55,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.8998918533325195
2023-01-07 09:12:55,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,847 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1063.445556640625
2023-01-07 09:12:55,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.919360160827637
2023-01-07 09:12:55,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,848 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -11.457868576049805
2023-01-07 09:12:55,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,848 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 62.3565673828125
2023-01-07 09:12:55,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.623658180236816
2023-01-07 09:12:55,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,850 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1626.66015625
2023-01-07 09:12:55,850 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.775967597961426
2023-01-07 09:12:55,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,851 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -2.3962478637695312
2023-01-07 09:12:55,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,852 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1715.43212890625
2023-01-07 09:12:55,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.94145584106445
2023-01-07 09:12:55,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,853 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1836.0992431640625
2023-01-07 09:12:55,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9347925186157227
2023-01-07 09:12:55,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,855 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 226.587646484375
2023-01-07 09:12:55,855 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.05194091796875
2023-01-07 09:12:55,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,856 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2056.4169921875
2023-01-07 09:12:55,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.37702178955078
2023-01-07 09:12:55,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,858 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 21.878761291503906
2023-01-07 09:12:55,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,858 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 155.04483032226562
2023-01-07 09:12:55,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,858 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2522.467041015625
2023-01-07 09:12:55,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,859 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1166.3590087890625
2023-01-07 09:12:55,859 > [DEBUG] 0 :: before allreduce fusion buffer :: 83.90904235839844
2023-01-07 09:12:55,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,860 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2152.1220703125
2023-01-07 09:12:55,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.751014709472656
2023-01-07 09:12:55,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,862 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 177.58941650390625
2023-01-07 09:12:55,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.24089813232422
2023-01-07 09:12:55,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,863 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2174.37353515625
2023-01-07 09:12:55,863 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.894919395446777
2023-01-07 09:12:55,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,864 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 285.88299560546875
2023-01-07 09:12:55,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,865 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1019.521484375
2023-01-07 09:12:55,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 113.5721435546875
2023-01-07 09:12:55,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,867 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -146.47434997558594
2023-01-07 09:12:55,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.58216094970703
2023-01-07 09:12:55,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,868 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 17.645530700683594
2023-01-07 09:12:55,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,868 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -228.86172485351562
2023-01-07 09:12:55,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -151.30313110351562
2023-01-07 09:12:55,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,870 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -845.0426025390625
2023-01-07 09:12:55,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.528337478637695
2023-01-07 09:12:55,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:12:55,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:12:55,871 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -845.0426025390625
2023-01-07 09:12:55,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.79780578613281
