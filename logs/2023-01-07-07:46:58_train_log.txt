2023-01-07 07:47:05,125 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:47:05,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,164 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.9398233890533447
2023-01-07 07:47:05,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,165 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:47:05,165 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,165 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,165 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 07:47:05,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,886 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,887 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,887 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,887 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 07:47:05,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,889 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -2.3299436569213867
2023-01-07 07:47:05,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,889 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:47:05,889 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,889 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 07:47:05,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,890 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,890 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,890 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,890 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,890 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 07:47:05,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,891 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -3.5802595615386963
2023-01-07 07:47:05,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,892 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:47:05,892 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,892 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 07:47:05,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,927 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,927 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,927 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,927 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,927 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 07:47:05,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,929 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.297992706298828
2023-01-07 07:47:05,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,929 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,929 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,929 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,929 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 07:47:05,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,930 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,931 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,931 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,931 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,931 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 07:47:05,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,931 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.131089448928833
2023-01-07 07:47:05,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,932 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,932 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,932 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 07:47:05,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,933 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,933 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,933 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,933 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,933 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 07:47:05,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,934 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 15.103982925415039
2023-01-07 07:47:05,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,934 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,934 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,934 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,935 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 07:47:05,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,935 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,936 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,936 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,936 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,936 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 07:47:05,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,937 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7413601875305176
2023-01-07 07:47:05,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,937 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:47:05,937 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,937 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,937 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 07:47:05,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,938 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,938 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,938 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,938 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,938 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 07:47:05,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,939 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 9.13563346862793
2023-01-07 07:47:05,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,940 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,940 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,940 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,940 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 07:47:05,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,941 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,941 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,941 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,941 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,941 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 07:47:05,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,942 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 8.413798332214355
2023-01-07 07:47:05,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,942 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,942 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,942 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,942 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 07:47:05,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,943 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,943 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,943 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,944 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,944 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 07:47:05,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,944 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 6.427009582519531
2023-01-07 07:47:05,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,945 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:47:05,945 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,945 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,945 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 07:47:05,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,946 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:05,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,946 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:47:05,946 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,946 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,946 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 07:47:05,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,947 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 18.908489227294922
2023-01-07 07:47:05,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,947 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:47:05,947 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,947 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,948 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 07:47:05,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,948 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,949 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,949 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,949 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,949 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 07:47:05,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,950 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -7.513669013977051
2023-01-07 07:47:05,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,950 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:47:05,950 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,950 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,950 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 07:47:05,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,951 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,951 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,951 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,951 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,951 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 07:47:05,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,952 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.761693000793457
2023-01-07 07:47:05,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,953 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:47:05,953 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,953 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,953 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 07:47:05,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,954 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,954 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,954 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,954 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 07:47:05,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,955 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 18.67743492126465
2023-01-07 07:47:05,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,955 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,956 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,956 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 07:47:05,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,956 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:05,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,957 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:05,957 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,957 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 07:47:05,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,958 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 2.4820995330810547
2023-01-07 07:47:05,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,958 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:47:05,958 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,958 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,958 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 07:47:05,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,959 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:05,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,960 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:05,960 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,960 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,960 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 07:47:05,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,961 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.42095947265625
2023-01-07 07:47:05,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,961 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,961 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,961 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,961 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 07:47:05,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,962 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,962 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,962 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,962 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 07:47:05,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,963 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.0223565101623535
2023-01-07 07:47:05,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,964 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:47:05,964 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,964 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,964 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 07:47:05,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,965 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,965 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,965 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,965 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 07:47:05,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,966 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: 12.97024154663086
2023-01-07 07:47:05,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,966 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,966 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,966 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,966 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 07:47:05,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,967 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:05,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,967 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:05,968 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,968 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,968 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 07:47:05,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,969 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -4.600156784057617
2023-01-07 07:47:05,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,969 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,969 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,969 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,969 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 07:47:05,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,970 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,970 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,970 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,970 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 07:47:05,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,971 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 4.026489734649658
2023-01-07 07:47:05,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,971 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:47:05,972 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,972 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,972 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 07:47:05,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,972 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,973 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,973 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,973 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 07:47:05,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,974 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -20.595718383789062
2023-01-07 07:47:05,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,974 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,974 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,974 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,974 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 07:47:05,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,975 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:05,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,975 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:05,976 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,976 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 07:47:05,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,976 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 21.338003158569336
2023-01-07 07:47:05,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,977 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,977 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,977 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,977 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 07:47:05,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,978 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,978 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,978 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,978 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 07:47:05,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,979 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 1.1036479473114014
2023-01-07 07:47:05,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,979 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:47:05,980 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,980 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,980 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 07:47:05,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,980 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:05,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,981 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:47:05,981 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,981 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,981 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 07:47:05,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,982 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -5.814108848571777
2023-01-07 07:47:05,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,982 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:47:05,982 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,982 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,982 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 07:47:05,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,983 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:05,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,983 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:05,983 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,983 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,983 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 07:47:05,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,984 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -59.17209243774414
2023-01-07 07:47:05,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,985 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:47:05,985 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,985 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,985 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 07:47:05,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,986 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,986 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,986 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,986 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,986 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 07:47:05,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,987 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -20.98597526550293
2023-01-07 07:47:05,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,987 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:05,987 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,987 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,987 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 07:47:05,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,988 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,989 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,989 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,989 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,989 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 07:47:05,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,990 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 34.32874298095703
2023-01-07 07:47:05,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,990 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:05,990 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,990 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,990 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 07:47:05,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,991 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:05,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,991 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:05,991 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,992 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,992 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 07:47:05,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,992 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -4.484246253967285
2023-01-07 07:47:05,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,993 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:47:05,993 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,993 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,993 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 07:47:05,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,994 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:05,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,994 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:05,994 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,994 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,994 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 07:47:05,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,995 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -67.99465942382812
2023-01-07 07:47:05,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,995 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:05,995 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,995 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,995 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 07:47:05,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,996 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,997 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,997 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,997 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,997 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 07:47:05,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,998 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 39.38418197631836
2023-01-07 07:47:05,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,998 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:05,998 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,998 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,998 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 07:47:05,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,999 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:05,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:05,999 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:05,999 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:05,999 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:05,999 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 07:47:06,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,000 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 40.379638671875
2023-01-07 07:47:06,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,001 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,001 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,001 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,001 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 07:47:06,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,001 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:06,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,002 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:06,002 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,002 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,002 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 07:47:06,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,003 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 27.753822326660156
2023-01-07 07:47:06,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,003 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,003 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,003 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,003 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 07:47:06,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,004 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,004 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,004 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,005 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,005 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 07:47:06,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,005 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.584294319152832
2023-01-07 07:47:06,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,006 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:06,006 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,006 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,006 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 07:47:06,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,007 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,007 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,007 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,007 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,007 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 07:47:06,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,008 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 24.87024688720703
2023-01-07 07:47:06,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,008 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,008 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,008 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,008 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 07:47:06,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,009 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:06,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,009 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:06,009 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,010 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,010 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 07:47:06,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,010 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -50.23155975341797
2023-01-07 07:47:06,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,011 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,011 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,011 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,011 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 07:47:06,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,012 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,012 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,012 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,012 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,012 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 07:47:06,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,013 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -13.924392700195312
2023-01-07 07:47:06,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,013 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:06,014 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,014 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,014 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 07:47:06,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,014 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,015 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,015 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,015 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,015 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 07:47:06,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,016 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -13.077762603759766
2023-01-07 07:47:06,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,016 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,016 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,016 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,016 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 07:47:06,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,017 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:06,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,017 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:06,017 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,017 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,017 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 07:47:06,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,018 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8933010101318359
2023-01-07 07:47:06,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,018 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,019 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,019 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,019 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 07:47:06,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,019 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,020 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,020 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,020 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,020 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 07:47:06,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,021 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 11.21274185180664
2023-01-07 07:47:06,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,021 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:06,021 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,021 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,021 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 07:47:06,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,022 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,022 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,022 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,022 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,022 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 07:47:06,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,023 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -15.147509574890137
2023-01-07 07:47:06,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,024 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,024 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,024 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 07:47:06,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,025 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:06,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,025 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:06,025 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,025 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,025 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 07:47:06,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,026 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -31.26508140563965
2023-01-07 07:47:06,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,026 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,026 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,026 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,026 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 07:47:06,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,027 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,027 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,027 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,027 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,028 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 07:47:06,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,028 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 19.3409423828125
2023-01-07 07:47:06,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,029 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:47:06,029 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,029 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 07:47:06,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,030 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,030 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:47:06,030 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,030 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,030 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 07:47:06,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,031 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 24.903684616088867
2023-01-07 07:47:06,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,031 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:47:06,031 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,031 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,031 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 07:47:06,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,032 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:47:06,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,032 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:47:06,032 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,032 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 07:47:06,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,033 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 61.16167449951172
2023-01-07 07:47:06,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,034 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:47:06,034 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,034 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,034 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 07:47:06,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,035 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,035 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,035 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,035 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,035 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 07:47:06,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,036 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -54.096534729003906
2023-01-07 07:47:06,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,036 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:47:06,036 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,036 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,037 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 07:47:06,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,037 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,038 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,038 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,038 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,038 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 07:47:06,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,039 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 6.148664474487305
2023-01-07 07:47:06,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,039 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:47:06,039 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,039 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,039 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 07:47:06,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,040 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:47:06,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,040 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:47:06,040 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,040 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,040 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 07:47:06,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,041 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -36.63011932373047
2023-01-07 07:47:06,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,041 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:47:06,041 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,041 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,042 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 07:47:06,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,042 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:47:06,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,043 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:47:06,043 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,043 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,043 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 07:47:06,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,044 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -23.45296859741211
2023-01-07 07:47:06,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,044 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:47:06,044 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,044 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,044 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 07:47:06,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,045 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,045 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,045 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,045 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,045 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 07:47:06,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,046 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: 0.5939793586730957
2023-01-07 07:47:06,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,047 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:47:06,047 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,047 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,047 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 07:47:06,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,048 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,048 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,048 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,048 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,048 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 07:47:06,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,049 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 12.994152069091797
2023-01-07 07:47:06,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,049 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:47:06,049 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,049 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,049 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 07:47:06,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,050 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:47:06,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,051 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:47:06,051 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,051 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,051 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 07:47:06,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,052 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -39.005638122558594
2023-01-07 07:47:06,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,052 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:47:06,052 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,052 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,052 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 07:47:06,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,053 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,053 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,053 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,053 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,053 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 07:47:06,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,054 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -63.155609130859375
2023-01-07 07:47:06,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,054 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:47:06,055 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,055 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,055 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 07:47:06,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,055 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,056 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:47:06,056 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,056 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,056 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 07:47:06,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,057 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -44.23688507080078
2023-01-07 07:47:06,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,057 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:47:06,057 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,057 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,057 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 07:47:06,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,058 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:47:06,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,058 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:47:06,058 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,058 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,058 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 07:47:06,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,059 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 18.121429443359375
2023-01-07 07:47:06,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:06,060 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:47:06,060 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,060 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:47:06,060 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 07:47:06,061 > [DEBUG] 0 :: 7.262264728546143
2023-01-07 07:47:06,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,065 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -370.9739990234375
2023-01-07 07:47:06,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,068 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -356.148681640625
2023-01-07 07:47:06,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1894996166229248
2023-01-07 07:47:06,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,081 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.72035813331604
2023-01-07 07:47:06,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,083 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3170497417449951
2023-01-07 07:47:06,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,084 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3493168354034424
2023-01-07 07:47:06,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,086 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2836328148841858
2023-01-07 07:47:06,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,087 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.664024829864502
2023-01-07 07:47:06,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,088 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005772780627012253
2023-01-07 07:47:06,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,089 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9103248119354248
2023-01-07 07:47:06,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19540664553642273
2023-01-07 07:47:06,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,092 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.060415491461753845
2023-01-07 07:47:06,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,093 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20416828989982605
2023-01-07 07:47:06,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,094 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.052764892578125
2023-01-07 07:47:06,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,096 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4871540367603302
2023-01-07 07:47:06,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,097 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0645337104797363
2023-01-07 07:47:06,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,098 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43552082777023315
2023-01-07 07:47:06,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,099 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8788637518882751
2023-01-07 07:47:06,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05967046320438385
2023-01-07 07:47:06,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,102 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4834163188934326
2023-01-07 07:47:06,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,104 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4466019868850708
2023-01-07 07:47:06,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,105 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47288280725479126
2023-01-07 07:47:06,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,106 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03539576008915901
2023-01-07 07:47:06,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,107 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,107 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03510722517967224
2023-01-07 07:47:06,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10031883418560028
2023-01-07 07:47:06,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,110 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3287193775177002
2023-01-07 07:47:06,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,112 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24469612538814545
2023-01-07 07:47:06,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,113 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03832968324422836
2023-01-07 07:47:06,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,114 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7501071691513062
2023-01-07 07:47:06,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,115 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.429581642150879
2023-01-07 07:47:06,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,116 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10283467173576355
2023-01-07 07:47:06,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,117 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.934739589691162
2023-01-07 07:47:06,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6939783096313477
2023-01-07 07:47:06,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3113434314727783
2023-01-07 07:47:06,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.882439374923706
2023-01-07 07:47:06,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,122 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7046529054641724
2023-01-07 07:47:06,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,123 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4501469135284424
2023-01-07 07:47:06,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,124 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9915217161178589
2023-01-07 07:47:06,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,125 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6648683547973633
2023-01-07 07:47:06,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,126 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1257591247558594
2023-01-07 07:47:06,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0107431411743164
2023-01-07 07:47:06,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4852380752563477
2023-01-07 07:47:06,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,130 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.461503505706787
2023-01-07 07:47:06,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,131 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4208139479160309
2023-01-07 07:47:06,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.390690565109253
2023-01-07 07:47:06,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,133 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.708342432975769
2023-01-07 07:47:06,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,135 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1841108798980713
2023-01-07 07:47:06,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,135 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0193679332733154
2023-01-07 07:47:06,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,137 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9660683870315552
2023-01-07 07:47:06,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17141318321228027
2023-01-07 07:47:06,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,139 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4822074770927429
2023-01-07 07:47:06,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,140 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.7992706298828125
2023-01-07 07:47:06,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,141 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3813246488571167
2023-01-07 07:47:06,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,142 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.359971523284912
2023-01-07 07:47:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,144 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3343303203582764
2023-01-07 07:47:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,144 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5477206707000732
2023-01-07 07:47:06,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,146 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.58559513092041
2023-01-07 07:47:06,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.87242317199707
2023-01-07 07:47:06,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,148 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.066359519958496
2023-01-07 07:47:06,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.809529781341553
2023-01-07 07:47:06,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,150 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2894325256347656
2023-01-07 07:47:06,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.077955722808838
2023-01-07 07:47:06,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,152 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,152 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6463091373443604
2023-01-07 07:47:06,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.361297607421875
2023-01-07 07:47:06,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,154 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,154 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6627181768417358
2023-01-07 07:47:06,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8017876148223877
2023-01-07 07:47:06,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,156 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.4903130531311035
2023-01-07 07:47:06,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30359601974487305
2023-01-07 07:47:06,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,158 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.023334503173828
2023-01-07 07:47:06,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.620677947998047
2023-01-07 07:47:06,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,160 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3425629138946533
2023-01-07 07:47:06,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,161 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.030672073364258
2023-01-07 07:47:06,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,162 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5390913486480713
2023-01-07 07:47:06,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2670361995697021
2023-01-07 07:47:06,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,164 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.166784286499023
2023-01-07 07:47:06,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.75699234008789
2023-01-07 07:47:06,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,166 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,166 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.609509468078613
2023-01-07 07:47:06,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,166 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.17477035522461
2023-01-07 07:47:06,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,168 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,168 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1643732190132141
2023-01-07 07:47:06,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,169 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.955501556396484
2023-01-07 07:47:06,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,170 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5531090497970581
2023-01-07 07:47:06,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8291382789611816
2023-01-07 07:47:06,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,172 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.605870246887207
2023-01-07 07:47:06,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,173 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.1407470703125
2023-01-07 07:47:06,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.496758460998535
2023-01-07 07:47:06,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,175 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,175 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.733951568603516
2023-01-07 07:47:06,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.088626861572266
2023-01-07 07:47:06,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.881230354309082
2023-01-07 07:47:06,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,180 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,180 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.51346206665039
2023-01-07 07:47:06,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,180 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.036861419677734
2023-01-07 07:47:06,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.560184478759766
2023-01-07 07:47:06,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,183 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,183 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,183 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.785780429840088
2023-01-07 07:47:06,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,184 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.465368270874023
2023-01-07 07:47:06,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,185 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.238147735595703
2023-01-07 07:47:06,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.766502380371094
2023-01-07 07:47:06,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.902057647705078
2023-01-07 07:47:06,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.590259552001953
2023-01-07 07:47:06,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,190 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,190 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.47010612487793
2023-01-07 07:47:06,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.79074478149414
2023-01-07 07:47:06,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,192 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.197319030761719
2023-01-07 07:47:06,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.923095703125
2023-01-07 07:47:06,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.15236282348633
2023-01-07 07:47:06,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,195 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.755024909973145
2023-01-07 07:47:06,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,196 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 140.01458740234375
2023-01-07 07:47:06,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.850181579589844
2023-01-07 07:47:06,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,199 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.55316925048828
2023-01-07 07:47:06,204 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:47:06,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,205 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 1464.4029541015625
2023-01-07 07:47:06,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 64.90043640136719
2023-01-07 07:47:06,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.39973258972168
2023-01-07 07:47:06,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.846199035644531
2023-01-07 07:47:06,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 726.4443359375
2023-01-07 07:47:06,218 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.9398233890533447
2023-01-07 07:47:06,218 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,218 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,218 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,218 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:47:06,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,218 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -337.8275451660156
2023-01-07 07:47:06,219 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:06,219 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,219 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -2.3299436569213867
2023-01-07 07:47:06,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,220 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.724727630615234
2023-01-07 07:47:06,221 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -2.3299436569213867
2023-01-07 07:47:06,221 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,221 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -3.5802595615386963
2023-01-07 07:47:06,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.258148193359375
2023-01-07 07:47:06,222 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 63.600006103515625
2023-01-07 07:47:06,222 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,222 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -3.5802595615386963
2023-01-07 07:47:06,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,222 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.993213653564453
2023-01-07 07:47:06,223 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -3.5802595615386963
2023-01-07 07:47:06,223 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,224 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:47:06,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,224 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,224 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,224 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.173377990722656
2023-01-07 07:47:06,225 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:06,225 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,225 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.427009582519531
2023-01-07 07:47:06,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.611909866333008
2023-01-07 07:47:06,226 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.297992706298828
2023-01-07 07:47:06,226 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,227 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:47:06,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,227 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.884098052978516
2023-01-07 07:47:06,228 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,228 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,228 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7413601875305176
2023-01-07 07:47:06,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.80485534667969
2023-01-07 07:47:06,229 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 15.126008987426758
2023-01-07 07:47:06,229 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,229 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7413601875305176
2023-01-07 07:47:06,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.625598907470703
2023-01-07 07:47:06,230 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 260.79998779296875
2023-01-07 07:47:06,230 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,230 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7413601875305176
2023-01-07 07:47:06,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.966552734375
2023-01-07 07:47:06,231 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 15.103982925415039
2023-01-07 07:47:06,232 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,232 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,232 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:47:06,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,232 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -144.9993133544922
2023-01-07 07:47:06,233 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:06,233 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,233 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7413601875305176
2023-01-07 07:47:06,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,233 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.552640914916992
2023-01-07 07:47:06,234 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7413601875305176
2023-01-07 07:47:06,234 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,235 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:47:06,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,235 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.435890197753906
2023-01-07 07:47:06,236 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:06,236 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,236 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,236 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,236 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.427009582519531
2023-01-07 07:47:06,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.542823791503906
2023-01-07 07:47:06,237 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 9.13563346862793
2023-01-07 07:47:06,237 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,237 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 8.413798332214355
2023-01-07 07:47:06,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.68655776977539
2023-01-07 07:47:06,238 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 264.20001220703125
2023-01-07 07:47:06,238 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,238 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 8.413798332214355
2023-01-07 07:47:06,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -121.71983337402344
2023-01-07 07:47:06,240 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 8.413798332214355
2023-01-07 07:47:06,240 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,240 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.427009582519531
2023-01-07 07:47:06,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.22927474975586
2023-01-07 07:47:06,241 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 66.19999694824219
2023-01-07 07:47:06,241 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,241 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.427009582519531
2023-01-07 07:47:06,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 102.90882873535156
2023-01-07 07:47:06,243 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 6.427009582519531
2023-01-07 07:47:06,243 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,243 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:47:06,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,243 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,243 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.52151489257812
2023-01-07 07:47:06,244 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:47:06,244 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,245 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.761693000793457
2023-01-07 07:47:06,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,245 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.456298828125
2023-01-07 07:47:06,246 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 18.908489227294922
2023-01-07 07:47:06,246 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,246 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -7.513669013977051
2023-01-07 07:47:06,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.162960052490234
2023-01-07 07:47:06,247 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 258.79931640625
2023-01-07 07:47:06,247 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,247 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.761693000793457
2023-01-07 07:47:06,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.12330627441406
2023-01-07 07:47:06,248 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -7.513669013977051
2023-01-07 07:47:06,248 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,248 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.761693000793457
2023-01-07 07:47:06,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.183916091918945
2023-01-07 07:47:06,249 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 130.59999084472656
2023-01-07 07:47:06,249 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,249 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.761693000793457
2023-01-07 07:47:06,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.022560119628906
2023-01-07 07:47:06,251 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.761693000793457
2023-01-07 07:47:06,251 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,251 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,251 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.300125122070312
2023-01-07 07:47:06,252 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,252 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,252 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 18.67743492126465
2023-01-07 07:47:06,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.000125408172607
2023-01-07 07:47:06,254 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 18.67743492126465
2023-01-07 07:47:06,254 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,254 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:47:06,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,254 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.987714767456055
2023-01-07 07:47:06,255 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,255 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,255 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 2.4820995330810547
2023-01-07 07:47:06,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,256 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.449375629425049
2023-01-07 07:47:06,256 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 2.4820995330810547
2023-01-07 07:47:06,256 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,256 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.42095947265625
2023-01-07 07:47:06,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.197437286376953
2023-01-07 07:47:06,257 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 507.39984130859375
2023-01-07 07:47:06,258 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,258 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.42095947265625
2023-01-07 07:47:06,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8118081092834473
2023-01-07 07:47:06,259 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.42095947265625
2023-01-07 07:47:06,259 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,259 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,259 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.42263793945312
2023-01-07 07:47:06,261 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,261 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,261 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.0223565101623535
2023-01-07 07:47:06,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.84671020507812
2023-01-07 07:47:06,262 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.0223565101623535
2023-01-07 07:47:06,262 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,262 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,263 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.37771224975586
2023-01-07 07:47:06,264 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,264 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,264 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 12.97024154663086
2023-01-07 07:47:06,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.3105974197387695
2023-01-07 07:47:06,265 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: 12.97024154663086
2023-01-07 07:47:06,265 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,265 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -4.600156784057617
2023-01-07 07:47:06,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.931123733520508
2023-01-07 07:47:06,266 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 538.800048828125
2023-01-07 07:47:06,266 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,266 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -4.600156784057617
2023-01-07 07:47:06,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.67563533782959
2023-01-07 07:47:06,268 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -4.600156784057617
2023-01-07 07:47:06,268 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,268 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,268 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.019621849060059
2023-01-07 07:47:06,269 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,269 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,269 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 4.026489734649658
2023-01-07 07:47:06,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.188278198242188
2023-01-07 07:47:06,271 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 4.026489734649658
2023-01-07 07:47:06,271 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,271 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,271 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8777899742126465
2023-01-07 07:47:06,272 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,272 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,272 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -20.595718383789062
2023-01-07 07:47:06,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,273 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.783010959625244
2023-01-07 07:47:06,274 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -20.595718383789062
2023-01-07 07:47:06,274 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,274 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:47:06,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,274 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.700681686401367
2023-01-07 07:47:06,275 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,275 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,275 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 21.338003158569336
2023-01-07 07:47:06,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.074342966079712
2023-01-07 07:47:06,277 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 21.338003158569336
2023-01-07 07:47:06,277 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,277 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,277 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.004684448242188
2023-01-07 07:47:06,278 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,278 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,278 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1.1036479473114014
2023-01-07 07:47:06,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.884299278259277
2023-01-07 07:47:06,280 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 1.1036479473114014
2023-01-07 07:47:06,280 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,280 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,280 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:47:06,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,280 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.351583480834961
2023-01-07 07:47:06,281 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:47:06,281 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,281 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -5.814108848571777
2023-01-07 07:47:06,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8353952169418335
2023-01-07 07:47:06,282 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -5.814108848571777
2023-01-07 07:47:06,282 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,283 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:47:06,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,283 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.8660125732421875
2023-01-07 07:47:06,284 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:47:06,284 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,284 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -59.17209243774414
2023-01-07 07:47:06,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3362011909484863
2023-01-07 07:47:06,286 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -59.17209243774414
2023-01-07 07:47:06,286 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,286 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:47:06,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,286 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.856685161590576
2023-01-07 07:47:06,287 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,287 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,287 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -20.98597526550293
2023-01-07 07:47:06,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.550457000732422
2023-01-07 07:47:06,289 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -20.98597526550293
2023-01-07 07:47:06,289 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,289 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.32874298095703
2023-01-07 07:47:06,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.961752414703369
2023-01-07 07:47:06,290 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 290.0001220703125
2023-01-07 07:47:06,290 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,290 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 34.32874298095703
2023-01-07 07:47:06,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0814404487609863
2023-01-07 07:47:06,292 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 34.32874298095703
2023-01-07 07:47:06,292 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,292 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -4.484246253967285
2023-01-07 07:47:06,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1826385259628296
2023-01-07 07:47:06,293 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1065.799560546875
2023-01-07 07:47:06,293 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,293 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -4.484246253967285
2023-01-07 07:47:06,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.547330379486084
2023-01-07 07:47:06,294 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -4.484246253967285
2023-01-07 07:47:06,294 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,294 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -67.99465942382812
2023-01-07 07:47:06,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14911949634552002
2023-01-07 07:47:06,295 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1091.39404296875
2023-01-07 07:47:06,295 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,295 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,295 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -67.99465942382812
2023-01-07 07:47:06,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.397432565689087
2023-01-07 07:47:06,297 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -67.99465942382812
2023-01-07 07:47:06,297 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,297 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:47:06,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,297 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:47:06,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6456802487373352
2023-01-07 07:47:06,298 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:47:06,298 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,298 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 39.38418197631836
2023-01-07 07:47:06,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9091811180114746
2023-01-07 07:47:06,300 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 39.38418197631836
2023-01-07 07:47:06,300 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,300 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 40.379638671875
2023-01-07 07:47:06,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6301393508911133
2023-01-07 07:47:06,301 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 297.1998291015625
2023-01-07 07:47:06,301 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,301 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 40.379638671875
2023-01-07 07:47:06,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1458420753479004
2023-01-07 07:47:06,302 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 40.379638671875
2023-01-07 07:47:06,302 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,303 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,303 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.753822326660156
2023-01-07 07:47:06,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0492949485778809
2023-01-07 07:47:06,303 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1103.7967529296875
2023-01-07 07:47:06,304 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,304 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 27.753822326660156
2023-01-07 07:47:06,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.239919185638428
2023-01-07 07:47:06,305 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 27.753822326660156
2023-01-07 07:47:06,305 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,305 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.584294319152832
2023-01-07 07:47:06,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4070091247558594
2023-01-07 07:47:06,306 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 297.3997802734375
2023-01-07 07:47:06,306 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,306 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.584294319152832
2023-01-07 07:47:06,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1818056106567383
2023-01-07 07:47:06,308 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.584294319152832
2023-01-07 07:47:06,308 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,308 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.87024688720703
2023-01-07 07:47:06,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7245640754699707
2023-01-07 07:47:06,309 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 298.3999328613281
2023-01-07 07:47:06,309 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,309 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 24.87024688720703
2023-01-07 07:47:06,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.159576654434204
2023-01-07 07:47:06,310 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 24.87024688720703
2023-01-07 07:47:06,310 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,311 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -50.23155975341797
2023-01-07 07:47:06,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2028567790985107
2023-01-07 07:47:06,311 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1104.200439453125
2023-01-07 07:47:06,312 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,312 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -50.23155975341797
2023-01-07 07:47:06,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9594568014144897
2023-01-07 07:47:06,313 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -50.23155975341797
2023-01-07 07:47:06,313 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,313 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -13.924392700195312
2023-01-07 07:47:06,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4255541563034058
2023-01-07 07:47:06,314 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 300.79962158203125
2023-01-07 07:47:06,314 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,314 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -13.924392700195312
2023-01-07 07:47:06,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.089602470397949
2023-01-07 07:47:06,316 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -13.924392700195312
2023-01-07 07:47:06,316 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,316 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.077762603759766
2023-01-07 07:47:06,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1548164337873459
2023-01-07 07:47:06,317 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 300.1998291015625
2023-01-07 07:47:06,317 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,317 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,317 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.077762603759766
2023-01-07 07:47:06,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5699584484100342
2023-01-07 07:47:06,319 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -13.077762603759766
2023-01-07 07:47:06,319 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,319 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 0.8933010101318359
2023-01-07 07:47:06,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7479301691055298
2023-01-07 07:47:06,320 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1105.403076171875
2023-01-07 07:47:06,320 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,320 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 0.8933010101318359
2023-01-07 07:47:06,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.025856271386146545
2023-01-07 07:47:06,321 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8933010101318359
2023-01-07 07:47:06,321 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,322 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.21274185180664
2023-01-07 07:47:06,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.05354905128479
2023-01-07 07:47:06,322 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 301.1998291015625
2023-01-07 07:47:06,323 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,323 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.21274185180664
2023-01-07 07:47:06,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27429836988449097
2023-01-07 07:47:06,324 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 11.21274185180664
2023-01-07 07:47:06,324 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,324 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -15.147509574890137
2023-01-07 07:47:06,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.270406723022461
2023-01-07 07:47:06,325 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 302.6004333496094
2023-01-07 07:47:06,325 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,325 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,325 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,325 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -15.147509574890137
2023-01-07 07:47:06,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1128296852111816
2023-01-07 07:47:06,327 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -15.147509574890137
2023-01-07 07:47:06,327 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,327 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -31.26508140563965
2023-01-07 07:47:06,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0737247467041016
2023-01-07 07:47:06,328 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1113.597900390625
2023-01-07 07:47:06,328 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,328 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,328 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,328 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -31.26508140563965
2023-01-07 07:47:06,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7748339772224426
2023-01-07 07:47:06,329 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -31.26508140563965
2023-01-07 07:47:06,330 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,330 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,330 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,330 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 19.3409423828125
2023-01-07 07:47:06,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9591168165206909
2023-01-07 07:47:06,331 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 303.39971923828125
2023-01-07 07:47:06,331 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,331 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 19.3409423828125
2023-01-07 07:47:06,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.152557373046875
2023-01-07 07:47:06,332 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 19.3409423828125
2023-01-07 07:47:06,332 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,332 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,332 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 24.903684616088867
2023-01-07 07:47:06,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.48118218779563904
2023-01-07 07:47:06,333 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 304.59954833984375
2023-01-07 07:47:06,333 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,333 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 24.903684616088867
2023-01-07 07:47:06,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5953354835510254
2023-01-07 07:47:06,335 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 24.903684616088867
2023-01-07 07:47:06,335 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,335 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 61.16167449951172
2023-01-07 07:47:06,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5795866250991821
2023-01-07 07:47:06,336 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1107.596923828125
2023-01-07 07:47:06,336 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,336 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 61.16167449951172
2023-01-07 07:47:06,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6760966777801514
2023-01-07 07:47:06,337 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 61.16167449951172
2023-01-07 07:47:06,337 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,337 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,337 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -54.096534729003906
2023-01-07 07:47:06,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22781088948249817
2023-01-07 07:47:06,339 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 597.7988891601562
2023-01-07 07:47:06,339 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,339 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,339 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -54.096534729003906
2023-01-07 07:47:06,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8741790652275085
2023-01-07 07:47:06,340 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -54.096534729003906
2023-01-07 07:47:06,340 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,340 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,341 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 6.148664474487305
2023-01-07 07:47:06,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27262353897094727
2023-01-07 07:47:06,341 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 509.1944580078125
2023-01-07 07:47:06,342 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,342 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 6.148664474487305
2023-01-07 07:47:06,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6506583094596863
2023-01-07 07:47:06,343 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 6.148664474487305
2023-01-07 07:47:06,343 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,343 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -36.63011932373047
2023-01-07 07:47:06,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.66801518201828
2023-01-07 07:47:06,344 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2115.39990234375
2023-01-07 07:47:06,344 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,344 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -36.63011932373047
2023-01-07 07:47:06,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2046474814414978
2023-01-07 07:47:06,346 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -36.63011932373047
2023-01-07 07:47:06,346 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,346 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -23.45296859741211
2023-01-07 07:47:06,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24986544251441956
2023-01-07 07:47:06,347 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2104.19091796875
2023-01-07 07:47:06,347 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,347 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -23.45296859741211
2023-01-07 07:47:06,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.314113974571228
2023-01-07 07:47:06,348 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -23.45296859741211
2023-01-07 07:47:06,348 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,348 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,348 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 0.5939793586730957
2023-01-07 07:47:06,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5116797089576721
2023-01-07 07:47:06,349 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 599.6024169921875
2023-01-07 07:47:06,349 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,349 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 0.5939793586730957
2023-01-07 07:47:06,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05758032202720642
2023-01-07 07:47:06,351 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: 0.5939793586730957
2023-01-07 07:47:06,351 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,351 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 12.994152069091797
2023-01-07 07:47:06,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1027907282114029
2023-01-07 07:47:06,352 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 509.0047912597656
2023-01-07 07:47:06,352 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,352 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 12.994152069091797
2023-01-07 07:47:06,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42269793152809143
2023-01-07 07:47:06,354 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 12.994152069091797
2023-01-07 07:47:06,354 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,354 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -39.005638122558594
2023-01-07 07:47:06,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6182878017425537
2023-01-07 07:47:06,355 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2112.19091796875
2023-01-07 07:47:06,355 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,355 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -39.005638122558594
2023-01-07 07:47:06,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.022683054208755493
2023-01-07 07:47:06,356 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -39.005638122558594
2023-01-07 07:47:06,356 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,356 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -63.155609130859375
2023-01-07 07:47:06,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11078571528196335
2023-01-07 07:47:06,357 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 601.998291015625
2023-01-07 07:47:06,357 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,358 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -63.155609130859375
2023-01-07 07:47:06,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06677818298339844
2023-01-07 07:47:06,359 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -63.155609130859375
2023-01-07 07:47:06,359 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,359 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -44.23688507080078
2023-01-07 07:47:06,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24813978374004364
2023-01-07 07:47:06,360 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 601.598388671875
2023-01-07 07:47:06,360 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,360 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -44.23688507080078
2023-01-07 07:47:06,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44811511039733887
2023-01-07 07:47:06,362 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -44.23688507080078
2023-01-07 07:47:06,362 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,362 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 18.121429443359375
2023-01-07 07:47:06,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.70048713684082
2023-01-07 07:47:06,363 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2450.191650390625
2023-01-07 07:47:06,363 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,363 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 18.121429443359375
2023-01-07 07:47:06,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1381579637527466
2023-01-07 07:47:06,365 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 18.121429443359375
2023-01-07 07:47:06,365 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:06,365 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:06,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:06,366 > [DEBUG] 0 :: 7.3740692138671875
2023-01-07 07:47:06,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,368 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -594.3759765625
2023-01-07 07:47:06,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,370 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -608.1936645507812
2023-01-07 07:47:06,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,374 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.527584075927734
2023-01-07 07:47:06,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,376 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -262.2187194824219
2023-01-07 07:47:06,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,379 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.58598709106445
2023-01-07 07:47:06,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,380 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6406678557395935
2023-01-07 07:47:06,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,382 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.48691940307617
2023-01-07 07:47:06,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,382 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.41098403930664
2023-01-07 07:47:06,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,384 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.77606201171875
2023-01-07 07:47:06,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,385 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.8872175216674805
2023-01-07 07:47:06,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,387 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,387 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0771031379699707
2023-01-07 07:47:06,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,388 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4905325174331665
2023-01-07 07:47:06,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,389 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.007564067840576
2023-01-07 07:47:06,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,390 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.839839935302734
2023-01-07 07:47:06,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,392 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15800967812538147
2023-01-07 07:47:06,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,393 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9802303314208984
2023-01-07 07:47:06,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,394 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.31610870361328
2023-01-07 07:47:06,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,395 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.753284454345703
2023-01-07 07:47:06,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,397 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45873111486434937
2023-01-07 07:47:06,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,398 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7127838134765625
2023-01-07 07:47:06,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,399 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.1920166015625
2023-01-07 07:47:06,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,400 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.580784320831299
2023-01-07 07:47:06,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,410 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.735965728759766
2023-01-07 07:47:06,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,411 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.632828712463379
2023-01-07 07:47:06,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,413 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.81932830810547
2023-01-07 07:47:06,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,414 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.457731246948242
2023-01-07 07:47:06,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,416 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.99329376220703
2023-01-07 07:47:06,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,417 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.90181565284729
2023-01-07 07:47:06,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,418 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,418 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.333709716796875
2023-01-07 07:47:06,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,419 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.758227348327637
2023-01-07 07:47:06,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,595 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.646690368652344
2023-01-07 07:47:06,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,596 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.035308837890625
2023-01-07 07:47:06,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,777 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,777 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.63896942138672
2023-01-07 07:47:06,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,778 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:06,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:06,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:06,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3641295433044434
2023-01-07 07:47:07,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,077 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.74298095703125
2023-01-07 07:47:07,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,078 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.827266693115234
2023-01-07 07:47:07,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,080 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,080 > [DEBUG] 0 :: before allreduce fusion buffer :: -433.94256591796875
2023-01-07 07:47:07,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,080 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.059837341308594
2023-01-07 07:47:07,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,082 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.75952911376953
2023-01-07 07:47:07,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,083 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -1797.514404296875
2023-01-07 07:47:07,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,084 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.6517448425293
2023-01-07 07:47:07,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,085 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,086 > [DEBUG] 0 :: before allreduce fusion buffer :: -3953.626708984375
2023-01-07 07:47:07,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,087 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.04399108886719
2023-01-07 07:47:07,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,088 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,088 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.821226119995117
2023-01-07 07:47:07,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,089 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.95506286621094
2023-01-07 07:47:07,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,090 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7155768871307373
2023-01-07 07:47:07,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,092 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,092 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.679832458496094
2023-01-07 07:47:07,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,092 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.154074668884277
2023-01-07 07:47:07,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,094 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,094 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.054744720458984
2023-01-07 07:47:07,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,095 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.37349319458008
2023-01-07 07:47:07,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,096 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.35883331298828
2023-01-07 07:47:07,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,097 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -103887.390625
2023-01-07 07:47:07,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,099 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -243911.625
2023-01-07 07:47:07,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,100 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.3776750564575195
2023-01-07 07:47:07,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,101 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.742816925048828
2023-01-07 07:47:07,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,102 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -3566.02001953125
2023-01-07 07:47:07,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,103 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.66291046142578
2023-01-07 07:47:07,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5737031698226929
2023-01-07 07:47:07,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,105 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -8333602.5
2023-01-07 07:47:07,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -16825756.0
2023-01-07 07:47:07,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,107 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7896649837493896
2023-01-07 07:47:07,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.126779079437256
2023-01-07 07:47:07,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,109 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.675597906112671
2023-01-07 07:47:07,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.94624328613281
2023-01-07 07:47:07,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,111 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.404417037963867
2023-01-07 07:47:07,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,112 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2146663665771484
2023-01-07 07:47:07,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,113 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.259622812271118
2023-01-07 07:47:07,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32386302947998047
2023-01-07 07:47:07,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,115 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.65095520019531
2023-01-07 07:47:07,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.744544982910156
2023-01-07 07:47:07,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,117 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 415.08941650390625
2023-01-07 07:47:07,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,117 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,118 > [DEBUG] 0 :: before allreduce fusion buffer :: -428.24871826171875
2023-01-07 07:47:07,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,119 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 157.47215270996094
2023-01-07 07:47:07,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -180.04432678222656
2023-01-07 07:47:07,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,121 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -985.595703125
2023-01-07 07:47:07,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -3858.150146484375
2023-01-07 07:47:07,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,123 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -2266.133056640625
2023-01-07 07:47:07,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,124 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -6824.37646484375
2023-01-07 07:47:07,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,125 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -481.62005615234375
2023-01-07 07:47:07,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -7655.39794921875
2023-01-07 07:47:07,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,127 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -1585.0528564453125
2023-01-07 07:47:07,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 3684.4716796875
2023-01-07 07:47:07,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,129 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -12619.55859375
2023-01-07 07:47:07,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,130 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -43825.66015625
2023-01-07 07:47:07,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 3076.823974609375
2023-01-07 07:47:07,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,132 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,132 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 3719249.5
2023-01-07 07:47:07,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 9565.318359375
2023-01-07 07:47:07,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -25740.69140625
2023-01-07 07:47:07,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,136 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -2561945.0
2023-01-07 07:47:07,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,137 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 110644.3359375
2023-01-07 07:47:07,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -35003.96484375
2023-01-07 07:47:07,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,139 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,139 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -37166.6875
2023-01-07 07:47:07,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -49709.4609375
2023-01-07 07:47:07,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,141 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,141 > [DEBUG] 0 :: before allreduce fusion buffer :: -66693.46875
2023-01-07 07:47:07,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 48164.09765625
2023-01-07 07:47:07,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,143 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -549547.5625
2023-01-07 07:47:07,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -31120.177734375
2023-01-07 07:47:07,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,145 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,145 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 270812.625
2023-01-07 07:47:07,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 21723.671875
2023-01-07 07:47:07,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,148 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 74687936.0
2023-01-07 07:47:07,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -13170.4169921875
2023-01-07 07:47:07,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -307970.25
2023-01-07 07:47:07,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,151 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -440432.5
2023-01-07 07:47:07,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,152 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,152 > [DEBUG] 0 :: before allreduce fusion buffer :: -2114643.0
2023-01-07 07:47:07,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -1488108.25
2023-01-07 07:47:07,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 952378.25
2023-01-07 07:47:07,156 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:47:07,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,157 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 3311384576.0
2023-01-07 07:47:07,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 843945148416.0
2023-01-07 07:47:07,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 855914315776.0
2023-01-07 07:47:07,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 1703322583040.0
2023-01-07 07:47:07,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 1704214790144.0
2023-01-07 07:47:07,174 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -1.7289466857910156
2023-01-07 07:47:07,174 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,174 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.19999694824219
2023-01-07 07:47:07,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,175 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -174392377344.0
2023-01-07 07:47:07,176 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.19999694824219
2023-01-07 07:47:07,176 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,176 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 13.269834518432617
2023-01-07 07:47:07,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,176 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 242478.25
2023-01-07 07:47:07,177 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 13.269834518432617
2023-01-07 07:47:07,177 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,178 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 33.19242858886719
2023-01-07 07:47:07,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -1686196.0
2023-01-07 07:47:07,178 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 63.600006103515625
2023-01-07 07:47:07,179 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,179 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 33.19242858886719
2023-01-07 07:47:07,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 1302999.75
2023-01-07 07:47:07,180 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 33.19242858886719
2023-01-07 07:47:07,180 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,180 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 65.59811401367188
2023-01-07 07:47:07,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,180 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,180 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -636891.625
2023-01-07 07:47:07,182 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 65.59811401367188
2023-01-07 07:47:07,182 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,182 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 226.6214599609375
2023-01-07 07:47:07,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 69815.5078125
2023-01-07 07:47:07,183 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -25.008296966552734
2023-01-07 07:47:07,183 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,183 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 300.399658203125
2023-01-07 07:47:07,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,183 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 931524316758016.0
2023-01-07 07:47:07,184 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 300.399658203125
2023-01-07 07:47:07,185 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,185 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,185 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 80.49341583251953
2023-01-07 07:47:07,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 179090.203125
2023-01-07 07:47:07,186 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 15.126008987426758
2023-01-07 07:47:07,186 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,186 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 80.49341583251953
2023-01-07 07:47:07,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,186 > [DEBUG] 0 :: before allreduce fusion buffer :: -215323.875
2023-01-07 07:47:07,187 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 260.79998779296875
2023-01-07 07:47:07,187 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,187 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,187 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,187 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 80.49341583251953
2023-01-07 07:47:07,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 51211.17578125
2023-01-07 07:47:07,188 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 39.91474914550781
2023-01-07 07:47:07,188 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,188 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 74.19988250732422
2023-01-07 07:47:07,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,188 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,188 > [DEBUG] 0 :: before allreduce fusion buffer :: -1123116.75
2023-01-07 07:47:07,189 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 74.19988250732422
2023-01-07 07:47:07,189 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,189 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 80.49341583251953
2023-01-07 07:47:07,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -48872.9140625
2023-01-07 07:47:07,191 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: 80.49341583251953
2023-01-07 07:47:07,191 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,191 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.4000015258789
2023-01-07 07:47:07,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,191 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,191 > [DEBUG] 0 :: before allreduce fusion buffer :: -661954.1875
2023-01-07 07:47:07,192 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.4000015258789
2023-01-07 07:47:07,192 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,192 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,193 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 226.6214599609375
2023-01-07 07:47:07,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 37512.00390625
2023-01-07 07:47:07,193 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 54.540340423583984
2023-01-07 07:47:07,194 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,194 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 58.02334213256836
2023-01-07 07:47:07,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 67802.3984375
2023-01-07 07:47:07,195 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 264.20001220703125
2023-01-07 07:47:07,195 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,195 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 58.02334213256836
2023-01-07 07:47:07,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -86706.609375
2023-01-07 07:47:07,196 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 58.02334213256836
2023-01-07 07:47:07,196 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,196 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 226.6214599609375
2023-01-07 07:47:07,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -1056.8466796875
2023-01-07 07:47:07,197 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 66.19999694824219
2023-01-07 07:47:07,197 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,197 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,197 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 226.6214599609375
2023-01-07 07:47:07,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 33791.76171875
2023-01-07 07:47:07,198 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 226.6214599609375
2023-01-07 07:47:07,199 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,199 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.198974609375
2023-01-07 07:47:07,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,199 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,199 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,199 > [DEBUG] 0 :: before allreduce fusion buffer :: -1571416.5
2023-01-07 07:47:07,200 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.198974609375
2023-01-07 07:47:07,200 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,200 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,200 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 179.79649353027344
2023-01-07 07:47:07,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,201 > [DEBUG] 0 :: before allreduce fusion buffer :: -2672.496826171875
2023-01-07 07:47:07,201 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.575857162475586
2023-01-07 07:47:07,202 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,202 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 321.68731689453125
2023-01-07 07:47:07,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -862.135986328125
2023-01-07 07:47:07,203 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 258.79931640625
2023-01-07 07:47:07,203 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,203 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,203 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 179.79649353027344
2023-01-07 07:47:07,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -7858.6240234375
2023-01-07 07:47:07,204 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 321.68731689453125
2023-01-07 07:47:07,204 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,204 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 179.79649353027344
2023-01-07 07:47:07,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,204 > [DEBUG] 0 :: before allreduce fusion buffer :: -9057.2890625
2023-01-07 07:47:07,205 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 130.59999084472656
2023-01-07 07:47:07,205 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,205 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 179.79649353027344
2023-01-07 07:47:07,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 17481.99609375
2023-01-07 07:47:07,206 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 179.79649353027344
2023-01-07 07:47:07,206 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,206 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,207 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,207 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 127.80003356933594
2023-01-07 07:47:07,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,207 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 8969.4169921875
2023-01-07 07:47:07,208 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 127.80003356933594
2023-01-07 07:47:07,208 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,208 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 217.57794189453125
2023-01-07 07:47:07,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -3357.95654296875
2023-01-07 07:47:07,209 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 217.57794189453125
2023-01-07 07:47:07,209 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,209 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 571.5991821289062
2023-01-07 07:47:07,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,210 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 4127.376953125
2023-01-07 07:47:07,211 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 571.5991821289062
2023-01-07 07:47:07,211 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,211 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.35414695739746
2023-01-07 07:47:07,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -1700.137939453125
2023-01-07 07:47:07,212 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.35414695739746
2023-01-07 07:47:07,212 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,212 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -92.26412200927734
2023-01-07 07:47:07,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -2782.218017578125
2023-01-07 07:47:07,213 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 503.056884765625
2023-01-07 07:47:07,213 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,213 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,213 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -92.26412200927734
2023-01-07 07:47:07,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -2702.773193359375
2023-01-07 07:47:07,214 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -92.26412200927734
2023-01-07 07:47:07,214 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,214 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 141.00018310546875
2023-01-07 07:47:07,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,215 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 110.43707275390625
2023-01-07 07:47:07,216 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 141.00018310546875
2023-01-07 07:47:07,216 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,216 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,216 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,216 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -252.68258666992188
2023-01-07 07:47:07,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,216 > [DEBUG] 0 :: before allreduce fusion buffer :: -612.9453125
2023-01-07 07:47:07,217 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -252.68258666992188
2023-01-07 07:47:07,217 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,217 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 141.60000610351562
2023-01-07 07:47:07,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,218 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -356.703125
2023-01-07 07:47:07,219 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 141.60000610351562
2023-01-07 07:47:07,219 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,219 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -35.41257095336914
2023-01-07 07:47:07,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 170.153076171875
2023-01-07 07:47:07,220 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -35.41257095336914
2023-01-07 07:47:07,220 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,220 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.719728469848633
2023-01-07 07:47:07,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 409.4075622558594
2023-01-07 07:47:07,221 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 542.0469970703125
2023-01-07 07:47:07,221 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,221 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -9.719728469848633
2023-01-07 07:47:07,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 666.7957763671875
2023-01-07 07:47:07,223 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -9.719728469848633
2023-01-07 07:47:07,223 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,223 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 141.59999084472656
2023-01-07 07:47:07,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,223 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,223 > [DEBUG] 0 :: before allreduce fusion buffer :: -86.33692932128906
2023-01-07 07:47:07,224 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 141.59999084472656
2023-01-07 07:47:07,224 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,224 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 353.333984375
2023-01-07 07:47:07,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 96.15165710449219
2023-01-07 07:47:07,226 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 353.333984375
2023-01-07 07:47:07,226 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,226 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,226 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,226 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 127.00021362304688
2023-01-07 07:47:07,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,226 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 84.85809326171875
2023-01-07 07:47:07,227 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.00021362304688
2023-01-07 07:47:07,227 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,227 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 71.05392456054688
2023-01-07 07:47:07,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.93907356262207
2023-01-07 07:47:07,228 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 71.05392456054688
2023-01-07 07:47:07,228 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,229 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 548.59814453125
2023-01-07 07:47:07,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,229 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.09815216064453
2023-01-07 07:47:07,230 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 548.59814453125
2023-01-07 07:47:07,230 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,230 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 188.9721221923828
2023-01-07 07:47:07,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,230 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0706219673156738
2023-01-07 07:47:07,231 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 188.9721221923828
2023-01-07 07:47:07,231 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,231 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,232 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 140.3999481201172
2023-01-07 07:47:07,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,232 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 78.71501922607422
2023-01-07 07:47:07,233 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 140.3999481201172
2023-01-07 07:47:07,233 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,233 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 122.78369140625
2023-01-07 07:47:07,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3205041885375977
2023-01-07 07:47:07,234 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 122.78369140625
2023-01-07 07:47:07,234 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,234 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 141.60000610351562
2023-01-07 07:47:07,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,235 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3287968635559082
2023-01-07 07:47:07,236 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 141.60000610351562
2023-01-07 07:47:07,236 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,236 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,236 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,236 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 74.98457336425781
2023-01-07 07:47:07,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6213037967681885
2023-01-07 07:47:07,237 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 74.98457336425781
2023-01-07 07:47:07,237 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,237 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,238 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 546.5999755859375
2023-01-07 07:47:07,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,238 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,239 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 546.5999755859375
2023-01-07 07:47:07,239 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,239 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -98.14380645751953
2023-01-07 07:47:07,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,240 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -98.14380645751953
2023-01-07 07:47:07,240 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,240 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 284.0
2023-01-07 07:47:07,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,242 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 284.0
2023-01-07 07:47:07,242 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,242 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -446.4768371582031
2023-01-07 07:47:07,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,243 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -446.4768371582031
2023-01-07 07:47:07,243 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,243 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -84.23905944824219
2023-01-07 07:47:07,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,244 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 292.18438720703125
2023-01-07 07:47:07,244 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,244 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -84.23905944824219
2023-01-07 07:47:07,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,246 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -84.23905944824219
2023-01-07 07:47:07,246 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,246 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -734.6387939453125
2023-01-07 07:47:07,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,247 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1056.72119140625
2023-01-07 07:47:07,247 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,247 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -734.6387939453125
2023-01-07 07:47:07,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,248 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -734.6387939453125
2023-01-07 07:47:07,248 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,248 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -994.499755859375
2023-01-07 07:47:07,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,249 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1080.2333984375
2023-01-07 07:47:07,249 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,249 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -994.499755859375
2023-01-07 07:47:07,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,250 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -994.499755859375
2023-01-07 07:47:07,251 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,251 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 242.8000030517578
2023-01-07 07:47:07,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,251 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:47:07,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,252 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 242.8000030517578
2023-01-07 07:47:07,252 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,252 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -1949.774658203125
2023-01-07 07:47:07,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,253 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1949.774658203125
2023-01-07 07:47:07,253 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,253 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -122.90110778808594
2023-01-07 07:47:07,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,254 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 296.0269775390625
2023-01-07 07:47:07,255 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,255 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -122.90110778808594
2023-01-07 07:47:07,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,256 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: -122.90110778808594
2023-01-07 07:47:07,256 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,256 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 155.5643310546875
2023-01-07 07:47:07,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,257 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1094.71826171875
2023-01-07 07:47:07,257 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,257 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 155.5643310546875
2023-01-07 07:47:07,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,258 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 155.5643310546875
2023-01-07 07:47:07,258 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,258 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 113.13695526123047
2023-01-07 07:47:07,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,259 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 297.2509460449219
2023-01-07 07:47:07,259 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,260 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 113.13695526123047
2023-01-07 07:47:07,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,261 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 113.13695526123047
2023-01-07 07:47:07,261 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,261 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 170.36550903320312
2023-01-07 07:47:07,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,262 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 298.2511291503906
2023-01-07 07:47:07,262 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,262 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 170.36550903320312
2023-01-07 07:47:07,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,263 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 170.36550903320312
2023-01-07 07:47:07,263 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,263 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -18.7242431640625
2023-01-07 07:47:07,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,264 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1095.1220703125
2023-01-07 07:47:07,264 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,264 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -18.7242431640625
2023-01-07 07:47:07,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,266 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -18.7242431640625
2023-01-07 07:47:07,266 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,266 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.15602874755859375
2023-01-07 07:47:07,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,267 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 300.65081787109375
2023-01-07 07:47:07,267 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,267 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.15602874755859375
2023-01-07 07:47:07,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,268 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.15602874755859375
2023-01-07 07:47:07,268 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,268 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.79953384399414
2023-01-07 07:47:07,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,269 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 303.774658203125
2023-01-07 07:47:07,269 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,269 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.79953384399414
2023-01-07 07:47:07,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,271 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 8.79953384399414
2023-01-07 07:47:07,271 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,271 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -260.39508056640625
2023-01-07 07:47:07,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,272 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1098.5609130859375
2023-01-07 07:47:07,272 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,272 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -260.39508056640625
2023-01-07 07:47:07,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,273 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -260.39508056640625
2023-01-07 07:47:07,273 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,273 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,273 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 510.61187744140625
2023-01-07 07:47:07,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,274 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 307.89874267578125
2023-01-07 07:47:07,274 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,274 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 510.61187744140625
2023-01-07 07:47:07,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,276 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 510.61187744140625
2023-01-07 07:47:07,276 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,276 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -2930.26416015625
2023-01-07 07:47:07,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,277 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 300.5191650390625
2023-01-07 07:47:07,277 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,277 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -2930.26416015625
2023-01-07 07:47:07,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,278 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -2930.26416015625
2023-01-07 07:47:07,278 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,278 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -20788.62890625
2023-01-07 07:47:07,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,279 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1104.51953125
2023-01-07 07:47:07,279 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,279 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -20788.62890625
2023-01-07 07:47:07,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,281 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -20788.62890625
2023-01-07 07:47:07,281 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,281 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -9086.5234375
2023-01-07 07:47:07,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,282 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 335.1602478027344
2023-01-07 07:47:07,282 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,282 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,282 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,282 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -9086.5234375
2023-01-07 07:47:07,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,283 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -9086.5234375
2023-01-07 07:47:07,283 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,283 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23649.861328125
2023-01-07 07:47:07,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,284 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 304.45074462890625
2023-01-07 07:47:07,284 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,284 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23649.861328125
2023-01-07 07:47:07,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,286 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23649.861328125
2023-01-07 07:47:07,286 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,286 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -50202.5390625
2023-01-07 07:47:07,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,287 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1098.5185546875
2023-01-07 07:47:07,287 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,287 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -50202.5390625
2023-01-07 07:47:07,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,288 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -50202.5390625
2023-01-07 07:47:07,288 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,288 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -42835.53125
2023-01-07 07:47:07,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,289 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 580.0884399414062
2023-01-07 07:47:07,289 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,289 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -42835.53125
2023-01-07 07:47:07,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,291 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -42835.53125
2023-01-07 07:47:07,291 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,291 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,291 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -293.8789978027344
2023-01-07 07:47:07,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,292 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 507.3145751953125
2023-01-07 07:47:07,292 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,292 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -293.8789978027344
2023-01-07 07:47:07,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,294 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -293.8789978027344
2023-01-07 07:47:07,294 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,294 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -123207.8671875
2023-01-07 07:47:07,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,295 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2106.02392578125
2023-01-07 07:47:07,295 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,295 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,295 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -123207.8671875
2023-01-07 07:47:07,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,296 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -123207.8671875
2023-01-07 07:47:07,296 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,296 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -26975.9296875
2023-01-07 07:47:07,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,297 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2094.81494140625
2023-01-07 07:47:07,297 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,297 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -26975.9296875
2023-01-07 07:47:07,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,298 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -26975.9296875
2023-01-07 07:47:07,298 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,299 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -65.1595687866211
2023-01-07 07:47:07,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,299 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 581.8919677734375
2023-01-07 07:47:07,300 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,300 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -65.1595687866211
2023-01-07 07:47:07,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,301 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -65.1595687866211
2023-01-07 07:47:07,301 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,301 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -34051.2890625
2023-01-07 07:47:07,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,302 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 491.2943420410156
2023-01-07 07:47:07,302 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,302 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -34051.2890625
2023-01-07 07:47:07,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,303 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -34051.2890625
2023-01-07 07:47:07,304 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,304 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -102858.1953125
2023-01-07 07:47:07,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,305 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2102.665771484375
2023-01-07 07:47:07,305 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,305 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -102858.1953125
2023-01-07 07:47:07,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,306 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -102858.1953125
2023-01-07 07:47:07,306 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,306 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -12831.14453125
2023-01-07 07:47:07,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,307 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 584.2880859375
2023-01-07 07:47:07,307 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,307 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -12831.14453125
2023-01-07 07:47:07,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,308 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -12831.14453125
2023-01-07 07:47:07,309 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,309 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 388.51617431640625
2023-01-07 07:47:07,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,310 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 588.8009033203125
2023-01-07 07:47:07,310 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,310 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,310 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,310 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 388.51617431640625
2023-01-07 07:47:07,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,311 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 388.51617431640625
2023-01-07 07:47:07,311 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,311 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -148857.625
2023-01-07 07:47:07,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,312 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2447.1337890625
2023-01-07 07:47:07,312 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,312 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -148857.625
2023-01-07 07:47:07,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:07,314 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: -148857.625
2023-01-07 07:47:07,314 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:07,314 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:07,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:07,315 > [DEBUG] 0 :: 18.566547393798828
2023-01-07 07:47:07,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -3829516.75
2023-01-07 07:47:07,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 431312.90625
2023-01-07 07:47:07,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 835548.9375
2023-01-07 07:47:07,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -5115130.5
2023-01-07 07:47:07,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,327 > [DEBUG] 0 :: before allreduce fusion buffer :: -2723992.0
2023-01-07 07:47:07,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 2075232.0
2023-01-07 07:47:07,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 1180005.5
2023-01-07 07:47:07,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 2301225.0
2023-01-07 07:47:07,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 26273272.0
2023-01-07 07:47:07,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -85145136.0
2023-01-07 07:47:07,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,334 > [DEBUG] 0 :: before allreduce fusion buffer :: 11841713.0
2023-01-07 07:47:07,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -318219392.0
2023-01-07 07:47:07,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 489311776.0
2023-01-07 07:47:07,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 134111968.0
2023-01-07 07:47:07,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 1931625984.0
2023-01-07 07:47:07,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 986076992.0
2023-01-07 07:47:07,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 6721659392.0
2023-01-07 07:47:07,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -329379.40625
2023-01-07 07:47:07,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 12076064768.0
2023-01-07 07:47:07,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -35110944768.0
2023-01-07 07:47:07,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 50622595072.0
2023-01-07 07:47:07,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -5483628032.0
2023-01-07 07:47:07,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 213934587904.0
2023-01-07 07:47:07,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 401441882112.0
2023-01-07 07:47:07,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 427957092352.0
2023-01-07 07:47:07,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -446135872.0
2023-01-07 07:47:07,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 852107395072.0
2023-01-07 07:47:07,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -1151808256.0
2023-01-07 07:47:07,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 1704214790144.0
2023-01-07 07:47:07,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -2390213632.0
2023-01-07 07:47:07,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 3408429580288.0
2023-01-07 07:47:07,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -1982394112.0
2023-01-07 07:47:07,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 6816859160576.0
2023-01-07 07:47:07,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -6741028864.0
2023-01-07 07:47:07,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 13487024635904.0
2023-01-07 07:47:07,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,363 > [DEBUG] 0 :: before allreduce fusion buffer :: -13702374400.0
2023-01-07 07:47:07,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 26974049271808.0
2023-01-07 07:47:07,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 51380224000000.0
2023-01-07 07:47:07,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 54534873284608.0
2023-01-07 07:47:07,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -4204912384.0
2023-01-07 07:47:07,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 218139509915648.0
2023-01-07 07:47:07,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,370 > [DEBUG] 0 :: before allreduce fusion buffer :: 207869035151360.0
2023-01-07 07:47:07,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 465762158379008.0
2023-01-07 07:47:07,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 415754579083264.0
2023-01-07 07:47:07,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 872558039662592.0
2023-01-07 07:47:07,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 2188761088.0
2023-01-07 07:47:07,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -531858752274432.0
2023-01-07 07:47:07,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -8211008512.0
2023-01-07 07:47:07,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -1113260086525952.0
2023-01-07 07:47:07,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -2160661412970496.0
2023-01-07 07:47:07,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4749617774133248e+16
2023-01-07 07:47:07,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -16505868288.0
2023-01-07 07:47:07,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,405 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.418308690824397e+16
2023-01-07 07:47:07,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 51532372.0
2023-01-07 07:47:07,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.031765874638848e+17
2023-01-07 07:47:07,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -3811931.75
2023-01-07 07:47:07,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 4781.71875
2023-01-07 07:47:07,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -178340.515625
2023-01-07 07:47:07,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 66927.453125
2023-01-07 07:47:07,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 20024.55859375
2023-01-07 07:47:07,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 138538.109375
2023-01-07 07:47:07,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -74404.1484375
2023-01-07 07:47:07,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 244656.8125
2023-01-07 07:47:07,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 119696.890625
2023-01-07 07:47:07,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 1378072.0
2023-01-07 07:47:07,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 818360.0
2023-01-07 07:47:07,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 2128525.75
2023-01-07 07:47:07,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 3728134.25
2023-01-07 07:47:07,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 8616662.0
2023-01-07 07:47:07,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,771 > [DEBUG] 0 :: before allreduce fusion buffer :: 8067140.0
2023-01-07 07:47:07,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 17806298.0
2023-01-07 07:47:07,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -773461.625
2023-01-07 07:47:07,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -2156.05517578125
2023-01-07 07:47:07,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,956 > [DEBUG] 0 :: before allreduce fusion buffer :: -1503656.75
2023-01-07 07:47:07,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:07,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:07,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -929.03125
2023-01-07 07:47:08,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -3003241.0
2023-01-07 07:47:08,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -9066.29296875
2023-01-07 07:47:08,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -2934799.0
2023-01-07 07:47:08,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -1674605601226752.0
2023-01-07 07:47:08,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -2770426.5
2023-01-07 07:47:08,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 6352.3125
2023-01-07 07:47:08,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -5694087.0
2023-01-07 07:47:08,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 13182.1015625
2023-01-07 07:47:08,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -5683002.5
2023-01-07 07:47:08,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 41676.59375
2023-01-07 07:47:08,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 4138.1796875
2023-01-07 07:47:08,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 1905.618408203125
2023-01-07 07:47:08,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 202428.28125
2023-01-07 07:47:08,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.561135292053223
2023-01-07 07:47:08,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 3046478.75
2023-01-07 07:47:08,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.9159650802612305
2023-01-07 07:47:08,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,088 > [DEBUG] 0 :: before allreduce fusion buffer :: -6204299.0
2023-01-07 07:47:08,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.544567108154297
2023-01-07 07:47:08,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.413002967834473
2023-01-07 07:47:08,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3420634269714355
2023-01-07 07:47:08,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.105365753173828
2023-01-07 07:47:08,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.340094566345215
2023-01-07 07:47:08,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 766983552.0
2023-01-07 07:47:08,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.435223579406738
2023-01-07 07:47:08,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.25226593017578
2023-01-07 07:47:08,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.414321899414062
2023-01-07 07:47:08,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -428.61981201171875
2023-01-07 07:47:08,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8947343826293945
2023-01-07 07:47:08,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -174.16476440429688
2023-01-07 07:47:08,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 129.81419372558594
2023-01-07 07:47:08,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 152.78021240234375
2023-01-07 07:47:08,109 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:47:08,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.342826491563213e+16
2023-01-07 07:47:08,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.685652983126426e+16
2023-01-07 07:47:08,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.628947630915584e+16
2023-01-07 07:47:08,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -3634822769541120.0
2023-01-07 07:47:08,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -1211663021768704.0
2023-01-07 07:47:08,122 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 30.061189651489258
2023-01-07 07:47:08,122 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,122 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,122 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 65.69673919677734
2023-01-07 07:47:08,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.07331848144531
2023-01-07 07:47:08,123 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 65.69673919677734
2023-01-07 07:47:08,123 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,123 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 14.847862243652344
2023-01-07 07:47:08,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -4846385262231552.0
2023-01-07 07:47:08,125 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 14.847862243652344
2023-01-07 07:47:08,125 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,125 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 23.865680694580078
2023-01-07 07:47:08,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 86.92832946777344
2023-01-07 07:47:08,126 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.79499816894531
2023-01-07 07:47:08,126 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,126 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 23.865680694580078
2023-01-07 07:47:08,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.34117126464844
2023-01-07 07:47:08,127 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 23.865680694580078
2023-01-07 07:47:08,127 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,127 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 67.08638763427734
2023-01-07 07:47:08,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.204959869384766
2023-01-07 07:47:08,129 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 67.08638763427734
2023-01-07 07:47:08,129 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,129 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 249.25399780273438
2023-01-07 07:47:08,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.568185329437256
2023-01-07 07:47:08,130 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.287985801696777
2023-01-07 07:47:08,130 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,130 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 302.86895751953125
2023-01-07 07:47:08,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5068588256835938
2023-01-07 07:47:08,132 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 302.86895751953125
2023-01-07 07:47:08,132 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,132 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 226.57054138183594
2023-01-07 07:47:08,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.99337387084961
2023-01-07 07:47:08,133 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 34.300514221191406
2023-01-07 07:47:08,133 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,133 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 226.57054138183594
2023-01-07 07:47:08,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.64146041870117
2023-01-07 07:47:08,134 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 265.1702880859375
2023-01-07 07:47:08,134 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,134 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,134 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,134 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 226.57054138183594
2023-01-07 07:47:08,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.15768814086914
2023-01-07 07:47:08,136 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 89.98121643066406
2023-01-07 07:47:08,136 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,136 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 74.04151916503906
2023-01-07 07:47:08,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -178.46420288085938
2023-01-07 07:47:08,137 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 74.04151916503906
2023-01-07 07:47:08,137 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,137 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 226.57054138183594
2023-01-07 07:47:08,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.47238540649414
2023-01-07 07:47:08,139 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: 226.57054138183594
2023-01-07 07:47:08,139 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,139 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.71409606933594
2023-01-07 07:47:08,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.65982437133789
2023-01-07 07:47:08,141 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.71409606933594
2023-01-07 07:47:08,141 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,141 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 249.25399780273438
2023-01-07 07:47:08,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,141 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.18814468383789
2023-01-07 07:47:08,142 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 91.30177307128906
2023-01-07 07:47:08,142 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,142 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,142 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,142 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.4764404296875
2023-01-07 07:47:08,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.778656005859375
2023-01-07 07:47:08,143 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 263.50830078125
2023-01-07 07:47:08,143 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,143 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 44.4764404296875
2023-01-07 07:47:08,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.714759826660156
2023-01-07 07:47:08,145 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 44.4764404296875
2023-01-07 07:47:08,145 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,145 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,145 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,145 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 249.25399780273438
2023-01-07 07:47:08,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.33699607849121
2023-01-07 07:47:08,146 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 67.17601013183594
2023-01-07 07:47:08,146 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,146 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 249.25399780273438
2023-01-07 07:47:08,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.406532287597656
2023-01-07 07:47:08,147 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 249.25399780273438
2023-01-07 07:47:08,147 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,147 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,147 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.37612915039062
2023-01-07 07:47:08,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,149 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.37612915039062
2023-01-07 07:47:08,149 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,149 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 195.31817626953125
2023-01-07 07:47:08,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,150 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.4367008209228516
2023-01-07 07:47:08,150 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,150 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 333.2182922363281
2023-01-07 07:47:08,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,151 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.1334228515625
2023-01-07 07:47:08,151 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,151 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 195.31817626953125
2023-01-07 07:47:08,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,152 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 333.2182922363281
2023-01-07 07:47:08,152 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,153 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 195.31817626953125
2023-01-07 07:47:08,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,153 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.56080627441406
2023-01-07 07:47:08,154 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,154 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 195.31817626953125
2023-01-07 07:47:08,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,155 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 195.31817626953125
2023-01-07 07:47:08,155 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,155 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 129.88360595703125
2023-01-07 07:47:08,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,156 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 129.88360595703125
2023-01-07 07:47:08,156 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,156 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 319.85125732421875
2023-01-07 07:47:08,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,158 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 319.85125732421875
2023-01-07 07:47:08,158 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,158 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 570.5592651367188
2023-01-07 07:47:08,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,159 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 570.5592651367188
2023-01-07 07:47:08,159 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,159 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,159 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 64.5879135131836
2023-01-07 07:47:08,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,160 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 64.5879135131836
2023-01-07 07:47:08,160 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,160 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 2.5656747817993164
2023-01-07 07:47:08,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,161 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 503.056884765625
2023-01-07 07:47:08,162 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,162 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,162 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,162 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 2.5656747817993164
2023-01-07 07:47:08,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,163 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 2.5656747817993164
2023-01-07 07:47:08,163 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,163 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 143.08375549316406
2023-01-07 07:47:08,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,164 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 143.08375549316406
2023-01-07 07:47:08,164 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,164 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -358.42694091796875
2023-01-07 07:47:08,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,166 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -358.42694091796875
2023-01-07 07:47:08,166 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,166 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 143.68359375
2023-01-07 07:47:08,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,167 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 143.68359375
2023-01-07 07:47:08,167 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,167 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,167 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,167 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 15.794073104858398
2023-01-07 07:47:08,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,169 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: 15.794073104858398
2023-01-07 07:47:08,169 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,169 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,169 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.155181884765625
2023-01-07 07:47:08,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,170 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 542.0469970703125
2023-01-07 07:47:08,170 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,170 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.155181884765625
2023-01-07 07:47:08,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,171 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.155181884765625
2023-01-07 07:47:08,171 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,171 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,171 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 143.68359375
2023-01-07 07:47:08,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,173 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 143.68359375
2023-01-07 07:47:08,173 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,173 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,173 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 583.4763793945312
2023-01-07 07:47:08,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,174 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 583.4763793945312
2023-01-07 07:47:08,174 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,174 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 129.0837860107422
2023-01-07 07:47:08,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,175 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 129.0837860107422
2023-01-07 07:47:08,175 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,176 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 135.72596740722656
2023-01-07 07:47:08,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,177 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 135.72596740722656
2023-01-07 07:47:08,177 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,177 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 555.1942749023438
2023-01-07 07:47:08,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,178 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 555.1942749023438
2023-01-07 07:47:08,178 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,178 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 243.36199951171875
2023-01-07 07:47:08,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,180 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 243.36199951171875
2023-01-07 07:47:08,180 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,180 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 144.242431640625
2023-01-07 07:47:08,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 30790.7890625
2023-01-07 07:47:08,181 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 144.242431640625
2023-01-07 07:47:08,182 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,182 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 191.11386108398438
2023-01-07 07:47:08,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,183 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 191.11386108398438
2023-01-07 07:47:08,183 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,183 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 151.10064697265625
2023-01-07 07:47:08,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,184 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 151.10064697265625
2023-01-07 07:47:08,185 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,185 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,185 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 143.77125549316406
2023-01-07 07:47:08,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,186 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 143.77125549316406
2023-01-07 07:47:08,186 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,186 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 569.7505493164062
2023-01-07 07:47:08,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,187 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 569.7505493164062
2023-01-07 07:47:08,187 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,187 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,187 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -100.905517578125
2023-01-07 07:47:08,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,189 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -100.905517578125
2023-01-07 07:47:08,189 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,189 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 302.9644470214844
2023-01-07 07:47:08,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,190 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 302.9644470214844
2023-01-07 07:47:08,190 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,190 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,190 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,190 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -7968.2421875
2023-01-07 07:47:08,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,192 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7968.2421875
2023-01-07 07:47:08,192 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,192 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -373.13641357421875
2023-01-07 07:47:08,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,193 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 293.8729248046875
2023-01-07 07:47:08,193 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,193 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,193 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,193 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -373.13641357421875
2023-01-07 07:47:08,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,194 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -373.13641357421875
2023-01-07 07:47:08,194 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,194 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -11681.701171875
2023-01-07 07:47:08,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,195 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1049.7034912109375
2023-01-07 07:47:08,195 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,195 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -11681.701171875
2023-01-07 07:47:08,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,197 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -11681.701171875
2023-01-07 07:47:08,197 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,197 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,197 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -11049.8818359375
2023-01-07 07:47:08,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,198 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1071.606201171875
2023-01-07 07:47:08,198 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,198 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -11049.8818359375
2023-01-07 07:47:08,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,199 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -11049.8818359375
2023-01-07 07:47:08,199 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,199 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 246.2230224609375
2023-01-07 07:47:08,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,201 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 246.2230224609375
2023-01-07 07:47:08,201 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,201 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -25977.5703125
2023-01-07 07:47:08,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,202 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -25977.5703125
2023-01-07 07:47:08,202 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,202 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -7502.9677734375
2023-01-07 07:47:08,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,203 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 295.120361328125
2023-01-07 07:47:08,203 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,203 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,203 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -7502.9677734375
2023-01-07 07:47:08,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,205 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: -7502.9677734375
2023-01-07 07:47:08,205 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,205 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 157.92758178710938
2023-01-07 07:47:08,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,206 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1087.70068359375
2023-01-07 07:47:08,206 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,206 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 157.92758178710938
2023-01-07 07:47:08,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,207 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 157.92758178710938
2023-01-07 07:47:08,207 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,207 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,207 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,207 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 17.0419921875
2023-01-07 07:47:08,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,208 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 297.1358947753906
2023-01-07 07:47:08,208 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,208 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 17.0419921875
2023-01-07 07:47:08,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,210 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 17.0419921875
2023-01-07 07:47:08,210 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,210 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,210 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 184.639404296875
2023-01-07 07:47:08,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,211 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 298.13604736328125
2023-01-07 07:47:08,211 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,211 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 184.639404296875
2023-01-07 07:47:08,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,212 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 184.639404296875
2023-01-07 07:47:08,212 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,212 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -9.82781982421875
2023-01-07 07:47:08,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,213 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1088.104248046875
2023-01-07 07:47:08,213 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,213 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,214 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -9.82781982421875
2023-01-07 07:47:08,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,215 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -9.82781982421875
2023-01-07 07:47:08,215 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,215 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 57.657257080078125
2023-01-07 07:47:08,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,216 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 300.5357666015625
2023-01-07 07:47:08,216 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,216 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,216 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,216 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 57.657257080078125
2023-01-07 07:47:08,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,217 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: 57.657257080078125
2023-01-07 07:47:08,217 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,217 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,218 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -37.54609680175781
2023-01-07 07:47:08,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,218 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 306.5380554199219
2023-01-07 07:47:08,218 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,218 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -37.54609680175781
2023-01-07 07:47:08,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,220 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -37.54609680175781
2023-01-07 07:47:08,220 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,220 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -641.210205078125
2023-01-07 07:47:08,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,221 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1093.271728515625
2023-01-07 07:47:08,221 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,221 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -641.210205078125
2023-01-07 07:47:08,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,222 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -641.210205078125
2023-01-07 07:47:08,222 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,222 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 677.2301025390625
2023-01-07 07:47:08,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,223 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 313.0770263671875
2023-01-07 07:47:08,224 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,224 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 677.2301025390625
2023-01-07 07:47:08,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,225 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 677.2301025390625
2023-01-07 07:47:08,225 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,225 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -5314.03369140625
2023-01-07 07:47:08,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,226 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 298.91033935546875
2023-01-07 07:47:08,226 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,226 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,226 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,226 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -5314.03369140625
2023-01-07 07:47:08,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,227 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -5314.03369140625
2023-01-07 07:47:08,227 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,228 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -37196.0390625
2023-01-07 07:47:08,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,228 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1097.5018310546875
2023-01-07 07:47:08,229 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,229 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -37196.0390625
2023-01-07 07:47:08,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,230 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -37196.0390625
2023-01-07 07:47:08,230 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,230 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -16299.3515625
2023-01-07 07:47:08,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,231 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 359.7112731933594
2023-01-07 07:47:08,231 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,231 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -16299.3515625
2023-01-07 07:47:08,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,233 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -16299.3515625
2023-01-07 07:47:08,233 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,233 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -42254.7734375
2023-01-07 07:47:08,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,234 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 304.335693359375
2023-01-07 07:47:08,234 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,234 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -42254.7734375
2023-01-07 07:47:08,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,235 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -42254.7734375
2023-01-07 07:47:08,235 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,235 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -89685.9609375
2023-01-07 07:47:08,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,236 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1091.500732421875
2023-01-07 07:47:08,236 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,236 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,236 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,236 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -89685.9609375
2023-01-07 07:47:08,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,238 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -89685.9609375
2023-01-07 07:47:08,238 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,238 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,238 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -76381.171875
2023-01-07 07:47:08,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,239 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 566.398193359375
2023-01-07 07:47:08,239 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,239 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -76381.171875
2023-01-07 07:47:08,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,240 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -76381.171875
2023-01-07 07:47:08,240 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,240 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -455.9896240234375
2023-01-07 07:47:08,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,241 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 505.8614501953125
2023-01-07 07:47:08,242 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,242 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -455.9896240234375
2023-01-07 07:47:08,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,243 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -455.9896240234375
2023-01-07 07:47:08,243 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,243 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -219808.53125
2023-01-07 07:47:08,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,244 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2098.77587890625
2023-01-07 07:47:08,244 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,244 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -219808.53125
2023-01-07 07:47:08,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,245 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -219808.53125
2023-01-07 07:47:08,245 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,245 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -48071.29296875
2023-01-07 07:47:08,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,246 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2087.56689453125
2023-01-07 07:47:08,246 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,247 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -48071.29296875
2023-01-07 07:47:08,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,248 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -48071.29296875
2023-01-07 07:47:08,248 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,248 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -30.72930908203125
2023-01-07 07:47:08,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,249 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 568.2017211914062
2023-01-07 07:47:08,249 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,249 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -30.72930908203125
2023-01-07 07:47:08,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,250 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -30.72930908203125
2023-01-07 07:47:08,251 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,251 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -60823.8203125
2023-01-07 07:47:08,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,252 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 477.6040954589844
2023-01-07 07:47:08,252 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,252 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -60823.8203125
2023-01-07 07:47:08,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,253 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -60823.8203125
2023-01-07 07:47:08,253 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,253 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -183550.28125
2023-01-07 07:47:08,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,254 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2095.259521484375
2023-01-07 07:47:08,254 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,254 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -183550.28125
2023-01-07 07:47:08,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,256 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -183550.28125
2023-01-07 07:47:08,256 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,256 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -22832.9375
2023-01-07 07:47:08,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,257 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 570.5980224609375
2023-01-07 07:47:08,257 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,257 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -22832.9375
2023-01-07 07:47:08,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,258 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -22832.9375
2023-01-07 07:47:08,258 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,258 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 267.8126525878906
2023-01-07 07:47:08,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,259 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 578.9083251953125
2023-01-07 07:47:08,259 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,259 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 267.8126525878906
2023-01-07 07:47:08,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,261 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 267.8126525878906
2023-01-07 07:47:08,261 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,261 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -266540.09375
2023-01-07 07:47:08,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,262 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2444.765380859375
2023-01-07 07:47:08,262 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,262 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -266540.09375
2023-01-07 07:47:08,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:08,264 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: -266540.09375
2023-01-07 07:47:08,264 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:08,264 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:08,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:08,264 > [DEBUG] 0 :: 102.2549057006836
2023-01-07 07:47:08,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 12598.533203125
2023-01-07 07:47:08,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -147.59048461914062
2023-01-07 07:47:08,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.266480922698975
2023-01-07 07:47:08,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 130351.53125
2023-01-07 07:47:08,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 51591.26953125
2023-01-07 07:47:08,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 103191.5546875
2023-01-07 07:47:08,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 206370.6875
2023-01-07 07:47:08,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.599098205566406
2023-01-07 07:47:08,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.402549743652344
2023-01-07 07:47:08,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 664784.8125
2023-01-07 07:47:08,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 314942.84375
2023-01-07 07:47:08,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 314926.0625
2023-01-07 07:47:08,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 629894.9375
2023-01-07 07:47:08,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 505401.3125
2023-01-07 07:47:08,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 74.99131774902344
2023-01-07 07:47:08,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.075515747070312
2023-01-07 07:47:08,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 147.19351196289062
2023-01-07 07:47:08,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 1709502.875
2023-01-07 07:47:08,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7729926109313965
2023-01-07 07:47:08,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,293 > [DEBUG] 0 :: before allreduce fusion buffer :: -413.54730224609375
2023-01-07 07:47:08,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.191334247589111
2023-01-07 07:47:08,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -543.50146484375
2023-01-07 07:47:08,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.06431245803833
2023-01-07 07:47:08,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -824.71826171875
2023-01-07 07:47:08,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5193239450454712
2023-01-07 07:47:08,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.2584228515625
2023-01-07 07:47:08,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5689541101455688
2023-01-07 07:47:08,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -1659.144287109375
2023-01-07 07:47:08,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5706270933151245
2023-01-07 07:47:08,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -1965.7401123046875
2023-01-07 07:47:08,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13299766182899475
2023-01-07 07:47:08,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.1672058105469
2023-01-07 07:47:08,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.596759557723999
2023-01-07 07:47:08,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -3199.16650390625
2023-01-07 07:47:08,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7321131229400635
2023-01-07 07:47:08,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -7439.59375
2023-01-07 07:47:08,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7400013208389282
2023-01-07 07:47:08,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -258.0342712402344
2023-01-07 07:47:08,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.231492042541504
2023-01-07 07:47:08,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -9260.873046875
2023-01-07 07:47:08,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0508395433425903
2023-01-07 07:47:08,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -23788.03125
2023-01-07 07:47:08,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1514767408370972
2023-01-07 07:47:08,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.742063045501709
2023-01-07 07:47:08,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5997572541236877
2023-01-07 07:47:08,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 7933.08984375
2023-01-07 07:47:08,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6992692947387695
2023-01-07 07:47:08,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -39314.078125
2023-01-07 07:47:08,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9911444187164307
2023-01-07 07:47:08,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,397 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.75192928314209
2023-01-07 07:47:08,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.79060935974121
2023-01-07 07:47:08,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -92917.625
2023-01-07 07:47:08,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2624761462211609
2023-01-07 07:47:08,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.2710394859313965
2023-01-07 07:47:08,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5324506759643555
2023-01-07 07:47:08,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -241642.46875
2023-01-07 07:47:08,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02280230075120926
2023-01-07 07:47:08,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7822158336639404
2023-01-07 07:47:08,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7262516021728516
2023-01-07 07:47:08,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8442642092704773
2023-01-07 07:47:08,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.390906572341919
2023-01-07 07:47:08,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.811153411865234
2023-01-07 07:47:08,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43863070011138916
2023-01-07 07:47:08,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.092950820922852
2023-01-07 07:47:08,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.338808059692383
2023-01-07 07:47:08,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.352129578590393
2023-01-07 07:47:08,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.265802264213562
2023-01-07 07:47:08,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,761 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.75662612915039
2023-01-07 07:47:08,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47073787450790405
2023-01-07 07:47:08,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4403600692749023
2023-01-07 07:47:08,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8317363262176514
2023-01-07 07:47:08,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.578536987304688
2023-01-07 07:47:08,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,949 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1556429862976074
2023-01-07 07:47:08,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,950 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.161482810974121
2023-01-07 07:47:08,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:08,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:08,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2177798748016357
2023-01-07 07:47:09,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.430870056152344
2023-01-07 07:47:09,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5713541507720947
2023-01-07 07:47:09,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 104.2091064453125
2023-01-07 07:47:09,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8632621169090271
2023-01-07 07:47:09,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.640445709228516
2023-01-07 07:47:09,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,070 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.617426335811615
2023-01-07 07:47:09,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9425549507141113
2023-01-07 07:47:09,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0375373363494873
2023-01-07 07:47:09,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.54621887207031
2023-01-07 07:47:09,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0289268493652344
2023-01-07 07:47:09,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.253063201904297
2023-01-07 07:47:09,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.293190002441406
2023-01-07 07:47:09,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.194833278656006
2023-01-07 07:47:09,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0308583974838257
2023-01-07 07:47:09,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.610985279083252
2023-01-07 07:47:09,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.527913093566895
2023-01-07 07:47:09,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -88.33702087402344
2023-01-07 07:47:09,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.743867874145508
2023-01-07 07:47:09,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,086 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.91881561279297
2023-01-07 07:47:09,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.221242904663086
2023-01-07 07:47:09,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 113.85192108154297
2023-01-07 07:47:09,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.802698135375977
2023-01-07 07:47:09,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 883916.6875
2023-01-07 07:47:09,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3274893760681152
2023-01-07 07:47:09,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -367.91973876953125
2023-01-07 07:47:09,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.697859048843384
2023-01-07 07:47:09,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -343.0195617675781
2023-01-07 07:47:09,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.71566772460938
2023-01-07 07:47:09,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 85.47603607177734
2023-01-07 07:47:09,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.44471740722656
2023-01-07 07:47:09,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 176.7763671875
2023-01-07 07:47:09,102 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:47:09,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 2436687104.0
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 19483234304.0
2023-01-07 07:47:09,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 38977347584.0
2023-01-07 07:47:09,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 90647920640.0
2023-01-07 07:47:09,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -1353241856.0
2023-01-07 07:47:09,116 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 53.578033447265625
2023-01-07 07:47:09,116 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,116 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 66.85391235351562
2023-01-07 07:47:09,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 318.1534423828125
2023-01-07 07:47:09,117 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 66.85391235351562
2023-01-07 07:47:09,118 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,118 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 16.07678985595703
2023-01-07 07:47:09,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.044818878173828
2023-01-07 07:47:09,119 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 16.07678985595703
2023-01-07 07:47:09,119 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,119 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 35.13816833496094
2023-01-07 07:47:09,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 107.71597290039062
2023-01-07 07:47:09,120 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 65.71871948242188
2023-01-07 07:47:09,120 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,120 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,120 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 35.13816833496094
2023-01-07 07:47:09,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.351646423339844
2023-01-07 07:47:09,122 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 35.13816833496094
2023-01-07 07:47:09,122 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,122 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,122 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 68.23683166503906
2023-01-07 07:47:09,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -47310127104.0
2023-01-07 07:47:09,123 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 68.23683166503906
2023-01-07 07:47:09,124 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,124 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,124 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 264.01312255859375
2023-01-07 07:47:09,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.104299545288086
2023-01-07 07:47:09,125 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 46.57948684692383
2023-01-07 07:47:09,125 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,125 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 304.8591003417969
2023-01-07 07:47:09,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.31320571899414
2023-01-07 07:47:09,126 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 304.8591003417969
2023-01-07 07:47:09,126 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,126 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 339.5652770996094
2023-01-07 07:47:09,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -57175949312.0
2023-01-07 07:47:09,127 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 49.12247085571289
2023-01-07 07:47:09,127 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,127 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 339.5652770996094
2023-01-07 07:47:09,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,128 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.30194854736328
2023-01-07 07:47:09,129 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 268.548583984375
2023-01-07 07:47:09,129 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,129 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 339.5652770996094
2023-01-07 07:47:09,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.920209884643555
2023-01-07 07:47:09,130 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 128.91259765625
2023-01-07 07:47:09,130 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,130 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 73.9297103881836
2023-01-07 07:47:09,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -114351898624.0
2023-01-07 07:47:09,131 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 73.9297103881836
2023-01-07 07:47:09,131 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,131 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 339.5652770996094
2023-01-07 07:47:09,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.59568977355957
2023-01-07 07:47:09,133 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: 339.5652770996094
2023-01-07 07:47:09,133 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,133 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.94493865966797
2023-01-07 07:47:09,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 150.80552673339844
2023-01-07 07:47:09,134 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.94493865966797
2023-01-07 07:47:09,134 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,134 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,134 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 264.01312255859375
2023-01-07 07:47:09,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.385068416595459
2023-01-07 07:47:09,136 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 120.40335083007812
2023-01-07 07:47:09,136 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,136 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 33.77295684814453
2023-01-07 07:47:09,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.970232009887695
2023-01-07 07:47:09,137 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 262.9736328125
2023-01-07 07:47:09,137 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,137 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 33.77295684814453
2023-01-07 07:47:09,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 234.5472412109375
2023-01-07 07:47:09,138 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 33.77295684814453
2023-01-07 07:47:09,138 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,138 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,138 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 264.01312255859375
2023-01-07 07:47:09,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.631752014160156
2023-01-07 07:47:09,139 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 67.93046569824219
2023-01-07 07:47:09,139 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,139 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 264.01312255859375
2023-01-07 07:47:09,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.360616683959961
2023-01-07 07:47:09,141 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 264.01312255859375
2023-01-07 07:47:09,141 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,141 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.41561889648438
2023-01-07 07:47:09,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.34070587158203
2023-01-07 07:47:09,142 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.41561889648438
2023-01-07 07:47:09,142 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,143 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 198.58963012695312
2023-01-07 07:47:09,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.0797271728515625
2023-01-07 07:47:09,144 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 2.946470260620117
2023-01-07 07:47:09,144 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,144 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 343.68695068359375
2023-01-07 07:47:09,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,145 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 254.07264709472656
2023-01-07 07:47:09,145 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,145 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,145 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,145 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 198.58963012695312
2023-01-07 07:47:09,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,146 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 343.68695068359375
2023-01-07 07:47:09,146 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,146 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 198.58963012695312
2023-01-07 07:47:09,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,147 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 126.98451232910156
2023-01-07 07:47:09,147 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,147 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,147 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 198.58963012695312
2023-01-07 07:47:09,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,149 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 198.58963012695312
2023-01-07 07:47:09,149 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,149 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 131.49423217773438
2023-01-07 07:47:09,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,150 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 131.49423217773438
2023-01-07 07:47:09,150 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,150 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 363.353271484375
2023-01-07 07:47:09,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,151 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 363.353271484375
2023-01-07 07:47:09,151 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,152 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 569.754638671875
2023-01-07 07:47:09,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,153 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 569.754638671875
2023-01-07 07:47:09,153 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,153 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 2.0409088134765625
2023-01-07 07:47:09,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,154 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 2.0409088134765625
2023-01-07 07:47:09,154 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,154 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.065799713134766
2023-01-07 07:47:09,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,155 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 499.6997375488281
2023-01-07 07:47:09,155 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,155 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.065799713134766
2023-01-07 07:47:09,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,157 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.065799713134766
2023-01-07 07:47:09,157 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,157 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 144.69439697265625
2023-01-07 07:47:09,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,158 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 144.69439697265625
2023-01-07 07:47:09,158 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,158 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -503.3551940917969
2023-01-07 07:47:09,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,159 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -503.3551940917969
2023-01-07 07:47:09,160 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,160 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,160 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 145.29421997070312
2023-01-07 07:47:09,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,161 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 145.29421997070312
2023-01-07 07:47:09,161 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,161 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -27.778676986694336
2023-01-07 07:47:09,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,162 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -27.778676986694336
2023-01-07 07:47:09,162 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,162 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,162 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -51.92011260986328
2023-01-07 07:47:09,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,163 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 544.5570068359375
2023-01-07 07:47:09,163 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,163 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,164 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,164 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -51.92011260986328
2023-01-07 07:47:09,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,165 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -51.92011260986328
2023-01-07 07:47:09,165 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,165 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 145.29425048828125
2023-01-07 07:47:09,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,166 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 145.29425048828125
2023-01-07 07:47:09,166 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,166 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 767.0836791992188
2023-01-07 07:47:09,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,168 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 767.0836791992188
2023-01-07 07:47:09,168 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,168 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 130.69442749023438
2023-01-07 07:47:09,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,169 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 130.69442749023438
2023-01-07 07:47:09,169 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,169 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,169 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 171.70880126953125
2023-01-07 07:47:09,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,171 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 171.70880126953125
2023-01-07 07:47:09,171 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,171 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,171 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 560.2155151367188
2023-01-07 07:47:09,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,172 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 560.2155151367188
2023-01-07 07:47:09,172 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,172 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 284.30377197265625
2023-01-07 07:47:09,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,173 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 284.30377197265625
2023-01-07 07:47:09,173 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,174 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 146.85015869140625
2023-01-07 07:47:09,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,175 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 146.85015869140625
2023-01-07 07:47:09,175 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,175 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,175 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,175 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 269.0556640625
2023-01-07 07:47:09,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,176 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 269.0556640625
2023-01-07 07:47:09,176 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,176 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 158.4784698486328
2023-01-07 07:47:09,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,178 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 158.4784698486328
2023-01-07 07:47:09,178 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,178 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 198.04364013671875
2023-01-07 07:47:09,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,180 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 198.04364013671875
2023-01-07 07:47:09,180 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,180 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 582.669677734375
2023-01-07 07:47:09,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,181 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 582.669677734375
2023-01-07 07:47:09,181 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,181 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -72.24655151367188
2023-01-07 07:47:09,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,182 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -72.24655151367188
2023-01-07 07:47:09,183 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,183 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,183 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 317.6044006347656
2023-01-07 07:47:09,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,184 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 317.6044006347656
2023-01-07 07:47:09,184 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,184 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -23774.6875
2023-01-07 07:47:09,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,185 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -23774.6875
2023-01-07 07:47:09,185 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,185 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -637.9432983398438
2023-01-07 07:47:09,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,186 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 295.25604248046875
2023-01-07 07:47:09,187 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,187 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,187 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,187 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -637.9432983398438
2023-01-07 07:47:09,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,188 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -637.9432983398438
2023-01-07 07:47:09,188 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,188 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -32758.66796875
2023-01-07 07:47:09,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,189 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1043.955078125
2023-01-07 07:47:09,189 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,189 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -32758.66796875
2023-01-07 07:47:09,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,190 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -32758.66796875
2023-01-07 07:47:09,190 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,190 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,190 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -26586.85546875
2023-01-07 07:47:09,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,191 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1064.5394287109375
2023-01-07 07:47:09,191 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,191 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -26586.85546875
2023-01-07 07:47:09,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,193 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -26586.85546875
2023-01-07 07:47:09,193 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,193 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,193 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,193 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 248.86904907226562
2023-01-07 07:47:09,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,194 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 248.86904907226562
2023-01-07 07:47:09,194 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,194 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -63103.65625
2023-01-07 07:47:09,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,196 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -63103.65625
2023-01-07 07:47:09,196 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,196 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -15674.91015625
2023-01-07 07:47:09,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,197 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 294.37774658203125
2023-01-07 07:47:09,197 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,197 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,197 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -15674.91015625
2023-01-07 07:47:09,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,198 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: -15674.91015625
2023-01-07 07:47:09,198 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,198 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 156.253662109375
2023-01-07 07:47:09,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,199 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1081.9521484375
2023-01-07 07:47:09,199 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,199 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,200 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 156.253662109375
2023-01-07 07:47:09,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,201 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: 156.253662109375
2023-01-07 07:47:09,201 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,201 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -82.83233642578125
2023-01-07 07:47:09,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,202 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 297.04168701171875
2023-01-07 07:47:09,202 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,202 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -82.83233642578125
2023-01-07 07:47:09,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,204 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: -82.83233642578125
2023-01-07 07:47:09,204 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,204 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 191.99554443359375
2023-01-07 07:47:09,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,205 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 298.04180908203125
2023-01-07 07:47:09,205 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,205 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 191.99554443359375
2023-01-07 07:47:09,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,206 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 191.99554443359375
2023-01-07 07:47:09,206 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,206 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2.486175537109375
2023-01-07 07:47:09,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,207 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1082.35595703125
2023-01-07 07:47:09,207 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,207 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,207 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,207 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2.486175537109375
2023-01-07 07:47:09,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,208 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 2.486175537109375
2023-01-07 07:47:09,209 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,209 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 126.30079650878906
2023-01-07 07:47:09,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,210 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 300.4415283203125
2023-01-07 07:47:09,210 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,210 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,210 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 126.30079650878906
2023-01-07 07:47:09,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,211 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: 126.30079650878906
2023-01-07 07:47:09,211 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,211 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -111.10771179199219
2023-01-07 07:47:09,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,212 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 308.8016357421875
2023-01-07 07:47:09,212 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,212 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -111.10771179199219
2023-01-07 07:47:09,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,213 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -111.10771179199219
2023-01-07 07:47:09,213 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,214 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,214 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,214 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -986.6622314453125
2023-01-07 07:47:09,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,214 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1088.9393310546875
2023-01-07 07:47:09,215 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,215 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -986.6622314453125
2023-01-07 07:47:09,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,216 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -986.6622314453125
2023-01-07 07:47:09,216 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,216 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,216 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,216 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 773.9094848632812
2023-01-07 07:47:09,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,217 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 317.3187561035156
2023-01-07 07:47:09,217 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,217 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 773.9094848632812
2023-01-07 07:47:09,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,218 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 773.9094848632812
2023-01-07 07:47:09,218 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,219 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7337.1953125
2023-01-07 07:47:09,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,219 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 297.592529296875
2023-01-07 07:47:09,220 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,220 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7337.1953125
2023-01-07 07:47:09,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,221 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7337.1953125
2023-01-07 07:47:09,221 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,221 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -50993.6015625
2023-01-07 07:47:09,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,222 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1091.75341796875
2023-01-07 07:47:09,222 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,222 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -50993.6015625
2023-01-07 07:47:09,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,223 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -50993.6015625
2023-01-07 07:47:09,223 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,223 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -22374.765625
2023-01-07 07:47:09,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,224 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 379.82196044921875
2023-01-07 07:47:09,225 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,225 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -22374.765625
2023-01-07 07:47:09,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,226 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -22374.765625
2023-01-07 07:47:09,226 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,226 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,226 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,226 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -57854.0234375
2023-01-07 07:47:09,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,227 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 304.241455078125
2023-01-07 07:47:09,227 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,227 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -57854.0234375
2023-01-07 07:47:09,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,228 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -57854.0234375
2023-01-07 07:47:09,228 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,228 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -122783.2578125
2023-01-07 07:47:09,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,229 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1085.7523193359375
2023-01-07 07:47:09,229 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,230 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -122783.2578125
2023-01-07 07:47:09,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,231 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -122783.2578125
2023-01-07 07:47:09,231 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,231 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -104504.859375
2023-01-07 07:47:09,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,232 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 555.1839599609375
2023-01-07 07:47:09,232 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,232 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,232 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -104504.859375
2023-01-07 07:47:09,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,233 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -104504.859375
2023-01-07 07:47:09,233 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,233 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -553.2152709960938
2023-01-07 07:47:09,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,234 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 504.6711120605469
2023-01-07 07:47:09,235 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,235 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -553.2152709960938
2023-01-07 07:47:09,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,236 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -553.2152709960938
2023-01-07 07:47:09,236 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,236 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,236 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,236 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -300764.3125
2023-01-07 07:47:09,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,237 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2092.916259765625
2023-01-07 07:47:09,237 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,237 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -300764.3125
2023-01-07 07:47:09,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,238 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -300764.3125
2023-01-07 07:47:09,239 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,239 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -65747.6796875
2023-01-07 07:47:09,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,240 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2081.67236328125
2023-01-07 07:47:09,240 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,240 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -65747.6796875
2023-01-07 07:47:09,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,241 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -65747.6796875
2023-01-07 07:47:09,241 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,241 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.675788879394531
2023-01-07 07:47:09,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,242 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 556.987548828125
2023-01-07 07:47:09,242 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,242 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.675788879394531
2023-01-07 07:47:09,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,244 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -7.675788879394531
2023-01-07 07:47:09,244 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,244 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -83265.9140625
2023-01-07 07:47:09,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,245 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 466.389892578125
2023-01-07 07:47:09,245 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,245 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -83265.9140625
2023-01-07 07:47:09,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,246 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -83265.9140625
2023-01-07 07:47:09,246 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,246 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -251182.125
2023-01-07 07:47:09,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,247 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2089.26220703125
2023-01-07 07:47:09,247 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,247 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -251182.125
2023-01-07 07:47:09,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,249 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -251182.125
2023-01-07 07:47:09,249 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,249 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -31219.16796875
2023-01-07 07:47:09,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,250 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 559.3839111328125
2023-01-07 07:47:09,250 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,250 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -31219.16796875
2023-01-07 07:47:09,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,251 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -31219.16796875
2023-01-07 07:47:09,251 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,251 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 141.56051635742188
2023-01-07 07:47:09,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,252 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 570.8053588867188
2023-01-07 07:47:09,252 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,252 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 141.56051635742188
2023-01-07 07:47:09,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,254 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 141.56051635742188
2023-01-07 07:47:09,254 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,254 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -365174.3125
2023-01-07 07:47:09,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,255 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2442.8056640625
2023-01-07 07:47:09,255 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,255 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -365174.3125
2023-01-07 07:47:09,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:47:09,256 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: -365174.3125
2023-01-07 07:47:09,256 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 07:47:09,257 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:47:09,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:47:09,257 > [DEBUG] 0 :: 225.1961669921875
2023-01-07 07:47:09,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -629.9964599609375
2023-01-07 07:47:09,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -93.03176879882812
2023-01-07 07:47:09,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,265 > [DEBUG] 0 :: before allreduce fusion buffer :: -307.2352294921875
2023-01-07 07:47:09,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 47678832.0
2023-01-07 07:47:09,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -398.3813171386719
2023-01-07 07:47:09,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.866302490234375
2023-01-07 07:47:09,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -805.4116821289062
2023-01-07 07:47:09,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 875.7847900390625
2023-01-07 07:47:09,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.323617458343506
2023-01-07 07:47:09,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 65490680.0
2023-01-07 07:47:09,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -2755.193359375
2023-01-07 07:47:09,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -2008.78857421875
2023-01-07 07:47:09,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -5266.89794921875
2023-01-07 07:47:09,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -5015000.0
2023-01-07 07:47:09,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -10134.9150390625
2023-01-07 07:47:09,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 4713.86669921875
2023-01-07 07:47:09,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.5086669921875
2023-01-07 07:47:09,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 120943816.0
2023-01-07 07:47:09,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.184029579162598
2023-01-07 07:47:09,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 131.7040557861328
2023-01-07 07:47:09,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8935389518737793
2023-01-07 07:47:09,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 309.43231201171875
2023-01-07 07:47:09,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7157224416732788
2023-01-07 07:47:09,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.19122314453125
2023-01-07 07:47:09,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4906761050224304
2023-01-07 07:47:09,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,296 > [DEBUG] 0 :: before allreduce fusion buffer :: -187.63897705078125
2023-01-07 07:47:09,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.100407600402832
2023-01-07 07:47:09,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.433456420898438
2023-01-07 07:47:09,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.190337657928467
2023-01-07 07:47:09,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 194.7813720703125
2023-01-07 07:47:09,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8321738243103027
2023-01-07 07:47:09,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,303 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.4198913574219
2023-01-07 07:47:09,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23966115713119507
2023-01-07 07:47:09,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 1853.37548828125
2023-01-07 07:47:09,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6276347637176514
2023-01-07 07:47:09,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 652.0217895507812
2023-01-07 07:47:09,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.03727912902832
2023-01-07 07:47:09,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -1418.98876953125
2023-01-07 07:47:09,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4124507904052734
2023-01-07 07:47:09,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 2611.23779296875
2023-01-07 07:47:09,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4459733963012695
2023-01-07 07:47:09,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 6025.9404296875
2023-01-07 07:47:09,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4566922187805176
2023-01-07 07:47:09,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3497889041900635
2023-01-07 07:47:09,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3542802333831787
2023-01-07 07:47:09,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 7159.9931640625
2023-01-07 07:47:09,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.346174240112305
2023-01-07 07:47:09,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 21485.330078125
2023-01-07 07:47:09,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1494961977005005
2023-01-07 07:47:09,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.58155059814453
2023-01-07 07:47:09,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.247676372528076
2023-01-07 07:47:09,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 40457.0546875
2023-01-07 07:47:09,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.421313762664795
2023-01-07 07:47:09,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.389774322509766
2023-01-07 07:47:09,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8350417613983154
2023-01-07 07:47:09,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 51920.66015625
2023-01-07 07:47:09,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.109841346740723
2023-01-07 07:47:09,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.38209915161133
2023-01-07 07:47:09,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.874429702758789
2023-01-07 07:47:09,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.864012718200684
2023-01-07 07:47:09,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.41862964630127
2023-01-07 07:47:09,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9110970497131348
2023-01-07 07:47:09,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.875670433044434
2023-01-07 07:47:09,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.63055992126465
2023-01-07 07:47:09,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.101602554321289
2023-01-07 07:47:09,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0468950271606445
2023-01-07 07:47:09,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8716045618057251
2023-01-07 07:47:09,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.7140474319458
2023-01-07 07:47:09,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,753 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.997786521911621
2023-01-07 07:47:09,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4050562381744385
2023-01-07 07:47:09,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.658331871032715
2023-01-07 07:47:09,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.68099975585938
2023-01-07 07:47:09,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.115139961242676
2023-01-07 07:47:09,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5817859172821045
2023-01-07 07:47:09,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:09,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:09,938 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.008596420288086
2023-01-07 07:47:10,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.146437644958496
2023-01-07 07:47:10,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.766359329223633
2023-01-07 07:47:10,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.73633575439453
2023-01-07 07:47:10,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.254438877105713
2023-01-07 07:47:10,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.836524963378906
2023-01-07 07:47:10,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.640350341796875
2023-01-07 07:47:10,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.267039775848389
2023-01-07 07:47:10,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8305420875549316
2023-01-07 07:47:10,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.67555618286133
2023-01-07 07:47:10,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.429950714111328
2023-01-07 07:47:10,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 2167.207763671875
2023-01-07 07:47:10,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.136152267456055
2023-01-07 07:47:10,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,065 > [DEBUG] 0 :: before allreduce fusion buffer :: -79.9864730834961
2023-01-07 07:47:10,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9629547595977783
2023-01-07 07:47:10,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,067 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.885398864746094
2023-01-07 07:47:10,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10577917098999023
2023-01-07 07:47:10,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,070 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.206993103027344
2023-01-07 07:47:10,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.07795715332031
2023-01-07 07:47:10,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 1011.2152709960938
2023-01-07 07:47:10,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.13974380493164
2023-01-07 07:47:10,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 147.12899780273438
2023-01-07 07:47:10,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.50758743286133
2023-01-07 07:47:10,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 264965.28125
2023-01-07 07:47:10,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.885297775268555
2023-01-07 07:47:10,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 646.9971923828125
2023-01-07 07:47:10,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.44593811035156
2023-01-07 07:47:10,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -2532344.5
2023-01-07 07:47:10,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 241.30345153808594
2023-01-07 07:47:10,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.69792175292969
2023-01-07 07:47:10,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,086 > [DEBUG] 0 :: before allreduce fusion buffer :: -151.59405517578125
2023-01-07 07:47:10,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:47:10,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:47:10,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -151.89419555664062
