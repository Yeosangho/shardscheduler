2023-01-07 07:51:22,398 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:51:22,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:22,437 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.808320164680481
2023-01-07 07:51:22,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:22,437 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:51:22,437 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:22,437 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:22,437 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 07:51:22,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,283 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,284 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,284 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,284 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,284 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 07:51:23,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,297 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.14872932434082
2023-01-07 07:51:23,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,297 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:23,297 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,297 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,297 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 07:51:23,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,298 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,299 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,299 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,299 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,299 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 07:51:23,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,301 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.014472007751465
2023-01-07 07:51:23,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,302 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:23,302 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,302 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,302 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 07:51:23,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,341 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,342 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,342 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,342 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,342 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 07:51:23,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,344 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -13.641298294067383
2023-01-07 07:51:23,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,344 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,344 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,344 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,344 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 07:51:23,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,345 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,346 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,346 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,346 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,346 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 07:51:23,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,347 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -2.916147232055664
2023-01-07 07:51:23,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,347 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,347 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,347 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,348 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 07:51:23,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,349 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,349 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,349 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,349 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,349 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 07:51:23,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,350 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.799287796020508
2023-01-07 07:51:23,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,351 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,351 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,351 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,351 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 07:51:23,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,352 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,352 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,352 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,352 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,352 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 07:51:23,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,354 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.5919523239135742
2023-01-07 07:51:23,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,354 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:23,354 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,354 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,354 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 07:51:23,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,355 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,356 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,356 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,356 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,356 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 07:51:23,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,357 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.788993835449219
2023-01-07 07:51:23,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,357 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,357 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,358 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,358 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 07:51:23,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,359 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,359 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,359 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,359 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,359 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 07:51:23,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,360 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 30.836856842041016
2023-01-07 07:51:23,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,361 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,361 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,361 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,361 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 07:51:23,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,362 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,362 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,362 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,362 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,362 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 07:51:23,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,364 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 5.077452659606934
2023-01-07 07:51:23,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,364 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:23,364 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,364 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,364 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 07:51:23,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,365 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:23,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,366 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:23,366 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,366 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,366 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 07:51:23,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,367 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -5.2926025390625
2023-01-07 07:51:23,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,367 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:23,367 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,367 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,368 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 07:51:23,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,369 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,369 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,369 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,369 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,369 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 07:51:23,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,370 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 57.94667434692383
2023-01-07 07:51:23,370 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,371 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:51:23,371 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,371 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,371 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 07:51:23,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,372 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,372 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,372 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,373 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,373 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 07:51:23,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,374 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 11.273445129394531
2023-01-07 07:51:23,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,374 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:23,374 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,374 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,374 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 07:51:23,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,376 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,376 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,376 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,376 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,376 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 07:51:23,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,377 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.352242469787598
2023-01-07 07:51:23,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,378 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,378 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,378 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,378 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 07:51:23,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,379 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,379 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,379 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,379 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,380 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 07:51:23,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,381 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 43.89067077636719
2023-01-07 07:51:23,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,381 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:23,381 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,381 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,381 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 07:51:23,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,382 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,383 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,383 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,383 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,383 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 07:51:23,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,384 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 17.112319946289062
2023-01-07 07:51:23,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,385 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,385 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,385 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,385 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 07:51:23,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,386 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,386 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,386 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,386 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,386 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 07:51:23,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,388 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.345441818237305
2023-01-07 07:51:23,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,388 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:23,388 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,388 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,388 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 07:51:23,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,389 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,390 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,390 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,390 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,390 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 07:51:23,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,391 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -22.915189743041992
2023-01-07 07:51:23,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,392 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,392 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,392 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,392 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 07:51:23,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,393 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,393 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,393 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,393 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,393 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 07:51:23,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,394 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -16.715007781982422
2023-01-07 07:51:23,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,395 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,395 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,395 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,395 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 07:51:23,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,396 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,396 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,396 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,397 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,397 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 07:51:23,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,398 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 0.8379263877868652
2023-01-07 07:51:23,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,398 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:23,398 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,398 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,398 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 07:51:23,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,399 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,400 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,400 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,400 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,400 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 07:51:23,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,401 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -9.415725708007812
2023-01-07 07:51:23,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,401 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,402 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,402 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,402 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 07:51:23,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,403 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,403 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,403 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,403 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,403 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 07:51:23,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,405 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 13.198230743408203
2023-01-07 07:51:23,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,405 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,405 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,405 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,405 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 07:51:23,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,406 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,406 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,407 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,407 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,407 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 07:51:23,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,408 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 15.31062126159668
2023-01-07 07:51:23,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,408 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:23,408 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,408 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,409 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 07:51:23,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,410 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:51:23,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,410 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:23,410 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,410 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,410 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 07:51:23,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,411 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -7.202464580535889
2023-01-07 07:51:23,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,412 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:23,412 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,412 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,412 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 07:51:23,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,413 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,413 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,413 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,413 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,413 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 07:51:23,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,415 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.41171932220459
2023-01-07 07:51:23,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,415 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:23,415 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,415 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,415 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 07:51:23,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,416 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,417 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,417 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,417 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,417 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 07:51:23,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,418 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 1.5752387046813965
2023-01-07 07:51:23,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,418 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,419 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,419 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,419 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 07:51:23,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,420 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,420 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,420 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,420 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,420 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 07:51:23,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,421 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 0.1080312728881836
2023-01-07 07:51:23,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,422 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,422 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,422 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,422 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 07:51:23,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,423 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,424 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,424 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,424 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,424 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 07:51:23,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,425 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -23.51654815673828
2023-01-07 07:51:23,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,425 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:23,425 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,425 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,425 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 07:51:23,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,426 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,427 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,427 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,427 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,427 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 07:51:23,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,428 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.361846923828125
2023-01-07 07:51:23,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,428 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,429 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,429 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,429 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 07:51:23,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,430 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,430 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,430 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,430 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,430 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 07:51:23,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,431 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.066387176513672
2023-01-07 07:51:23,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,432 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,432 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,432 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,432 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 07:51:23,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,433 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,434 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,434 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,434 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,434 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 07:51:23,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,435 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 8.936910629272461
2023-01-07 07:51:23,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,435 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,436 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,436 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,436 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 07:51:23,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,437 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,437 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,437 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,437 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,437 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 07:51:23,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,438 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.354618072509766
2023-01-07 07:51:23,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,439 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,439 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,439 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,439 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 07:51:23,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,440 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,440 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,440 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,440 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,441 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 07:51:23,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,442 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 31.165061950683594
2023-01-07 07:51:23,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,442 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,442 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,442 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,442 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 07:51:23,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,443 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,444 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,444 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,444 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,444 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 07:51:23,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,445 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 22.767589569091797
2023-01-07 07:51:23,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,445 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,445 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,445 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,445 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 07:51:23,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,446 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,447 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,447 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,447 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,447 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 07:51:23,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,448 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -53.85360336303711
2023-01-07 07:51:23,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,449 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,449 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,449 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,449 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 07:51:23,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,450 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,450 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,450 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,450 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,450 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 07:51:23,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,452 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -16.88102149963379
2023-01-07 07:51:23,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,452 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,452 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,452 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,452 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 07:51:23,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,453 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,454 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,454 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,454 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,454 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 07:51:23,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,455 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -9.834939956665039
2023-01-07 07:51:23,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,455 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,455 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,455 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,455 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 07:51:23,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,456 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,457 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,457 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,457 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,457 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 07:51:23,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,458 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 58.93596649169922
2023-01-07 07:51:23,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,459 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,459 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,459 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,459 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 07:51:23,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,460 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,460 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,460 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,460 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,460 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 07:51:23,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,461 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 33.402408599853516
2023-01-07 07:51:23,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,462 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,462 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,462 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,462 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 07:51:23,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,463 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,463 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,463 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,463 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,463 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 07:51:23,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,465 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.320283889770508
2023-01-07 07:51:23,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,465 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,465 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,465 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,465 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 07:51:23,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,466 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,467 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,467 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,467 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,467 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 07:51:23,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,468 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 78.32042694091797
2023-01-07 07:51:23,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,468 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,469 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,469 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,469 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 07:51:23,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,470 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,470 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,470 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,470 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,470 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 07:51:23,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,471 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 15.271093368530273
2023-01-07 07:51:23,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,472 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:23,472 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,472 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,472 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 07:51:23,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,473 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:51:23,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,473 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:23,473 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,473 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,473 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 07:51:23,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,475 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 4.744016647338867
2023-01-07 07:51:23,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,475 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:23,475 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,475 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,475 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 07:51:23,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,476 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:51:23,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,476 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:23,477 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,477 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,477 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 07:51:23,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,478 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 95.67549133300781
2023-01-07 07:51:23,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,478 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:23,478 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,478 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,478 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 07:51:23,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,480 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,480 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,480 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,480 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,480 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 07:51:23,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,481 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -38.519996643066406
2023-01-07 07:51:23,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,482 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:23,482 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,482 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,482 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 07:51:23,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,483 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,483 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,483 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,483 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,484 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 07:51:23,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,485 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -1.7011375427246094
2023-01-07 07:51:23,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,485 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:23,485 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,485 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,485 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 07:51:23,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,486 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:51:23,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,487 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:23,487 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,487 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,487 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 07:51:23,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,488 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -46.188575744628906
2023-01-07 07:51:23,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,488 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:51:23,488 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,488 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,488 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 07:51:23,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,489 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:51:23,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,490 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:23,490 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,490 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,490 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 07:51:23,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,491 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -52.732200622558594
2023-01-07 07:51:23,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,492 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:23,492 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,492 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,492 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 07:51:23,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,493 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,493 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,493 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,493 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,493 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 07:51:23,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,494 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.901889801025391
2023-01-07 07:51:23,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,495 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:23,495 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,495 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,495 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 07:51:23,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,496 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,497 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,497 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,497 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,497 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 07:51:23,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,498 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 41.01795959472656
2023-01-07 07:51:23,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,498 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:23,498 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,498 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,498 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 07:51:23,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,500 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:51:23,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,500 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:23,500 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,500 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,500 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 07:51:23,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,501 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -25.142528533935547
2023-01-07 07:51:23,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,502 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:23,502 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,502 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,502 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 07:51:23,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,503 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,503 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,503 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,503 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,503 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 07:51:23,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,505 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 51.97746276855469
2023-01-07 07:51:23,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,505 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:23,505 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,505 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,505 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 07:51:23,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,506 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:51:23,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,507 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:23,507 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,507 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,507 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 07:51:23,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,508 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.2801971435546875
2023-01-07 07:51:23,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,508 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:23,508 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,508 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,508 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 07:51:23,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,509 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:51:23,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,510 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:23,510 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,510 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,510 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 07:51:23,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,512 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 4.861334800720215
2023-01-07 07:51:23,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,512 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:51:23,512 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,512 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:51:23,512 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 07:51:23,514 > [DEBUG] 0 :: 7.033421516418457
2023-01-07 07:51:23,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,522 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,522 > [DEBUG] 0 :: before allreduce fusion buffer :: -382.9586486816406
2023-01-07 07:51:23,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,525 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,525 > [DEBUG] 0 :: before allreduce fusion buffer :: -345.593505859375
2023-01-07 07:51:23,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,536 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16557270288467407
2023-01-07 07:51:23,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,537 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.924119770526886
2023-01-07 07:51:23,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,540 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3465500473976135
2023-01-07 07:51:23,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,541 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.011867426335811615
2023-01-07 07:51:23,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,543 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2695376873016357
2023-01-07 07:51:23,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,544 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3293682932853699
2023-01-07 07:51:23,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,547 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01752648502588272
2023-01-07 07:51:23,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,548 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1478755474090576
2023-01-07 07:51:23,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,550 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9058282375335693
2023-01-07 07:51:23,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,552 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08665652573108673
2023-01-07 07:51:23,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,554 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07855020463466644
2023-01-07 07:51:23,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,555 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.64587140083313
2023-01-07 07:51:23,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,557 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.505234956741333
2023-01-07 07:51:23,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,558 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.897851586341858
2023-01-07 07:51:23,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,560 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2228931188583374
2023-01-07 07:51:23,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,561 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.037683963775635
2023-01-07 07:51:23,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,564 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.090453028678894
2023-01-07 07:51:23,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,565 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5758072733879089
2023-01-07 07:51:23,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,567 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1247379779815674
2023-01-07 07:51:23,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,568 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4527704119682312
2023-01-07 07:51:23,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,571 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42732053995132446
2023-01-07 07:51:23,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,572 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9740617871284485
2023-01-07 07:51:23,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,575 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8502386808395386
2023-01-07 07:51:23,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,576 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34368520975112915
2023-01-07 07:51:23,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,578 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9838802218437195
2023-01-07 07:51:23,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,579 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07616851478815079
2023-01-07 07:51:23,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,581 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16647571325302124
2023-01-07 07:51:23,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,582 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46400704979896545
2023-01-07 07:51:23,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,584 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6882301568984985
2023-01-07 07:51:23,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,585 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7877504825592041
2023-01-07 07:51:23,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,587 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0527522563934326
2023-01-07 07:51:23,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,588 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5862531661987305
2023-01-07 07:51:23,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,590 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2693679332733154
2023-01-07 07:51:23,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,591 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42044973373413086
2023-01-07 07:51:23,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,593 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0926294326782227
2023-01-07 07:51:23,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,594 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.354381561279297
2023-01-07 07:51:23,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,596 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1304295063018799
2023-01-07 07:51:23,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,597 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.126778602600098
2023-01-07 07:51:23,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,599 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.055617570877075
2023-01-07 07:51:23,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,600 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5460193753242493
2023-01-07 07:51:23,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,603 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6029818058013916
2023-01-07 07:51:23,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,604 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.558318614959717
2023-01-07 07:51:23,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,606 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1539807319641113
2023-01-07 07:51:23,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,607 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.216681957244873
2023-01-07 07:51:23,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,609 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.272437334060669
2023-01-07 07:51:23,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,610 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1946654319763184
2023-01-07 07:51:23,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,612 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1537723541259766
2023-01-07 07:51:23,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.652771949768066
2023-01-07 07:51:23,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,614 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.837198257446289
2023-01-07 07:51:23,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,615 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.6632304191589355
2023-01-07 07:51:23,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,618 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.706905364990234
2023-01-07 07:51:23,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,619 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2543816566467285
2023-01-07 07:51:23,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,621 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9116058349609375
2023-01-07 07:51:23,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,622 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3229265213012695
2023-01-07 07:51:23,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,624 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8171491622924805
2023-01-07 07:51:23,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.751778602600098
2023-01-07 07:51:23,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,627 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46661028265953064
2023-01-07 07:51:23,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.024996757507324
2023-01-07 07:51:23,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,629 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45365190505981445
2023-01-07 07:51:23,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5359773635864258
2023-01-07 07:51:23,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,632 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.129493713378906
2023-01-07 07:51:23,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.275192260742188
2023-01-07 07:51:23,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,635 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.192718967795372
2023-01-07 07:51:23,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.977748870849609
2023-01-07 07:51:23,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,637 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9213835000991821
2023-01-07 07:51:23,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.987076759338379
2023-01-07 07:51:23,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,640 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.431258201599121
2023-01-07 07:51:23,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.853641510009766
2023-01-07 07:51:23,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,642 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7008988857269287
2023-01-07 07:51:23,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,643 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.090386152267456
2023-01-07 07:51:23,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,645 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.535855770111084
2023-01-07 07:51:23,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,646 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.713921546936035
2023-01-07 07:51:23,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,647 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8427391052246094
2023-01-07 07:51:23,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.229527473449707
2023-01-07 07:51:23,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,650 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6034436225891113
2023-01-07 07:51:23,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,651 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.150531768798828
2023-01-07 07:51:23,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,653 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6218057870864868
2023-01-07 07:51:23,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.099335670471191
2023-01-07 07:51:23,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,655 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,656 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.6968255043029785
2023-01-07 07:51:23,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.971881866455078
2023-01-07 07:51:23,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,658 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.018575668334961
2023-01-07 07:51:23,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,659 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 119.77021789550781
2023-01-07 07:51:23,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.42450714111328
2023-01-07 07:51:23,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,662 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,663 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.003480434417725
2023-01-07 07:51:23,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.075559616088867
2023-01-07 07:51:23,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.707009315490723
2023-01-07 07:51:23,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,668 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.788223266601562
2023-01-07 07:51:23,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,670 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.31074905395508
2023-01-07 07:51:23,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,672 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.55122375488281
2023-01-07 07:51:23,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,673 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,674 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.918033599853516
2023-01-07 07:51:23,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.35908508300781
2023-01-07 07:51:23,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,676 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -118.92942810058594
2023-01-07 07:51:23,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.93055725097656
2023-01-07 07:51:23,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,679 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.39947509765625
2023-01-07 07:51:23,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.926603317260742
2023-01-07 07:51:23,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,681 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,681 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.443870544433594
2023-01-07 07:51:23,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.495969772338867
2023-01-07 07:51:23,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,685 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.516145706176758
2023-01-07 07:51:23,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,686 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.490200996398926
2023-01-07 07:51:23,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.2754020690918
2023-01-07 07:51:23,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,689 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,689 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.49203872680664
2023-01-07 07:51:23,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,690 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.74639129638672
2023-01-07 07:51:23,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -81.65631866455078
2023-01-07 07:51:23,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 127.90592193603516
2023-01-07 07:51:23,695 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:51:23,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,695 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 3954.95703125
2023-01-07 07:51:23,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.818197250366211
2023-01-07 07:51:23,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.153246879577637
2023-01-07 07:51:23,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.738014221191406
2023-01-07 07:51:23,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:23,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:23,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 721.75146484375
2023-01-07 07:51:23,708 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.808320164680481
2023-01-07 07:51:23,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:23,709 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:51:23,709 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:23,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,565 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:51:24,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,566 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.379642486572266
2023-01-07 07:51:24,568 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 66.0
2023-01-07 07:51:24,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,568 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,568 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 9.14872932434082
2023-01-07 07:51:24,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,568 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.4654541015625
2023-01-07 07:51:24,571 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 25.346630096435547
2023-01-07 07:51:24,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,571 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:24,571 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:24,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.66796875
2023-01-07 07:51:24,572 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 66.00000762939453
2023-01-07 07:51:24,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,573 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,573 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:24,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 136.37271118164062
2023-01-07 07:51:24,574 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.014472007751465
2023-01-07 07:51:24,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,575 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:24,575 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:51:24,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,575 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,575 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.594097137451172
2023-01-07 07:51:24,577 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 65.00004577636719
2023-01-07 07:51:24,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,578 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,578 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:24,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.590993881225586
2023-01-07 07:51:24,579 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -5.240215301513672
2023-01-07 07:51:24,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,580 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,580 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:51:24,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,580 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.88617706298828
2023-01-07 07:51:24,582 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 257.0
2023-01-07 07:51:24,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,582 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,582 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:24,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,583 > [DEBUG] 0 :: before allreduce fusion buffer :: -64.03882598876953
2023-01-07 07:51:24,584 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -2.916147232055664
2023-01-07 07:51:24,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,584 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,584 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,584 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:24,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.900897979736328
2023-01-07 07:51:24,585 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 266.59991455078125
2023-01-07 07:51:24,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,586 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,586 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,586 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:24,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.17982482910156
2023-01-07 07:51:24,587 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.799287796020508
2023-01-07 07:51:24,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,588 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,588 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:51:24,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,588 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.2723388671875
2023-01-07 07:51:24,590 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:51:24,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,590 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,590 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,590 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:24,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.29621887207031
2023-01-07 07:51:24,592 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.5919523239135742
2023-01-07 07:51:24,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,592 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:24,592 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,592 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:51:24,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,593 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9621734619140625
2023-01-07 07:51:24,594 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.19999694824219
2023-01-07 07:51:24,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,595 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,595 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:24,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.80329132080078
2023-01-07 07:51:24,596 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.788993835449219
2023-01-07 07:51:24,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,597 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,597 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:24,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.72910499572754
2023-01-07 07:51:24,598 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 259.3999938964844
2023-01-07 07:51:24,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,598 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,599 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:24,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.940223693847656
2023-01-07 07:51:24,600 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 30.836856842041016
2023-01-07 07:51:24,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,601 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,601 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:24,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 121.77925109863281
2023-01-07 07:51:24,602 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 65.19999694824219
2023-01-07 07:51:24,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,602 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,602 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:24,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 100.83686065673828
2023-01-07 07:51:24,604 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 5.077452659606934
2023-01-07 07:51:24,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,604 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:24,604 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:51:24,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,605 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,605 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.73677062988281
2023-01-07 07:51:24,607 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.59999084472656
2023-01-07 07:51:24,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,607 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:24,607 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,607 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:24,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.119041442871094
2023-01-07 07:51:24,610 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -14.098064422607422
2023-01-07 07:51:24,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,610 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:24,610 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,610 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,610 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 57.94667434692383
2023-01-07 07:51:24,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.72150421142578
2023-01-07 07:51:24,612 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 262.8000183105469
2023-01-07 07:51:24,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,612 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,612 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:24,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.020599365234375
2023-01-07 07:51:24,614 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 57.94667434692383
2023-01-07 07:51:24,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,614 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:51:24,614 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,614 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:24,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.99823760986328
2023-01-07 07:51:24,615 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 132.80018615722656
2023-01-07 07:51:24,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,616 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,616 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:24,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.80861282348633
2023-01-07 07:51:24,617 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 11.273445129394531
2023-01-07 07:51:24,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,618 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:24,618 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,618 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.57647705078125
2023-01-07 07:51:24,620 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 126.99999237060547
2023-01-07 07:51:24,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,620 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,620 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,620 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,620 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.352242469787598
2023-01-07 07:51:24,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.531923294067383
2023-01-07 07:51:24,622 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.352242469787598
2023-01-07 07:51:24,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,622 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,622 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,622 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:51:24,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,623 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.795661926269531
2023-01-07 07:51:24,624 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.6004028320312
2023-01-07 07:51:24,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,625 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,625 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 43.89067077636719
2023-01-07 07:51:24,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5827038288116455
2023-01-07 07:51:24,626 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 43.89067077636719
2023-01-07 07:51:24,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,626 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:24,626 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:24,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.841276168823242
2023-01-07 07:51:24,628 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 529.39990234375
2023-01-07 07:51:24,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,628 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,628 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:24,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.655439376831055
2023-01-07 07:51:24,630 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 17.112319946289062
2023-01-07 07:51:24,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,630 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,630 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,630 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.276947021484375
2023-01-07 07:51:24,632 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 127.19998168945312
2023-01-07 07:51:24,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,633 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,633 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.345441818237305
2023-01-07 07:51:24,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.999834060668945
2023-01-07 07:51:24,634 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.345441818237305
2023-01-07 07:51:24,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,635 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:24,635 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,635 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,635 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.8458499908447266
2023-01-07 07:51:24,637 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 126.20006561279297
2023-01-07 07:51:24,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,637 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,637 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -22.915189743041992
2023-01-07 07:51:24,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.455221176147461
2023-01-07 07:51:24,639 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -22.915189743041992
2023-01-07 07:51:24,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,639 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,639 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,639 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:24,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.323076248168945
2023-01-07 07:51:24,641 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 538.9991455078125
2023-01-07 07:51:24,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,641 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,641 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:24,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.885258436203003
2023-01-07 07:51:24,643 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -16.715007781982422
2023-01-07 07:51:24,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,643 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,643 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,643 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.301576614379883
2023-01-07 07:51:24,645 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 126.79986572265625
2023-01-07 07:51:24,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,646 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,646 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8379263877868652
2023-01-07 07:51:24,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.341983795166016
2023-01-07 07:51:24,647 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 0.8379263877868652
2023-01-07 07:51:24,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,647 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:24,648 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,648 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.122353434562683
2023-01-07 07:51:24,649 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.60020446777344
2023-01-07 07:51:24,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,650 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,650 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,650 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,650 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -9.415725708007812
2023-01-07 07:51:24,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1543221473693848
2023-01-07 07:51:24,651 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -9.415725708007812
2023-01-07 07:51:24,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,652 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,652 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:51:24,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,652 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3567070960998535
2023-01-07 07:51:24,654 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 514.998291015625
2023-01-07 07:51:24,654 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,654 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,654 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 13.198230743408203
2023-01-07 07:51:24,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1391037702560425
2023-01-07 07:51:24,656 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 13.198230743408203
2023-01-07 07:51:24,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,656 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,656 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,657 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,657 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.32223892211914
2023-01-07 07:51:24,658 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.40054321289062
2023-01-07 07:51:24,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,659 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,659 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.31062126159668
2023-01-07 07:51:24,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.440120697021484
2023-01-07 07:51:24,660 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 15.31062126159668
2023-01-07 07:51:24,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,661 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:24,661 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,661 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,661 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:51:24,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,661 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.469182968139648
2023-01-07 07:51:24,663 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 131.00009155273438
2023-01-07 07:51:24,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,663 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:24,663 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -7.202464580535889
2023-01-07 07:51:24,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5331413745880127
2023-01-07 07:51:24,664 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -7.202464580535889
2023-01-07 07:51:24,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,665 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:24,665 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:51:24,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,665 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.6300323009490967
2023-01-07 07:51:24,667 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 509.97918701171875
2023-01-07 07:51:24,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,667 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,667 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -5.41171932220459
2023-01-07 07:51:24,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3070051670074463
2023-01-07 07:51:24,669 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.41171932220459
2023-01-07 07:51:24,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,669 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:24,669 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:51:24,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,670 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.885345458984375
2023-01-07 07:51:24,672 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 254.99942016601562
2023-01-07 07:51:24,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,672 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,672 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 1.5752387046813965
2023-01-07 07:51:24,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.629405498504639
2023-01-07 07:51:24,674 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 1.5752387046813965
2023-01-07 07:51:24,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,674 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,674 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:24,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.673807144165039
2023-01-07 07:51:24,675 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 282.39947509765625
2023-01-07 07:51:24,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,676 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,676 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:24,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5074994564056396
2023-01-07 07:51:24,677 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 0.1080312728881836
2023-01-07 07:51:24,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,678 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,678 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:24,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.820110321044922
2023-01-07 07:51:24,679 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1158.60009765625
2023-01-07 07:51:24,679 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,680 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,680 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:24,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32513976097106934
2023-01-07 07:51:24,681 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -23.51654815673828
2023-01-07 07:51:24,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,681 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:24,681 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:24,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.877489686012268
2023-01-07 07:51:24,683 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1167.9974365234375
2023-01-07 07:51:24,683 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,683 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,683 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:24,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.372288465499878
2023-01-07 07:51:24,685 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.361846923828125
2023-01-07 07:51:24,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,685 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,685 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:51:24,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,686 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:51:24,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.764867782592773
2023-01-07 07:51:24,687 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 258.00030517578125
2023-01-07 07:51:24,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,688 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,688 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,688 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.066387176513672
2023-01-07 07:51:24,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9787812232971191
2023-01-07 07:51:24,689 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.066387176513672
2023-01-07 07:51:24,689 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,690 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,690 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,690 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,690 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:24,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.652551651000977
2023-01-07 07:51:24,691 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 293.3998718261719
2023-01-07 07:51:24,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,692 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,692 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,692 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:24,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7883167266845703
2023-01-07 07:51:24,693 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 8.936910629272461
2023-01-07 07:51:24,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,693 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,694 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,694 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:24,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.889991760253906
2023-01-07 07:51:24,695 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1187.399658203125
2023-01-07 07:51:24,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,695 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,695 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:24,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.958366394042969
2023-01-07 07:51:24,697 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.354618072509766
2023-01-07 07:51:24,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,697 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,697 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:24,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.310635089874268
2023-01-07 07:51:24,699 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 295.5997314453125
2023-01-07 07:51:24,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,699 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,699 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:24,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1041464805603027
2023-01-07 07:51:24,701 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 31.165061950683594
2023-01-07 07:51:24,701 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,701 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,701 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:24,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12733761966228485
2023-01-07 07:51:24,703 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 295.999755859375
2023-01-07 07:51:24,703 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,703 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,703 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,703 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:24,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2758556306362152
2023-01-07 07:51:24,705 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 22.767589569091797
2023-01-07 07:51:24,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,705 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,705 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:24,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8685375452041626
2023-01-07 07:51:24,706 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1197.3974609375
2023-01-07 07:51:24,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,707 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,707 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:24,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7121410369873047
2023-01-07 07:51:24,708 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -53.85360336303711
2023-01-07 07:51:24,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,709 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,709 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:24,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.606222152709961
2023-01-07 07:51:24,710 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 297.000732421875
2023-01-07 07:51:24,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,711 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,711 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:24,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.3451972007751465
2023-01-07 07:51:24,712 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -16.88102149963379
2023-01-07 07:51:24,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,713 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,713 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:24,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6378605365753174
2023-01-07 07:51:24,714 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 299.19976806640625
2023-01-07 07:51:24,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,714 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,714 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:24,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.627442479133606
2023-01-07 07:51:24,716 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -9.834939956665039
2023-01-07 07:51:24,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,716 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,716 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:24,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.620715141296387
2023-01-07 07:51:24,718 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1207.799072265625
2023-01-07 07:51:24,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,718 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,718 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,719 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:24,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8523335456848145
2023-01-07 07:51:24,720 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 58.93596649169922
2023-01-07 07:51:24,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,720 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,720 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:24,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5119166374206543
2023-01-07 07:51:24,722 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 300.60137939453125
2023-01-07 07:51:24,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,722 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,722 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:24,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.013091742992401123
2023-01-07 07:51:24,724 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 33.402408599853516
2023-01-07 07:51:24,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,724 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,724 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:24,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15315033495426178
2023-01-07 07:51:24,725 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 302.79949951171875
2023-01-07 07:51:24,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,726 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,726 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:24,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.817960798740387
2023-01-07 07:51:24,727 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.320283889770508
2023-01-07 07:51:24,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,728 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,728 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,728 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:24,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15412822365760803
2023-01-07 07:51:24,729 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1212.394775390625
2023-01-07 07:51:24,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,730 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,730 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:24,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3282536268234253
2023-01-07 07:51:24,731 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 78.32042694091797
2023-01-07 07:51:24,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,732 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,732 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:24,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8523010015487671
2023-01-07 07:51:24,733 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 302.3998718261719
2023-01-07 07:51:24,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,734 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,734 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,734 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:24,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030568420886993408
2023-01-07 07:51:24,735 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 15.271093368530273
2023-01-07 07:51:24,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,736 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:24,736 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:24,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,736 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6007483601570129
2023-01-07 07:51:24,737 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 303.3995666503906
2023-01-07 07:51:24,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,738 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:24,738 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:24,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,738 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11533001810312271
2023-01-07 07:51:24,739 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 4.744016647338867
2023-01-07 07:51:24,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,740 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:24,740 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:24,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5712593793869019
2023-01-07 07:51:24,741 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1217.1968994140625
2023-01-07 07:51:24,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,741 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:24,741 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:24,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44323796033859253
2023-01-07 07:51:24,743 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 95.67549133300781
2023-01-07 07:51:24,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,743 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:24,743 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:24,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8079417943954468
2023-01-07 07:51:24,745 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 608.3995971679688
2023-01-07 07:51:24,745 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,745 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,745 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:24,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3987978994846344
2023-01-07 07:51:24,747 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -38.519996643066406
2023-01-07 07:51:24,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,747 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:24,747 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:24,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9952840209007263
2023-01-07 07:51:24,749 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 610.598876953125
2023-01-07 07:51:24,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,749 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,749 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:24,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.025332249701023102
2023-01-07 07:51:24,751 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -1.7011375427246094
2023-01-07 07:51:24,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,751 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:24,751 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:24,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,752 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2006080597639084
2023-01-07 07:51:24,752 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2449.989501953125
2023-01-07 07:51:24,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,753 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:24,753 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:24,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,753 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5322480797767639
2023-01-07 07:51:24,754 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -46.188575744628906
2023-01-07 07:51:24,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,755 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:51:24,755 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,755 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,755 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:24,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40543296933174133
2023-01-07 07:51:24,756 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2450.19775390625
2023-01-07 07:51:24,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,756 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:24,756 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,757 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:24,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07170380651950836
2023-01-07 07:51:24,758 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -52.732200622558594
2023-01-07 07:51:24,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,758 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:24,758 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,759 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:24,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.019161708652973175
2023-01-07 07:51:24,760 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 610.9989624023438
2023-01-07 07:51:24,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,760 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,760 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,760 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:24,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5967917442321777
2023-01-07 07:51:24,762 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.901889801025391
2023-01-07 07:51:24,762 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,762 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:24,762 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:24,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1575523316860199
2023-01-07 07:51:24,763 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 610.7996826171875
2023-01-07 07:51:24,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,764 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,764 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:24,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15121568739414215
2023-01-07 07:51:24,765 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 41.01795959472656
2023-01-07 07:51:24,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,766 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:24,766 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:24,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9763723611831665
2023-01-07 07:51:24,767 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2451.19873046875
2023-01-07 07:51:24,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,768 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:24,768 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:24,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5600622892379761
2023-01-07 07:51:24,770 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -25.142528533935547
2023-01-07 07:51:24,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,770 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:24,770 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:24,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3304317593574524
2023-01-07 07:51:24,771 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 611.9993896484375
2023-01-07 07:51:24,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,772 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,772 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,772 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,772 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:24,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5254098176956177
2023-01-07 07:51:24,773 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 51.97746276855469
2023-01-07 07:51:24,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,774 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:24,774 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,774 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:24,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,774 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12421693652868271
2023-01-07 07:51:24,775 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 611.7993774414062
2023-01-07 07:51:24,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,776 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:24,776 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:24,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.029832497239112854
2023-01-07 07:51:24,777 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.2801971435546875
2023-01-07 07:51:24,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,778 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:24,778 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:24,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.3709588050842285
2023-01-07 07:51:24,779 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2453.1923828125
2023-01-07 07:51:24,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,779 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:24,780 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,780 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,780 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:24,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8401862382888794
2023-01-07 07:51:24,781 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 4.861334800720215
2023-01-07 07:51:24,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,782 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:51:24,782 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:24,783 > [DEBUG] 0 :: 7.489527225494385
2023-01-07 07:51:24,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,786 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -643.473388671875
2023-01-07 07:51:24,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,788 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -638.7243041992188
2023-01-07 07:51:24,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,793 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.39514923095703
2023-01-07 07:51:24,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,795 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -414.6531066894531
2023-01-07 07:51:24,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,797 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -178.51756286621094
2023-01-07 07:51:24,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,799 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,799 > [DEBUG] 0 :: before allreduce fusion buffer :: -334.65679931640625
2023-01-07 07:51:24,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,801 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,801 > [DEBUG] 0 :: before allreduce fusion buffer :: -206.64808654785156
2023-01-07 07:51:24,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,802 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -248.26202392578125
2023-01-07 07:51:24,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,804 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,804 > [DEBUG] 0 :: before allreduce fusion buffer :: -91.53059387207031
2023-01-07 07:51:24,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,805 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,805 > [DEBUG] 0 :: before allreduce fusion buffer :: -413.81890869140625
2023-01-07 07:51:24,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,807 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,807 > [DEBUG] 0 :: before allreduce fusion buffer :: -186.46058654785156
2023-01-07 07:51:24,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,808 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -360.78369140625
2023-01-07 07:51:24,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,810 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -251.39651489257812
2023-01-07 07:51:24,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,811 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -245.18052673339844
2023-01-07 07:51:24,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,814 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -257.06988525390625
2023-01-07 07:51:24,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,815 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -250.001708984375
2023-01-07 07:51:24,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,817 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,817 > [DEBUG] 0 :: before allreduce fusion buffer :: -125.96614837646484
2023-01-07 07:51:24,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,818 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,819 > [DEBUG] 0 :: before allreduce fusion buffer :: -411.5789794921875
2023-01-07 07:51:24,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,820 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,820 > [DEBUG] 0 :: before allreduce fusion buffer :: -193.05667114257812
2023-01-07 07:51:24,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,821 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,822 > [DEBUG] 0 :: before allreduce fusion buffer :: -383.2310791015625
2023-01-07 07:51:24,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,823 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -190.64617919921875
2023-01-07 07:51:24,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,825 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -337.21990966796875
2023-01-07 07:51:24,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,827 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -124.79631042480469
2023-01-07 07:51:24,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,828 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -409.167724609375
2023-01-07 07:51:24,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,830 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,830 > [DEBUG] 0 :: before allreduce fusion buffer :: -120.80256652832031
2023-01-07 07:51:24,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,831 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -369.67059326171875
2023-01-07 07:51:24,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,833 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,833 > [DEBUG] 0 :: before allreduce fusion buffer :: -181.5650177001953
2023-01-07 07:51:24,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,834 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -332.3089599609375
2023-01-07 07:51:24,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,836 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,836 > [DEBUG] 0 :: before allreduce fusion buffer :: -121.30408477783203
2023-01-07 07:51:24,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,837 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -409.83984375
2023-01-07 07:51:24,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,839 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,839 > [DEBUG] 0 :: before allreduce fusion buffer :: -117.45801544189453
2023-01-07 07:51:24,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,840 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,841 > [DEBUG] 0 :: before allreduce fusion buffer :: -364.89483642578125
2023-01-07 07:51:24,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,842 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,842 > [DEBUG] 0 :: before allreduce fusion buffer :: -178.64511108398438
2023-01-07 07:51:24,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,843 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,844 > [DEBUG] 0 :: before allreduce fusion buffer :: -336.205810546875
2023-01-07 07:51:24,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,845 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -115.38983154296875
2023-01-07 07:51:24,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,846 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -410.4089660644531
2023-01-07 07:51:24,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,848 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -114.56813049316406
2023-01-07 07:51:24,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,850 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,850 > [DEBUG] 0 :: before allreduce fusion buffer :: -364.24249267578125
2023-01-07 07:51:24,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,851 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -177.833251953125
2023-01-07 07:51:24,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,853 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -336.5600891113281
2023-01-07 07:51:24,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,855 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -103.61390686035156
2023-01-07 07:51:24,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,856 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,856 > [DEBUG] 0 :: before allreduce fusion buffer :: -404.68145751953125
2023-01-07 07:51:24,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,858 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -95.47630310058594
2023-01-07 07:51:24,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,859 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -344.82086181640625
2023-01-07 07:51:24,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,861 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,861 > [DEBUG] 0 :: before allreduce fusion buffer :: -159.99020385742188
2023-01-07 07:51:24,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,862 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -327.59246826171875
2023-01-07 07:51:24,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,864 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -77.37467956542969
2023-01-07 07:51:24,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,865 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,865 > [DEBUG] 0 :: before allreduce fusion buffer :: -405.8194580078125
2023-01-07 07:51:24,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,867 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,867 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.19831848144531
2023-01-07 07:51:24,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -333.6553649902344
2023-01-07 07:51:24,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,869 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -128.78445434570312
2023-01-07 07:51:24,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,871 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -371.47515869140625
2023-01-07 07:51:24,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,873 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,873 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.207664489746094
2023-01-07 07:51:24,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,874 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -299.06707763671875
2023-01-07 07:51:24,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,876 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,876 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.981964111328125
2023-01-07 07:51:24,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,877 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -430.2304382324219
2023-01-07 07:51:24,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,879 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,879 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.105262756347656
2023-01-07 07:51:24,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,880 > [DEBUG] 0 :: before allreduce fusion buffer :: -306.07562255859375
2023-01-07 07:51:24,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,881 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -73.86975860595703
2023-01-07 07:51:24,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -243.50228881835938
2023-01-07 07:51:24,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,884 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,884 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.49000549316406
2023-01-07 07:51:24,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -281.8978271484375
2023-01-07 07:51:24,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,886 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.68402099609375
2023-01-07 07:51:24,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,887 > [DEBUG] 0 :: before allreduce fusion buffer :: -315.08447265625
2023-01-07 07:51:24,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,888 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.69127655029297
2023-01-07 07:51:24,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,890 > [DEBUG] 0 :: before allreduce fusion buffer :: -236.92457580566406
2023-01-07 07:51:24,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,891 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,891 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.63043975830078
2023-01-07 07:51:24,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -273.8370056152344
2023-01-07 07:51:24,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,893 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,893 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.69587707519531
2023-01-07 07:51:24,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,895 > [DEBUG] 0 :: before allreduce fusion buffer :: -325.1922607421875
2023-01-07 07:51:24,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,896 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.863006591796875
2023-01-07 07:51:24,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,897 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -197.14358520507812
2023-01-07 07:51:24,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,899 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,899 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.226905822753906
2023-01-07 07:51:24,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,900 > [DEBUG] 0 :: before allreduce fusion buffer :: -278.37115478515625
2023-01-07 07:51:24,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,902 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,902 > [DEBUG] 0 :: before allreduce fusion buffer :: -78.6370849609375
2023-01-07 07:51:24,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,903 > [DEBUG] 0 :: before allreduce fusion buffer :: -350.86187744140625
2023-01-07 07:51:24,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,904 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,904 > [DEBUG] 0 :: before allreduce fusion buffer :: -65.07551574707031
2023-01-07 07:51:24,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,905 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,906 > [DEBUG] 0 :: before allreduce fusion buffer :: -274.897705078125
2023-01-07 07:51:24,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,907 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,907 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.90655517578125
2023-01-07 07:51:24,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -262.761474609375
2023-01-07 07:51:24,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,909 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,910 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.95401382446289
2023-01-07 07:51:24,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,911 > [DEBUG] 0 :: before allreduce fusion buffer :: -346.65545654296875
2023-01-07 07:51:24,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,912 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,912 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.31402587890625
2023-01-07 07:51:24,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,913 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,913 > [DEBUG] 0 :: before allreduce fusion buffer :: -208.6102294921875
2023-01-07 07:51:24,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.50531005859375
2023-01-07 07:51:24,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,916 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,916 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,917 > [DEBUG] 0 :: before allreduce fusion buffer :: -348.9587097167969
2023-01-07 07:51:24,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,918 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.35869598388672
2023-01-07 07:51:24,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -304.24517822265625
2023-01-07 07:51:24,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,921 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,921 > [DEBUG] 0 :: before allreduce fusion buffer :: -77.35183715820312
2023-01-07 07:51:24,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,922 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,922 > [DEBUG] 0 :: before allreduce fusion buffer :: -174.5475311279297
2023-01-07 07:51:24,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,924 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.56658935546875
2023-01-07 07:51:24,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,925 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,925 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,925 > [DEBUG] 0 :: before allreduce fusion buffer :: -363.95245361328125
2023-01-07 07:51:24,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.373905181884766
2023-01-07 07:51:24,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,928 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,928 > [DEBUG] 0 :: before allreduce fusion buffer :: -384.9413146972656
2023-01-07 07:51:24,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,930 > [DEBUG] 0 :: before allreduce fusion buffer :: -101.28418731689453
2023-01-07 07:51:24,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,931 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -328.9320068359375
2023-01-07 07:51:24,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,932 > [DEBUG] 0 :: before allreduce fusion buffer :: -134.3629150390625
2023-01-07 07:51:24,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,933 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,934 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,934 > [DEBUG] 0 :: before allreduce fusion buffer :: -516.1396484375
2023-01-07 07:51:24,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,936 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.32560729980469
2023-01-07 07:51:24,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,937 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,937 > [DEBUG] 0 :: before allreduce fusion buffer :: -382.1005859375
2023-01-07 07:51:24,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,938 > [DEBUG] 0 :: before allreduce fusion buffer :: -91.54986572265625
2023-01-07 07:51:24,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,939 > [DEBUG] 0 :: before allreduce fusion buffer :: -458.1307373046875
2023-01-07 07:51:24,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,941 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,941 > [DEBUG] 0 :: before allreduce fusion buffer :: -115.90534973144531
2023-01-07 07:51:24,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,942 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,942 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.40976333618164
2023-01-07 07:51:24,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,944 > [DEBUG] 0 :: before allreduce fusion buffer :: -386.4580078125
2023-01-07 07:51:24,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -198.06195068359375
2023-01-07 07:51:24,950 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:51:24,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,950 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:24,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 969.6982421875
2023-01-07 07:51:24,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 432.86505126953125
2023-01-07 07:51:24,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 1018.9561767578125
2023-01-07 07:51:24,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 2001.314453125
2023-01-07 07:51:24,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:24,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:24,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 4735.3544921875
2023-01-07 07:51:24,968 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 123.19091033935547
2023-01-07 07:51:24,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:24,968 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:51:24,968 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:24,968 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,811 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 66.0
2023-01-07 07:51:25,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,811 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -213.58087158203125
2023-01-07 07:51:25,813 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 66.66548156738281
2023-01-07 07:51:25,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,813 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,813 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,813 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,814 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 25.346630096435547
2023-01-07 07:51:25,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,814 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.3267822265625
2023-01-07 07:51:25,816 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 21.554309844970703
2023-01-07 07:51:25,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,816 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:25,816 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,816 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,816 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:25,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.58904457092285
2023-01-07 07:51:25,817 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 69.61276245117188
2023-01-07 07:51:25,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,818 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,818 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,818 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,818 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:25,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 266.059814453125
2023-01-07 07:51:25,820 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.014472007751465
2023-01-07 07:51:25,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,820 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:25,820 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,820 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 65.00004577636719
2023-01-07 07:51:25,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,820 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,821 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,821 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.31639099121094
2023-01-07 07:51:25,823 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 66.23779296875
2023-01-07 07:51:25,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,823 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,823 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,823 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,823 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:25,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.8168182373046875
2023-01-07 07:51:25,825 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -14.238682746887207
2023-01-07 07:51:25,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,825 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,825 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,825 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,825 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 257.0
2023-01-07 07:51:25,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,825 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.30104446411133
2023-01-07 07:51:25,827 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 258.24591064453125
2023-01-07 07:51:25,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,827 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,827 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,828 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,828 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:25,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -83.75697326660156
2023-01-07 07:51:25,829 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -2.916147232055664
2023-01-07 07:51:25,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,829 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,829 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,829 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:25,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 101.94835662841797
2023-01-07 07:51:25,831 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 280.1888732910156
2023-01-07 07:51:25,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,831 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,831 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,831 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,831 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:25,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,832 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.69454574584961
2023-01-07 07:51:25,833 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.799287796020508
2023-01-07 07:51:25,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,833 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,833 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,833 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,833 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 63.800018310546875
2023-01-07 07:51:25,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,834 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.73957824707031
2023-01-07 07:51:25,835 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 63.800018310546875
2023-01-07 07:51:25,835 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,836 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,836 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,836 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,836 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:25,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,836 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.77082824707031
2023-01-07 07:51:25,837 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.5919523239135742
2023-01-07 07:51:25,838 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,838 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:25,838 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,838 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,838 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.19999694824219
2023-01-07 07:51:25,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,838 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.761310577392578
2023-01-07 07:51:25,840 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.46360778808594
2023-01-07 07:51:25,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,840 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,840 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,840 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,841 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:25,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,841 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.662433624267578
2023-01-07 07:51:25,842 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.788993835449219
2023-01-07 07:51:25,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,842 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,842 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,842 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,843 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:25,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.465334892272949
2023-01-07 07:51:25,844 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 272.5864562988281
2023-01-07 07:51:25,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,844 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,844 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,844 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,844 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:25,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.96707534790039
2023-01-07 07:51:25,846 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 30.836856842041016
2023-01-07 07:51:25,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,846 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,846 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,847 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:25,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.219230651855469
2023-01-07 07:51:25,848 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 67.81228637695312
2023-01-07 07:51:25,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,848 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,848 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,848 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,848 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:25,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.90850830078125
2023-01-07 07:51:25,850 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 5.077452659606934
2023-01-07 07:51:25,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,850 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:25,850 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,850 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,851 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.59999084472656
2023-01-07 07:51:25,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,851 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,851 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.464542388916016
2023-01-07 07:51:25,853 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.62458801269531
2023-01-07 07:51:25,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,853 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:25,853 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,853 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,854 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:25,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.58995819091797
2023-01-07 07:51:25,855 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -30.000669479370117
2023-01-07 07:51:25,855 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,855 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:25,855 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,856 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 57.94667434692383
2023-01-07 07:51:25,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6769208908081055
2023-01-07 07:51:25,857 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 278.97802734375
2023-01-07 07:51:25,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,857 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,857 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,857 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,858 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:25,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.977500915527344
2023-01-07 07:51:25,859 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 57.94667434692383
2023-01-07 07:51:25,859 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,859 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:51:25,859 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,859 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,860 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:25,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.119270324707031
2023-01-07 07:51:25,861 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 140.39932250976562
2023-01-07 07:51:25,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,861 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,861 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,861 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:25,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.69331359863281
2023-01-07 07:51:25,863 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 11.273445129394531
2023-01-07 07:51:25,863 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,863 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:25,863 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 126.99999237060547
2023-01-07 07:51:25,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,864 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.35979461669922
2023-01-07 07:51:25,865 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 125.87745666503906
2023-01-07 07:51:25,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,866 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,866 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,866 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,866 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.352242469787598
2023-01-07 07:51:25,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,866 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.081113815307617
2023-01-07 07:51:25,867 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.352242469787598
2023-01-07 07:51:25,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,869 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,869 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,869 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,869 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.6004028320312
2023-01-07 07:51:25,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,869 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,869 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.863493919372559
2023-01-07 07:51:25,871 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 513.2608642578125
2023-01-07 07:51:25,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,871 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,871 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,871 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,871 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 43.89067077636719
2023-01-07 07:51:25,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.569127082824707
2023-01-07 07:51:25,873 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 43.89067077636719
2023-01-07 07:51:25,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,873 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:25,873 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,873 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,873 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:25,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.539312362670898
2023-01-07 07:51:25,874 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 559.7200927734375
2023-01-07 07:51:25,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,875 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,875 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,875 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,875 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:25,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.962871551513672
2023-01-07 07:51:25,876 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 17.112319946289062
2023-01-07 07:51:25,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,877 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,877 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,877 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,877 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.19998168945312
2023-01-07 07:51:25,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,877 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,878 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.88055419921875
2023-01-07 07:51:25,879 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 126.1116943359375
2023-01-07 07:51:25,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,879 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,879 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,879 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,880 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.345441818237305
2023-01-07 07:51:25,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.595401763916016
2023-01-07 07:51:25,881 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.345441818237305
2023-01-07 07:51:25,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,881 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:25,881 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,881 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,882 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 126.20006561279297
2023-01-07 07:51:25,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,882 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.009830474853516
2023-01-07 07:51:25,883 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 124.01240539550781
2023-01-07 07:51:25,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,884 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,884 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,884 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,884 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -22.915189743041992
2023-01-07 07:51:25,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,884 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.397043228149414
2023-01-07 07:51:25,885 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -22.915189743041992
2023-01-07 07:51:25,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,886 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,886 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,886 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,886 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:25,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1544618606567383
2023-01-07 07:51:25,887 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 570.6831665039062
2023-01-07 07:51:25,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,888 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,888 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,888 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,888 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:25,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.058026313781738
2023-01-07 07:51:25,889 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -16.715007781982422
2023-01-07 07:51:25,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,890 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,890 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,890 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,890 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 126.79986572265625
2023-01-07 07:51:25,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,890 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.15835952758789
2023-01-07 07:51:25,892 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 126.15503692626953
2023-01-07 07:51:25,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,892 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,892 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,892 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,892 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8379263877868652
2023-01-07 07:51:25,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.45420837402344
2023-01-07 07:51:25,894 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 0.8379263877868652
2023-01-07 07:51:25,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,894 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:25,894 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,894 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,894 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 127.60020446777344
2023-01-07 07:51:25,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,895 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,895 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.487649917602539
2023-01-07 07:51:25,896 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.55474853515625
2023-01-07 07:51:25,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,897 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,897 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,897 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,897 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -9.415725708007812
2023-01-07 07:51:25,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7880175113677979
2023-01-07 07:51:25,898 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -9.415725708007812
2023-01-07 07:51:25,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,899 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,899 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,899 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,899 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 514.998291015625
2023-01-07 07:51:25,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,899 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6248282194137573
2023-01-07 07:51:25,901 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 518.4764404296875
2023-01-07 07:51:25,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,901 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,901 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,901 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,901 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 13.198230743408203
2023-01-07 07:51:25,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,901 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.495291709899902
2023-01-07 07:51:25,903 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 13.198230743408203
2023-01-07 07:51:25,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,903 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,903 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,903 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,903 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.40054321289062
2023-01-07 07:51:25,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,903 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.732589721679688
2023-01-07 07:51:25,905 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.462646484375
2023-01-07 07:51:25,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,905 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,905 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,906 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,906 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.31062126159668
2023-01-07 07:51:25,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.577348709106445
2023-01-07 07:51:25,907 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 15.31062126159668
2023-01-07 07:51:25,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,908 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:25,908 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,908 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,908 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 131.00009155273438
2023-01-07 07:51:25,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,908 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,909 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.782495498657227
2023-01-07 07:51:25,910 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 130.0646209716797
2023-01-07 07:51:25,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,910 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:25,910 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,910 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,910 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -7.202464580535889
2023-01-07 07:51:25,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,911 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8216851949691772
2023-01-07 07:51:25,912 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -7.202464580535889
2023-01-07 07:51:25,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,912 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:25,912 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,912 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,912 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 509.97918701171875
2023-01-07 07:51:25,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,913 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,913 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.8238584995269775
2023-01-07 07:51:25,914 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 508.42218017578125
2023-01-07 07:51:25,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,915 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,915 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,915 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,915 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -5.41171932220459
2023-01-07 07:51:25,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.549198150634766
2023-01-07 07:51:25,916 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.41171932220459
2023-01-07 07:51:25,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,917 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:25,917 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,917 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,917 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 254.99942016601562
2023-01-07 07:51:25,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,917 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.202237129211426
2023-01-07 07:51:25,919 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 252.71026611328125
2023-01-07 07:51:25,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,919 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,919 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,919 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,919 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 1.5752387046813965
2023-01-07 07:51:25,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3909432888031006
2023-01-07 07:51:25,921 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 1.5752387046813965
2023-01-07 07:51:25,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,921 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,921 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,921 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,921 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:25,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,922 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18644773960113525
2023-01-07 07:51:25,923 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 312.89691162109375
2023-01-07 07:51:25,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,923 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,923 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,923 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,923 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:25,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,924 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13816609978675842
2023-01-07 07:51:25,925 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 0.1080312728881836
2023-01-07 07:51:25,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,925 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,925 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,925 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,925 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:25,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,926 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.454690933227539
2023-01-07 07:51:25,926 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1206.61962890625
2023-01-07 07:51:25,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,927 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,927 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,927 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,927 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:25,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6368722915649414
2023-01-07 07:51:25,928 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -23.51654815673828
2023-01-07 07:51:25,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,929 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:25,929 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,929 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,929 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:25,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,929 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.080294370651245
2023-01-07 07:51:25,930 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1257.7464599609375
2023-01-07 07:51:25,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,931 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,931 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,931 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,931 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:25,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5293229818344116
2023-01-07 07:51:25,932 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.361846923828125
2023-01-07 07:51:25,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,933 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,933 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,933 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,933 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 258.00030517578125
2023-01-07 07:51:25,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,933 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:51:25,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9652692079544067
2023-01-07 07:51:25,935 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 258.60089111328125
2023-01-07 07:51:25,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,935 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,935 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,935 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,935 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.066387176513672
2023-01-07 07:51:25,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.646133303642273
2023-01-07 07:51:25,937 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.066387176513672
2023-01-07 07:51:25,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,937 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,937 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,937 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,937 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:25,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,938 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9021103382110596
2023-01-07 07:51:25,939 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 328.79534912109375
2023-01-07 07:51:25,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,939 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,939 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,939 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,939 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:25,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6224413514137268
2023-01-07 07:51:25,941 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 8.936910629272461
2023-01-07 07:51:25,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,941 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,941 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,941 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,941 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:25,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.801257610321045
2023-01-07 07:51:25,942 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1313.76416015625
2023-01-07 07:51:25,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,943 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,943 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,943 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,943 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:25,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5276174545288086
2023-01-07 07:51:25,944 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.354618072509766
2023-01-07 07:51:25,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,945 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,945 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,945 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,945 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:25,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.199716091156006
2023-01-07 07:51:25,946 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 331.8461608886719
2023-01-07 07:51:25,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,947 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,947 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,947 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,947 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:25,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,947 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3381048440933228
2023-01-07 07:51:25,948 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 31.165061950683594
2023-01-07 07:51:25,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,949 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,949 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,949 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,949 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:25,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4704068899154663
2023-01-07 07:51:25,950 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 333.44305419921875
2023-01-07 07:51:25,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,951 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,951 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,951 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,951 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:25,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.583088755607605
2023-01-07 07:51:25,952 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 22.767589569091797
2023-01-07 07:51:25,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,953 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,953 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,953 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,953 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:25,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8061339855194092
2023-01-07 07:51:25,954 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1340.455322265625
2023-01-07 07:51:25,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,954 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,954 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,954 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,955 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:25,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8271114826202393
2023-01-07 07:51:25,956 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -53.85360336303711
2023-01-07 07:51:25,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,956 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,956 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,957 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,957 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:25,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7447311282157898
2023-01-07 07:51:25,958 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 336.37548828125
2023-01-07 07:51:25,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,958 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,958 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,958 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,958 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:25,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,959 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.802069664001465
2023-01-07 07:51:25,960 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -16.88102149963379
2023-01-07 07:51:25,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,960 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,960 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,960 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,960 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:25,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.828930377960205
2023-01-07 07:51:25,962 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 340.2831115722656
2023-01-07 07:51:25,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,962 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,962 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,962 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,962 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:25,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,963 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03840634226799011
2023-01-07 07:51:25,964 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -9.834939956665039
2023-01-07 07:51:25,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,964 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,964 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,964 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,964 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:25,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,965 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1264742612838745
2023-01-07 07:51:25,965 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1377.7994384765625
2023-01-07 07:51:25,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,966 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,966 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,966 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,966 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:25,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37879422307014465
2023-01-07 07:51:25,968 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 58.93596649169922
2023-01-07 07:51:25,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,968 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,968 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,968 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,968 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:25,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8216782808303833
2023-01-07 07:51:25,969 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 342.4794921875
2023-01-07 07:51:25,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,970 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,970 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,970 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,970 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:25,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5017825365066528
2023-01-07 07:51:25,971 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 33.402408599853516
2023-01-07 07:51:25,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,972 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,972 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,972 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,972 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:25,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,972 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.873578667640686
2023-01-07 07:51:25,973 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 343.08697509765625
2023-01-07 07:51:25,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,974 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,974 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,974 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,974 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:25,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.857729434967041
2023-01-07 07:51:25,975 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.320283889770508
2023-01-07 07:51:25,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,976 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,976 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,976 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,976 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:25,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,976 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0010786056518555
2023-01-07 07:51:25,977 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1363.0859375
2023-01-07 07:51:25,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,978 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,978 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,978 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,978 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:25,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5491631031036377
2023-01-07 07:51:25,979 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 78.32042694091797
2023-01-07 07:51:25,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,980 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,980 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,980 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,980 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:25,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,980 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2906994819641113
2023-01-07 07:51:25,981 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 347.1927795410156
2023-01-07 07:51:25,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,981 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,982 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,982 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,982 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:25,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16736909747123718
2023-01-07 07:51:25,983 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 15.271093368530273
2023-01-07 07:51:25,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,983 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:25,984 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,984 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,984 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:25,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24873086810112
2023-01-07 07:51:25,985 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 347.8962097167969
2023-01-07 07:51:25,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,985 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:25,985 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,985 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,986 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:25,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2447679042816162
2023-01-07 07:51:25,987 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 4.744016647338867
2023-01-07 07:51:25,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,987 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:25,987 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,987 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,988 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:25,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,988 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5753623247146606
2023-01-07 07:51:25,989 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1404.6549072265625
2023-01-07 07:51:25,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,989 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:25,989 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,989 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,989 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:25,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2642406225204468
2023-01-07 07:51:25,991 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 95.67549133300781
2023-01-07 07:51:25,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,991 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:25,991 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,991 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,991 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:25,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.088867425918579
2023-01-07 07:51:25,993 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 691.2769775390625
2023-01-07 07:51:25,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,993 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,993 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,993 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,993 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:25,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5270751118659973
2023-01-07 07:51:25,995 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -38.519996643066406
2023-01-07 07:51:25,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,995 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:25,995 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,995 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,995 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:25,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06382317841053009
2023-01-07 07:51:25,997 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 701.385498046875
2023-01-07 07:51:25,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,997 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:25,997 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,997 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,997 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:25,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:25,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:25,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33969050645828247
2023-01-07 07:51:25,999 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -1.7011375427246094
2023-01-07 07:51:25,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:25,999 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:25,999 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:25,999 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:25,999 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:25,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7065831422805786
2023-01-07 07:51:26,001 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2826.574462890625
2023-01-07 07:51:26,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,001 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:26,001 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,001 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,001 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:26,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6142183542251587
2023-01-07 07:51:26,002 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -46.188575744628906
2023-01-07 07:51:26,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,003 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:51:26,003 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,003 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,003 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:26,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.485215425491333
2023-01-07 07:51:26,004 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2836.03125
2023-01-07 07:51:26,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,005 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:26,005 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,005 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,005 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:26,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44889721274375916
2023-01-07 07:51:26,006 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -52.732200622558594
2023-01-07 07:51:26,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,007 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:26,007 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,007 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,007 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:26,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,007 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03966040536761284
2023-01-07 07:51:26,008 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 706.654052734375
2023-01-07 07:51:26,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,009 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:26,009 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,009 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,009 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:26,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5735414624214172
2023-01-07 07:51:26,010 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.901889801025391
2023-01-07 07:51:26,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,011 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:26,011 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,011 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,011 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:26,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,011 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5632159113883972
2023-01-07 07:51:26,012 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 708.8382568359375
2023-01-07 07:51:26,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,012 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:26,013 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,013 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,013 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:26,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18664035201072693
2023-01-07 07:51:26,014 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 41.01795959472656
2023-01-07 07:51:26,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,014 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:26,015 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,015 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:26,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1200677156448364
2023-01-07 07:51:26,016 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2840.0107421875
2023-01-07 07:51:26,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,016 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:26,016 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,016 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:26,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09211591631174088
2023-01-07 07:51:26,018 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -25.142528533935547
2023-01-07 07:51:26,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,018 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:26,018 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,018 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:26,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45742571353912354
2023-01-07 07:51:26,020 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 711.151611328125
2023-01-07 07:51:26,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,020 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:26,020 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,020 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,020 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:26,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3626747727394104
2023-01-07 07:51:26,022 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 51.97746276855469
2023-01-07 07:51:26,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,022 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:26,022 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,022 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,022 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:26,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05592760071158409
2023-01-07 07:51:26,024 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 712.1198120117188
2023-01-07 07:51:26,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,024 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:26,024 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,024 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:26,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,025 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06567847728729248
2023-01-07 07:51:26,026 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.2801971435546875
2023-01-07 07:51:26,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,026 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:26,026 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:26,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.82093620300293
2023-01-07 07:51:26,027 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2836.0087890625
2023-01-07 07:51:26,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,028 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:26,028 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,028 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,028 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:26,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9129486083984375
2023-01-07 07:51:26,030 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 4.861334800720215
2023-01-07 07:51:26,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,030 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:51:26,030 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:26,031 > [DEBUG] 0 :: 7.826211929321289
2023-01-07 07:51:26,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,035 > [DEBUG] 0 :: before allreduce fusion buffer :: -1131.183349609375
2023-01-07 07:51:26,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -1124.845703125
2023-01-07 07:51:26,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.19538879394531
2023-01-07 07:51:26,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -370.9195861816406
2023-01-07 07:51:26,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -228.3316650390625
2023-01-07 07:51:26,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -431.81512451171875
2023-01-07 07:51:26,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -144.68136596679688
2023-01-07 07:51:26,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -249.31405639648438
2023-01-07 07:51:26,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -117.0393295288086
2023-01-07 07:51:26,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,053 > [DEBUG] 0 :: before allreduce fusion buffer :: -568.197265625
2023-01-07 07:51:26,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -304.59552001953125
2023-01-07 07:51:26,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -551.5595092773438
2023-01-07 07:51:26,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -439.50946044921875
2023-01-07 07:51:26,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,059 > [DEBUG] 0 :: before allreduce fusion buffer :: -290.60772705078125
2023-01-07 07:51:26,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -504.9490661621094
2023-01-07 07:51:26,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -285.5456237792969
2023-01-07 07:51:26,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -151.18756103515625
2023-01-07 07:51:26,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -386.25372314453125
2023-01-07 07:51:26,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,067 > [DEBUG] 0 :: before allreduce fusion buffer :: -152.4736328125
2023-01-07 07:51:26,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -386.1595458984375
2023-01-07 07:51:26,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,070 > [DEBUG] 0 :: before allreduce fusion buffer :: -209.4257354736328
2023-01-07 07:51:26,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -369.88079833984375
2023-01-07 07:51:26,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -123.16947937011719
2023-01-07 07:51:26,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -464.6405944824219
2023-01-07 07:51:26,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -159.31732177734375
2023-01-07 07:51:26,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,079 > [DEBUG] 0 :: before allreduce fusion buffer :: -431.6208801269531
2023-01-07 07:51:26,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,081 > [DEBUG] 0 :: before allreduce fusion buffer :: -293.5615234375
2023-01-07 07:51:26,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -330.864501953125
2023-01-07 07:51:26,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,084 > [DEBUG] 0 :: before allreduce fusion buffer :: -119.91282653808594
2023-01-07 07:51:26,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -525.419677734375
2023-01-07 07:51:26,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -153.96292114257812
2023-01-07 07:51:26,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,088 > [DEBUG] 0 :: before allreduce fusion buffer :: -488.9173889160156
2023-01-07 07:51:26,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,090 > [DEBUG] 0 :: before allreduce fusion buffer :: -278.6127014160156
2023-01-07 07:51:26,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -475.64886474609375
2023-01-07 07:51:26,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -213.04779052734375
2023-01-07 07:51:26,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,094 > [DEBUG] 0 :: before allreduce fusion buffer :: -668.19287109375
2023-01-07 07:51:26,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -265.91766357421875
2023-01-07 07:51:26,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -636.398681640625
2023-01-07 07:51:26,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,098 > [DEBUG] 0 :: before allreduce fusion buffer :: -369.845947265625
2023-01-07 07:51:26,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -317.56597900390625
2023-01-07 07:51:26,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -281.89166259765625
2023-01-07 07:51:26,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -609.1845092773438
2023-01-07 07:51:26,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -349.33380126953125
2023-01-07 07:51:26,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -547.8368530273438
2023-01-07 07:51:26,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,107 > [DEBUG] 0 :: before allreduce fusion buffer :: -457.15765380859375
2023-01-07 07:51:26,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -296.7397155761719
2023-01-07 07:51:26,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,110 > [DEBUG] 0 :: before allreduce fusion buffer :: -347.1258850097656
2023-01-07 07:51:26,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,112 > [DEBUG] 0 :: before allreduce fusion buffer :: -604.697998046875
2023-01-07 07:51:26,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,113 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -325.82440185546875
2023-01-07 07:51:26,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,115 > [DEBUG] 0 :: before allreduce fusion buffer :: -270.4803161621094
2023-01-07 07:51:26,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -452.827392578125
2023-01-07 07:51:26,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -860.4398803710938
2023-01-07 07:51:26,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,119 > [DEBUG] 0 :: before allreduce fusion buffer :: -157.51864624023438
2023-01-07 07:51:26,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -2151.30029296875
2023-01-07 07:51:26,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -608.86767578125
2023-01-07 07:51:26,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -3516.38818359375
2023-01-07 07:51:26,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -553.4010009765625
2023-01-07 07:51:26,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -2684.063232421875
2023-01-07 07:51:26,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -843.93798828125
2023-01-07 07:51:26,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,128 > [DEBUG] 0 :: before allreduce fusion buffer :: -1787.4566650390625
2023-01-07 07:51:26,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.08917236328125
2023-01-07 07:51:26,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -2627.272705078125
2023-01-07 07:51:26,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -379.2418212890625
2023-01-07 07:51:26,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -3181.518798828125
2023-01-07 07:51:26,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -1001.9862060546875
2023-01-07 07:51:26,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -2381.593505859375
2023-01-07 07:51:26,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -460.50250244140625
2023-01-07 07:51:26,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -3068.87841796875
2023-01-07 07:51:26,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -539.7490844726562
2023-01-07 07:51:26,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -3662.7216796875
2023-01-07 07:51:26,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -1182.840087890625
2023-01-07 07:51:26,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -2377.558837890625
2023-01-07 07:51:26,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -602.9635009765625
2023-01-07 07:51:26,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -3532.343994140625
2023-01-07 07:51:26,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,147 > [DEBUG] 0 :: before allreduce fusion buffer :: -636.876220703125
2023-01-07 07:51:26,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,148 > [DEBUG] 0 :: before allreduce fusion buffer :: -4136.7744140625
2023-01-07 07:51:26,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -1360.51806640625
2023-01-07 07:51:26,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -3368.773193359375
2023-01-07 07:51:26,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -412.11541748046875
2023-01-07 07:51:26,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,154 > [DEBUG] 0 :: before allreduce fusion buffer :: -3351.38818359375
2023-01-07 07:51:26,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,155 > [DEBUG] 0 :: before allreduce fusion buffer :: -818.8486328125
2023-01-07 07:51:26,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -4374.74560546875
2023-01-07 07:51:26,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -892.6273193359375
2023-01-07 07:51:26,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -4337.76416015625
2023-01-07 07:51:26,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -1836.3768310546875
2023-01-07 07:51:26,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,162 > [DEBUG] 0 :: before allreduce fusion buffer :: -5493.8271484375
2023-01-07 07:51:26,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -972.1494140625
2023-01-07 07:51:26,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -5720.8203125
2023-01-07 07:51:26,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,166 > [DEBUG] 0 :: before allreduce fusion buffer :: -1056.803955078125
2023-01-07 07:51:26,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -4042.857177734375
2023-01-07 07:51:26,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,169 > [DEBUG] 0 :: before allreduce fusion buffer :: -1919.4539794921875
2023-01-07 07:51:26,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -5982.935546875
2023-01-07 07:51:26,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -1100.751953125
2023-01-07 07:51:26,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -6283.39892578125
2023-01-07 07:51:26,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -1288.64794921875
2023-01-07 07:51:26,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -4781.986328125
2023-01-07 07:51:26,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,177 > [DEBUG] 0 :: before allreduce fusion buffer :: -2291.863037109375
2023-01-07 07:51:26,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -6681.9130859375
2023-01-07 07:51:26,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -811.813232421875
2023-01-07 07:51:26,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -7319.4970703125
2023-01-07 07:51:26,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,183 > [DEBUG] 0 :: before allreduce fusion buffer :: -1318.248291015625
2023-01-07 07:51:26,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -7270.7578125
2023-01-07 07:51:26,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,186 > [DEBUG] 0 :: before allreduce fusion buffer :: -1556.822021484375
2023-01-07 07:51:26,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,187 > [DEBUG] 0 :: before allreduce fusion buffer :: -5972.5595703125
2023-01-07 07:51:26,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -4312.33447265625
2023-01-07 07:51:26,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -2881.5703125
2023-01-07 07:51:26,193 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:51:26,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 9528.375
2023-01-07 07:51:26,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,206 > [DEBUG] 0 :: before allreduce fusion buffer :: -3680.240234375
2023-01-07 07:51:26,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -7388.708984375
2023-01-07 07:51:26,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -14799.24609375
2023-01-07 07:51:26,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:26,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:26,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -28509.046875
2023-01-07 07:51:26,211 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 245.82618713378906
2023-01-07 07:51:26,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:26,212 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:51:26,212 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:26,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,054 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 66.66548156738281
2023-01-07 07:51:27,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -309.27178955078125
2023-01-07 07:51:27,056 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 67.19257354736328
2023-01-07 07:51:27,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,057 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,057 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,057 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,057 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 21.554309844970703
2023-01-07 07:51:27,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -116.22838592529297
2023-01-07 07:51:27,059 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 31.056650161743164
2023-01-07 07:51:27,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,060 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:27,060 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,060 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,060 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:27,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.115478515625
2023-01-07 07:51:27,061 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 72.72013092041016
2023-01-07 07:51:27,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,061 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,061 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,062 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,062 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:27,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 108.90449523925781
2023-01-07 07:51:27,063 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.014472007751465
2023-01-07 07:51:27,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,064 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:27,064 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,064 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,064 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 66.23779296875
2023-01-07 07:51:27,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.983245849609375
2023-01-07 07:51:27,066 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 67.01689147949219
2023-01-07 07:51:27,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,067 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,067 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,067 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,067 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:27,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.127059936523438
2023-01-07 07:51:27,068 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -18.07119369506836
2023-01-07 07:51:27,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,069 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,069 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,069 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 258.24591064453125
2023-01-07 07:51:27,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.9423713684082
2023-01-07 07:51:27,071 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 259.1194152832031
2023-01-07 07:51:27,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,071 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,071 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,071 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,071 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:27,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -100.4584732055664
2023-01-07 07:51:27,072 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -2.916147232055664
2023-01-07 07:51:27,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,073 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,073 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,073 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,073 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:27,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.920100212097168
2023-01-07 07:51:27,074 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 298.29827880859375
2023-01-07 07:51:27,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,075 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,075 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,075 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,075 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:27,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.88776397705078
2023-01-07 07:51:27,076 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.799287796020508
2023-01-07 07:51:27,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,077 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,077 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,077 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 62.994140625
2023-01-07 07:51:27,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -110.44409942626953
2023-01-07 07:51:27,079 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 62.994140625
2023-01-07 07:51:27,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,079 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,079 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,079 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:27,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,080 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.020416259765625
2023-01-07 07:51:27,081 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.5919523239135742
2023-01-07 07:51:27,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,082 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:27,082 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.46360778808594
2023-01-07 07:51:27,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.1380615234375
2023-01-07 07:51:27,084 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.49723052978516
2023-01-07 07:51:27,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,084 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,084 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,084 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,084 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:27,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.2196044921875
2023-01-07 07:51:27,086 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.788993835449219
2023-01-07 07:51:27,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,086 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,086 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,086 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,086 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:27,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.768611907958984
2023-01-07 07:51:27,087 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 291.5835266113281
2023-01-07 07:51:27,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,088 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,088 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,088 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:27,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.634385108947754
2023-01-07 07:51:27,090 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 30.836856842041016
2023-01-07 07:51:27,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,090 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,090 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:27,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.548664093017578
2023-01-07 07:51:27,092 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 71.84236907958984
2023-01-07 07:51:27,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,092 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,092 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:27,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.693809509277344
2023-01-07 07:51:27,094 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 5.077452659606934
2023-01-07 07:51:27,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,094 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:27,094 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,094 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.62458801269531
2023-01-07 07:51:27,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.785125732421875
2023-01-07 07:51:27,097 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.18022918701172
2023-01-07 07:51:27,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,097 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:27,097 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,097 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,097 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:27,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 82.6874771118164
2023-01-07 07:51:27,099 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -37.59766387939453
2023-01-07 07:51:27,099 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,099 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:27,099 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,099 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 57.94667434692383
2023-01-07 07:51:27,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.20684242248535
2023-01-07 07:51:27,100 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 299.8287353515625
2023-01-07 07:51:27,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,101 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,101 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:27,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.19972038269043
2023-01-07 07:51:27,103 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 57.94667434692383
2023-01-07 07:51:27,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,103 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:51:27,103 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:27,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.778677463531494
2023-01-07 07:51:27,104 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 149.65834045410156
2023-01-07 07:51:27,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,105 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,105 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,105 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:27,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.10344696044922
2023-01-07 07:51:27,106 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 11.273445129394531
2023-01-07 07:51:27,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,107 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:27,107 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 125.87745666503906
2023-01-07 07:51:27,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.99017333984375
2023-01-07 07:51:27,109 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 125.20014953613281
2023-01-07 07:51:27,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,109 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,109 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.352242469787598
2023-01-07 07:51:27,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.11983299255371
2023-01-07 07:51:27,111 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.352242469787598
2023-01-07 07:51:27,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,111 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,111 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 513.2608642578125
2023-01-07 07:51:27,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.253759384155273
2023-01-07 07:51:27,113 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 515.1482543945312
2023-01-07 07:51:27,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,114 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,114 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 43.89067077636719
2023-01-07 07:51:27,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1018476486206055
2023-01-07 07:51:27,115 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 43.89067077636719
2023-01-07 07:51:27,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,115 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:27,115 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:27,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.34981918334961
2023-01-07 07:51:27,117 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 592.3282470703125
2023-01-07 07:51:27,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,117 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,117 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,117 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:27,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.177748203277588
2023-01-07 07:51:27,119 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 17.112319946289062
2023-01-07 07:51:27,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,119 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,119 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,120 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 126.1116943359375
2023-01-07 07:51:27,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.97052001953125
2023-01-07 07:51:27,121 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 125.27608489990234
2023-01-07 07:51:27,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,122 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,122 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,122 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.345441818237305
2023-01-07 07:51:27,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.460394859313965
2023-01-07 07:51:27,123 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.345441818237305
2023-01-07 07:51:27,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,124 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:27,124 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,124 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 124.01240539550781
2023-01-07 07:51:27,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.158994674682617
2023-01-07 07:51:27,126 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 122.89971160888672
2023-01-07 07:51:27,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,126 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,126 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -22.915189743041992
2023-01-07 07:51:27,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.757392883300781
2023-01-07 07:51:27,128 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -22.915189743041992
2023-01-07 07:51:27,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,129 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,129 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:27,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.817030191421509
2023-01-07 07:51:27,130 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 604.2333374023438
2023-01-07 07:51:27,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,131 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,131 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,131 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,131 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:27,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.355789184570312
2023-01-07 07:51:27,133 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -16.715007781982422
2023-01-07 07:51:27,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,133 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,133 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 126.15503692626953
2023-01-07 07:51:27,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.8035807609558105
2023-01-07 07:51:27,135 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 125.57244873046875
2023-01-07 07:51:27,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,135 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,135 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8379263877868652
2023-01-07 07:51:27,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.684038162231445
2023-01-07 07:51:27,137 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 0.8379263877868652
2023-01-07 07:51:27,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,137 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:27,137 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 127.55474853515625
2023-01-07 07:51:27,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8080445528030396
2023-01-07 07:51:27,139 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.16403198242188
2023-01-07 07:51:27,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,140 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,140 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -9.415725708007812
2023-01-07 07:51:27,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2824974060058594
2023-01-07 07:51:27,141 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -9.415725708007812
2023-01-07 07:51:27,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,142 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,142 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,142 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,142 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 518.4764404296875
2023-01-07 07:51:27,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.240274906158447
2023-01-07 07:51:27,144 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 520.2080078125
2023-01-07 07:51:27,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,144 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,144 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 13.198230743408203
2023-01-07 07:51:27,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.822443962097168
2023-01-07 07:51:27,146 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 13.198230743408203
2023-01-07 07:51:27,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,146 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,146 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.462646484375
2023-01-07 07:51:27,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.17119026184082
2023-01-07 07:51:27,148 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.1612548828125
2023-01-07 07:51:27,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,149 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,149 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.31062126159668
2023-01-07 07:51:27,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.454547882080078
2023-01-07 07:51:27,150 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 15.31062126159668
2023-01-07 07:51:27,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,151 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:27,151 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 130.0646209716797
2023-01-07 07:51:27,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.0962347984313965
2023-01-07 07:51:27,153 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.96031188964844
2023-01-07 07:51:27,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,153 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:27,153 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -7.202464580535889
2023-01-07 07:51:27,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.358551979064941
2023-01-07 07:51:27,155 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -7.202464580535889
2023-01-07 07:51:27,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,155 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:27,155 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 508.42218017578125
2023-01-07 07:51:27,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5193958282470703
2023-01-07 07:51:27,157 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 507.605224609375
2023-01-07 07:51:27,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,158 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,158 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -5.41171932220459
2023-01-07 07:51:27,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.539689064025879
2023-01-07 07:51:27,159 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.41171932220459
2023-01-07 07:51:27,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,160 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:27,160 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,160 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 252.71026611328125
2023-01-07 07:51:27,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4009082317352295
2023-01-07 07:51:27,162 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 252.271484375
2023-01-07 07:51:27,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,162 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,162 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,162 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,162 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 1.5752387046813965
2023-01-07 07:51:27,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.647781372070312
2023-01-07 07:51:27,164 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 1.5752387046813965
2023-01-07 07:51:27,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,164 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,164 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,164 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:27,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.329067230224609
2023-01-07 07:51:27,166 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 344.34552001953125
2023-01-07 07:51:27,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,166 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,166 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:27,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.155796527862549
2023-01-07 07:51:27,168 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 0.1080312728881836
2023-01-07 07:51:27,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,168 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,168 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:27,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,169 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.559326171875
2023-01-07 07:51:27,170 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1267.1298828125
2023-01-07 07:51:27,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,170 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,170 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:27,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.991908848285675
2023-01-07 07:51:27,171 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -23.51654815673828
2023-01-07 07:51:27,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,172 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:27,172 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:27,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.554784774780273
2023-01-07 07:51:27,173 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1336.156494140625
2023-01-07 07:51:27,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,174 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,174 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:27,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.322352886199951
2023-01-07 07:51:27,175 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.361846923828125
2023-01-07 07:51:27,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,176 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,176 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 258.60089111328125
2023-01-07 07:51:27,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.433840751647949
2023-01-07 07:51:27,178 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 258.7864074707031
2023-01-07 07:51:27,178 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,178 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,178 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.066387176513672
2023-01-07 07:51:27,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2621270418167114
2023-01-07 07:51:27,180 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.066387176513672
2023-01-07 07:51:27,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,180 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,180 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:27,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.561892032623291
2023-01-07 07:51:27,182 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 364.21673583984375
2023-01-07 07:51:27,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,182 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,182 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:27,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.469785690307617
2023-01-07 07:51:27,184 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 8.936910629272461
2023-01-07 07:51:27,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,184 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,184 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:27,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.926649570465088
2023-01-07 07:51:27,185 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1412.102783203125
2023-01-07 07:51:27,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,186 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,186 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:27,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.91566801071167
2023-01-07 07:51:27,188 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.354618072509766
2023-01-07 07:51:27,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,188 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,188 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:27,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9394495487213135
2023-01-07 07:51:27,189 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 366.87530517578125
2023-01-07 07:51:27,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,190 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,190 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,190 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,190 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:27,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.948838710784912
2023-01-07 07:51:27,191 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 31.165061950683594
2023-01-07 07:51:27,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,192 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,192 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:27,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15734358131885529
2023-01-07 07:51:27,193 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 365.056884765625
2023-01-07 07:51:27,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,194 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,194 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:27,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.897762656211853
2023-01-07 07:51:27,195 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 22.767589569091797
2023-01-07 07:51:27,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,196 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,196 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:27,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4084968864917755
2023-01-07 07:51:27,197 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1439.714111328125
2023-01-07 07:51:27,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,198 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,198 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:27,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.561925172805786
2023-01-07 07:51:27,199 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -53.85360336303711
2023-01-07 07:51:27,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,200 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,200 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,200 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,200 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:27,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.575961112976074
2023-01-07 07:51:27,201 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 371.997802734375
2023-01-07 07:51:27,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,201 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,202 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:27,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.793663024902344
2023-01-07 07:51:27,203 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -16.88102149963379
2023-01-07 07:51:27,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,204 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,204 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:27,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6755607724189758
2023-01-07 07:51:27,205 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 376.5059814453125
2023-01-07 07:51:27,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,205 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,205 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:27,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9888595342636108
2023-01-07 07:51:27,207 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -9.834939956665039
2023-01-07 07:51:27,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,207 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,207 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:27,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4528675079345703
2023-01-07 07:51:27,209 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1480.6875
2023-01-07 07:51:27,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,209 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,209 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:27,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5717031955718994
2023-01-07 07:51:27,211 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 58.93596649169922
2023-01-07 07:51:27,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,211 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,211 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:27,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0479381084442139
2023-01-07 07:51:27,213 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 378.3800048828125
2023-01-07 07:51:27,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,213 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,213 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,213 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:27,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5259057283401489
2023-01-07 07:51:27,215 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 33.402408599853516
2023-01-07 07:51:27,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,215 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,215 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:27,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,216 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7916810512542725
2023-01-07 07:51:27,217 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 378.2391357421875
2023-01-07 07:51:27,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,217 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,217 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:27,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6376658082008362
2023-01-07 07:51:27,219 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.320283889770508
2023-01-07 07:51:27,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,219 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,219 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:27,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9876015186309814
2023-01-07 07:51:27,220 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1473.12158203125
2023-01-07 07:51:27,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,221 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,221 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:27,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8941812515258789
2023-01-07 07:51:27,223 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 78.32042694091797
2023-01-07 07:51:27,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,223 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,223 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:27,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,224 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.890282154083252
2023-01-07 07:51:27,225 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 387.4591369628906
2023-01-07 07:51:27,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,225 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,225 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:27,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40509071946144104
2023-01-07 07:51:27,227 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 15.271093368530273
2023-01-07 07:51:27,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,227 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:27,227 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:27,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2024478912353516
2023-01-07 07:51:27,228 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 387.694091796875
2023-01-07 07:51:27,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,229 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:27,229 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:27,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26123499870300293
2023-01-07 07:51:27,230 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 4.744016647338867
2023-01-07 07:51:27,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,231 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:27,231 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:27,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1432982683181763
2023-01-07 07:51:27,232 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1570.94970703125
2023-01-07 07:51:27,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,233 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:27,233 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:27,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,233 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.402712881565094
2023-01-07 07:51:27,234 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 95.67549133300781
2023-01-07 07:51:27,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,235 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:27,235 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:27,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2742137908935547
2023-01-07 07:51:27,236 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 770.0701904296875
2023-01-07 07:51:27,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,237 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,237 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:27,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.588306188583374
2023-01-07 07:51:27,238 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -38.519996643066406
2023-01-07 07:51:27,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,239 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:27,239 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:27,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45721086859703064
2023-01-07 07:51:27,240 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 782.2401733398438
2023-01-07 07:51:27,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,241 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,241 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:27,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9417150616645813
2023-01-07 07:51:27,242 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -1.7011375427246094
2023-01-07 07:51:27,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,243 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:27,243 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:27,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,243 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1146585941314697
2023-01-07 07:51:27,244 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 3209.426025390625
2023-01-07 07:51:27,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,245 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:27,245 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:27,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.308278203010559
2023-01-07 07:51:27,246 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -46.188575744628906
2023-01-07 07:51:27,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,246 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:51:27,247 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:27,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9531087875366211
2023-01-07 07:51:27,248 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 3225.35400390625
2023-01-07 07:51:27,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,248 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:27,248 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:27,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,249 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8574386835098267
2023-01-07 07:51:27,250 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -52.732200622558594
2023-01-07 07:51:27,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,250 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:27,250 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:27,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19455388188362122
2023-01-07 07:51:27,252 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 800.6488037109375
2023-01-07 07:51:27,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,252 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,252 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:27,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8587976098060608
2023-01-07 07:51:27,254 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.901889801025391
2023-01-07 07:51:27,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,254 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:27,254 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:27,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6445900201797485
2023-01-07 07:51:27,256 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 805.690185546875
2023-01-07 07:51:27,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,256 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,256 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:27,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23936451971530914
2023-01-07 07:51:27,258 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 41.01795959472656
2023-01-07 07:51:27,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,258 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:27,258 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:27,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5378369092941284
2023-01-07 07:51:27,259 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 3196.54736328125
2023-01-07 07:51:27,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,260 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:27,260 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:27,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3957889676094055
2023-01-07 07:51:27,262 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -25.142528533935547
2023-01-07 07:51:27,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,262 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:27,262 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:27,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5710242986679077
2023-01-07 07:51:27,263 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 810.552734375
2023-01-07 07:51:27,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,264 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,264 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:27,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30704253911972046
2023-01-07 07:51:27,265 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 51.97746276855469
2023-01-07 07:51:27,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,266 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:27,266 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:27,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18870311975479126
2023-01-07 07:51:27,267 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 799.197021484375
2023-01-07 07:51:27,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,268 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:27,268 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:27,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20906630158424377
2023-01-07 07:51:27,269 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.2801971435546875
2023-01-07 07:51:27,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,270 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:27,270 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:27,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.311206817626953
2023-01-07 07:51:27,271 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 3209.410400390625
2023-01-07 07:51:27,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,272 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:27,272 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:27,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4457783699035645
2023-01-07 07:51:27,273 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 4.861334800720215
2023-01-07 07:51:27,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,274 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:51:27,274 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:27,275 > [DEBUG] 0 :: 8.409561157226562
2023-01-07 07:51:27,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -1563.303466796875
2023-01-07 07:51:27,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -1239.003173828125
2023-01-07 07:51:27,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -162.8343505859375
2023-01-07 07:51:27,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -322.1897277832031
2023-01-07 07:51:27,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -220.53372192382812
2023-01-07 07:51:27,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -514.8359985351562
2023-01-07 07:51:27,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -322.26165771484375
2023-01-07 07:51:27,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -298.95166015625
2023-01-07 07:51:27,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -140.88372802734375
2023-01-07 07:51:27,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -480.7012634277344
2023-01-07 07:51:27,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -369.5902404785156
2023-01-07 07:51:27,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,296 > [DEBUG] 0 :: before allreduce fusion buffer :: -649.9472045898438
2023-01-07 07:51:27,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -355.755615234375
2023-01-07 07:51:27,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -338.97052001953125
2023-01-07 07:51:27,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -546.9067993164062
2023-01-07 07:51:27,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -351.7677001953125
2023-01-07 07:51:27,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -343.27105712890625
2023-01-07 07:51:27,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -480.90618896484375
2023-01-07 07:51:27,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -490.940185546875
2023-01-07 07:51:27,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -699.9168701171875
2023-01-07 07:51:27,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -507.8282775878906
2023-01-07 07:51:27,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -419.85870361328125
2023-01-07 07:51:27,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -412.61029052734375
2023-01-07 07:51:27,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -730.0814208984375
2023-01-07 07:51:27,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -500.0909423828125
2023-01-07 07:51:27,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -754.3984375
2023-01-07 07:51:27,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -420.49847412109375
2023-01-07 07:51:27,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -985.6726684570312
2023-01-07 07:51:27,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.8846435546875
2023-01-07 07:51:27,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -1905.605224609375
2023-01-07 07:51:27,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -671.83447265625
2023-01-07 07:51:27,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -2120.12548828125
2023-01-07 07:51:27,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -1027.186767578125
2023-01-07 07:51:27,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -1938.015380859375
2023-01-07 07:51:27,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -935.797607421875
2023-01-07 07:51:27,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,333 > [DEBUG] 0 :: before allreduce fusion buffer :: -2997.7392578125
2023-01-07 07:51:27,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -1144.9593505859375
2023-01-07 07:51:27,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -3007.718994140625
2023-01-07 07:51:27,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -1690.32275390625
2023-01-07 07:51:27,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -2466.865966796875
2023-01-07 07:51:27,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -1552.8656005859375
2023-01-07 07:51:27,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -3936.83154296875
2023-01-07 07:51:27,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -1668.591796875
2023-01-07 07:51:27,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -3505.521728515625
2023-01-07 07:51:27,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -2090.890869140625
2023-01-07 07:51:27,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -2489.07177734375
2023-01-07 07:51:27,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -1696.8077392578125
2023-01-07 07:51:27,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -4242.62890625
2023-01-07 07:51:27,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,353 > [DEBUG] 0 :: before allreduce fusion buffer :: -1978.275390625
2023-01-07 07:51:27,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -4124.76318359375
2023-01-07 07:51:27,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -2270.1396484375
2023-01-07 07:51:27,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -3255.77001953125
2023-01-07 07:51:27,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -1768.2578125
2023-01-07 07:51:27,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -3833.754150390625
2023-01-07 07:51:27,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -1419.8134765625
2023-01-07 07:51:27,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -2956.35693359375
2023-01-07 07:51:27,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -3841.8330078125
2023-01-07 07:51:27,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -44261.1171875
2023-01-07 07:51:27,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -6360.6298828125
2023-01-07 07:51:27,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -24522.404296875
2023-01-07 07:51:27,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -2822.062744140625
2023-01-07 07:51:27,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -38934.109375
2023-01-07 07:51:27,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -3059.728271484375
2023-01-07 07:51:27,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,373 > [DEBUG] 0 :: before allreduce fusion buffer :: -48298.203125
2023-01-07 07:51:27,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -8485.126953125
2023-01-07 07:51:27,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -31528.515625
2023-01-07 07:51:27,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,376 > [DEBUG] 0 :: before allreduce fusion buffer :: -3900.99951171875
2023-01-07 07:51:27,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -43448.015625
2023-01-07 07:51:27,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -4807.2109375
2023-01-07 07:51:27,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -53915.6015625
2023-01-07 07:51:27,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -11344.3896484375
2023-01-07 07:51:27,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -33903.0546875
2023-01-07 07:51:27,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -5669.947265625
2023-01-07 07:51:27,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -49832.34375
2023-01-07 07:51:27,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,387 > [DEBUG] 0 :: before allreduce fusion buffer :: -6279.9599609375
2023-01-07 07:51:27,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -60111.0703125
2023-01-07 07:51:27,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -14251.92578125
2023-01-07 07:51:27,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,391 > [DEBUG] 0 :: before allreduce fusion buffer :: -49664.6953125
2023-01-07 07:51:27,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -4227.57275390625
2023-01-07 07:51:27,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -43042.2578125
2023-01-07 07:51:27,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,395 > [DEBUG] 0 :: before allreduce fusion buffer :: -8505.3125
2023-01-07 07:51:27,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -61444.55078125
2023-01-07 07:51:27,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,397 > [DEBUG] 0 :: before allreduce fusion buffer :: -9829.548828125
2023-01-07 07:51:27,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,398 > [DEBUG] 0 :: before allreduce fusion buffer :: -61969.56640625
2023-01-07 07:51:27,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,400 > [DEBUG] 0 :: before allreduce fusion buffer :: -21199.265625
2023-01-07 07:51:27,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -84460.7265625
2023-01-07 07:51:27,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -12063.748046875
2023-01-07 07:51:27,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -86381.546875
2023-01-07 07:51:27,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -13323.986328125
2023-01-07 07:51:27,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,407 > [DEBUG] 0 :: before allreduce fusion buffer :: -56704.1171875
2023-01-07 07:51:27,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -25275.59765625
2023-01-07 07:51:27,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -93484.984375
2023-01-07 07:51:27,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -14108.732421875
2023-01-07 07:51:27,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -96322.1875
2023-01-07 07:51:27,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -17099.669921875
2023-01-07 07:51:27,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -67893.9921875
2023-01-07 07:51:27,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -30188.73828125
2023-01-07 07:51:27,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -103663.3828125
2023-01-07 07:51:27,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -10806.0986328125
2023-01-07 07:51:27,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,422 > [DEBUG] 0 :: before allreduce fusion buffer :: -113492.953125
2023-01-07 07:51:27,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -16774.791015625
2023-01-07 07:51:27,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -111714.7109375
2023-01-07 07:51:27,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -20466.35546875
2023-01-07 07:51:27,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -90823.9765625
2023-01-07 07:51:27,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -64452.5703125
2023-01-07 07:51:27,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -37610.4375
2023-01-07 07:51:27,435 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:51:27,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 139015.140625
2023-01-07 07:51:27,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,449 > [DEBUG] 0 :: before allreduce fusion buffer :: -161460.71875
2023-01-07 07:51:27,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,451 > [DEBUG] 0 :: before allreduce fusion buffer :: -322831.3125
2023-01-07 07:51:27,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,452 > [DEBUG] 0 :: before allreduce fusion buffer :: -5771941.5
2023-01-07 07:51:27,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:27,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:27,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -1290435.5
2023-01-07 07:51:27,454 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 362.4066162109375
2023-01-07 07:51:27,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:27,454 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:51:27,454 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:27,455 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 67.19257354736328
2023-01-07 07:51:28,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,296 > [DEBUG] 0 :: before allreduce fusion buffer :: -444.43658447265625
2023-01-07 07:51:28,297 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 67.92953491210938
2023-01-07 07:51:28,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,298 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,298 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 31.056650161743164
2023-01-07 07:51:28,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.8404436111450195
2023-01-07 07:51:28,300 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 35.17362976074219
2023-01-07 07:51:28,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,301 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:28,301 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:28,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -133.7631378173828
2023-01-07 07:51:28,302 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 76.91117095947266
2023-01-07 07:51:28,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,302 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,302 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.014472007751465
2023-01-07 07:51:28,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 104.20852661132812
2023-01-07 07:51:28,304 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.014472007751465
2023-01-07 07:51:28,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,304 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:28,305 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 67.01689147949219
2023-01-07 07:51:28,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -92.52911376953125
2023-01-07 07:51:28,307 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 67.57512664794922
2023-01-07 07:51:28,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,307 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,308 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:28,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -29.17064666748047
2023-01-07 07:51:28,309 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -21.40392303466797
2023-01-07 07:51:28,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,310 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,310 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,310 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,310 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 259.1194152832031
2023-01-07 07:51:28,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.143385887145996
2023-01-07 07:51:28,312 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 259.8605041503906
2023-01-07 07:51:28,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,312 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,312 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:28,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.190128326416016
2023-01-07 07:51:28,313 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -2.916147232055664
2023-01-07 07:51:28,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,314 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,314 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:28,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.55760192871094
2023-01-07 07:51:28,315 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 318.3614501953125
2023-01-07 07:51:28,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,316 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,316 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:28,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -98.82317352294922
2023-01-07 07:51:28,317 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.799287796020508
2023-01-07 07:51:28,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,318 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,318 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 63.025901794433594
2023-01-07 07:51:28,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -231.03488159179688
2023-01-07 07:51:28,320 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 63.025901794433594
2023-01-07 07:51:28,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,320 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,320 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5919523239135742
2023-01-07 07:51:28,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.19679260253906
2023-01-07 07:51:28,322 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.5919523239135742
2023-01-07 07:51:28,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,322 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:28,323 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.49723052978516
2023-01-07 07:51:28,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.74012756347656
2023-01-07 07:51:28,324 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.39697265625
2023-01-07 07:51:28,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,325 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,325 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,325 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,325 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:28,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.997377395629883
2023-01-07 07:51:28,326 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: -8.788993835449219
2023-01-07 07:51:28,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,327 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,327 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:28,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.035125732421875
2023-01-07 07:51:28,328 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 312.90496826171875
2023-01-07 07:51:28,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,329 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,329 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 30.836856842041016
2023-01-07 07:51:28,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.392351150512695
2023-01-07 07:51:28,331 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 30.836856842041016
2023-01-07 07:51:28,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,331 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,331 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:28,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.9571418762207
2023-01-07 07:51:28,332 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 76.52969360351562
2023-01-07 07:51:28,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,333 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,333 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.077452659606934
2023-01-07 07:51:28,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.6449089050293
2023-01-07 07:51:28,334 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: 5.077452659606934
2023-01-07 07:51:28,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,335 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:51:28,335 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.18022918701172
2023-01-07 07:51:28,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.61655044555664
2023-01-07 07:51:28,337 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.36964416503906
2023-01-07 07:51:28,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,338 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:51:28,338 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:28,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.98950958251953
2023-01-07 07:51:28,339 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -45.26696014404297
2023-01-07 07:51:28,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,340 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:51:28,340 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 57.94667434692383
2023-01-07 07:51:28,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.225786209106445
2023-01-07 07:51:28,341 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 321.61474609375
2023-01-07 07:51:28,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,342 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,342 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:28,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.48379135131836
2023-01-07 07:51:28,343 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: 57.94667434692383
2023-01-07 07:51:28,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,344 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:51:28,344 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:28,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.861968994140625
2023-01-07 07:51:28,345 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 159.16006469726562
2023-01-07 07:51:28,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,346 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,346 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 11.273445129394531
2023-01-07 07:51:28,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.116859436035156
2023-01-07 07:51:28,347 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 11.273445129394531
2023-01-07 07:51:28,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,348 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:28,348 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,348 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 125.20014953613281
2023-01-07 07:51:28,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.24285125732422
2023-01-07 07:51:28,350 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 125.13856506347656
2023-01-07 07:51:28,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,350 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,350 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.352242469787598
2023-01-07 07:51:28,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.3641414642334
2023-01-07 07:51:28,352 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 15.352242469787598
2023-01-07 07:51:28,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,352 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,352 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 515.1482543945312
2023-01-07 07:51:28,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.396951675415039
2023-01-07 07:51:28,354 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 514.7249145507812
2023-01-07 07:51:28,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,354 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,354 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 43.89067077636719
2023-01-07 07:51:28,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.4433417320251465
2023-01-07 07:51:28,356 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 43.89067077636719
2023-01-07 07:51:28,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,356 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:28,356 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:28,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.354307174682617
2023-01-07 07:51:28,358 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 625.188232421875
2023-01-07 07:51:28,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,358 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,358 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 17.112319946289062
2023-01-07 07:51:28,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3232009410858154
2023-01-07 07:51:28,360 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 17.112319946289062
2023-01-07 07:51:28,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,360 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,360 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 125.27608489990234
2023-01-07 07:51:28,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.68162155151367
2023-01-07 07:51:28,362 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 124.78181457519531
2023-01-07 07:51:28,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,363 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,363 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.345441818237305
2023-01-07 07:51:28,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.678343772888184
2023-01-07 07:51:28,364 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.345441818237305
2023-01-07 07:51:28,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,365 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:28,365 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 122.89971160888672
2023-01-07 07:51:28,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.630487442016602
2023-01-07 07:51:28,367 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 121.75848388671875
2023-01-07 07:51:28,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,367 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,367 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -22.915189743041992
2023-01-07 07:51:28,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.530573844909668
2023-01-07 07:51:28,369 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -22.915189743041992
2023-01-07 07:51:28,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,369 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,369 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:28,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1727139949798584
2023-01-07 07:51:28,370 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 638.19189453125
2023-01-07 07:51:28,370 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,371 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,371 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -16.715007781982422
2023-01-07 07:51:28,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.10963249206543
2023-01-07 07:51:28,372 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -16.715007781982422
2023-01-07 07:51:28,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,373 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,373 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 125.57244873046875
2023-01-07 07:51:28,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.613314628601074
2023-01-07 07:51:28,375 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 125.0893325805664
2023-01-07 07:51:28,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,375 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,375 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8379263877868652
2023-01-07 07:51:28,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.827739715576172
2023-01-07 07:51:28,377 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 0.8379263877868652
2023-01-07 07:51:28,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,377 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:28,377 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 127.16403198242188
2023-01-07 07:51:28,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7327325344085693
2023-01-07 07:51:28,379 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.02285766601562
2023-01-07 07:51:28,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,380 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,380 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -9.415725708007812
2023-01-07 07:51:28,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.360422134399414
2023-01-07 07:51:28,381 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -9.415725708007812
2023-01-07 07:51:28,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,382 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,382 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 520.2080078125
2023-01-07 07:51:28,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.098853349685669
2023-01-07 07:51:28,384 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 521.990478515625
2023-01-07 07:51:28,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,385 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,385 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 13.198230743408203
2023-01-07 07:51:28,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.526834487915039
2023-01-07 07:51:28,386 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 13.198230743408203
2023-01-07 07:51:28,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,387 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,387 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.1612548828125
2023-01-07 07:51:28,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.677014350891113
2023-01-07 07:51:28,389 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.56979370117188
2023-01-07 07:51:28,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,390 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,390 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 15.31062126159668
2023-01-07 07:51:28,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.44039249420166
2023-01-07 07:51:28,391 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 15.31062126159668
2023-01-07 07:51:28,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,392 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:51:28,392 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 129.96031188964844
2023-01-07 07:51:28,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.960090160369873
2023-01-07 07:51:28,394 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.96665954589844
2023-01-07 07:51:28,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,394 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:51:28,394 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: -7.202464580535889
2023-01-07 07:51:28,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.660903215408325
2023-01-07 07:51:28,396 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: -7.202464580535889
2023-01-07 07:51:28,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,396 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:51:28,397 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 507.605224609375
2023-01-07 07:51:28,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,397 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.688451766967773
2023-01-07 07:51:28,398 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 505.7851867675781
2023-01-07 07:51:28,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,399 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,399 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -5.41171932220459
2023-01-07 07:51:28,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21668002009391785
2023-01-07 07:51:28,401 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -5.41171932220459
2023-01-07 07:51:28,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,401 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:51:28,401 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 252.271484375
2023-01-07 07:51:28,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.804162979125977
2023-01-07 07:51:28,403 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 252.0487060546875
2023-01-07 07:51:28,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,403 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,403 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 1.5752387046813965
2023-01-07 07:51:28,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.742450714111328
2023-01-07 07:51:28,405 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 1.5752387046813965
2023-01-07 07:51:28,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,405 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,406 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:28,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.021638870239258
2023-01-07 07:51:28,407 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 368.5318603515625
2023-01-07 07:51:28,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,407 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,407 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 0.1080312728881836
2023-01-07 07:51:28,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.470029830932617
2023-01-07 07:51:28,409 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 0.1080312728881836
2023-01-07 07:51:28,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,409 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,409 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:28,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2585091590881348
2023-01-07 07:51:28,411 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1310.197509765625
2023-01-07 07:51:28,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,411 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,411 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -23.51654815673828
2023-01-07 07:51:28,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8332743644714355
2023-01-07 07:51:28,413 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -23.51654815673828
2023-01-07 07:51:28,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,413 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:28,413 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:28,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5217530727386475
2023-01-07 07:51:28,414 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1380.8431396484375
2023-01-07 07:51:28,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,415 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,415 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -41.361846923828125
2023-01-07 07:51:28,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2263894081115723
2023-01-07 07:51:28,417 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -41.361846923828125
2023-01-07 07:51:28,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,417 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,417 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 258.7864074707031
2023-01-07 07:51:28,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31875234842300415
2023-01-07 07:51:28,419 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 258.8421630859375
2023-01-07 07:51:28,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,419 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,420 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.066387176513672
2023-01-07 07:51:28,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7011847496032715
2023-01-07 07:51:28,421 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 3.066387176513672
2023-01-07 07:51:28,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,422 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,422 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:28,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,422 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.453879356384277
2023-01-07 07:51:28,423 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 385.59454345703125
2023-01-07 07:51:28,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,423 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,423 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,423 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 8.936910629272461
2023-01-07 07:51:28,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.630972862243652
2023-01-07 07:51:28,425 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 8.936910629272461
2023-01-07 07:51:28,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,425 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,425 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:28,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5397913455963135
2023-01-07 07:51:28,427 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1507.00048828125
2023-01-07 07:51:28,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,427 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,427 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,427 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -34.354618072509766
2023-01-07 07:51:28,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.565477967262268
2023-01-07 07:51:28,429 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -34.354618072509766
2023-01-07 07:51:28,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,429 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,429 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:28,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7447996139526367
2023-01-07 07:51:28,431 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 404.1588134765625
2023-01-07 07:51:28,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,431 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,431 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 31.165061950683594
2023-01-07 07:51:28,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.986682891845703
2023-01-07 07:51:28,433 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 31.165061950683594
2023-01-07 07:51:28,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,433 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,433 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:28,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2194771766662598
2023-01-07 07:51:28,435 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 400.0396728515625
2023-01-07 07:51:28,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,435 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,435 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 22.767589569091797
2023-01-07 07:51:28,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4243130683898926
2023-01-07 07:51:28,437 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: 22.767589569091797
2023-01-07 07:51:28,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,437 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,437 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:28,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,438 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5510741472244263
2023-01-07 07:51:28,438 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1518.2996826171875
2023-01-07 07:51:28,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,439 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,439 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,439 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -53.85360336303711
2023-01-07 07:51:28,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6809945106506348
2023-01-07 07:51:28,440 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: -53.85360336303711
2023-01-07 07:51:28,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,441 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,441 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:28,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,441 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.075770616531372
2023-01-07 07:51:28,442 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 406.220703125
2023-01-07 07:51:28,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,443 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,443 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -16.88102149963379
2023-01-07 07:51:28,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.900264739990234
2023-01-07 07:51:28,444 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -16.88102149963379
2023-01-07 07:51:28,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,445 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,445 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:28,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3504831790924072
2023-01-07 07:51:28,446 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 411.34521484375
2023-01-07 07:51:28,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,447 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,447 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -9.834939956665039
2023-01-07 07:51:28,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.574052095413208
2023-01-07 07:51:28,448 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: -9.834939956665039
2023-01-07 07:51:28,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,449 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,449 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:28,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9824259281158447
2023-01-07 07:51:28,450 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1565.85791015625
2023-01-07 07:51:28,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,450 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,451 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,451 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 58.93596649169922
2023-01-07 07:51:28,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.410565584897995
2023-01-07 07:51:28,452 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: 58.93596649169922
2023-01-07 07:51:28,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,453 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,453 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,453 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,453 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:28,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11437949538230896
2023-01-07 07:51:28,454 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 414.3191833496094
2023-01-07 07:51:28,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,454 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,454 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 33.402408599853516
2023-01-07 07:51:28,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1446328163146973
2023-01-07 07:51:28,456 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 33.402408599853516
2023-01-07 07:51:28,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,456 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,456 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:28,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4606971740722656
2023-01-07 07:51:28,458 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 415.00653076171875
2023-01-07 07:51:28,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,458 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,458 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.320283889770508
2023-01-07 07:51:28,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4613336324691772
2023-01-07 07:51:28,460 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.320283889770508
2023-01-07 07:51:28,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,460 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,460 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:28,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8900400400161743
2023-01-07 07:51:28,462 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1558.7855224609375
2023-01-07 07:51:28,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,462 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,462 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 78.32042694091797
2023-01-07 07:51:28,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4247909188270569
2023-01-07 07:51:28,464 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: 78.32042694091797
2023-01-07 07:51:28,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,464 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,464 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:28,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6741609573364258
2023-01-07 07:51:28,465 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 430.85394287109375
2023-01-07 07:51:28,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,466 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,466 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,466 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,466 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 15.271093368530273
2023-01-07 07:51:28,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5958518981933594
2023-01-07 07:51:28,468 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: 15.271093368530273
2023-01-07 07:51:28,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,468 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:51:28,468 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,468 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:28,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9363869428634644
2023-01-07 07:51:28,469 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 429.2333679199219
2023-01-07 07:51:28,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,470 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:51:28,470 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,470 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.744016647338867
2023-01-07 07:51:28,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34740424156188965
2023-01-07 07:51:28,471 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: 4.744016647338867
2023-01-07 07:51:28,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,472 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:51:28,472 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:28,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0798586830496788
2023-01-07 07:51:28,473 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1698.137939453125
2023-01-07 07:51:28,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,474 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:51:28,474 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 95.67549133300781
2023-01-07 07:51:28,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,474 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23083804547786713
2023-01-07 07:51:28,475 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: 95.67549133300781
2023-01-07 07:51:28,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,476 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:51:28,476 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:28,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9752617478370667
2023-01-07 07:51:28,477 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 849.0899047851562
2023-01-07 07:51:28,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,478 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,478 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -38.519996643066406
2023-01-07 07:51:28,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0559163093566895
2023-01-07 07:51:28,479 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -38.519996643066406
2023-01-07 07:51:28,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,480 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:28,480 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:28,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.018837332725525
2023-01-07 07:51:28,481 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 866.1427612304688
2023-01-07 07:51:28,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,482 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,482 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,482 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,482 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -1.7011375427246094
2023-01-07 07:51:28,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2406603842973709
2023-01-07 07:51:28,483 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: -1.7011375427246094
2023-01-07 07:51:28,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,484 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:28,484 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:28,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7965902090072632
2023-01-07 07:51:28,485 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 3569.1318359375
2023-01-07 07:51:28,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,486 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:28,486 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -46.188575744628906
2023-01-07 07:51:28,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.913844108581543
2023-01-07 07:51:28,487 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -46.188575744628906
2023-01-07 07:51:28,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,487 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:51:28,487 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,488 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:28,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.246767282485962
2023-01-07 07:51:28,489 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 3596.857177734375
2023-01-07 07:51:28,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,489 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:28,489 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -52.732200622558594
2023-01-07 07:51:28,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6195178031921387
2023-01-07 07:51:28,491 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -52.732200622558594
2023-01-07 07:51:28,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,491 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:28,491 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:28,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7963539958000183
2023-01-07 07:51:28,493 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 890.5892333984375
2023-01-07 07:51:28,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,493 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,493 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -4.901889801025391
2023-01-07 07:51:28,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27578455209732056
2023-01-07 07:51:28,495 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -4.901889801025391
2023-01-07 07:51:28,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,495 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:28,495 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,495 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:28,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1049301624298096
2023-01-07 07:51:28,497 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 896.421630859375
2023-01-07 07:51:28,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,497 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,497 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 41.01795959472656
2023-01-07 07:51:28,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.173647940158844
2023-01-07 07:51:28,499 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: 41.01795959472656
2023-01-07 07:51:28,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,499 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:28,499 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:28,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,500 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46102771162986755
2023-01-07 07:51:28,500 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 3555.796875
2023-01-07 07:51:28,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,501 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:28,501 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -25.142528533935547
2023-01-07 07:51:28,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23261883854866028
2023-01-07 07:51:28,502 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -25.142528533935547
2023-01-07 07:51:28,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,503 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:28,503 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:28,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9343565106391907
2023-01-07 07:51:28,504 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 906.5909423828125
2023-01-07 07:51:28,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,505 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,505 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 51.97746276855469
2023-01-07 07:51:28,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06264711916446686
2023-01-07 07:51:28,506 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: 51.97746276855469
2023-01-07 07:51:28,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,507 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:51:28,507 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:28,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,507 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22446855902671814
2023-01-07 07:51:28,508 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 887.8349609375
2023-01-07 07:51:28,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,509 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:51:28,509 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1.2801971435546875
2023-01-07 07:51:28,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4991656541824341
2023-01-07 07:51:28,510 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 1.2801971435546875
2023-01-07 07:51:28,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,511 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:51:28,511 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:28,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.111526489257812
2023-01-07 07:51:28,512 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 3579.974853515625
2023-01-07 07:51:28,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,512 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:51:28,513 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,513 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 4.861334800720215
2023-01-07 07:51:28,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3146932125091553
2023-01-07 07:51:28,514 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 4.861334800720215
2023-01-07 07:51:28,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:51:28,515 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:51:28,515 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:51:28,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:51:28,516 > [DEBUG] 0 :: 8.428932189941406
2023-01-07 07:51:28,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -2356.27587890625
2023-01-07 07:51:28,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -2394.630615234375
2023-01-07 07:51:28,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -89.45213317871094
2023-01-07 07:51:28,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -320.1434326171875
2023-01-07 07:51:28,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -277.27423095703125
2023-01-07 07:51:28,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -421.0465087890625
2023-01-07 07:51:28,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -413.81689453125
2023-01-07 07:51:28,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -357.52325439453125
2023-01-07 07:51:28,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,536 > [DEBUG] 0 :: before allreduce fusion buffer :: -346.9211120605469
2023-01-07 07:51:28,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -740.2225341796875
2023-01-07 07:51:28,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -673.399169921875
2023-01-07 07:51:28,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -1005.0478515625
2023-01-07 07:51:28,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -683.42529296875
2023-01-07 07:51:28,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -425.31439208984375
2023-01-07 07:51:28,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -1000.208984375
2023-01-07 07:51:28,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -416.21453857421875
2023-01-07 07:51:28,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,549 > [DEBUG] 0 :: before allreduce fusion buffer :: -520.8335571289062
2023-01-07 07:51:28,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -928.3876342773438
2023-01-07 07:51:28,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -791.095703125
2023-01-07 07:51:28,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -1608.257568359375
2023-01-07 07:51:28,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -1114.3721923828125
2023-01-07 07:51:28,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -1357.566162109375
2023-01-07 07:51:28,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -293.5794677734375
2023-01-07 07:51:28,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -1846.964599609375
2023-01-07 07:51:28,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -672.1396484375
2023-01-07 07:51:28,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -2046.4227294921875
2023-01-07 07:51:28,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -1416.9351806640625
2023-01-07 07:51:28,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -1810.33447265625
2023-01-07 07:51:28,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -1265.0118408203125
2023-01-07 07:51:28,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -3995.059326171875
2023-01-07 07:51:28,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -3472.94580078125
2023-01-07 07:51:28,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -23582.9609375
2023-01-07 07:51:28,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -5387.68896484375
2023-01-07 07:51:28,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -22174.4296875
2023-01-07 07:51:28,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -5182.126953125
2023-01-07 07:51:28,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -28638.09375
2023-01-07 07:51:28,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -6401.31640625
2023-01-07 07:51:28,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -30482.16796875
2023-01-07 07:51:28,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -9539.4287109375
2023-01-07 07:51:28,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -27175.982421875
2023-01-07 07:51:28,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -6984.9267578125
2023-01-07 07:51:28,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -35496.7109375
2023-01-07 07:51:28,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -8462.669921875
2023-01-07 07:51:28,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -38480.0
2023-01-07 07:51:28,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -9678.8603515625
2023-01-07 07:51:28,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,594 > [DEBUG] 0 :: before allreduce fusion buffer :: -38490.66015625
2023-01-07 07:51:28,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -13018.947265625
2023-01-07 07:51:28,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -51918.19921875
2023-01-07 07:51:28,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -15093.49609375
2023-01-07 07:51:28,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -49528.109375
2023-01-07 07:51:28,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -16944.673828125
2023-01-07 07:51:28,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -45975.7578125
2023-01-07 07:51:28,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -9102.251953125
2023-01-07 07:51:28,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -16320.880859375
2023-01-07 07:51:28,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -35726.6875
2023-01-07 07:51:28,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -309060.625
2023-01-07 07:51:28,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -197829.671875
2023-01-07 07:51:28,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -1277707.25
2023-01-07 07:51:28,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -323949.3125
2023-01-07 07:51:28,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -865987.25
2023-01-07 07:51:28,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -91651.703125
2023-01-07 07:51:28,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -1157683.5
2023-01-07 07:51:28,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -87020.3828125
2023-01-07 07:51:28,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -1329438.75
2023-01-07 07:51:28,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -336434.3125
2023-01-07 07:51:28,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,621 > [DEBUG] 0 :: before allreduce fusion buffer :: -978684.75
2023-01-07 07:51:28,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -93051.3125
2023-01-07 07:51:28,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -1212496.75
2023-01-07 07:51:28,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -115051.703125
2023-01-07 07:51:28,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -1403413.75
2023-01-07 07:51:28,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -366371.9375
2023-01-07 07:51:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -999369.375
2023-01-07 07:51:28,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -122383.6328125
2023-01-07 07:51:28,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -1301260.875
2023-01-07 07:51:28,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -128495.15625
2023-01-07 07:51:28,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,634 > [DEBUG] 0 :: before allreduce fusion buffer :: -1486418.75
2023-01-07 07:51:28,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -395931.8125
2023-01-07 07:51:28,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -1292532.625
2023-01-07 07:51:28,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -32059.896484375
2023-01-07 07:51:28,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -1114668.25
2023-01-07 07:51:28,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -162153.875
2023-01-07 07:51:28,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -1474651.75
2023-01-07 07:51:28,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -187203.171875
2023-01-07 07:51:28,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -1488265.125
2023-01-07 07:51:28,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,646 > [DEBUG] 0 :: before allreduce fusion buffer :: -526915.25
2023-01-07 07:51:28,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -1899946.125
2023-01-07 07:51:28,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -225267.875
2023-01-07 07:51:28,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -1889387.375
2023-01-07 07:51:28,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -246422.5
2023-01-07 07:51:28,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -1317743.875
2023-01-07 07:51:28,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -570762.1875
2023-01-07 07:51:28,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,656 > [DEBUG] 0 :: before allreduce fusion buffer :: -2033596.75
2023-01-07 07:51:28,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -235635.328125
2023-01-07 07:51:28,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -2035095.0
2023-01-07 07:51:28,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -302478.09375
2023-01-07 07:51:28,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -1479885.875
2023-01-07 07:51:28,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -630177.4375
2023-01-07 07:51:28,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -2184286.75
2023-01-07 07:51:28,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -110375.2734375
2023-01-07 07:51:28,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,667 > [DEBUG] 0 :: before allreduce fusion buffer :: -2301381.25
2023-01-07 07:51:28,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -245333.265625
2023-01-07 07:51:28,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -2263827.5
2023-01-07 07:51:28,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -322958.375
2023-01-07 07:51:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -1875735.375
2023-01-07 07:51:28,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -1301173.125
2023-01-07 07:51:28,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:51:28,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:51:28,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -682641.75
