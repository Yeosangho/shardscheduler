[00;33m 	'cuda/11.3' does not supports the {CUDA_MPI}. [0m
[00;34m 	{CUDA_MPI} is only supported in cuda 11.4 version. [0m
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/75 [00:00<?, ?it/s]  0%|          | 0/75 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/hpc72a03/shardscheduler/dp_custom.py", line 941, in __getattr__
    return super().__getattr__(name)  # defer to nn.Module's logic
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'DataParallel_Custom' object has no attribute 'is_root'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/hpc72a03/shardscheduler/fairscale/fairscale/nn/misc/flatten_params_wrapper.py", line 392, in __getattr__
    return super().__getattr__(name)  # defer to nn.Module's logic
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'FlattenParamsWrapper' object has no attribute 'is_root'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 478, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 333, in benchmark_step
    output = self.sharded_module( b_input_ids,
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1046, in forward
    transformer_outputs = self.transformer(
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 832, in forward
    inputs_embeds = self.wte(input_ids)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/hpc72a03/shardscheduler/dp_custom.py", line 663, in forward
    self._rebuild_full_params()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/hpc72a03/shardscheduler/dp_custom.py", line 596, in _rebuild_full_params
    print(f"Allgather forward : {self.model_parameter_names} is root : {self.is_root}")
  File "/scratch/hpc72a03/shardscheduler/dp_custom.py", line 943, in __getattr__
    return getattr(self.module, name)
  File "/scratch/hpc72a03/shardscheduler/fairscale/fairscale/nn/misc/flatten_params_wrapper.py", line 394, in __getattr__
    return getattr(self.module, name)  # fallback to wrapped module
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'Embedding' object has no attribute 'is_root'
srun: error: gpu20: task 0: Exited with exit code 1
  0%|          | 0/75 [00:00<?, ?it/s]  0%|          | 0/75 [00:01<?, ?it/s]
  0%|          | 0/75 [00:00<?, ?it/s]  0%|          | 0/75 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 478, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 350, in benchmark_step
    dist.all_reduce(grad_clone, async_op=False)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 489, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 478, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 350, in benchmark_step
    dist.all_reduce(grad_clone, async_op=False)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: [3] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 489, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  0%|          | 0/75 [00:00<?, ?it/s]  0%|          | 0/75 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 478, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 350, in benchmark_step
    dist.all_reduce(grad_clone, async_op=False)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1314, in all_reduce
    work = default_pg.allreduce([tensor], opts)
RuntimeError: [2] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 489, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
srun: error: gpu20: tasks 1,3: Exited with exit code 1
srun: error: gpu20: task 2: Exited with exit code 1
