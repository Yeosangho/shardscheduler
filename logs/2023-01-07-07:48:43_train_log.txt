2023-01-07 07:48:50,157 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:48:50,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,193 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -1.922404408454895
2023-01-07 07:48:50,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,194 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:48:50,194 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,194 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,194 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 07:48:50,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,912 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,912 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,912 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,912 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,912 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 07:48:50,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,914 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -8.645812034606934
2023-01-07 07:48:50,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,914 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:50,915 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,915 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,915 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 07:48:50,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,915 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,916 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,916 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,916 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,916 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 07:48:50,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,917 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -14.367234230041504
2023-01-07 07:48:50,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,917 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:50,917 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,917 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 07:48:50,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,953 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,953 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,954 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,954 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 07:48:50,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,955 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 8.114887237548828
2023-01-07 07:48:50,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,955 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,955 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,955 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,955 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 07:48:50,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,957 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:50,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,957 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:50,957 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,957 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 07:48:50,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,958 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 6.535425186157227
2023-01-07 07:48:50,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,958 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,958 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,958 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,958 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 07:48:50,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,959 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:50,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,959 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:50,959 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,959 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,959 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 07:48:50,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,960 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 24.492828369140625
2023-01-07 07:48:50,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,961 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,961 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,961 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,961 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 07:48:50,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,962 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,962 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,962 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,962 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 07:48:50,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,963 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.8943326473236084
2023-01-07 07:48:50,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,963 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:50,963 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,963 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,963 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 07:48:50,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,964 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,964 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,964 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,965 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 07:48:50,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,966 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 19.24296760559082
2023-01-07 07:48:50,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,966 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,966 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,966 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,966 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 07:48:50,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,967 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:50,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,967 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:50,967 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,967 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 07:48:50,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,968 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 9.861352920532227
2023-01-07 07:48:50,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,968 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,968 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,969 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,969 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 07:48:50,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,969 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,970 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,970 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,970 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 07:48:50,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,971 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.2355763912200928
2023-01-07 07:48:50,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,971 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:50,971 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,971 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 07:48:50,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,972 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:50,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,972 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:50,972 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,972 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,972 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 07:48:50,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,973 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 19.3155460357666
2023-01-07 07:48:50,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,974 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:50,974 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,974 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,974 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 07:48:50,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,974 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:50,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,975 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:50,975 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,975 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,975 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 07:48:50,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,976 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -20.39568328857422
2023-01-07 07:48:50,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,976 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:48:50,976 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,976 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 07:48:50,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,977 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,977 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,977 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,978 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 07:48:50,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,979 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 21.890974044799805
2023-01-07 07:48:50,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,979 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:50,979 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,979 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,979 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 07:48:50,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,980 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,980 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,980 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,980 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,981 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 07:48:50,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,981 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.37729549407959
2023-01-07 07:48:50,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,982 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:50,982 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,982 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,982 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 07:48:50,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,983 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:50,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,983 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:50,983 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,983 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,983 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 07:48:50,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,984 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 10.350849151611328
2023-01-07 07:48:50,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,984 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:50,984 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,984 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,984 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 07:48:50,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,985 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:50,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,986 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:50,986 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,986 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,986 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 07:48:50,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,987 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -74.88262939453125
2023-01-07 07:48:50,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,987 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:50,987 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,987 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,987 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 07:48:50,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,988 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,988 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,988 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,989 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,989 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 07:48:50,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,989 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 5.852261543273926
2023-01-07 07:48:50,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,990 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:50,990 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,990 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,990 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 07:48:50,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,991 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,991 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,991 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,991 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,991 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 07:48:50,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,992 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -9.101005554199219
2023-01-07 07:48:50,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,993 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:50,993 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,993 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,993 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 07:48:50,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,993 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:50,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,994 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:50,994 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,994 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,994 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 07:48:50,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,995 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -42.156307220458984
2023-01-07 07:48:50,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,995 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:50,995 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,995 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,995 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 07:48:50,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,996 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,996 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,996 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,996 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,996 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 07:48:50,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,997 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -28.29714012145996
2023-01-07 07:48:50,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,998 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:50,998 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,998 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,998 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 07:48:50,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,999 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:50,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:50,999 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:50,999 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:50,999 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:50,999 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 07:48:50,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,000 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 17.017976760864258
2023-01-07 07:48:51,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,000 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:51,000 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,000 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,000 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 07:48:51,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,001 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,002 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,002 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,002 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,002 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 07:48:51,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,003 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 9.676246643066406
2023-01-07 07:48:51,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,003 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:51,003 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,003 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,003 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 07:48:51,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,004 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:51,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,004 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:51,004 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,004 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,004 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 07:48:51,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,005 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 6.567827224731445
2023-01-07 07:48:51,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,005 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:51,006 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,006 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,006 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 07:48:51,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,006 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:51,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,007 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:51,007 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,007 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,007 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 07:48:51,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,008 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 18.30142593383789
2023-01-07 07:48:51,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,008 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:51,008 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,008 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,008 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 07:48:51,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,009 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,009 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,009 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,009 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,009 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 07:48:51,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,010 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 47.5260124206543
2023-01-07 07:48:51,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,011 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:51,011 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,011 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,011 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 07:48:51,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,012 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,012 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,012 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,012 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,012 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 07:48:51,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,013 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.410390853881836
2023-01-07 07:48:51,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,013 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,013 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,013 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,013 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 07:48:51,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,014 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,015 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,015 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,015 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,015 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 07:48:51,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,016 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -15.702336311340332
2023-01-07 07:48:51,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,016 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,016 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,016 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,016 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 07:48:51,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,017 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,017 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,018 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,018 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,018 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 07:48:51,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,018 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 61.0592155456543
2023-01-07 07:48:51,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,019 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:51,019 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,019 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,019 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 07:48:51,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,020 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,020 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,020 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,020 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,020 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 07:48:51,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,021 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.296527862548828
2023-01-07 07:48:51,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,021 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,021 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,021 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,021 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 07:48:51,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,022 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,023 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,023 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,023 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,023 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 07:48:51,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,024 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.677282810211182
2023-01-07 07:48:51,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,024 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,024 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,024 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,024 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 07:48:51,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,025 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,025 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,025 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,025 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,025 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 07:48:51,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,026 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 3.829287528991699
2023-01-07 07:48:51,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,027 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,027 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,027 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,027 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 07:48:51,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,027 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,028 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,028 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,028 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,028 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 07:48:51,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,029 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -26.810039520263672
2023-01-07 07:48:51,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,029 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,029 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,029 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,029 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 07:48:51,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,030 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,030 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,030 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,030 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,031 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 07:48:51,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,031 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 24.95868492126465
2023-01-07 07:48:51,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,032 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,032 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,032 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,032 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 07:48:51,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,033 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,033 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,033 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,033 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,033 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 07:48:51,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,034 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.2516183853149414
2023-01-07 07:48:51,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,034 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,034 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,034 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,034 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 07:48:51,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,035 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,035 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,035 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,036 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,036 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 07:48:51,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,036 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 39.66939926147461
2023-01-07 07:48:51,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,037 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,037 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,037 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,037 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 07:48:51,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,038 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,038 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,038 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,038 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,038 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 07:48:51,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,039 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85781192779541
2023-01-07 07:48:51,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,039 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,040 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,040 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,040 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 07:48:51,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,040 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,041 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,041 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,041 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,041 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 07:48:51,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,042 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 2.4547319412231445
2023-01-07 07:48:51,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,042 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,042 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,042 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,042 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 07:48:51,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,043 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,043 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,043 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,043 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,043 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 07:48:51,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,044 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -26.943378448486328
2023-01-07 07:48:51,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,044 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,044 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,045 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,045 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 07:48:51,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,045 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,046 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,046 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,046 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,046 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 07:48:51,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,047 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 29.764585494995117
2023-01-07 07:48:51,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,047 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,047 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,047 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,047 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 07:48:51,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,048 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,048 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,048 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,048 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,048 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 07:48:51,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,049 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.420494079589844
2023-01-07 07:48:51,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,049 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,050 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,050 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,050 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 07:48:51,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,050 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,051 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,051 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,051 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,051 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 07:48:51,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,052 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -81.59500122070312
2023-01-07 07:48:51,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,052 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,052 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,052 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,052 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 07:48:51,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,053 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,053 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,053 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,053 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,053 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 07:48:51,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,054 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -25.087913513183594
2023-01-07 07:48:51,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,055 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:51,055 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,055 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,055 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 07:48:51,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,055 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:51,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,056 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:51,056 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,056 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,056 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 07:48:51,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,057 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23.904560089111328
2023-01-07 07:48:51,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,057 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:51,057 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,057 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,057 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 07:48:51,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,058 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 07:48:51,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,058 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:51,058 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,058 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,058 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 07:48:51,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,059 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -68.88726806640625
2023-01-07 07:48:51,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,060 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:51,060 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,060 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,060 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 07:48:51,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,061 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,061 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,061 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,061 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,061 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 07:48:51,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,062 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.03201866149902344
2023-01-07 07:48:51,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,062 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:51,062 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,062 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,062 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 07:48:51,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,063 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,064 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,064 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,064 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,064 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 07:48:51,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,065 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 5.009161949157715
2023-01-07 07:48:51,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,065 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:51,065 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,065 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,065 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 07:48:51,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,066 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:48:51,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,066 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:51,066 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,066 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,066 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 07:48:51,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,067 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.8457889556884766
2023-01-07 07:48:51,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,067 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:48:51,067 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,067 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,067 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 07:48:51,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,068 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:48:51,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,068 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:51,069 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,069 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,069 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 07:48:51,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,069 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 12.287728309631348
2023-01-07 07:48:51,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,070 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:51,070 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,070 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,070 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 07:48:51,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,071 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,071 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,071 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,071 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,071 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 07:48:51,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,072 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -45.334171295166016
2023-01-07 07:48:51,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,072 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:51,072 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,073 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,073 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 07:48:51,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,073 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,074 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,074 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,074 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,074 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 07:48:51,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,075 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -41.74021911621094
2023-01-07 07:48:51,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,075 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:51,075 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,075 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,075 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 07:48:51,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,076 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:48:51,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,076 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:51,076 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,077 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,077 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 07:48:51,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,077 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -130.3599395751953
2023-01-07 07:48:51,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,078 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:51,078 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,078 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,078 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 07:48:51,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,079 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,079 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,079 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,079 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,079 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 07:48:51,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,080 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.4918365478515625
2023-01-07 07:48:51,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,080 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:51,080 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,080 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,080 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 07:48:51,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,081 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:51,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,081 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:51,081 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,082 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,082 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 07:48:51,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,082 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -16.49246597290039
2023-01-07 07:48:51,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,083 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:51,083 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,083 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,083 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 07:48:51,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,084 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 07:48:51,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,084 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:51,084 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,084 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,084 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 07:48:51,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,085 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.924813270568848
2023-01-07 07:48:51,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,086 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:48:51,086 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,086 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 07:48:51,086 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 07:48:51,087 > [DEBUG] 0 :: 7.401515007019043
2023-01-07 07:48:51,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -358.92535400390625
2023-01-07 07:48:51,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,094 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,095 > [DEBUG] 0 :: before allreduce fusion buffer :: -351.79022216796875
2023-01-07 07:48:51,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,108 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.022848740220069885
2023-01-07 07:48:51,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9792137742042542
2023-01-07 07:48:51,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,111 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13026629388332367
2023-01-07 07:48:51,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17710036039352417
2023-01-07 07:48:51,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,113 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7786242365837097
2023-01-07 07:48:51,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,114 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5287396907806396
2023-01-07 07:48:51,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,116 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18900156021118164
2023-01-07 07:48:51,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,117 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5906467437744141
2023-01-07 07:48:51,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13516950607299805
2023-01-07 07:48:51,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06387212872505188
2023-01-07 07:48:51,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6460385322570801
2023-01-07 07:48:51,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,121 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47695261240005493
2023-01-07 07:48:51,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,123 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05942850187420845
2023-01-07 07:48:51,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,124 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1856589317321777
2023-01-07 07:48:51,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,126 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,126 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23904797434806824
2023-01-07 07:48:51,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,126 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,127 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.088089942932129
2023-01-07 07:48:51,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,128 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4201669692993164
2023-01-07 07:48:51,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,129 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -189.52639770507812
2023-01-07 07:48:51,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,131 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.615147113800049
2023-01-07 07:48:51,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07773042470216751
2023-01-07 07:48:51,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,133 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17392298579216003
2023-01-07 07:48:51,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,134 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1527364253997803
2023-01-07 07:48:51,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,136 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012037277221679688
2023-01-07 07:48:51,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,137 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.703195095062256
2023-01-07 07:48:51,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,139 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4682237505912781
2023-01-07 07:48:51,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,140 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5289647579193115
2023-01-07 07:48:51,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,141 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,141 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5457154512405396
2023-01-07 07:48:51,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,142 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.235780715942383
2023-01-07 07:48:51,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.945148468017578
2023-01-07 07:48:51,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,144 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5890334844589233
2023-01-07 07:48:51,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,145 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18665307760238647
2023-01-07 07:48:51,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,146 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5612339973449707
2023-01-07 07:48:51,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,148 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,148 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5231732726097107
2023-01-07 07:48:51,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,148 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2340662479400635
2023-01-07 07:48:51,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,150 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8840633034706116
2023-01-07 07:48:51,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,151 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.538015604019165
2023-01-07 07:48:51,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,152 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.575730323791504
2023-01-07 07:48:51,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,153 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.676544189453125
2023-01-07 07:48:51,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,154 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,154 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6833772659301758
2023-01-07 07:48:51,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,155 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3574785888195038
2023-01-07 07:48:51,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,156 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3669874668121338
2023-01-07 07:48:51,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,157 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06041300296783447
2023-01-07 07:48:51,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,159 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.281856536865234
2023-01-07 07:48:51,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,159 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9068732857704163
2023-01-07 07:48:51,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,161 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,161 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2957912087440491
2023-01-07 07:48:51,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,162 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1330699920654297
2023-01-07 07:48:51,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,163 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3442537486553192
2023-01-07 07:48:51,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.742346286773682
2023-01-07 07:48:51,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,165 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.634756326675415
2023-01-07 07:48:51,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,166 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.675725936889648
2023-01-07 07:48:51,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,168 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,168 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.626469612121582
2023-01-07 07:48:51,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,169 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.229083061218262
2023-01-07 07:48:51,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,170 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.380526065826416
2023-01-07 07:48:51,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,171 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14700031280517578
2023-01-07 07:48:51,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,172 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.043537139892578
2023-01-07 07:48:51,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.321745872497559
2023-01-07 07:48:51,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,175 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23848813772201538
2023-01-07 07:48:51,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.426555871963501
2023-01-07 07:48:51,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,176 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8140392303466797
2023-01-07 07:48:51,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.766350746154785
2023-01-07 07:48:51,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,178 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.68531608581543
2023-01-07 07:48:51,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.699713230133057
2023-01-07 07:48:51,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,180 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9193625450134277
2023-01-07 07:48:51,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.351327896118164
2023-01-07 07:48:51,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,182 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7601518630981445
2023-01-07 07:48:51,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,183 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.512423038482666
2023-01-07 07:48:51,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,184 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9369675517082214
2023-01-07 07:48:51,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.1500959396362305
2023-01-07 07:48:51,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,186 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,186 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1542582511901855
2023-01-07 07:48:51,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,187 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.077185153961182
2023-01-07 07:48:51,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,188 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,188 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.731952667236328
2023-01-07 07:48:51,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8644942045211792
2023-01-07 07:48:51,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,190 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.181410789489746
2023-01-07 07:48:51,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17793512344360352
2023-01-07 07:48:51,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,192 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.800865650177002
2023-01-07 07:48:51,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,192 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.932552337646484
2023-01-07 07:48:51,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,194 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.808645248413086
2023-01-07 07:48:51,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3514504432678223
2023-01-07 07:48:51,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,196 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,196 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1811020374298096
2023-01-07 07:48:51,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.857280731201172
2023-01-07 07:48:51,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,198 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.39471960067749
2023-01-07 07:48:51,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,199 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.9299898147583
2023-01-07 07:48:51,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.248041152954102
2023-01-07 07:48:51,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,201 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,201 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.47996520996094
2023-01-07 07:48:51,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.374513626098633
2023-01-07 07:48:51,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.698081970214844
2023-01-07 07:48:51,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,205 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.954734802246094
2023-01-07 07:48:51,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,206 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.89680099487305
2023-01-07 07:48:51,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.358053207397461
2023-01-07 07:48:51,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,208 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,209 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.889947891235352
2023-01-07 07:48:51,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6706264019012451
2023-01-07 07:48:51,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,211 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.515385627746582
2023-01-07 07:48:51,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.59355926513672
2023-01-07 07:48:51,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,214 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.51522445678711
2023-01-07 07:48:51,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,215 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.232940673828125
2023-01-07 07:48:51,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,215 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,216 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.289416313171387
2023-01-07 07:48:51,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.217788696289062
2023-01-07 07:48:51,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,218 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.586769104003906
2023-01-07 07:48:51,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,219 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.669599533081055
2023-01-07 07:48:51,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.27769470214844
2023-01-07 07:48:51,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,221 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.480388641357422
2023-01-07 07:48:51,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,221 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.05977249145508
2023-01-07 07:48:51,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.145633697509766
2023-01-07 07:48:51,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.33968734741211
2023-01-07 07:48:51,230 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:48:51,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,230 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:51,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 1673.582275390625
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -120.0829849243164
2023-01-07 07:48:51,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,240 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.888877868652344
2023-01-07 07:48:51,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.65308380126953
2023-01-07 07:48:51,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:51,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:51,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 700.5570068359375
2023-01-07 07:48:51,242 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -1.922404408454895
2023-01-07 07:48:51,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:51,242 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:48:51,242 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:51,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:48:52,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,102 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,102 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.05027770996094
2023-01-07 07:48:52,103 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,103 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,104 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,104 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,104 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -8.645812034606934
2023-01-07 07:48:52,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,104 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,104 > [DEBUG] 0 :: before allreduce fusion buffer :: -67.960693359375
2023-01-07 07:48:52,105 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -6.646210193634033
2023-01-07 07:48:52,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,106 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:52,106 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,106 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,106 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:52,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 165.07620239257812
2023-01-07 07:48:52,107 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,107 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,107 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:52,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.242185592651367
2023-01-07 07:48:52,108 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -14.367234230041504
2023-01-07 07:48:52,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,108 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:52,109 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:48:52,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,109 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -70.28623962402344
2023-01-07 07:48:52,110 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,111 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,111 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:52,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.222949028015137
2023-01-07 07:48:52,112 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 36.116695404052734
2023-01-07 07:48:52,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,112 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,112 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,112 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,112 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:48:52,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,112 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.959553718566895
2023-01-07 07:48:52,113 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:52,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,114 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,114 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:52,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.8102912902832
2023-01-07 07:48:52,115 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 6.535425186157227
2023-01-07 07:48:52,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,115 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,115 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,115 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,115 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:52,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.610511779785156
2023-01-07 07:48:52,116 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 261.5999450683594
2023-01-07 07:48:52,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,116 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,117 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,117 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,117 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:52,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.068090438842773
2023-01-07 07:48:52,118 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 24.492828369140625
2023-01-07 07:48:52,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,118 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,118 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:48:52,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,118 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,119 > [DEBUG] 0 :: before allreduce fusion buffer :: -215.17105102539062
2023-01-07 07:48:52,119 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,120 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,120 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,120 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,120 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:52,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.87321662902832
2023-01-07 07:48:52,121 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.8943326473236084
2023-01-07 07:48:52,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,121 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:52,121 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:48:52,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,122 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.556236267089844
2023-01-07 07:48:52,123 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,123 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,123 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:52,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.54829216003418
2023-01-07 07:48:52,124 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 19.24296760559082
2023-01-07 07:48:52,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,124 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,124 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:52,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.269275665283203
2023-01-07 07:48:52,125 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 261.4000244140625
2023-01-07 07:48:52,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,126 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,126 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:52,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.945953369140625
2023-01-07 07:48:52,127 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 9.861352920532227
2023-01-07 07:48:52,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,127 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,127 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:52,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.774606704711914
2023-01-07 07:48:52,128 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 65.4000015258789
2023-01-07 07:48:52,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,129 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,129 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,129 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,129 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:52,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,129 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.984119415283203
2023-01-07 07:48:52,130 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.2355763912200928
2023-01-07 07:48:52,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,130 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:52,130 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 07:48:52,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,130 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,131 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.86061668395996
2023-01-07 07:48:52,132 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 07:48:52,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,132 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:52,132 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:52,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.635150909423828
2023-01-07 07:48:52,133 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 23.116010665893555
2023-01-07 07:48:52,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,134 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:52,134 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,134 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,134 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -20.39568328857422
2023-01-07 07:48:52,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.58026885986328
2023-01-07 07:48:52,135 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 262.40008544921875
2023-01-07 07:48:52,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,135 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,135 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:52,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.508905410766602
2023-01-07 07:48:52,136 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -20.39568328857422
2023-01-07 07:48:52,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,137 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:48:52,137 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:52,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.607067108154297
2023-01-07 07:48:52,138 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 130.00003051757812
2023-01-07 07:48:52,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,138 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,138 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,138 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:52,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.139695167541504
2023-01-07 07:48:52,139 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 21.890974044799805
2023-01-07 07:48:52,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,139 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:52,140 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,140 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,140 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7582027912139893
2023-01-07 07:48:52,141 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,141 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,141 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -11.37729549407959
2023-01-07 07:48:52,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.601271152496338
2023-01-07 07:48:52,142 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.37729549407959
2023-01-07 07:48:52,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,143 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,143 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:48:52,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.587078094482422
2023-01-07 07:48:52,144 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:52,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,144 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,144 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,145 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 10.350849151611328
2023-01-07 07:48:52,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.46389102935791
2023-01-07 07:48:52,145 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 10.350849151611328
2023-01-07 07:48:52,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,146 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:52,146 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:52,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,146 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.639290809631348
2023-01-07 07:48:52,147 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 540.5999755859375
2023-01-07 07:48:52,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,147 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,147 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,147 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:52,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,147 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.696361541748047
2023-01-07 07:48:52,148 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -74.88262939453125
2023-01-07 07:48:52,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,149 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,149 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,149 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34229421615600586
2023-01-07 07:48:52,150 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,150 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,150 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 5.852261543273926
2023-01-07 07:48:52,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.762818336486816
2023-01-07 07:48:52,152 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 5.852261543273926
2023-01-07 07:48:52,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,152 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:52,152 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,152 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.69904613494873
2023-01-07 07:48:52,153 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,154 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,154 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -9.101005554199219
2023-01-07 07:48:52,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5977529287338257
2023-01-07 07:48:52,155 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -9.101005554199219
2023-01-07 07:48:52,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,155 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,155 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:52,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.8964757919311523
2023-01-07 07:48:52,156 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 549.60107421875
2023-01-07 07:48:52,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,156 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,157 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:52,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0294110774993896
2023-01-07 07:48:52,158 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -42.156307220458984
2023-01-07 07:48:52,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,158 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,158 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,158 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.633919715881348
2023-01-07 07:48:52,159 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,160 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,160 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,160 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.29714012145996
2023-01-07 07:48:52,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.448659896850586
2023-01-07 07:48:52,161 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -28.29714012145996
2023-01-07 07:48:52,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,161 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:52,161 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,162 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10638183355331421
2023-01-07 07:48:52,163 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,163 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,163 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 17.017976760864258
2023-01-07 07:48:52,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.844193696975708
2023-01-07 07:48:52,164 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 17.017976760864258
2023-01-07 07:48:52,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,164 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,164 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:48:52,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,165 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.189496040344238
2023-01-07 07:48:52,166 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:52,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,166 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,166 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 9.676246643066406
2023-01-07 07:48:52,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2270777225494385
2023-01-07 07:48:52,167 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 9.676246643066406
2023-01-07 07:48:52,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,168 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,168 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,168 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,168 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7629103660583496
2023-01-07 07:48:52,169 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 07:48:52,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,169 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,169 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.567827224731445
2023-01-07 07:48:52,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.052694320678711
2023-01-07 07:48:52,170 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 6.567827224731445
2023-01-07 07:48:52,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,171 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:52,171 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,171 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,171 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 07:48:52,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,171 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8786029815673828
2023-01-07 07:48:52,172 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.40005493164062
2023-01-07 07:48:52,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,173 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:52,173 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,173 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 18.30142593383789
2023-01-07 07:48:52,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.84022855758667
2023-01-07 07:48:52,174 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 18.30142593383789
2023-01-07 07:48:52,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,174 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:52,174 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 07:48:52,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,174 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.727907180786133
2023-01-07 07:48:52,175 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 07:48:52,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,176 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,176 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 47.5260124206543
2023-01-07 07:48:52,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.891597270965576
2023-01-07 07:48:52,177 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 47.5260124206543
2023-01-07 07:48:52,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,177 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:52,177 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:48:52,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,177 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.764610290527344
2023-01-07 07:48:52,179 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 257.6048583984375
2023-01-07 07:48:52,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,179 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,179 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.410390853881836
2023-01-07 07:48:52,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,180 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5802835822105408
2023-01-07 07:48:52,180 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.410390853881836
2023-01-07 07:48:52,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,181 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,181 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,181 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,181 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:52,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6871371269226074
2023-01-07 07:48:52,182 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 296.9998779296875
2023-01-07 07:48:52,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,182 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,182 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:52,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.1422576904296875
2023-01-07 07:48:52,183 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -15.702336311340332
2023-01-07 07:48:52,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,183 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,183 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,183 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:52,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.729718208312988
2023-01-07 07:48:52,184 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1192.1983642578125
2023-01-07 07:48:52,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,185 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,185 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,185 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:52,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2673040628433228
2023-01-07 07:48:52,186 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 61.0592155456543
2023-01-07 07:48:52,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,186 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:52,186 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,186 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:52,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,186 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.24904203414917
2023-01-07 07:48:52,187 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1195.199462890625
2023-01-07 07:48:52,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,188 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,188 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:52,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,188 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.593606948852539
2023-01-07 07:48:52,189 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.296527862548828
2023-01-07 07:48:52,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,189 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,189 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 07:48:52,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,189 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 07:48:52,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.266101837158203
2023-01-07 07:48:52,190 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 07:48:52,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,191 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,191 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.677282810211182
2023-01-07 07:48:52,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6695884466171265
2023-01-07 07:48:52,192 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.677282810211182
2023-01-07 07:48:52,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,192 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,192 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:52,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,193 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7587485313415527
2023-01-07 07:48:52,193 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 299.19989013671875
2023-01-07 07:48:52,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,194 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,194 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:52,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0099315643310547
2023-01-07 07:48:52,195 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 3.829287528991699
2023-01-07 07:48:52,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,195 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,195 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:52,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1465363502502441
2023-01-07 07:48:52,196 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1207.5997314453125
2023-01-07 07:48:52,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,196 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,196 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:52,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7872085571289062
2023-01-07 07:48:52,198 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -26.810039520263672
2023-01-07 07:48:52,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,198 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,198 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:52,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9011669158935547
2023-01-07 07:48:52,199 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 300.79974365234375
2023-01-07 07:48:52,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,199 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,199 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:52,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,200 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.18891716003418
2023-01-07 07:48:52,200 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 24.95868492126465
2023-01-07 07:48:52,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,201 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,201 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:52,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,201 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6342663764953613
2023-01-07 07:48:52,202 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 301.5995788574219
2023-01-07 07:48:52,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,202 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,202 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:52,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.323518753051758
2023-01-07 07:48:52,203 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.2516183853149414
2023-01-07 07:48:52,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,204 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,204 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:52,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.392111301422119
2023-01-07 07:48:52,205 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1211.02490234375
2023-01-07 07:48:52,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,205 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,205 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,205 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:52,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.034385427832603455
2023-01-07 07:48:52,206 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 39.66939926147461
2023-01-07 07:48:52,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,206 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,206 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,207 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,207 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:52,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.304879188537598
2023-01-07 07:48:52,208 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 302.7994689941406
2023-01-07 07:48:52,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,208 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,208 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:52,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7577229738235474
2023-01-07 07:48:52,209 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85781192779541
2023-01-07 07:48:52,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,209 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,209 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:52,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8749363422393799
2023-01-07 07:48:52,210 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 302.000732421875
2023-01-07 07:48:52,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,211 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,211 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:52,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40504521131515503
2023-01-07 07:48:52,212 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 2.4547319412231445
2023-01-07 07:48:52,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,212 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,212 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:52,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7242217063903809
2023-01-07 07:48:52,213 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1216.0006103515625
2023-01-07 07:48:52,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,214 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,214 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,214 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,214 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:52,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0393857955932617
2023-01-07 07:48:52,215 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -26.943378448486328
2023-01-07 07:48:52,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,215 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,215 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:52,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,216 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1236355304718018
2023-01-07 07:48:52,216 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 302.7999572753906
2023-01-07 07:48:52,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,217 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,217 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:52,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5198250412940979
2023-01-07 07:48:52,218 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 29.764585494995117
2023-01-07 07:48:52,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,218 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,218 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,218 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,218 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:52,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22703738510608673
2023-01-07 07:48:52,219 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 303.19976806640625
2023-01-07 07:48:52,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,219 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,219 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:52,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,220 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10485246032476425
2023-01-07 07:48:52,221 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.420494079589844
2023-01-07 07:48:52,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,221 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,221 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:52,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5905520915985107
2023-01-07 07:48:52,222 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1218.19873046875
2023-01-07 07:48:52,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,222 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,222 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:52,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0594865083694458
2023-01-07 07:48:52,224 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -81.59500122070312
2023-01-07 07:48:52,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,224 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,224 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:52,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,224 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9318111538887024
2023-01-07 07:48:52,225 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 303.3987731933594
2023-01-07 07:48:52,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,225 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,225 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:52,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.029200706630945206
2023-01-07 07:48:52,226 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -25.087913513183594
2023-01-07 07:48:52,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,227 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:52,227 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:52,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3368895053863525
2023-01-07 07:48:52,228 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 304.3995056152344
2023-01-07 07:48:52,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,228 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:52,228 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:52,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6890332698822021
2023-01-07 07:48:52,229 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23.904560089111328
2023-01-07 07:48:52,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,230 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:52,230 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:52,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1793501377105713
2023-01-07 07:48:52,231 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1220.5994873046875
2023-01-07 07:48:52,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,231 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:52,231 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:52,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,231 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6171941757202148
2023-01-07 07:48:52,232 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -68.88726806640625
2023-01-07 07:48:52,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,232 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:52,232 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:52,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.035360656678676605
2023-01-07 07:48:52,233 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 610.9994506835938
2023-01-07 07:48:52,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,234 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,234 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:52,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,234 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30019617080688477
2023-01-07 07:48:52,235 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.03201866149902344
2023-01-07 07:48:52,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,235 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:52,235 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:52,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2220573425292969
2023-01-07 07:48:52,236 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 517.7974853515625
2023-01-07 07:48:52,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,237 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,237 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:52,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04832844436168671
2023-01-07 07:48:52,238 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 5.009161949157715
2023-01-07 07:48:52,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,238 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:52,238 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,238 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:52,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4679110646247864
2023-01-07 07:48:52,239 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2450.597412109375
2023-01-07 07:48:52,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,239 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:52,239 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:52,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,240 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6868017315864563
2023-01-07 07:48:52,240 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.8457889556884766
2023-01-07 07:48:52,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,241 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:48:52,241 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:52,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9618210792541504
2023-01-07 07:48:52,242 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2451.39794921875
2023-01-07 07:48:52,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,242 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:52,242 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:52,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5247464179992676
2023-01-07 07:48:52,243 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 12.287728309631348
2023-01-07 07:48:52,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,244 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:52,244 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:52,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5396710634231567
2023-01-07 07:48:52,245 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 612.1972045898438
2023-01-07 07:48:52,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,245 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,245 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:52,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.025475695729255676
2023-01-07 07:48:52,246 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -45.334171295166016
2023-01-07 07:48:52,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,246 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:52,246 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:52,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4800493121147156
2023-01-07 07:48:52,247 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.1973876953125
2023-01-07 07:48:52,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,248 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,248 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:52,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11528733372688293
2023-01-07 07:48:52,249 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -41.74021911621094
2023-01-07 07:48:52,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,249 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:52,249 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:52,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0994277000427246
2023-01-07 07:48:52,250 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2452.798095703125
2023-01-07 07:48:52,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,251 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:52,251 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:52,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45978349447250366
2023-01-07 07:48:52,252 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -130.3599395751953
2023-01-07 07:48:52,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,252 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:52,252 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:52,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,253 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5299960374832153
2023-01-07 07:48:52,253 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 612.3994140625
2023-01-07 07:48:52,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,254 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,254 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:52,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03156007081270218
2023-01-07 07:48:52,255 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.4918365478515625
2023-01-07 07:48:52,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,255 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:52,255 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:52,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,256 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3589526414871216
2023-01-07 07:48:52,256 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 612.3995361328125
2023-01-07 07:48:52,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,256 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:52,257 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:52,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3260788917541504
2023-01-07 07:48:52,258 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -16.49246597290039
2023-01-07 07:48:52,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,258 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:52,258 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:52,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.472301483154297
2023-01-07 07:48:52,259 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2453.791748046875
2023-01-07 07:48:52,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,259 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:52,259 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:52,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21477791666984558
2023-01-07 07:48:52,261 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.924813270568848
2023-01-07 07:48:52,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,261 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:48:52,261 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:52,262 > [DEBUG] 0 :: 7.568788528442383
2023-01-07 07:48:52,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,264 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -639.517578125
2023-01-07 07:48:52,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,266 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -646.201416015625
2023-01-07 07:48:52,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,270 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.61760711669922
2023-01-07 07:48:52,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,272 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -406.121826171875
2023-01-07 07:48:52,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,275 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5029507875442505
2023-01-07 07:48:52,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,276 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -330.65777587890625
2023-01-07 07:48:52,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,277 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -202.8616943359375
2023-01-07 07:48:52,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,278 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -252.60122680664062
2023-01-07 07:48:52,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,280 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -103.32720184326172
2023-01-07 07:48:52,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,281 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -269.6429443359375
2023-01-07 07:48:52,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,282 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -134.79798889160156
2023-01-07 07:48:52,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,283 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -358.6875
2023-01-07 07:48:52,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,284 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -249.49563598632812
2023-01-07 07:48:52,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,285 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,285 > [DEBUG] 0 :: before allreduce fusion buffer :: -464.8127746582031
2023-01-07 07:48:52,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,287 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,287 > [DEBUG] 0 :: before allreduce fusion buffer :: -254.93186950683594
2023-01-07 07:48:52,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,288 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -252.19290161132812
2023-01-07 07:48:52,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,289 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -135.78192138671875
2023-01-07 07:48:52,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,290 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -268.960693359375
2023-01-07 07:48:52,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,291 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -93.18055725097656
2023-01-07 07:48:52,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,292 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -466.8266296386719
2023-01-07 07:48:52,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,294 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -279.84527587890625
2023-01-07 07:48:52,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,295 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -419.33221435546875
2023-01-07 07:48:52,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,296 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -169.07833862304688
2023-01-07 07:48:52,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,297 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -503.7803039550781
2023-01-07 07:48:52,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,299 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -162.98440551757812
2023-01-07 07:48:52,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,300 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -462.9466247558594
2023-01-07 07:48:52,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,301 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -268.6163330078125
2023-01-07 07:48:52,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,302 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -413.37835693359375
2023-01-07 07:48:52,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,303 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,303 > [DEBUG] 0 :: before allreduce fusion buffer :: -165.7359161376953
2023-01-07 07:48:52,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,304 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -503.18695068359375
2023-01-07 07:48:52,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,305 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -158.43807983398438
2023-01-07 07:48:52,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,306 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -461.9737548828125
2023-01-07 07:48:52,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,308 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -265.12823486328125
2023-01-07 07:48:52,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,309 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -416.60577392578125
2023-01-07 07:48:52,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,310 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -158.3775177001953
2023-01-07 07:48:52,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,311 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -503.8092041015625
2023-01-07 07:48:52,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,312 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,312 > [DEBUG] 0 :: before allreduce fusion buffer :: -155.7889862060547
2023-01-07 07:48:52,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,313 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -460.688720703125
2023-01-07 07:48:52,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,314 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -263.26739501953125
2023-01-07 07:48:52,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,315 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -418.2087097167969
2023-01-07 07:48:52,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,317 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -139.18505859375
2023-01-07 07:48:52,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,318 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -500.5
2023-01-07 07:48:52,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,319 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -125.11698913574219
2023-01-07 07:48:52,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,320 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -437.9248352050781
2023-01-07 07:48:52,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,321 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -233.8035888671875
2023-01-07 07:48:52,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,322 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -407.5882568359375
2023-01-07 07:48:52,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,324 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.92941284179688
2023-01-07 07:48:52,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,325 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -497.6793212890625
2023-01-07 07:48:52,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,326 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -92.82194519042969
2023-01-07 07:48:52,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,327 > [DEBUG] 0 :: before allreduce fusion buffer :: -434.73773193359375
2023-01-07 07:48:52,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,328 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -189.011474609375
2023-01-07 07:48:52,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,329 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -440.67822265625
2023-01-07 07:48:52,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,330 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.89301681518555
2023-01-07 07:48:52,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,331 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -374.2340087890625
2023-01-07 07:48:52,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,332 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.03839874267578
2023-01-07 07:48:52,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,333 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,333 > [DEBUG] 0 :: before allreduce fusion buffer :: -514.5537109375
2023-01-07 07:48:52,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,335 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.9312629699707
2023-01-07 07:48:52,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -419.2200927734375
2023-01-07 07:48:52,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,336 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.26133728027344
2023-01-07 07:48:52,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -285.4517822265625
2023-01-07 07:48:52,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,338 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.822357177734375
2023-01-07 07:48:52,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -380.32342529296875
2023-01-07 07:48:52,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,340 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.231651306152344
2023-01-07 07:48:52,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -444.1666564941406
2023-01-07 07:48:52,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,342 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -83.44462585449219
2023-01-07 07:48:52,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -334.36383056640625
2023-01-07 07:48:52,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,344 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.76036834716797
2023-01-07 07:48:52,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -375.25067138671875
2023-01-07 07:48:52,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,345 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.81067657470703
2023-01-07 07:48:52,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -404.8835144042969
2023-01-07 07:48:52,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,347 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.0826644897461
2023-01-07 07:48:52,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,348 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -306.5391845703125
2023-01-07 07:48:52,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,350 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,350 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.78205108642578
2023-01-07 07:48:52,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -370.44830322265625
2023-01-07 07:48:52,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,351 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.30802536010742
2023-01-07 07:48:52,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -437.5215148925781
2023-01-07 07:48:52,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,353 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,353 > [DEBUG] 0 :: before allreduce fusion buffer :: -98.29066467285156
2023-01-07 07:48:52,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,354 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -388.2711181640625
2023-01-07 07:48:52,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,355 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.209312438964844
2023-01-07 07:48:52,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,356 > [DEBUG] 0 :: before allreduce fusion buffer :: -336.7133483886719
2023-01-07 07:48:52,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,357 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.166236877441406
2023-01-07 07:48:52,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -414.4818115234375
2023-01-07 07:48:52,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,359 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.737083435058594
2023-01-07 07:48:52,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,360 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -380.8150329589844
2023-01-07 07:48:52,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -110.78457641601562
2023-01-07 07:48:52,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,362 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,362 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -505.0638122558594
2023-01-07 07:48:52,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.084285736083984
2023-01-07 07:48:52,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -434.1275329589844
2023-01-07 07:48:52,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,365 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,366 > [DEBUG] 0 :: before allreduce fusion buffer :: -71.59835815429688
2023-01-07 07:48:52,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,366 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,366 > [DEBUG] 0 :: before allreduce fusion buffer :: -296.0998229980469
2023-01-07 07:48:52,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -91.72857666015625
2023-01-07 07:48:52,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,368 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,369 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -490.88519287109375
2023-01-07 07:48:52,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -86.27792358398438
2023-01-07 07:48:52,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,371 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -463.828857421875
2023-01-07 07:48:52,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -117.17753601074219
2023-01-07 07:48:52,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,373 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,373 > [DEBUG] 0 :: before allreduce fusion buffer :: -282.8391418457031
2023-01-07 07:48:52,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -206.5552520751953
2023-01-07 07:48:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,375 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,375 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -464.0525817871094
2023-01-07 07:48:52,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.42501449584961
2023-01-07 07:48:52,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,377 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -469.280517578125
2023-01-07 07:48:52,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -156.9246063232422
2023-01-07 07:48:52,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -604.1356201171875
2023-01-07 07:48:52,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,380 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -276.12481689453125
2023-01-07 07:48:52,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,381 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -407.93890380859375
2023-01-07 07:48:52,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -137.33486938476562
2023-01-07 07:48:52,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -187.265625
2023-01-07 07:48:52,385 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,385 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 844.3057861328125
2023-01-07 07:48:52,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 132.17999267578125
2023-01-07 07:48:52,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 259.34765625
2023-01-07 07:48:52,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 608.38525390625
2023-01-07 07:48:52,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:52,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:52,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 1847.6787109375
2023-01-07 07:48:52,394 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 119.88836669921875
2023-01-07 07:48:52,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:52,394 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:48:52,394 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:52,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 63.79999923706055
2023-01-07 07:48:53,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -245.21371459960938
2023-01-07 07:48:53,242 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0813217163086
2023-01-07 07:48:53,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,242 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,242 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.646210193634033
2023-01-07 07:48:53,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,243 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,243 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.752546310424805
2023-01-07 07:48:53,244 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: -1.8999290466308594
2023-01-07 07:48:53,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,244 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:53,244 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:53,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.07195281982422
2023-01-07 07:48:53,245 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 65.1386947631836
2023-01-07 07:48:53,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,245 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,246 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:53,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,246 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.12664031982422
2023-01-07 07:48:53,247 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -14.367234230041504
2023-01-07 07:48:53,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,247 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:53,247 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.00000762939453
2023-01-07 07:48:53,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,248 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.229849815368652
2023-01-07 07:48:53,249 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.00000762939453
2023-01-07 07:48:53,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,249 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,249 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:53,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,249 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.444427490234375
2023-01-07 07:48:53,250 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 36.516075134277344
2023-01-07 07:48:53,250 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,251 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,251 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,251 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.1998748779297
2023-01-07 07:48:53,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,251 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.025171279907227
2023-01-07 07:48:53,252 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 254.1998748779297
2023-01-07 07:48:53,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,252 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,252 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:53,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,253 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.781707763671875
2023-01-07 07:48:53,253 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 6.535425186157227
2023-01-07 07:48:53,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,254 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,254 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:53,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 171.25830078125
2023-01-07 07:48:53,255 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 271.0479736328125
2023-01-07 07:48:53,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,255 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,255 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:53,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -89.34306335449219
2023-01-07 07:48:53,256 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 24.492828369140625
2023-01-07 07:48:53,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,256 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,256 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.80000305175781
2023-01-07 07:48:53,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,257 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.438623428344727
2023-01-07 07:48:53,258 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 61.80000305175781
2023-01-07 07:48:53,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,258 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,258 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:53,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.502727508544922
2023-01-07 07:48:53,259 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.8943326473236084
2023-01-07 07:48:53,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,260 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:53,260 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 62.799991607666016
2023-01-07 07:48:53,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,260 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.342225551605225
2023-01-07 07:48:53,261 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 62.799991607666016
2023-01-07 07:48:53,261 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,261 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,261 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:53,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.22908020019531
2023-01-07 07:48:53,263 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 19.24296760559082
2023-01-07 07:48:53,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,263 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,263 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:53,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.002466201782227
2023-01-07 07:48:53,264 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 273.5506286621094
2023-01-07 07:48:53,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,264 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,264 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:53,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.059452056884766
2023-01-07 07:48:53,265 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 9.861352920532227
2023-01-07 07:48:53,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,266 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,266 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:53,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.764594078063965
2023-01-07 07:48:53,267 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 68.68264770507812
2023-01-07 07:48:53,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,267 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,267 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:53,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.97510528564453
2023-01-07 07:48:53,268 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.2355763912200928
2023-01-07 07:48:53,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,269 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:53,269 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 62.79999923706055
2023-01-07 07:48:53,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,269 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,269 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.21368408203125
2023-01-07 07:48:53,270 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 62.79999923706055
2023-01-07 07:48:53,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,271 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:53,271 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:53,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.180198669433594
2023-01-07 07:48:53,272 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 24.66999626159668
2023-01-07 07:48:53,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,272 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:53,272 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -20.39568328857422
2023-01-07 07:48:53,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,273 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.017574310302734
2023-01-07 07:48:53,273 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 275.6785583496094
2023-01-07 07:48:53,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,274 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,274 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:53,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.288326263427734
2023-01-07 07:48:53,275 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -20.39568328857422
2023-01-07 07:48:53,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,275 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:48:53,275 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:53,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.800621032714844
2023-01-07 07:48:53,276 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 136.26968383789062
2023-01-07 07:48:53,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,276 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,276 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:53,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.786449432373047
2023-01-07 07:48:53,278 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 21.890974044799805
2023-01-07 07:48:53,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,278 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:53,278 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.80172729492188
2023-01-07 07:48:53,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,278 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.424421310424805
2023-01-07 07:48:53,279 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.80172729492188
2023-01-07 07:48:53,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,280 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,280 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,280 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -11.37729549407959
2023-01-07 07:48:53,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3502254486083984
2023-01-07 07:48:53,281 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.37729549407959
2023-01-07 07:48:53,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,281 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,281 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 516.7991943359375
2023-01-07 07:48:53,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,281 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.215896606445312
2023-01-07 07:48:53,282 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 516.7991943359375
2023-01-07 07:48:53,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,283 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,283 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 10.350849151611328
2023-01-07 07:48:53,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.919963359832764
2023-01-07 07:48:53,284 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 10.350849151611328
2023-01-07 07:48:53,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,284 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:53,284 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:53,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.083131790161133
2023-01-07 07:48:53,285 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 568.80908203125
2023-01-07 07:48:53,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,285 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,285 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:53,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.249996185302734
2023-01-07 07:48:53,287 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -74.88262939453125
2023-01-07 07:48:53,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,287 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,287 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 126.99766540527344
2023-01-07 07:48:53,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,287 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 89.79722595214844
2023-01-07 07:48:53,288 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 126.99766540527344
2023-01-07 07:48:53,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,289 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,289 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 5.852261543273926
2023-01-07 07:48:53,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.704601287841797
2023-01-07 07:48:53,290 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 5.852261543273926
2023-01-07 07:48:53,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,290 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:53,290 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 126.5999526977539
2023-01-07 07:48:53,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,290 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.604794502258301
2023-01-07 07:48:53,292 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 126.5999526977539
2023-01-07 07:48:53,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,292 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,292 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -9.101005554199219
2023-01-07 07:48:53,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.888089656829834
2023-01-07 07:48:53,293 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -9.101005554199219
2023-01-07 07:48:53,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,293 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,293 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:53,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7213025093078613
2023-01-07 07:48:53,294 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 588.2008056640625
2023-01-07 07:48:53,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,295 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,295 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,295 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:53,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.22811222076416
2023-01-07 07:48:53,296 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -42.156307220458984
2023-01-07 07:48:53,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,296 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,296 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 129.19998168945312
2023-01-07 07:48:53,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,296 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2248021364212036
2023-01-07 07:48:53,298 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 129.19998168945312
2023-01-07 07:48:53,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,298 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,298 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.29714012145996
2023-01-07 07:48:53,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.984227180480957
2023-01-07 07:48:53,299 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -28.29714012145996
2023-01-07 07:48:53,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,299 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:53,299 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0000457763672
2023-01-07 07:48:53,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,300 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,300 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0955605506896973
2023-01-07 07:48:53,301 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0000457763672
2023-01-07 07:48:53,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,301 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,301 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 17.017976760864258
2023-01-07 07:48:53,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.748682975769043
2023-01-07 07:48:53,302 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 17.017976760864258
2023-01-07 07:48:53,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,303 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,303 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,303 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,303 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 509.99652099609375
2023-01-07 07:48:53,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,303 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.807138442993164
2023-01-07 07:48:53,304 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 509.99652099609375
2023-01-07 07:48:53,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,304 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,304 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 9.676246643066406
2023-01-07 07:48:53,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4450392723083496
2023-01-07 07:48:53,305 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 9.676246643066406
2023-01-07 07:48:53,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,306 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,306 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.39999389648438
2023-01-07 07:48:53,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,306 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.261690139770508
2023-01-07 07:48:53,307 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.39999389648438
2023-01-07 07:48:53,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,307 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,308 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.567827224731445
2023-01-07 07:48:53,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.074238300323486
2023-01-07 07:48:53,309 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 6.567827224731445
2023-01-07 07:48:53,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,309 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:53,309 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 129.40005493164062
2023-01-07 07:48:53,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,309 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2143622636795044
2023-01-07 07:48:53,311 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.8209228515625
2023-01-07 07:48:53,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,311 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:53,311 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 18.30142593383789
2023-01-07 07:48:53,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.331500768661499
2023-01-07 07:48:53,312 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 18.30142593383789
2023-01-07 07:48:53,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,312 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:53,312 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 510.80303955078125
2023-01-07 07:48:53,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,313 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9602301120758057
2023-01-07 07:48:53,314 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 510.80303955078125
2023-01-07 07:48:53,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,314 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,314 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 47.5260124206543
2023-01-07 07:48:53,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4074108600616455
2023-01-07 07:48:53,315 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 47.5260124206543
2023-01-07 07:48:53,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,316 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:53,316 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 257.6048583984375
2023-01-07 07:48:53,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,316 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.338801860809326
2023-01-07 07:48:53,317 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 257.6048583984375
2023-01-07 07:48:53,317 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,317 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,317 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.410390853881836
2023-01-07 07:48:53,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.71834945678711
2023-01-07 07:48:53,318 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.410390853881836
2023-01-07 07:48:53,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,319 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,319 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:53,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5666918754577637
2023-01-07 07:48:53,320 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 319.85797119140625
2023-01-07 07:48:53,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,320 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,320 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:53,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4505128264427185
2023-01-07 07:48:53,321 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -15.702336311340332
2023-01-07 07:48:53,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,322 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,322 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:53,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.554661512374878
2023-01-07 07:48:53,323 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1320.7952880859375
2023-01-07 07:48:53,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,323 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,323 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:53,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.126960754394531
2023-01-07 07:48:53,324 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 61.0592155456543
2023-01-07 07:48:53,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,324 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:53,324 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:53,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.210721969604492
2023-01-07 07:48:53,325 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1345.8697509765625
2023-01-07 07:48:53,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,326 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,326 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:53,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9819893836975098
2023-01-07 07:48:53,327 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.296527862548828
2023-01-07 07:48:53,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,327 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,327 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 255.60040283203125
2023-01-07 07:48:53,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,327 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 07:48:53,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.859768867492676
2023-01-07 07:48:53,329 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 255.60040283203125
2023-01-07 07:48:53,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,329 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,329 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.677282810211182
2023-01-07 07:48:53,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.576483964920044
2023-01-07 07:48:53,330 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.677282810211182
2023-01-07 07:48:53,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,330 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,330 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,330 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:53,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2539234161376953
2023-01-07 07:48:53,331 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 332.9886779785156
2023-01-07 07:48:53,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,332 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,332 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,332 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:53,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5661726593971252
2023-01-07 07:48:53,333 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 3.829287528991699
2023-01-07 07:48:53,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,333 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,333 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:53,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.338865280151367
2023-01-07 07:48:53,334 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1345.88623046875
2023-01-07 07:48:53,334 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,334 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,334 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,334 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:53,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1578654050827026
2023-01-07 07:48:53,336 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -26.810039520263672
2023-01-07 07:48:53,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,336 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,336 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:53,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07916167378425598
2023-01-07 07:48:53,337 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 341.6505126953125
2023-01-07 07:48:53,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,337 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,337 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,337 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,337 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:53,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.809438705444336
2023-01-07 07:48:53,338 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 24.95868492126465
2023-01-07 07:48:53,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,339 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,339 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,339 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:53,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4114272594451904
2023-01-07 07:48:53,340 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 343.54095458984375
2023-01-07 07:48:53,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,340 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,340 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:53,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31351426243782043
2023-01-07 07:48:53,341 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.2516183853149414
2023-01-07 07:48:53,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,341 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,341 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:53,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.498934745788574
2023-01-07 07:48:53,342 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1381.6513671875
2023-01-07 07:48:53,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,343 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,343 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:53,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.116544485092163
2023-01-07 07:48:53,344 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 39.66939926147461
2023-01-07 07:48:53,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,344 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,344 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:53,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08269988000392914
2023-01-07 07:48:53,345 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 346.62591552734375
2023-01-07 07:48:53,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,346 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,346 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:53,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.489091157913208
2023-01-07 07:48:53,347 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85781192779541
2023-01-07 07:48:53,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,347 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,347 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:53,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5811541676521301
2023-01-07 07:48:53,348 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 346.3919677734375
2023-01-07 07:48:53,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,348 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,348 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:53,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8710907101631165
2023-01-07 07:48:53,350 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 2.4547319412231445
2023-01-07 07:48:53,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,350 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,350 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:53,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6068969964981079
2023-01-07 07:48:53,351 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1399.558837890625
2023-01-07 07:48:53,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,351 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,351 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:53,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25535276532173157
2023-01-07 07:48:53,352 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -26.943378448486328
2023-01-07 07:48:53,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,353 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,353 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:53,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.159628987312317
2023-01-07 07:48:53,354 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 348.86053466796875
2023-01-07 07:48:53,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,354 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,354 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:53,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3091788291931152
2023-01-07 07:48:53,355 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 29.764585494995117
2023-01-07 07:48:53,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,356 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,356 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,356 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:53,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.778445839881897
2023-01-07 07:48:53,357 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 345.70098876953125
2023-01-07 07:48:53,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,357 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,357 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:53,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31971287727355957
2023-01-07 07:48:53,358 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.420494079589844
2023-01-07 07:48:53,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,358 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,358 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:53,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26882413029670715
2023-01-07 07:48:53,359 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1396.845458984375
2023-01-07 07:48:53,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,360 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,360 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:53,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4266843795776367
2023-01-07 07:48:53,361 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -81.59500122070312
2023-01-07 07:48:53,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,361 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,361 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:53,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.377312660217285
2023-01-07 07:48:53,362 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 350.48565673828125
2023-01-07 07:48:53,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,362 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,363 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:53,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6931447982788086
2023-01-07 07:48:53,364 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -25.087913513183594
2023-01-07 07:48:53,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,364 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:53,364 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:53,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6993201971054077
2023-01-07 07:48:53,365 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 351.64398193359375
2023-01-07 07:48:53,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,365 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:53,365 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:53,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.056690067052841187
2023-01-07 07:48:53,366 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23.904560089111328
2023-01-07 07:48:53,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,367 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:53,367 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:53,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.182014465332031
2023-01-07 07:48:53,368 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1405.2286376953125
2023-01-07 07:48:53,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,368 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:53,368 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:53,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8180341720581055
2023-01-07 07:48:53,369 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -68.88726806640625
2023-01-07 07:48:53,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,370 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:53,370 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:53,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,370 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1086808443069458
2023-01-07 07:48:53,371 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 690.9627075195312
2023-01-07 07:48:53,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,371 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,371 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:53,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2302803993225098
2023-01-07 07:48:53,372 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.03201866149902344
2023-01-07 07:48:53,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,372 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:53,372 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:53,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,373 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32473212480545044
2023-01-07 07:48:53,373 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 518.4207153320312
2023-01-07 07:48:53,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,374 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,374 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:53,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4294971823692322
2023-01-07 07:48:53,375 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 5.009161949157715
2023-01-07 07:48:53,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,375 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:53,375 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:53,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,376 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.189304828643799
2023-01-07 07:48:53,376 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2840.049560546875
2023-01-07 07:48:53,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,377 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:53,377 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:53,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8366515636444092
2023-01-07 07:48:53,378 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.8457889556884766
2023-01-07 07:48:53,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,378 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:48:53,378 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:53,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06976161897182465
2023-01-07 07:48:53,379 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2825.87841796875
2023-01-07 07:48:53,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,379 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:53,379 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:53,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6034967303276062
2023-01-07 07:48:53,380 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 12.287728309631348
2023-01-07 07:48:53,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,381 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:53,381 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:53,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09305725246667862
2023-01-07 07:48:53,382 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 704.6424560546875
2023-01-07 07:48:53,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,382 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,382 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:53,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08908141404390335
2023-01-07 07:48:53,383 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -45.334171295166016
2023-01-07 07:48:53,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,383 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:53,384 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:53,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40441352128982544
2023-01-07 07:48:53,385 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 511.95098876953125
2023-01-07 07:48:53,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,385 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,385 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:53,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21965523064136505
2023-01-07 07:48:53,386 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -41.74021911621094
2023-01-07 07:48:53,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,386 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:53,386 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:53,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5270439386367798
2023-01-07 07:48:53,387 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2812.602294921875
2023-01-07 07:48:53,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,388 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:53,388 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:53,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09208828210830688
2023-01-07 07:48:53,389 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -130.3599395751953
2023-01-07 07:48:53,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,389 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:53,389 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:53,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29808783531188965
2023-01-07 07:48:53,390 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 708.2310180664062
2023-01-07 07:48:53,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,390 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,390 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:53,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08098761737346649
2023-01-07 07:48:53,392 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.4918365478515625
2023-01-07 07:48:53,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,392 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:53,392 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:53,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34783291816711426
2023-01-07 07:48:53,393 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 708.3452758789062
2023-01-07 07:48:53,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,393 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:53,393 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:53,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5358055830001831
2023-01-07 07:48:53,394 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -16.49246597290039
2023-01-07 07:48:53,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,395 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:53,395 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:53,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.353548049926758
2023-01-07 07:48:53,396 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2835.884033203125
2023-01-07 07:48:53,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,396 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:53,396 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:53,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1596299409866333
2023-01-07 07:48:53,397 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.924813270568848
2023-01-07 07:48:53,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,398 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:48:53,398 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:53,398 > [DEBUG] 0 :: 7.56186580657959
2023-01-07 07:48:53,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -1103.60498046875
2023-01-07 07:48:53,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -1094.010498046875
2023-01-07 07:48:53,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -137.05364990234375
2023-01-07 07:48:53,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -371.0882873535156
2023-01-07 07:48:53,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,411 > [DEBUG] 0 :: before allreduce fusion buffer :: -203.49783325195312
2023-01-07 07:48:53,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -363.642333984375
2023-01-07 07:48:53,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -340.7208557128906
2023-01-07 07:48:53,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -276.44940185546875
2023-01-07 07:48:53,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -232.10153198242188
2023-01-07 07:48:53,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -338.76251220703125
2023-01-07 07:48:53,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,418 > [DEBUG] 0 :: before allreduce fusion buffer :: -157.13595581054688
2023-01-07 07:48:53,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -1014.3322143554688
2023-01-07 07:48:53,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -152.77508544921875
2023-01-07 07:48:53,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -281.873779296875
2023-01-07 07:48:53,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -629.17236328125
2023-01-07 07:48:53,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -716.14599609375
2023-01-07 07:48:53,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -227.9540252685547
2023-01-07 07:48:53,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,426 > [DEBUG] 0 :: before allreduce fusion buffer :: -362.04156494140625
2023-01-07 07:48:53,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -199.96585083007812
2023-01-07 07:48:53,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -598.9888305664062
2023-01-07 07:48:53,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -351.6002502441406
2023-01-07 07:48:53,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -450.638427734375
2023-01-07 07:48:53,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -242.51971435546875
2023-01-07 07:48:53,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -660.2978515625
2023-01-07 07:48:53,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -316.009765625
2023-01-07 07:48:53,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -673.885009765625
2023-01-07 07:48:53,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -486.10784912109375
2023-01-07 07:48:53,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -483.3920593261719
2023-01-07 07:48:53,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -127.96521759033203
2023-01-07 07:48:53,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -617.2796630859375
2023-01-07 07:48:53,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,441 > [DEBUG] 0 :: before allreduce fusion buffer :: -241.29727172851562
2023-01-07 07:48:53,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -543.0784301757812
2023-01-07 07:48:53,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -339.61920166015625
2023-01-07 07:48:53,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -446.85003662109375
2023-01-07 07:48:53,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -256.20330810546875
2023-01-07 07:48:53,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -642.4730224609375
2023-01-07 07:48:53,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -366.41021728515625
2023-01-07 07:48:53,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -758.7667846679688
2023-01-07 07:48:53,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -483.74114990234375
2023-01-07 07:48:53,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -568.4464721679688
2023-01-07 07:48:53,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,452 > [DEBUG] 0 :: before allreduce fusion buffer :: -367.6873779296875
2023-01-07 07:48:53,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -917.037841796875
2023-01-07 07:48:53,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -397.56256103515625
2023-01-07 07:48:53,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -723.714599609375
2023-01-07 07:48:53,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -667.8851318359375
2023-01-07 07:48:53,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,457 > [DEBUG] 0 :: before allreduce fusion buffer :: -639.7611083984375
2023-01-07 07:48:53,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -108.34283447265625
2023-01-07 07:48:53,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -688.9483642578125
2023-01-07 07:48:53,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -109.67939758300781
2023-01-07 07:48:53,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -347.43817138671875
2023-01-07 07:48:53,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -346.84613037109375
2023-01-07 07:48:53,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -823.5217895507812
2023-01-07 07:48:53,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -102.99048614501953
2023-01-07 07:48:53,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -372.6299133300781
2023-01-07 07:48:53,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.79619598388672
2023-01-07 07:48:53,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,468 > [DEBUG] 0 :: before allreduce fusion buffer :: -557.3238525390625
2023-01-07 07:48:53,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -75.52025604248047
2023-01-07 07:48:53,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,470 > [DEBUG] 0 :: before allreduce fusion buffer :: -443.2228088378906
2023-01-07 07:48:53,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -157.5116729736328
2023-01-07 07:48:53,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -404.6207275390625
2023-01-07 07:48:53,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -141.42825317382812
2023-01-07 07:48:53,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -480.87939453125
2023-01-07 07:48:53,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,474 > [DEBUG] 0 :: before allreduce fusion buffer :: -126.65299987792969
2023-01-07 07:48:53,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -482.254638671875
2023-01-07 07:48:53,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -220.34722900390625
2023-01-07 07:48:53,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -388.27288818359375
2023-01-07 07:48:53,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -201.32064819335938
2023-01-07 07:48:53,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -522.802978515625
2023-01-07 07:48:53,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,479 > [DEBUG] 0 :: before allreduce fusion buffer :: -278.9252624511719
2023-01-07 07:48:53,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -791.402587890625
2023-01-07 07:48:53,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -251.51260375976562
2023-01-07 07:48:53,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -366.658203125
2023-01-07 07:48:53,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -308.0539245605469
2023-01-07 07:48:53,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -529.571044921875
2023-01-07 07:48:53,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -638.257080078125
2023-01-07 07:48:53,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -2018.2191162109375
2023-01-07 07:48:53,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,487 > [DEBUG] 0 :: before allreduce fusion buffer :: -897.45654296875
2023-01-07 07:48:53,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -2654.970703125
2023-01-07 07:48:53,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -199.41087341308594
2023-01-07 07:48:53,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -1948.434326171875
2023-01-07 07:48:53,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,491 > [DEBUG] 0 :: before allreduce fusion buffer :: -789.21923828125
2023-01-07 07:48:53,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -3417.875
2023-01-07 07:48:53,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -833.0861206054688
2023-01-07 07:48:53,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,494 > [DEBUG] 0 :: before allreduce fusion buffer :: -3536.92724609375
2023-01-07 07:48:53,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -1273.07421875
2023-01-07 07:48:53,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -4662.720703125
2023-01-07 07:48:53,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -989.3248901367188
2023-01-07 07:48:53,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,498 > [DEBUG] 0 :: before allreduce fusion buffer :: -4841.81689453125
2023-01-07 07:48:53,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,499 > [DEBUG] 0 :: before allreduce fusion buffer :: -1058.5621337890625
2023-01-07 07:48:53,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,500 > [DEBUG] 0 :: before allreduce fusion buffer :: -3070.98681640625
2023-01-07 07:48:53,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -1451.368896484375
2023-01-07 07:48:53,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,502 > [DEBUG] 0 :: before allreduce fusion buffer :: -5349.14892578125
2023-01-07 07:48:53,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -1135.8116455078125
2023-01-07 07:48:53,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -5580.0302734375
2023-01-07 07:48:53,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -1248.58837890625
2023-01-07 07:48:53,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -3715.42578125
2023-01-07 07:48:53,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,507 > [DEBUG] 0 :: before allreduce fusion buffer :: -1920.325927734375
2023-01-07 07:48:53,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -6041.68212890625
2023-01-07 07:48:53,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -503.3898010253906
2023-01-07 07:48:53,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,511 > [DEBUG] 0 :: before allreduce fusion buffer :: -6765.2158203125
2023-01-07 07:48:53,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -1563.004638671875
2023-01-07 07:48:53,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -6662.6162109375
2023-01-07 07:48:53,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -1785.23193359375
2023-01-07 07:48:53,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,514 > [DEBUG] 0 :: before allreduce fusion buffer :: -5737.87744140625
2023-01-07 07:48:53,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -3473.36669921875
2023-01-07 07:48:53,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -3738.84521484375
2023-01-07 07:48:53,521 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:48:53,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 8816.646484375
2023-01-07 07:48:53,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,531 > [DEBUG] 0 :: before allreduce fusion buffer :: -5586.994140625
2023-01-07 07:48:53,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -11153.73828125
2023-01-07 07:48:53,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -22345.2578125
2023-01-07 07:48:53,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:53,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:53,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -44564.125
2023-01-07 07:48:53,534 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 240.49459838867188
2023-01-07 07:48:53,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:53,534 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:48:53,534 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:53,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0813217163086
2023-01-07 07:48:54,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.368843078613281
2023-01-07 07:48:54,379 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0813217163086
2023-01-07 07:48:54,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,380 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,380 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.8999290466308594
2023-01-07 07:48:54,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.81880187988281
2023-01-07 07:48:54,382 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 3.9681034088134766
2023-01-07 07:48:54,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,382 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:54,382 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:54,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 121.71432495117188
2023-01-07 07:48:54,383 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 67.72178649902344
2023-01-07 07:48:54,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,383 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,383 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:54,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -64.52958679199219
2023-01-07 07:48:54,384 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -14.367234230041504
2023-01-07 07:48:54,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,385 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:54,385 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.53782653808594
2023-01-07 07:48:54,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.7983169555664
2023-01-07 07:48:54,386 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.53782653808594
2023-01-07 07:48:54,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,387 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,387 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:54,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,387 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.187374114990234
2023-01-07 07:48:54,388 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 36.425445556640625
2023-01-07 07:48:54,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,388 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,388 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.0521240234375
2023-01-07 07:48:54,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4381980895996094
2023-01-07 07:48:54,389 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 254.0521240234375
2023-01-07 07:48:54,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,390 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,390 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:54,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.60224533081055
2023-01-07 07:48:54,391 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 6.535425186157227
2023-01-07 07:48:54,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,391 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,391 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:54,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 94.71332550048828
2023-01-07 07:48:54,392 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 283.4765319824219
2023-01-07 07:48:54,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,392 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,392 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:54,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.625185012817383
2023-01-07 07:48:54,394 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 24.492828369140625
2023-01-07 07:48:54,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,394 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,394 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 60.36752700805664
2023-01-07 07:48:54,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -217.39727783203125
2023-01-07 07:48:54,395 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 60.36752700805664
2023-01-07 07:48:54,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,396 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,396 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:54,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.913049697875977
2023-01-07 07:48:54,397 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.8943326473236084
2023-01-07 07:48:54,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,397 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:54,397 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 62.80894088745117
2023-01-07 07:48:54,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.300631523132324
2023-01-07 07:48:54,399 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 62.80894088745117
2023-01-07 07:48:54,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,399 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,399 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:54,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.503528594970703
2023-01-07 07:48:54,400 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 19.24296760559082
2023-01-07 07:48:54,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,400 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,400 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:54,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.508169174194336
2023-01-07 07:48:54,401 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 286.55828857421875
2023-01-07 07:48:54,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,402 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,402 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:54,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 84.75787353515625
2023-01-07 07:48:54,403 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 9.861352920532227
2023-01-07 07:48:54,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,403 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,403 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,403 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:54,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,404 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.569847106933594
2023-01-07 07:48:54,404 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 72.40185546875
2023-01-07 07:48:54,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,405 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,405 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:54,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,405 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.48786163330078
2023-01-07 07:48:54,406 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.2355763912200928
2023-01-07 07:48:54,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,406 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:54,406 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 62.488075256347656
2023-01-07 07:48:54,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.51837158203125
2023-01-07 07:48:54,408 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 62.488075256347656
2023-01-07 07:48:54,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,408 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:54,408 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:54,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.8922758102417
2023-01-07 07:48:54,409 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 30.41979217529297
2023-01-07 07:48:54,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,410 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:54,410 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -20.39568328857422
2023-01-07 07:48:54,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.077838897705078
2023-01-07 07:48:54,411 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 289.68359375
2023-01-07 07:48:54,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,411 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,411 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:54,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.705928802490234
2023-01-07 07:48:54,412 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -20.39568328857422
2023-01-07 07:48:54,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,412 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:48:54,412 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:54,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.8305535316467285
2023-01-07 07:48:54,413 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 143.3913116455078
2023-01-07 07:48:54,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,414 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,414 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,414 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:54,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.857918739318848
2023-01-07 07:48:54,415 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 21.890974044799805
2023-01-07 07:48:54,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,415 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:54,415 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.54925537109375
2023-01-07 07:48:54,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,416 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.329687118530273
2023-01-07 07:48:54,417 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.54925537109375
2023-01-07 07:48:54,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,417 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,417 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,417 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -11.37729549407959
2023-01-07 07:48:54,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.311270713806152
2023-01-07 07:48:54,418 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.37729549407959
2023-01-07 07:48:54,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,418 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,418 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,419 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 515.6105346679688
2023-01-07 07:48:54,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.019112586975098
2023-01-07 07:48:54,420 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 515.6105346679688
2023-01-07 07:48:54,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,420 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,420 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 10.350849151611328
2023-01-07 07:48:54,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.29184913635254
2023-01-07 07:48:54,421 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 10.350849151611328
2023-01-07 07:48:54,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,421 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:54,421 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,421 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:54,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7967593669891357
2023-01-07 07:48:54,422 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 598.7515869140625
2023-01-07 07:48:54,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,423 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,423 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,423 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,423 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:54,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.277475357055664
2023-01-07 07:48:54,424 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -74.88262939453125
2023-01-07 07:48:54,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,424 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,424 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 126.34175872802734
2023-01-07 07:48:54,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.800623893737793
2023-01-07 07:48:54,426 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 126.34175872802734
2023-01-07 07:48:54,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,426 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,426 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,426 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 5.852261543273926
2023-01-07 07:48:54,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,426 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.734228134155273
2023-01-07 07:48:54,427 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 5.852261543273926
2023-01-07 07:48:54,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,427 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:54,427 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 125.47367095947266
2023-01-07 07:48:54,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.167684555053711
2023-01-07 07:48:54,429 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 125.47367095947266
2023-01-07 07:48:54,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,429 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,429 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -9.101005554199219
2023-01-07 07:48:54,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.623002529144287
2023-01-07 07:48:54,430 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -9.101005554199219
2023-01-07 07:48:54,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,430 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,431 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:54,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,431 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5311533212661743
2023-01-07 07:48:54,432 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 622.830078125
2023-01-07 07:48:54,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,432 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,432 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,432 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,432 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:54,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.839964866638184
2023-01-07 07:48:54,433 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -42.156307220458984
2023-01-07 07:48:54,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,433 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,433 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,434 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.81491088867188
2023-01-07 07:48:54,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.340950965881348
2023-01-07 07:48:54,435 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.81491088867188
2023-01-07 07:48:54,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,435 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,435 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.29714012145996
2023-01-07 07:48:54,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.94590950012207
2023-01-07 07:48:54,436 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -28.29714012145996
2023-01-07 07:48:54,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,437 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:54,437 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 127.30826568603516
2023-01-07 07:48:54,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.587265491485596
2023-01-07 07:48:54,438 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 127.30826568603516
2023-01-07 07:48:54,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,438 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,438 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 17.017976760864258
2023-01-07 07:48:54,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6092731952667236
2023-01-07 07:48:54,439 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 17.017976760864258
2023-01-07 07:48:54,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,440 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,440 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 505.6556396484375
2023-01-07 07:48:54,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.664652824401855
2023-01-07 07:48:54,441 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 505.6556396484375
2023-01-07 07:48:54,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,441 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,441 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 9.676246643066406
2023-01-07 07:48:54,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6771454811096191
2023-01-07 07:48:54,442 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 9.676246643066406
2023-01-07 07:48:54,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,443 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,443 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.173583984375
2023-01-07 07:48:54,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.149429202079773
2023-01-07 07:48:54,444 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.1885528564453
2023-01-07 07:48:54,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,445 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,445 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.567827224731445
2023-01-07 07:48:54,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.717830657958984
2023-01-07 07:48:54,446 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 6.567827224731445
2023-01-07 07:48:54,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,446 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:54,446 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,446 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,446 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 129.8209228515625
2023-01-07 07:48:54,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0918378829956055
2023-01-07 07:48:54,448 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.8209228515625
2023-01-07 07:48:54,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,448 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:54,448 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,448 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,448 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 18.30142593383789
2023-01-07 07:48:54,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8916969299316406
2023-01-07 07:48:54,449 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 18.30142593383789
2023-01-07 07:48:54,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,449 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:54,450 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,450 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.4139404296875
2023-01-07 07:48:54,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.573359966278076
2023-01-07 07:48:54,451 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.4139404296875
2023-01-07 07:48:54,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,451 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,451 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,451 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 47.5260124206543
2023-01-07 07:48:54,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.189659118652344
2023-01-07 07:48:54,452 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 47.5260124206543
2023-01-07 07:48:54,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,453 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:54,453 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,453 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,453 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 258.4992370605469
2023-01-07 07:48:54,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.699676513671875
2023-01-07 07:48:54,454 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 258.4992370605469
2023-01-07 07:48:54,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,454 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,454 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.410390853881836
2023-01-07 07:48:54,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.643589973449707
2023-01-07 07:48:54,455 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.410390853881836
2023-01-07 07:48:54,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,456 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,456 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:54,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.690173625946045
2023-01-07 07:48:54,457 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 338.4906005859375
2023-01-07 07:48:54,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,457 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,457 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:54,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1341550052165985
2023-01-07 07:48:54,458 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -15.702336311340332
2023-01-07 07:48:54,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,459 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,459 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,459 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,459 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:54,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.242485046386719
2023-01-07 07:48:54,460 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1401.7550048828125
2023-01-07 07:48:54,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,460 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,460 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:54,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.504554748535156
2023-01-07 07:48:54,461 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 61.0592155456543
2023-01-07 07:48:54,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,461 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:54,461 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:54,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,462 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.258522987365723
2023-01-07 07:48:54,462 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1466.05029296875
2023-01-07 07:48:54,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,463 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,463 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:54,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.020442008972168
2023-01-07 07:48:54,464 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.296527862548828
2023-01-07 07:48:54,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,464 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,464 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 255.25833129882812
2023-01-07 07:48:54,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.921087265014648
2023-01-07 07:48:54,465 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 255.25833129882812
2023-01-07 07:48:54,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,466 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,466 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,466 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,466 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.677282810211182
2023-01-07 07:48:54,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.62893009185791
2023-01-07 07:48:54,467 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.677282810211182
2023-01-07 07:48:54,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,467 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,467 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:54,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.395484209060669
2023-01-07 07:48:54,468 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 365.77532958984375
2023-01-07 07:48:54,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,469 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,469 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:54,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.848406434059143
2023-01-07 07:48:54,470 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 3.829287528991699
2023-01-07 07:48:54,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,470 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,470 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,470 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:54,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.049978256225586
2023-01-07 07:48:54,471 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1480.774658203125
2023-01-07 07:48:54,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,471 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,472 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:54,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3048934936523438
2023-01-07 07:48:54,473 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -26.810039520263672
2023-01-07 07:48:54,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,473 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,473 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,473 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,473 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:54,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5935876369476318
2023-01-07 07:48:54,474 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 380.85479736328125
2023-01-07 07:48:54,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,474 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,474 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,475 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:54,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0619868040084839
2023-01-07 07:48:54,475 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 24.95868492126465
2023-01-07 07:48:54,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,476 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,476 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:54,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3312225341796875
2023-01-07 07:48:54,477 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 383.50439453125
2023-01-07 07:48:54,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,477 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,477 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,477 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,477 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:54,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2220932245254517
2023-01-07 07:48:54,478 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.2516183853149414
2023-01-07 07:48:54,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,479 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,479 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,479 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,479 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:54,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10086645185947418
2023-01-07 07:48:54,480 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1542.2548828125
2023-01-07 07:48:54,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,480 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,480 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:54,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.65842604637146
2023-01-07 07:48:54,481 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 39.66939926147461
2023-01-07 07:48:54,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,481 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,481 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,482 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:54,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.031839370727539
2023-01-07 07:48:54,482 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 388.0901184082031
2023-01-07 07:48:54,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,483 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,483 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:54,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8373470306396484
2023-01-07 07:48:54,484 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85781192779541
2023-01-07 07:48:54,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,484 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,484 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:54,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9072899222373962
2023-01-07 07:48:54,485 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 388.2952880859375
2023-01-07 07:48:54,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,486 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,486 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:54,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6176321506500244
2023-01-07 07:48:54,487 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 2.4547319412231445
2023-01-07 07:48:54,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,487 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,487 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:54,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5783674716949463
2023-01-07 07:48:54,488 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1562.6478271484375
2023-01-07 07:48:54,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,488 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,488 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:54,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9996278285980225
2023-01-07 07:48:54,489 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -26.943378448486328
2023-01-07 07:48:54,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,490 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,490 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,490 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:54,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0204498767852783
2023-01-07 07:48:54,491 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 391.1200256347656
2023-01-07 07:48:54,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,491 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,491 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:54,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7181164026260376
2023-01-07 07:48:54,492 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 29.764585494995117
2023-01-07 07:48:54,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,493 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,493 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:54,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9049816131591797
2023-01-07 07:48:54,494 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 386.46624755859375
2023-01-07 07:48:54,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,494 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,494 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:54,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,494 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0567203760147095
2023-01-07 07:48:54,495 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.420494079589844
2023-01-07 07:48:54,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,495 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,495 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:54,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4561214447021484
2023-01-07 07:48:54,496 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1570.3665771484375
2023-01-07 07:48:54,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,497 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,497 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:54,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.897874355316162
2023-01-07 07:48:54,498 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -81.59500122070312
2023-01-07 07:48:54,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,498 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,498 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,498 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:54,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.673041582107544
2023-01-07 07:48:54,499 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 397.6444396972656
2023-01-07 07:48:54,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,500 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,500 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,500 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,500 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:54,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2829817235469818
2023-01-07 07:48:54,501 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -25.087913513183594
2023-01-07 07:48:54,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,501 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:54,501 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:54,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.299546718597412
2023-01-07 07:48:54,502 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 397.30792236328125
2023-01-07 07:48:54,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,502 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:54,502 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:54,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5734783411026
2023-01-07 07:48:54,504 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23.904560089111328
2023-01-07 07:48:54,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,504 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:54,504 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,504 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:54,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.943193197250366
2023-01-07 07:48:54,505 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1573.902099609375
2023-01-07 07:48:54,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,505 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:54,505 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:54,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5134037137031555
2023-01-07 07:48:54,506 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -68.88726806640625
2023-01-07 07:48:54,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,507 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:54,507 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:54,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4365952014923096
2023-01-07 07:48:54,508 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 768.5071411132812
2023-01-07 07:48:54,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,508 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,508 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,508 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,508 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:54,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6950833797454834
2023-01-07 07:48:54,509 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.03201866149902344
2023-01-07 07:48:54,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,509 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:54,509 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,510 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:54,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09777580201625824
2023-01-07 07:48:54,511 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 545.9561767578125
2023-01-07 07:48:54,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,511 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,511 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:54,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31630563735961914
2023-01-07 07:48:54,512 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 5.009161949157715
2023-01-07 07:48:54,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,512 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:54,512 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:54,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8772222995758057
2023-01-07 07:48:54,513 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 3195.6845703125
2023-01-07 07:48:54,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,514 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:54,514 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:54,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,514 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7436615228652954
2023-01-07 07:48:54,515 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.8457889556884766
2023-01-07 07:48:54,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,515 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:48:54,515 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:54,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,515 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4268558621406555
2023-01-07 07:48:54,516 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 3178.112060546875
2023-01-07 07:48:54,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,516 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:54,516 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:54,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,517 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43032705783843994
2023-01-07 07:48:54,517 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 12.287728309631348
2023-01-07 07:48:54,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,518 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:54,518 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:54,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2631421983242035
2023-01-07 07:48:54,519 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 781.0006713867188
2023-01-07 07:48:54,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,519 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,519 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:54,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09693646430969238
2023-01-07 07:48:54,520 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -45.334171295166016
2023-01-07 07:48:54,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,521 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:54,521 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:54,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28510433435440063
2023-01-07 07:48:54,522 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 511.4736022949219
2023-01-07 07:48:54,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,522 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,522 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:54,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,522 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22642143070697784
2023-01-07 07:48:54,523 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -41.74021911621094
2023-01-07 07:48:54,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,523 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:54,523 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:54,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7616159915924072
2023-01-07 07:48:54,524 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 3178.546875
2023-01-07 07:48:54,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,525 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:54,525 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,525 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,525 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:54,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32703864574432373
2023-01-07 07:48:54,526 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -130.3599395751953
2023-01-07 07:48:54,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,526 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:54,526 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:54,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24436703324317932
2023-01-07 07:48:54,527 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 806.7174072265625
2023-01-07 07:48:54,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,527 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,528 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,528 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:54,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23443099856376648
2023-01-07 07:48:54,529 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.4918365478515625
2023-01-07 07:48:54,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,529 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:54,529 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,529 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:54,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00019960850477218628
2023-01-07 07:48:54,530 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 801.411865234375
2023-01-07 07:48:54,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,530 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:54,530 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:54,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20272931456565857
2023-01-07 07:48:54,531 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -16.49246597290039
2023-01-07 07:48:54,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,532 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:54,532 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:54,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.872705459594727
2023-01-07 07:48:54,533 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 3205.2529296875
2023-01-07 07:48:54,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,533 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:54,533 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:54,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6474311351776123
2023-01-07 07:48:54,534 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.924813270568848
2023-01-07 07:48:54,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,535 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:48:54,535 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:54,535 > [DEBUG] 0 :: 8.574687004089355
2023-01-07 07:48:54,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -1623.3173828125
2023-01-07 07:48:54,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -1647.1212158203125
2023-01-07 07:48:54,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -227.46240234375
2023-01-07 07:48:54,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -365.6210021972656
2023-01-07 07:48:54,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,549 > [DEBUG] 0 :: before allreduce fusion buffer :: -219.26025390625
2023-01-07 07:48:54,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -327.16357421875
2023-01-07 07:48:54,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -459.215576171875
2023-01-07 07:48:54,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -251.41836547851562
2023-01-07 07:48:54,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -347.9447021484375
2023-01-07 07:48:54,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -380.302490234375
2023-01-07 07:48:54,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -393.91827392578125
2023-01-07 07:48:54,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -609.468505859375
2023-01-07 07:48:54,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.515316009521484
2023-01-07 07:48:54,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -494.82421875
2023-01-07 07:48:54,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -310.9044189453125
2023-01-07 07:48:54,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -326.6474304199219
2023-01-07 07:48:54,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -546.8299560546875
2023-01-07 07:48:54,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -611.5886840820312
2023-01-07 07:48:54,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -353.7657470703125
2023-01-07 07:48:54,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -443.299560546875
2023-01-07 07:48:54,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -644.388427734375
2023-01-07 07:48:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -490.42425537109375
2023-01-07 07:48:54,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -452.3564758300781
2023-01-07 07:48:54,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -952.8948974609375
2023-01-07 07:48:54,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -647.591064453125
2023-01-07 07:48:54,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -1089.796875
2023-01-07 07:48:54,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -1060.16552734375
2023-01-07 07:48:54,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -560.1494750976562
2023-01-07 07:48:54,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -888.029296875
2023-01-07 07:48:54,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -1381.9423828125
2023-01-07 07:48:54,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -808.99951171875
2023-01-07 07:48:54,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -1105.6949462890625
2023-01-07 07:48:54,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -991.3236083984375
2023-01-07 07:48:54,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -641.0828857421875
2023-01-07 07:48:54,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -1055.899169921875
2023-01-07 07:48:54,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -1553.110107421875
2023-01-07 07:48:54,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -1117.5919189453125
2023-01-07 07:48:54,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -1626.4588623046875
2023-01-07 07:48:54,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -1376.1749267578125
2023-01-07 07:48:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -479.3300476074219
2023-01-07 07:48:54,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -1491.425537109375
2023-01-07 07:48:54,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -1825.3316650390625
2023-01-07 07:48:54,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -1718.8797607421875
2023-01-07 07:48:54,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -2062.200927734375
2023-01-07 07:48:54,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,594 > [DEBUG] 0 :: before allreduce fusion buffer :: -2194.674560546875
2023-01-07 07:48:54,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -666.261962890625
2023-01-07 07:48:54,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -1471.77294921875
2023-01-07 07:48:54,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -2015.753173828125
2023-01-07 07:48:54,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -1481.936767578125
2023-01-07 07:48:54,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -490.3876953125
2023-01-07 07:48:54,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -1691.155029296875
2023-01-07 07:48:54,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -1313.0870361328125
2023-01-07 07:48:54,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -894.67919921875
2023-01-07 07:48:54,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -1759.634765625
2023-01-07 07:48:54,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -450.1989440917969
2023-01-07 07:48:54,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -2564.44677734375
2023-01-07 07:48:54,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -499.9908142089844
2023-01-07 07:48:54,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -2160.964111328125
2023-01-07 07:48:54,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -739.992431640625
2023-01-07 07:48:54,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -1565.884521484375
2023-01-07 07:48:54,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -645.72216796875
2023-01-07 07:48:54,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -1615.9920654296875
2023-01-07 07:48:54,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -713.0047607421875
2023-01-07 07:48:54,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -1884.20166015625
2023-01-07 07:48:54,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -1097.379150390625
2023-01-07 07:48:54,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -2198.11669921875
2023-01-07 07:48:54,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -989.9733276367188
2023-01-07 07:48:54,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -2867.9189453125
2023-01-07 07:48:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -1209.783447265625
2023-01-07 07:48:54,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -3619.35400390625
2023-01-07 07:48:54,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -1670.357666015625
2023-01-07 07:48:54,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -1789.3759765625
2023-01-07 07:48:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -1650.206787109375
2023-01-07 07:48:54,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -3692.06787109375
2023-01-07 07:48:54,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -1964.0458984375
2023-01-07 07:48:54,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -5369.31396484375
2023-01-07 07:48:54,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -2730.920166015625
2023-01-07 07:48:54,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -3153.68603515625
2023-01-07 07:48:54,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -1051.758056640625
2023-01-07 07:48:54,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -4417.0078125
2023-01-07 07:48:54,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,629 > [DEBUG] 0 :: before allreduce fusion buffer :: -6079.4814453125
2023-01-07 07:48:54,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -44628.62109375
2023-01-07 07:48:54,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -6893.1669921875
2023-01-07 07:48:54,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -46657.5859375
2023-01-07 07:48:54,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -10277.5791015625
2023-01-07 07:48:54,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,634 > [DEBUG] 0 :: before allreduce fusion buffer :: -66316.28125
2023-01-07 07:48:54,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -8700.58203125
2023-01-07 07:48:54,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -65521.95703125
2023-01-07 07:48:54,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,637 > [DEBUG] 0 :: before allreduce fusion buffer :: -9848.7578125
2023-01-07 07:48:54,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -31519.25
2023-01-07 07:48:54,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -13785.76171875
2023-01-07 07:48:54,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -74265.3828125
2023-01-07 07:48:54,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -11725.62890625
2023-01-07 07:48:54,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -74312.5078125
2023-01-07 07:48:54,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -13192.3525390625
2023-01-07 07:48:54,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -39488.6796875
2023-01-07 07:48:54,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -18420.818359375
2023-01-07 07:48:54,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -82983.09375
2023-01-07 07:48:54,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -4228.482421875
2023-01-07 07:48:54,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -89328.53125
2023-01-07 07:48:54,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -16101.845703125
2023-01-07 07:48:54,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -87945.96875
2023-01-07 07:48:54,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -18234.9375
2023-01-07 07:48:54,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,652 > [DEBUG] 0 :: before allreduce fusion buffer :: -69830.703125
2023-01-07 07:48:54,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -31049.0859375
2023-01-07 07:48:54,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -34475.390625
2023-01-07 07:48:54,657 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 07:48:54,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 129678.84375
2023-01-07 07:48:54,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -127943.09375
2023-01-07 07:48:54,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -255785.5625
2023-01-07 07:48:54,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -511593.25
2023-01-07 07:48:54,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:54,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:54,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -1029983.5
2023-01-07 07:48:54,672 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: 354.64837646484375
2023-01-07 07:48:54,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:54,672 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 07:48:54,672 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:54,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 63.91912841796875
2023-01-07 07:48:55,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,517 > [DEBUG] 0 :: before allreduce fusion buffer :: -96.33290100097656
2023-01-07 07:48:55,518 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 63.91912841796875
2023-01-07 07:48:55,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,518 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,518 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 3.9681034088134766
2023-01-07 07:48:55,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,519 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.056421279907227
2023-01-07 07:48:55,520 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 3.9681034088134766
2023-01-07 07:48:55,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,520 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:55,520 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:55,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 175.66998291015625
2023-01-07 07:48:55,521 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 70.67134857177734
2023-01-07 07:48:55,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,522 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,522 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,522 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -14.367234230041504
2023-01-07 07:48:55,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,522 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.944519996643066
2023-01-07 07:48:55,523 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: -14.367234230041504
2023-01-07 07:48:55,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,523 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:55,523 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 65.27268981933594
2023-01-07 07:48:55,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.91019821166992
2023-01-07 07:48:55,525 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 65.27268981933594
2023-01-07 07:48:55,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,525 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,525 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,525 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,525 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:55,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.859956741333008
2023-01-07 07:48:55,526 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: 38.055442810058594
2023-01-07 07:48:55,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,527 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,527 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 253.3877410888672
2023-01-07 07:48:55,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.46940994262695
2023-01-07 07:48:55,528 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 253.3877410888672
2023-01-07 07:48:55,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,528 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,528 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:55,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.846226692199707
2023-01-07 07:48:55,529 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 6.535425186157227
2023-01-07 07:48:55,529 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,530 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,530 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:55,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 107.54753112792969
2023-01-07 07:48:55,531 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 295.76495361328125
2023-01-07 07:48:55,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,531 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,531 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,531 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:55,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.8790283203125
2023-01-07 07:48:55,532 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 24.492828369140625
2023-01-07 07:48:55,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,532 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,533 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 59.4636344909668
2023-01-07 07:48:55,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -100.5690689086914
2023-01-07 07:48:55,534 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 59.4636344909668
2023-01-07 07:48:55,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,534 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,534 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,534 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.8943326473236084
2023-01-07 07:48:55,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 94.31199645996094
2023-01-07 07:48:55,535 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.8943326473236084
2023-01-07 07:48:55,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,536 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:55,536 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 62.65212631225586
2023-01-07 07:48:55,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1279144287109375
2023-01-07 07:48:55,537 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 62.65212631225586
2023-01-07 07:48:55,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,537 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,537 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:55,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.38390350341797
2023-01-07 07:48:55,539 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 19.24296760559082
2023-01-07 07:48:55,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,539 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,539 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:55,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4859323501586914
2023-01-07 07:48:55,540 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 298.947998046875
2023-01-07 07:48:55,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,540 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,540 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 9.861352920532227
2023-01-07 07:48:55,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.22784423828125
2023-01-07 07:48:55,542 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: 9.861352920532227
2023-01-07 07:48:55,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,542 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,542 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:55,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.332096099853516
2023-01-07 07:48:55,543 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 76.03245544433594
2023-01-07 07:48:55,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,543 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,543 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.2355763912200928
2023-01-07 07:48:55,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.27339172363281
2023-01-07 07:48:55,544 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.2355763912200928
2023-01-07 07:48:55,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,545 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 07:48:55,545 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 61.966453552246094
2023-01-07 07:48:55,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.08147048950195
2023-01-07 07:48:55,546 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 61.966453552246094
2023-01-07 07:48:55,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,547 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 07:48:55,547 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:55,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.049095153808594
2023-01-07 07:48:55,548 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: 36.73274230957031
2023-01-07 07:48:55,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,548 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 07:48:55,548 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -20.39568328857422
2023-01-07 07:48:55,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.094873428344727
2023-01-07 07:48:55,549 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 302.2393798828125
2023-01-07 07:48:55,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,550 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,550 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:55,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.22169017791748
2023-01-07 07:48:55,551 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -20.39568328857422
2023-01-07 07:48:55,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,551 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 07:48:55,551 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,551 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:55,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.225074768066406
2023-01-07 07:48:55,552 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 150.35012817382812
2023-01-07 07:48:55,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,552 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,552 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,552 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 21.890974044799805
2023-01-07 07:48:55,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.001643180847168
2023-01-07 07:48:55,554 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 21.890974044799805
2023-01-07 07:48:55,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,554 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:55,554 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.01968383789062
2023-01-07 07:48:55,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.527671813964844
2023-01-07 07:48:55,555 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.01968383789062
2023-01-07 07:48:55,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,556 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,556 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -11.37729549407959
2023-01-07 07:48:55,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8514063358306885
2023-01-07 07:48:55,557 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: -11.37729549407959
2023-01-07 07:48:55,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,557 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,557 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 515.173583984375
2023-01-07 07:48:55,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.863353729248047
2023-01-07 07:48:55,558 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 515.173583984375
2023-01-07 07:48:55,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,559 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,559 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 10.350849151611328
2023-01-07 07:48:55,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.262495040893555
2023-01-07 07:48:55,560 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 10.350849151611328
2023-01-07 07:48:55,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,560 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:55,560 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:55,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.35845947265625
2023-01-07 07:48:55,561 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 625.8428344726562
2023-01-07 07:48:55,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,561 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,561 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,561 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -74.88262939453125
2023-01-07 07:48:55,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.992256164550781
2023-01-07 07:48:55,562 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: -74.88262939453125
2023-01-07 07:48:55,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,563 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,563 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,563 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,563 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 126.3624496459961
2023-01-07 07:48:55,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.17327117919922
2023-01-07 07:48:55,564 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 126.3624496459961
2023-01-07 07:48:55,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,565 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,565 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,565 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 5.852261543273926
2023-01-07 07:48:55,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.850229263305664
2023-01-07 07:48:55,566 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: 5.852261543273926
2023-01-07 07:48:55,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,566 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:55,566 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,566 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 124.9891357421875
2023-01-07 07:48:55,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.157744407653809
2023-01-07 07:48:55,567 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 124.9891357421875
2023-01-07 07:48:55,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,568 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,568 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -9.101005554199219
2023-01-07 07:48:55,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.759929656982422
2023-01-07 07:48:55,569 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -9.101005554199219
2023-01-07 07:48:55,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,569 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,569 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:55,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.053324699401855
2023-01-07 07:48:55,570 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 649.879150390625
2023-01-07 07:48:55,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,570 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,570 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -42.156307220458984
2023-01-07 07:48:55,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.193436622619629
2023-01-07 07:48:55,572 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: -42.156307220458984
2023-01-07 07:48:55,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,572 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,572 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.4842529296875
2023-01-07 07:48:55,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9471113681793213
2023-01-07 07:48:55,573 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.4842529296875
2023-01-07 07:48:55,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,574 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,574 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,574 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,574 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -28.29714012145996
2023-01-07 07:48:55,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.14305114746094
2023-01-07 07:48:55,575 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: -28.29714012145996
2023-01-07 07:48:55,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,575 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:55,575 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 126.53546142578125
2023-01-07 07:48:55,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.179320335388184
2023-01-07 07:48:55,577 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 126.53546142578125
2023-01-07 07:48:55,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,577 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,577 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 17.017976760864258
2023-01-07 07:48:55,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3345272541046143
2023-01-07 07:48:55,578 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: 17.017976760864258
2023-01-07 07:48:55,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,578 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,578 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 501.96630859375
2023-01-07 07:48:55,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.409324645996094
2023-01-07 07:48:55,580 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 501.96630859375
2023-01-07 07:48:55,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,580 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,580 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 9.676246643066406
2023-01-07 07:48:55,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6374038457870483
2023-01-07 07:48:55,581 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: 9.676246643066406
2023-01-07 07:48:55,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,581 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,582 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.1885528564453
2023-01-07 07:48:55,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.351607322692871
2023-01-07 07:48:55,583 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.1885528564453
2023-01-07 07:48:55,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,583 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,583 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.567827224731445
2023-01-07 07:48:55,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.336495399475098
2023-01-07 07:48:55,584 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: 6.567827224731445
2023-01-07 07:48:55,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,585 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 07:48:55,585 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 129.8135223388672
2023-01-07 07:48:55,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7106263041496277
2023-01-07 07:48:55,586 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 129.8135223388672
2023-01-07 07:48:55,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,587 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 07:48:55,587 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 18.30142593383789
2023-01-07 07:48:55,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.257844924926758
2023-01-07 07:48:55,588 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 18.30142593383789
2023-01-07 07:48:55,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,588 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 07:48:55,588 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 513.564697265625
2023-01-07 07:48:55,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.636545181274414
2023-01-07 07:48:55,589 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 513.564697265625
2023-01-07 07:48:55,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,590 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,590 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,590 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 47.5260124206543
2023-01-07 07:48:55,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.345760822296143
2023-01-07 07:48:55,591 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: 47.5260124206543
2023-01-07 07:48:55,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,591 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 07:48:55,591 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,591 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 258.5932312011719
2023-01-07 07:48:55,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.435457229614258
2023-01-07 07:48:55,593 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 258.5932312011719
2023-01-07 07:48:55,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,593 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,593 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.410390853881836
2023-01-07 07:48:55,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.140591621398926
2023-01-07 07:48:55,594 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: -7.410390853881836
2023-01-07 07:48:55,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,594 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,594 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:55,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.75422477722168
2023-01-07 07:48:55,595 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 365.750244140625
2023-01-07 07:48:55,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,596 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,596 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -15.702336311340332
2023-01-07 07:48:55,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16587811708450317
2023-01-07 07:48:55,597 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: -15.702336311340332
2023-01-07 07:48:55,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,597 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,597 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:55,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.65282940864563
2023-01-07 07:48:55,598 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1460.3809814453125
2023-01-07 07:48:55,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,599 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,599 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 61.0592155456543
2023-01-07 07:48:55,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.266611099243164
2023-01-07 07:48:55,600 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 61.0592155456543
2023-01-07 07:48:55,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,600 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:55,600 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,600 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:55,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.16546630859375
2023-01-07 07:48:55,601 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1556.9326171875
2023-01-07 07:48:55,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,601 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,601 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -9.296527862548828
2023-01-07 07:48:55,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.395527362823486
2023-01-07 07:48:55,602 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: -9.296527862548828
2023-01-07 07:48:55,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,603 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,603 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 255.37313842773438
2023-01-07 07:48:55,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0040313005447388
2023-01-07 07:48:55,604 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 255.37313842773438
2023-01-07 07:48:55,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,604 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,605 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 6.677282810211182
2023-01-07 07:48:55,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.399709701538086
2023-01-07 07:48:55,606 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: 6.677282810211182
2023-01-07 07:48:55,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,606 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,606 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,606 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:55,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0030503273010254
2023-01-07 07:48:55,607 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 401.28302001953125
2023-01-07 07:48:55,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,607 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,607 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,607 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,607 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 3.829287528991699
2023-01-07 07:48:55,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1539199352264404
2023-01-07 07:48:55,608 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 3.829287528991699
2023-01-07 07:48:55,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,609 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,609 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:55,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.25703239440918
2023-01-07 07:48:55,610 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1601.8465576171875
2023-01-07 07:48:55,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,610 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,610 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,610 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,610 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -26.810039520263672
2023-01-07 07:48:55,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4426456391811371
2023-01-07 07:48:55,611 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -26.810039520263672
2023-01-07 07:48:55,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,612 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,612 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:55,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8036152124404907
2023-01-07 07:48:55,613 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 420.51287841796875
2023-01-07 07:48:55,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,613 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,613 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,613 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 24.95868492126465
2023-01-07 07:48:55,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3122869729995728
2023-01-07 07:48:55,614 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 24.95868492126465
2023-01-07 07:48:55,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,614 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,614 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:55,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.726139545440674
2023-01-07 07:48:55,615 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 423.56982421875
2023-01-07 07:48:55,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,616 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,616 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -3.2516183853149414
2023-01-07 07:48:55,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5397826433181763
2023-01-07 07:48:55,617 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -3.2516183853149414
2023-01-07 07:48:55,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,617 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,617 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,617 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:55,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0198304653167725
2023-01-07 07:48:55,618 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1670.5032958984375
2023-01-07 07:48:55,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,619 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,619 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,619 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 39.66939926147461
2023-01-07 07:48:55,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,619 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.121952533721924
2023-01-07 07:48:55,620 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 39.66939926147461
2023-01-07 07:48:55,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,620 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,620 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,620 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,620 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:55,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1943678855895996
2023-01-07 07:48:55,621 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 426.28485107421875
2023-01-07 07:48:55,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,621 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,621 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -15.85781192779541
2023-01-07 07:48:55,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0465468168258667
2023-01-07 07:48:55,622 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85781192779541
2023-01-07 07:48:55,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,623 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,623 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:55,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9142887592315674
2023-01-07 07:48:55,624 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 432.4141845703125
2023-01-07 07:48:55,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,624 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,624 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,624 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2.4547319412231445
2023-01-07 07:48:55,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1548500061035156
2023-01-07 07:48:55,625 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 2.4547319412231445
2023-01-07 07:48:55,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,626 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,626 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,626 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:55,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.257735729217529
2023-01-07 07:48:55,627 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1726.8289794921875
2023-01-07 07:48:55,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,627 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,627 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -26.943378448486328
2023-01-07 07:48:55,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4068541526794434
2023-01-07 07:48:55,628 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -26.943378448486328
2023-01-07 07:48:55,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,628 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,628 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:55,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.877964973449707
2023-01-07 07:48:55,629 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 429.5390319824219
2023-01-07 07:48:55,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,630 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,630 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 29.764585494995117
2023-01-07 07:48:55,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5325477123260498
2023-01-07 07:48:55,631 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: 29.764585494995117
2023-01-07 07:48:55,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,631 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,631 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:55,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.456892967224121
2023-01-07 07:48:55,632 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 424.4124755859375
2023-01-07 07:48:55,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,632 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,633 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 18.420494079589844
2023-01-07 07:48:55,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5345115661621094
2023-01-07 07:48:55,634 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: 18.420494079589844
2023-01-07 07:48:55,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,634 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,634 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:55,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.497105598449707
2023-01-07 07:48:55,635 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1735.218505859375
2023-01-07 07:48:55,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,635 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,635 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,635 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -81.59500122070312
2023-01-07 07:48:55,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3185539245605469
2023-01-07 07:48:55,636 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -81.59500122070312
2023-01-07 07:48:55,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,637 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,637 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:55,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7117667198181152
2023-01-07 07:48:55,638 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 439.0351257324219
2023-01-07 07:48:55,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,638 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,638 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -25.087913513183594
2023-01-07 07:48:55,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.69369637966156
2023-01-07 07:48:55,639 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -25.087913513183594
2023-01-07 07:48:55,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,640 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 07:48:55,640 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:55,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35320883989334106
2023-01-07 07:48:55,641 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 436.4027099609375
2023-01-07 07:48:55,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,641 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 07:48:55,641 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -23.904560089111328
2023-01-07 07:48:55,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5017682313919067
2023-01-07 07:48:55,642 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -23.904560089111328
2023-01-07 07:48:55,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,642 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 07:48:55,642 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:55,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.207679271697998
2023-01-07 07:48:55,643 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1733.1376953125
2023-01-07 07:48:55,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,644 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 07:48:55,644 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -68.88726806640625
2023-01-07 07:48:55,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2661489248275757
2023-01-07 07:48:55,645 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -68.88726806640625
2023-01-07 07:48:55,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,645 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 07:48:55,645 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:55,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.797534465789795
2023-01-07 07:48:55,646 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 834.15185546875
2023-01-07 07:48:55,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,646 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,646 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -0.03201866149902344
2023-01-07 07:48:55,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9235055446624756
2023-01-07 07:48:55,648 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -0.03201866149902344
2023-01-07 07:48:55,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,648 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:55,648 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:55,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8271015882492065
2023-01-07 07:48:55,649 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 580.20751953125
2023-01-07 07:48:55,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,649 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,649 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 5.009161949157715
2023-01-07 07:48:55,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2399638444185257
2023-01-07 07:48:55,651 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 5.009161949157715
2023-01-07 07:48:55,651 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,651 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:55,651 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:55,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.259399175643921
2023-01-07 07:48:55,652 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 3541.931884765625
2023-01-07 07:48:55,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,652 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:55,652 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 3.8457889556884766
2023-01-07 07:48:55,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8106704950332642
2023-01-07 07:48:55,653 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 3.8457889556884766
2023-01-07 07:48:55,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,654 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 07:48:55,654 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:55,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30107784271240234
2023-01-07 07:48:55,654 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 3517.39892578125
2023-01-07 07:48:55,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,655 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:55,655 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 12.287728309631348
2023-01-07 07:48:55,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06351039558649063
2023-01-07 07:48:55,656 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: 12.287728309631348
2023-01-07 07:48:55,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,656 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:55,656 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:55,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,657 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44481033086776733
2023-01-07 07:48:55,657 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 860.3705444335938
2023-01-07 07:48:55,657 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,658 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,658 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -45.334171295166016
2023-01-07 07:48:55,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22413478791713715
2023-01-07 07:48:55,659 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -45.334171295166016
2023-01-07 07:48:55,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,659 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:55,659 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:55,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45244649052619934
2023-01-07 07:48:55,660 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 570.4927368164062
2023-01-07 07:48:55,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,660 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,660 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,661 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -41.74021911621094
2023-01-07 07:48:55,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,661 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.210720956325531
2023-01-07 07:48:55,661 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -41.74021911621094
2023-01-07 07:48:55,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,662 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:55,662 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:55,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9182369709014893
2023-01-07 07:48:55,663 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 3538.273193359375
2023-01-07 07:48:55,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,663 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:55,663 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -130.3599395751953
2023-01-07 07:48:55,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2771090865135193
2023-01-07 07:48:55,664 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -130.3599395751953
2023-01-07 07:48:55,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,665 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:55,665 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:55,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0021724924445152283
2023-01-07 07:48:55,666 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 894.0048828125
2023-01-07 07:48:55,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,666 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,666 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.4918365478515625
2023-01-07 07:48:55,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.28388381004333496
2023-01-07 07:48:55,667 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -2.4918365478515625
2023-01-07 07:48:55,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,667 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 07:48:55,667 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:55,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.010278977453708649
2023-01-07 07:48:55,668 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 894.2586669921875
2023-01-07 07:48:55,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,669 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 07:48:55,669 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -16.49246597290039
2023-01-07 07:48:55,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1883206069469452
2023-01-07 07:48:55,670 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: -16.49246597290039
2023-01-07 07:48:55,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,670 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 07:48:55,670 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:55,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.528995513916016
2023-01-07 07:48:55,671 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 3564.034912109375
2023-01-07 07:48:55,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,671 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 07:48:55,671 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,672 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.924813270568848
2023-01-07 07:48:55,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.284814834594727
2023-01-07 07:48:55,673 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.924813270568848
2023-01-07 07:48:55,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 07:48:55,673 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 07:48:55,673 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 07:48:55,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 07:48:55,674 > [DEBUG] 0 :: 8.205100059509277
2023-01-07 07:48:55,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -2116.1328125
2023-01-07 07:48:55,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -1637.84716796875
2023-01-07 07:48:55,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -362.841552734375
2023-01-07 07:48:55,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -370.5846862792969
2023-01-07 07:48:55,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,687 > [DEBUG] 0 :: before allreduce fusion buffer :: -540.296630859375
2023-01-07 07:48:55,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -457.43243408203125
2023-01-07 07:48:55,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -831.5496215820312
2023-01-07 07:48:55,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -275.79241943359375
2023-01-07 07:48:55,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -901.6729736328125
2023-01-07 07:48:55,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -500.32135009765625
2023-01-07 07:48:55,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -504.7566223144531
2023-01-07 07:48:55,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -865.9722900390625
2023-01-07 07:48:55,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.254371643066406
2023-01-07 07:48:55,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -505.1783752441406
2023-01-07 07:48:55,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -1417.786865234375
2023-01-07 07:48:55,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,699 > [DEBUG] 0 :: before allreduce fusion buffer :: -403.00830078125
2023-01-07 07:48:55,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -1316.5343017578125
2023-01-07 07:48:55,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -721.3972778320312
2023-01-07 07:48:55,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -636.5950927734375
2023-01-07 07:48:55,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -964.8838500976562
2023-01-07 07:48:55,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,705 > [DEBUG] 0 :: before allreduce fusion buffer :: -1409.109619140625
2023-01-07 07:48:55,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -608.4951782226562
2023-01-07 07:48:55,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,707 > [DEBUG] 0 :: before allreduce fusion buffer :: -1285.74658203125
2023-01-07 07:48:55,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -1884.281005859375
2023-01-07 07:48:55,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -1703.8720703125
2023-01-07 07:48:55,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -2278.858642578125
2023-01-07 07:48:55,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -2589.302734375
2023-01-07 07:48:55,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -669.0107421875
2023-01-07 07:48:55,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,714 > [DEBUG] 0 :: before allreduce fusion buffer :: -2962.149658203125
2023-01-07 07:48:55,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -3878.839599609375
2023-01-07 07:48:55,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -2836.13427734375
2023-01-07 07:48:55,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,718 > [DEBUG] 0 :: before allreduce fusion buffer :: -3485.04443359375
2023-01-07 07:48:55,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -615.5469360351562
2023-01-07 07:48:55,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -1321.94091796875
2023-01-07 07:48:55,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -1037.0540771484375
2023-01-07 07:48:55,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,722 > [DEBUG] 0 :: before allreduce fusion buffer :: -2670.3701171875
2023-01-07 07:48:55,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -968.1038208007812
2023-01-07 07:48:55,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,724 > [DEBUG] 0 :: before allreduce fusion buffer :: -2584.776123046875
2023-01-07 07:48:55,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -2214.95361328125
2023-01-07 07:48:55,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -1605.298828125
2023-01-07 07:48:55,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -2743.244140625
2023-01-07 07:48:55,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -4737.48388671875
2023-01-07 07:48:55,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -4210.158203125
2023-01-07 07:48:55,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -5902.607421875
2023-01-07 07:48:55,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -6204.2060546875
2023-01-07 07:48:55,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -2034.0018310546875
2023-01-07 07:48:55,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -4995.025390625
2023-01-07 07:48:55,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -7529.88134765625
2023-01-07 07:48:55,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -6320.30908203125
2023-01-07 07:48:55,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -2441.54931640625
2023-01-07 07:48:55,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,738 > [DEBUG] 0 :: before allreduce fusion buffer :: -7643.6572265625
2023-01-07 07:48:55,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -19964.654296875
2023-01-07 07:48:55,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -7079.32421875
2023-01-07 07:48:55,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -26750.55859375
2023-01-07 07:48:55,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,742 > [DEBUG] 0 :: before allreduce fusion buffer :: -5522.966796875
2023-01-07 07:48:55,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -37211.44921875
2023-01-07 07:48:55,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -7511.388671875
2023-01-07 07:48:55,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,745 > [DEBUG] 0 :: before allreduce fusion buffer :: -31858.71484375
2023-01-07 07:48:55,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -9279.791015625
2023-01-07 07:48:55,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -10746.32421875
2023-01-07 07:48:55,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -6744.06103515625
2023-01-07 07:48:55,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,749 > [DEBUG] 0 :: before allreduce fusion buffer :: -28018.01953125
2023-01-07 07:48:55,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -7358.24853515625
2023-01-07 07:48:55,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -35229.17578125
2023-01-07 07:48:55,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,752 > [DEBUG] 0 :: before allreduce fusion buffer :: -10250.466796875
2023-01-07 07:48:55,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -22428.015625
2023-01-07 07:48:55,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -9010.78125
2023-01-07 07:48:55,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -33679.171875
2023-01-07 07:48:55,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -10846.7666015625
2023-01-07 07:48:55,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -43804.83203125
2023-01-07 07:48:55,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,759 > [DEBUG] 0 :: before allreduce fusion buffer :: -15412.7568359375
2023-01-07 07:48:55,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -24147.55859375
2023-01-07 07:48:55,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -5163.30224609375
2023-01-07 07:48:55,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -1963.720947265625
2023-01-07 07:48:55,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -6967.6337890625
2023-01-07 07:48:55,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -10630.4794921875
2023-01-07 07:48:55,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -42762.84375
2023-01-07 07:48:55,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,767 > [DEBUG] 0 :: before allreduce fusion buffer :: -210662.0625
2023-01-07 07:48:55,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -27977.48828125
2023-01-07 07:48:55,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -368783.78125
2023-01-07 07:48:55,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -167686.4375
2023-01-07 07:48:55,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -970431.25
2023-01-07 07:48:55,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -174480.734375
2023-01-07 07:48:55,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,775 > [DEBUG] 0 :: before allreduce fusion buffer :: -1000622.125
2023-01-07 07:48:55,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,776 > [DEBUG] 0 :: before allreduce fusion buffer :: -244034.6875
2023-01-07 07:48:55,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,777 > [DEBUG] 0 :: before allreduce fusion buffer :: -1330624.0
2023-01-07 07:48:55,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -190865.15625
2023-01-07 07:48:55,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -1299951.125
2023-01-07 07:48:55,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -204041.765625
2023-01-07 07:48:55,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,782 > [DEBUG] 0 :: before allreduce fusion buffer :: -667629.9375
2023-01-07 07:48:55,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,784 > [DEBUG] 0 :: before allreduce fusion buffer :: -273096.90625
2023-01-07 07:48:55,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -1429125.75
2023-01-07 07:48:55,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -221275.8125
2023-01-07 07:48:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -1407523.375
2023-01-07 07:48:55,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -239884.59375
2023-01-07 07:48:55,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -754173.5
2023-01-07 07:48:55,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -318823.25
2023-01-07 07:48:55,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -1537713.25
2023-01-07 07:48:55,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,795 > [DEBUG] 0 :: before allreduce fusion buffer :: -54937.88671875
2023-01-07 07:48:55,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -1615207.0
2023-01-07 07:48:55,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,797 > [DEBUG] 0 :: before allreduce fusion buffer :: -261453.453125
2023-01-07 07:48:55,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -1574781.875
2023-01-07 07:48:55,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -287369.40625
2023-01-07 07:48:55,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,801 > [DEBUG] 0 :: before allreduce fusion buffer :: -786897.25
2023-01-07 07:48:55,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 22511.671875
2023-01-07 07:48:55,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 07:48:55,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 07:48:55,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -538502.375
