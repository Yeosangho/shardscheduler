[00;33m 	'cuda/11.3' does not supports the {CUDA_MPI}. [0m
[00;34m 	{CUDA_MPI} is only supported in cuda 11.4 version. [0m
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
  0%|          | 0/38 [00:00<?, ?it/s]OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
OMP: Warning #191: Forking a process while a parallel region is active is potentially unsafe.
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
  0%|          | 0/38 [00:00<?, ?it/s]/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
  0%|          | 0/38 [00:00<?, ?it/s]/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
/scratch/hpc72a03/shardscheduler/torch_scheduler.py:1046: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/python_arg_parser.cpp:1055.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
  3%|â–Ž         | 1/38 [00:03<01:58,  3.21s/it]  3%|â–Ž         | 1/38 [00:06<03:57,  6.42s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  3%|â–Ž         | 1/38 [00:03<01:59,  3.22s/it]  3%|â–Ž         | 1/38 [00:06<03:58,  6.46s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  3%|â–Ž         | 1/38 [00:03<01:55,  3.12s/it]  3%|â–Ž         | 1/38 [00:06<03:58,  6.44s/it]
  3%|â–Ž         | 1/38 [00:03<01:58,  3.21s/it]  3%|â–Ž         | 1/38 [00:06<03:58,  6.44s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  3%|â–Ž         | 1/38 [00:03<02:00,  3.24s/it]  3%|â–Ž         | 1/38 [00:06<03:59,  6.47s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  3%|â–Ž         | 1/38 [00:03<01:56,  3.16s/it]  3%|â–Ž         | 1/38 [00:06<03:56,  6.38s/it]
  3%|â–Ž         | 1/38 [00:03<01:56,  3.14s/it]  3%|â–Ž         | 1/38 [00:06<03:56,  6.39s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
  3%|â–Ž         | 1/38 [00:03<01:58,  3.20s/it]  3%|â–Ž         | 1/38 [00:06<03:58,  6.45s/it]
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 521, in <module>
    trainer.benchmark_step()
  File "main_gpt2_with_health_checker.py", line 396, in benchmark_step
    loss.backward()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: setStorage: sizes [3840, 1280], strides [1, 3840], storage offset 0, and itemsize 4 requiring a storage size of 19660800 are out of bounds for storage of size 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "main_gpt2_with_health_checker.py", line 532, in <module>
    thread.join()
  File "/home01/hpc72a03/.conda/envs/shard/lib/python3.8/threading.py", line 1006, in join
    raise RuntimeError("cannot join thread before it is started")
RuntimeError: cannot join thread before it is started
srun: error: gpu20: task 1: Exited with exit code 1
srun: error: gpu21: tasks 4-5: Exited with exit code 1
srun: error: gpu20: tasks 0,2: Exited with exit code 1
srun: error: gpu21: task 6: Exited with exit code 1
srun: error: gpu20: task 3: Exited with exit code 1
srun: error: gpu21: task 7: Exited with exit code 1
