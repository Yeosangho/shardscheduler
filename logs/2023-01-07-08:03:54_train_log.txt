2023-01-07 08:04:01,808 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:04:01,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:01,846 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.6732504367828369
2023-01-07 08:04:01,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:01,847 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:04:01,847 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:01,847 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:01,847 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:04:01,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,706 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,706 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,707 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,707 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,707 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:04:02,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,709 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.218955993652344
2023-01-07 08:04:02,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,709 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:02,709 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,709 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,709 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:04:02,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,710 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,711 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,711 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,711 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,711 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:04:02,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,712 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 7.042952537536621
2023-01-07 08:04:02,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,713 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:02,713 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,713 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,713 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:04:02,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,750 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,750 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,750 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,750 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,751 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:04:02,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,752 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.099423885345459
2023-01-07 08:04:02,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,752 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,752 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,753 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,753 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:04:02,753 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,754 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,754 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,754 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,754 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,754 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:04:02,755 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,755 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 5.568670272827148
2023-01-07 08:04:02,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,756 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,756 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,756 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,756 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:04:02,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,757 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,757 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,757 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,757 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,758 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:04:02,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,759 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8228445053100586
2023-01-07 08:04:02,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,759 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,759 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,759 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,759 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:04:02,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,760 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,761 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,761 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,761 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,761 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:04:02,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,762 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -6.840078353881836
2023-01-07 08:04:02,762 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,763 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:02,763 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,763 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,763 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:04:02,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,764 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,764 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,764 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,764 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,764 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:04:02,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,765 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 11.602226257324219
2023-01-07 08:04:02,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,766 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,766 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,766 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,766 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:04:02,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,767 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,767 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,768 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,768 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,768 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:04:02,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,769 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -38.38706970214844
2023-01-07 08:04:02,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,769 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,769 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,769 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,769 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:04:02,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,770 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,771 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,771 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,771 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,771 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:04:02,771 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,772 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -10.686333656311035
2023-01-07 08:04:02,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,773 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:02,773 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,773 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,773 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:04:02,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,774 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:02,774 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,774 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:02,774 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,774 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,774 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:04:02,774 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,776 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -4.334873676300049
2023-01-07 08:04:02,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,776 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:02,776 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,776 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,776 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:04:02,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,777 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,777 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,778 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,778 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,778 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:04:02,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,779 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -13.012504577636719
2023-01-07 08:04:02,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,779 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:04:02,779 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,779 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,779 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:04:02,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,780 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,781 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,781 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,781 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,781 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:04:02,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,782 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 2.7129430770874023
2023-01-07 08:04:02,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,783 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:02,783 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,783 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,783 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:04:02,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,784 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,785 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,785 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,785 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,785 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:04:02,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,786 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 10.566534042358398
2023-01-07 08:04:02,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,786 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,786 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,786 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,787 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:04:02,787 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,788 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,788 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,788 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,788 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,788 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,788 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:04:02,788 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,789 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.445682525634766
2023-01-07 08:04:02,789 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,790 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:02,790 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,790 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,790 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:04:02,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,791 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,791 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,791 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,792 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,792 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,792 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:04:02,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,793 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 18.37600326538086
2023-01-07 08:04:02,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,793 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,793 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,793 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,793 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:04:02,794 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,794 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,795 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,795 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,795 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,795 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:04:02,795 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,796 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85270881652832
2023-01-07 08:04:02,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,797 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:02,797 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,797 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,797 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:04:02,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,798 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,798 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,799 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,799 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,799 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:04:02,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,800 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -1.3110322952270508
2023-01-07 08:04:02,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,800 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,800 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,800 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,800 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:04:02,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,801 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,802 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,802 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,802 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,802 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:04:02,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,803 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 56.35940933227539
2023-01-07 08:04:02,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,804 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,804 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,804 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,804 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:04:02,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,805 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,805 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,805 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,805 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,805 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:04:02,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,806 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.077699661254883
2023-01-07 08:04:02,807 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,807 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:02,807 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,807 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,807 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:04:02,807 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,808 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,808 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,809 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,809 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,809 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:04:02,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,810 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.877731323242188
2023-01-07 08:04:02,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,810 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,810 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,810 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,810 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:04:02,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,811 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,812 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,812 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,812 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,812 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:04:02,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,813 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: -21.230371475219727
2023-01-07 08:04:02,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,814 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,814 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,814 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,814 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:04:02,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,815 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,815 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,815 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,815 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,815 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:04:02,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,817 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -19.277565002441406
2023-01-07 08:04:02,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,817 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:02,817 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,817 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,817 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:04:02,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,818 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:02,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,819 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:02,819 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,819 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,819 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:04:02,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,820 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 1.5568034648895264
2023-01-07 08:04:02,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,820 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:02,820 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,820 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,820 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:04:02,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,821 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,822 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,822 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,822 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,822 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:04:02,822 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,823 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -11.064445495605469
2023-01-07 08:04:02,823 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,824 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:02,824 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,824 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,824 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:04:02,824 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,825 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,825 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,825 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,825 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,826 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:04:02,826 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,827 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.501985549926758
2023-01-07 08:04:02,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,827 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,827 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,827 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,827 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:04:02,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,828 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,829 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,829 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,829 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,829 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:04:02,829 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,830 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 32.03540802001953
2023-01-07 08:04:02,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,831 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,831 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,831 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,831 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:04:02,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,832 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,832 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,832 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,832 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,832 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:04:02,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,833 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 1.161661148071289
2023-01-07 08:04:02,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,834 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:02,834 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,834 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,834 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:04:02,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,835 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,835 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,835 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,836 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,836 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,836 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:04:02,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,837 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: 6.488774299621582
2023-01-07 08:04:02,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,837 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,837 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,837 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,837 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:04:02,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,838 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,839 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,839 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,839 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,839 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:04:02,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,840 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -31.224315643310547
2023-01-07 08:04:02,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,841 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,841 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,841 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,841 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:04:02,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,842 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,842 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,842 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,843 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,843 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:04:02,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,844 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 24.167444229125977
2023-01-07 08:04:02,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,844 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,844 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,844 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,844 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:04:02,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,845 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,846 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,846 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,846 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,846 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:04:02,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,847 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -5.7928009033203125
2023-01-07 08:04:02,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,847 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,848 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,848 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,848 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:04:02,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,849 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,849 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,849 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,849 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,849 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:04:02,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,850 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 16.308856964111328
2023-01-07 08:04:02,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,851 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,851 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,851 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,851 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:04:02,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,852 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,852 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,852 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,853 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,853 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:04:02,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,854 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -6.959627151489258
2023-01-07 08:04:02,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,854 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,854 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,854 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,854 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:04:02,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,855 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,855 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,856 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,856 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,856 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,856 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:04:02,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,857 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 12.91061019897461
2023-01-07 08:04:02,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,857 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,857 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,857 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,858 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:04:02,858 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,859 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,859 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,859 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,859 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,859 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,859 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:04:02,859 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,860 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.9678564071655273
2023-01-07 08:04:02,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,861 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,861 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,861 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,861 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:04:02,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,862 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,862 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,862 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,863 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,863 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:04:02,863 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,864 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 19.857297897338867
2023-01-07 08:04:02,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,864 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,864 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,864 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,864 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:04:02,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,865 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,866 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,866 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,866 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,866 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:04:02,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,867 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -1.9362564086914062
2023-01-07 08:04:02,867 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,867 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,868 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,868 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,868 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:04:02,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,869 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,869 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,869 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,869 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,869 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:04:02,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,870 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: -15.344305038452148
2023-01-07 08:04:02,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,871 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,871 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,871 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,871 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:04:02,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,872 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,872 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,872 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,872 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,872 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:04:02,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,874 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.877996444702148
2023-01-07 08:04:02,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,874 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,874 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,874 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,874 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:04:02,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,875 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,876 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,876 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,876 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,876 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:04:02,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,877 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -22.828798294067383
2023-01-07 08:04:02,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,877 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,877 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,877 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,878 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:04:02,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,878 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,879 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,879 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,879 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,879 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:04:02,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,880 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -34.145172119140625
2023-01-07 08:04:02,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,881 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:02,881 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,881 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,881 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:04:02,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,882 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:02,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,882 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:02,882 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,882 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,882 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:04:02,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,884 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -11.372824668884277
2023-01-07 08:04:02,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,884 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:02,884 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,884 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:04:02,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,885 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:02,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,885 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:02,885 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,886 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,886 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:04:02,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,887 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -60.26964569091797
2023-01-07 08:04:02,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,887 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:02,887 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,887 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:04:02,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,888 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,889 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,889 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,889 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:04:02,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,890 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -24.857271194458008
2023-01-07 08:04:02,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,891 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:02,891 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,891 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,891 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:04:02,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,892 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,892 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,892 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,892 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:04:02,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,894 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 17.965049743652344
2023-01-07 08:04:02,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,894 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:02,894 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,894 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,894 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:04:02,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,895 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:02,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,896 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:02,896 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,896 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,896 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:04:02,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,897 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -34.256595611572266
2023-01-07 08:04:02,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,897 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:04:02,897 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,897 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,897 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:04:02,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,898 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:02,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,899 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:02,899 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,899 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,899 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:04:02,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,900 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.075042724609375
2023-01-07 08:04:02,900 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,900 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:02,901 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,901 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,901 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:04:02,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,902 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,902 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,902 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,902 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,902 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:04:02,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,903 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7992687225341797
2023-01-07 08:04:02,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,904 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:02,904 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,904 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,904 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:04:02,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,905 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,906 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,906 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,906 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,906 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:04:02,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,907 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -23.16168212890625
2023-01-07 08:04:02,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,907 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:02,907 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,907 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,907 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:04:02,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,909 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:02,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,909 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:02,909 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,909 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,909 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:04:02,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,910 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -62.17387390136719
2023-01-07 08:04:02,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,911 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:02,911 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,911 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,911 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:04:02,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,912 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,912 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,912 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,912 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,912 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:04:02,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,914 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -67.58294677734375
2023-01-07 08:04:02,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,914 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:02,914 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,914 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,914 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:04:02,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,915 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:02,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,916 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:02,916 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,916 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,916 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:04:02,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,917 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 42.423255920410156
2023-01-07 08:04:02,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,917 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:02,917 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,917 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:04:02,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,918 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:02,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,919 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:02,919 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,919 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,919 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:04:02,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,920 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.02621841430664
2023-01-07 08:04:02,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:02,921 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:04:02,921 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:02,921 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:04:02,921 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:04:02,922 > [DEBUG] 0 :: 7.175354480743408
2023-01-07 08:04:02,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,927 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,928 > [DEBUG] 0 :: before allreduce fusion buffer :: -361.46820068359375
2023-01-07 08:04:02,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,932 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,932 > [DEBUG] 0 :: before allreduce fusion buffer :: -351.7166442871094
2023-01-07 08:04:02,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,942 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03929971903562546
2023-01-07 08:04:02,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,943 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7185747623443604
2023-01-07 08:04:02,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,945 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07206523418426514
2023-01-07 08:04:02,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,946 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,947 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22996583580970764
2023-01-07 08:04:02,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,948 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,948 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40216702222824097
2023-01-07 08:04:02,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,949 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01720818504691124
2023-01-07 08:04:02,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,951 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3096615970134735
2023-01-07 08:04:02,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,952 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17342057824134827
2023-01-07 08:04:02,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,954 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8304731845855713
2023-01-07 08:04:02,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,955 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1307240724563599
2023-01-07 08:04:02,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,957 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16667044162750244
2023-01-07 08:04:02,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,958 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5145916938781738
2023-01-07 08:04:02,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,960 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8779886960983276
2023-01-07 08:04:02,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,961 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4818698465824127
2023-01-07 08:04:02,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,962 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6772592663764954
2023-01-07 08:04:02,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,963 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.489405870437622
2023-01-07 08:04:02,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,965 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09906891733407974
2023-01-07 08:04:02,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,966 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6043342351913452
2023-01-07 08:04:02,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,968 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.151909351348877
2023-01-07 08:04:02,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,969 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,969 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6563178300857544
2023-01-07 08:04:02,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,971 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3524814248085022
2023-01-07 08:04:02,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,972 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,972 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44625067710876465
2023-01-07 08:04:02,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,974 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,974 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05384429544210434
2023-01-07 08:04:02,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,975 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4500205516815186
2023-01-07 08:04:02,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,976 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9943411350250244
2023-01-07 08:04:02,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,977 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0292526483535767
2023-01-07 08:04:02,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,979 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,979 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17885535955429077
2023-01-07 08:04:02,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,980 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,980 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.736617922782898
2023-01-07 08:04:02,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,981 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004195958375930786
2023-01-07 08:04:02,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,982 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,983 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.143481969833374
2023-01-07 08:04:02,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,984 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,984 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8848389387130737
2023-01-07 08:04:02,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,985 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,985 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8707189559936523
2023-01-07 08:04:02,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,986 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5919952988624573
2023-01-07 08:04:02,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,987 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3858706951141357
2023-01-07 08:04:02,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,989 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20708274841308594
2023-01-07 08:04:02,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,990 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4822425842285156
2023-01-07 08:04:02,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,991 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,991 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0387768745422363
2023-01-07 08:04:02,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,992 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1812843084335327
2023-01-07 08:04:02,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,994 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,994 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9906435012817383
2023-01-07 08:04:02,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,995 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,995 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.442378997802734
2023-01-07 08:04:02,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,996 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,996 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2814364433288574
2023-01-07 08:04:02,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,997 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:02,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.1770477294921875
2023-01-07 08:04:02,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:02,999 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:02,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.4004621505737305
2023-01-07 08:04:03,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,000 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37300488352775574
2023-01-07 08:04:03,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,001 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,001 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8570646643638611
2023-01-07 08:04:03,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,002 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,002 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.3036394119262695
2023-01-07 08:04:03,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,004 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,004 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4988151788711548
2023-01-07 08:04:03,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.919291973114014
2023-01-07 08:04:03,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,006 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,006 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4177742004394531
2023-01-07 08:04:03,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,007 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,007 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.359869956970215
2023-01-07 08:04:03,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,009 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,009 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5277445316314697
2023-01-07 08:04:03,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,010 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4093689918518066
2023-01-07 08:04:03,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,011 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,011 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10060057044029236
2023-01-07 08:04:03,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,012 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1173746585845947
2023-01-07 08:04:03,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,014 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.601324558258057
2023-01-07 08:04:03,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.174922943115234
2023-01-07 08:04:03,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,017 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.018326759338379
2023-01-07 08:04:03,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.1021223068237305
2023-01-07 08:04:03,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,019 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23819541931152344
2023-01-07 08:04:03,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.922622680664062
2023-01-07 08:04:03,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,022 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.372292995452881
2023-01-07 08:04:03,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.08563995361328
2023-01-07 08:04:03,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,024 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8708136081695557
2023-01-07 08:04:03,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,025 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.899381637573242
2023-01-07 08:04:03,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,026 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,027 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6961071491241455
2023-01-07 08:04:03,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,028 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.481966018676758
2023-01-07 08:04:03,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,029 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.666468620300293
2023-01-07 08:04:03,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.011138916015625
2023-01-07 08:04:03,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,031 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.138956069946289
2023-01-07 08:04:03,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,032 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,033 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.603640079498291
2023-01-07 08:04:03,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,034 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5110759735107422
2023-01-07 08:04:03,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.938952445983887
2023-01-07 08:04:03,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,036 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.59737777709961
2023-01-07 08:04:03,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.12400436401367
2023-01-07 08:04:03,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,038 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1746063232421875
2023-01-07 08:04:03,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.953475952148438
2023-01-07 08:04:03,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,041 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,041 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7731523513793945
2023-01-07 08:04:03,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.52565574645996
2023-01-07 08:04:03,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,043 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3998647928237915
2023-01-07 08:04:03,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.81854820251465
2023-01-07 08:04:03,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,046 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.067146301269531
2023-01-07 08:04:03,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,047 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.819803237915039
2023-01-07 08:04:03,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.924983501434326
2023-01-07 08:04:03,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,050 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,050 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.240543842315674
2023-01-07 08:04:03,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.211211204528809
2023-01-07 08:04:03,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,053 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.99151611328125
2023-01-07 08:04:03,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,054 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.87898254394531
2023-01-07 08:04:03,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,055 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.71200942993164
2023-01-07 08:04:03,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.740267276763916
2023-01-07 08:04:03,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.904001235961914
2023-01-07 08:04:03,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.766655445098877
2023-01-07 08:04:03,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,062 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.892864227294922
2023-01-07 08:04:03,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.542900085449219
2023-01-07 08:04:03,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,064 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 95.66819763183594
2023-01-07 08:04:03,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.448634147644043
2023-01-07 08:04:03,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,066 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,067 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,067 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.828731536865234
2023-01-07 08:04:03,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,068 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.576136112213135
2023-01-07 08:04:03,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,069 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.60090255737305
2023-01-07 08:04:03,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.067255973815918
2023-01-07 08:04:03,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.59781646728516
2023-01-07 08:04:03,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,073 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.498367309570312
2023-01-07 08:04:03,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,074 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -118.44530487060547
2023-01-07 08:04:03,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,076 > [DEBUG] 0 :: before allreduce fusion buffer :: -260.83123779296875
2023-01-07 08:04:03,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.86495971679688
2023-01-07 08:04:03,079 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:04:03,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,079 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 2795.607666015625
2023-01-07 08:04:03,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.39270782470703
2023-01-07 08:04:03,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.552038192749023
2023-01-07 08:04:03,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.269586563110352
2023-01-07 08:04:03,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 720.7398681640625
2023-01-07 08:04:03,089 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.6732504367828369
2023-01-07 08:04:03,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,089 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:04:03,089 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,089 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,946 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:03,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,947 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.065136909484863
2023-01-07 08:04:03,948 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,949 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,949 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,949 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,949 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 9.218955993652344
2023-01-07 08:04:03,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,949 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.987098693847656
2023-01-07 08:04:03,950 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.218955993652344
2023-01-07 08:04:03,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,951 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:03,951 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,951 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,951 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:03,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -117.13025665283203
2023-01-07 08:04:03,952 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,953 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,953 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,953 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,953 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:03,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.490509033203125
2023-01-07 08:04:03,954 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 7.042952537536621
2023-01-07 08:04:03,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,954 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:03,955 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,955 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,955 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:03,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,955 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,955 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.475749969482422
2023-01-07 08:04:03,956 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,957 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,957 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,957 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,957 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:03,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.571189880371094
2023-01-07 08:04:03,958 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.099423885345459
2023-01-07 08:04:03,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,958 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,959 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,959 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,959 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:03,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,959 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,959 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.535433769226074
2023-01-07 08:04:03,960 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:03,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,960 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:03,960 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,961 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,961 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:03,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,961 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.160985946655273
2023-01-07 08:04:03,962 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 5.568670272827148
2023-01-07 08:04:03,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,962 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,962 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,962 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,962 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:03,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 91.22286224365234
2023-01-07 08:04:03,963 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:03,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,964 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:03,964 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,964 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,964 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:03,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.735132217407227
2023-01-07 08:04:03,965 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8228445053100586
2023-01-07 08:04:03,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,966 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,966 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,966 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,966 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:03,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,966 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,966 > [DEBUG] 0 :: before allreduce fusion buffer :: -122.07445526123047
2023-01-07 08:04:03,967 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,968 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,968 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,968 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,968 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:03,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0405704975128174
2023-01-07 08:04:03,969 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -6.840078353881836
2023-01-07 08:04:03,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,970 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:03,970 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,970 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,970 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:03,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,970 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.777637481689453
2023-01-07 08:04:03,971 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,972 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,972 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,972 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,972 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:03,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.181812286376953
2023-01-07 08:04:03,973 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 11.602226257324219
2023-01-07 08:04:03,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,973 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,974 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,974 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,974 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:03,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.718801498413086
2023-01-07 08:04:03,975 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:03,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,975 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:03,975 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,975 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,975 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:03,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.31428146362305
2023-01-07 08:04:03,977 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -38.38706970214844
2023-01-07 08:04:03,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,977 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,977 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,977 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,978 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:03,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.55211639404297
2023-01-07 08:04:03,979 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,979 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,979 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,979 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,979 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:03,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.246883392333984
2023-01-07 08:04:03,980 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -10.686333656311035
2023-01-07 08:04:03,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,981 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:03,981 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,981 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,981 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:03,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,981 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,981 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,982 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.2889289855957
2023-01-07 08:04:03,983 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:03,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,983 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:03,983 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,983 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,983 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:03,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.292308807373047
2023-01-07 08:04:03,984 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -4.334873676300049
2023-01-07 08:04:03,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,985 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:03,985 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,985 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,985 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.012504577636719
2023-01-07 08:04:03,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,985 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.069293022155762
2023-01-07 08:04:03,986 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:03,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,987 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:03,987 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,987 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,987 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:03,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,987 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.369699478149414
2023-01-07 08:04:03,988 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -13.012504577636719
2023-01-07 08:04:03,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,989 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:04:03,989 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,989 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,989 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:03,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.60075378417969
2023-01-07 08:04:03,990 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:03,990 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,990 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:03,990 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,990 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,990 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:03,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,991 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.55392837524414
2023-01-07 08:04:03,992 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 2.7129430770874023
2023-01-07 08:04:03,992 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,992 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:03,992 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,992 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,992 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:03,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,992 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.031562089920044
2023-01-07 08:04:03,994 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:03,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,994 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:03,994 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,994 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,994 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.566534042358398
2023-01-07 08:04:03,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.092001438140869
2023-01-07 08:04:03,996 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 10.566534042358398
2023-01-07 08:04:03,996 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,996 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:03,996 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,996 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,996 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:03,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,996 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:03,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.753522872924805
2023-01-07 08:04:03,997 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:03,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:03,998 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:03,998 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:03,998 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:03,998 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.445682525634766
2023-01-07 08:04:03,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:03,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:03,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.724795818328857
2023-01-07 08:04:03,999 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.445682525634766
2023-01-07 08:04:03,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,000 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:04,000 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,000 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,000 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:04,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.135965347290039
2023-01-07 08:04:04,001 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,001 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,001 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,001 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,001 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:04,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.023615837097168
2023-01-07 08:04:04,003 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 18.37600326538086
2023-01-07 08:04:04,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,003 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,003 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,003 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,003 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,003 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.985855102539062
2023-01-07 08:04:04,005 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,005 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,005 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,005 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,005 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,005 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.85270881652832
2023-01-07 08:04:04,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.411548614501953
2023-01-07 08:04:04,006 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85270881652832
2023-01-07 08:04:04,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,007 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:04,007 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,007 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,007 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,007 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.025262832641602
2023-01-07 08:04:04,008 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,009 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,009 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,009 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,009 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -1.3110322952270508
2023-01-07 08:04:04,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.250327110290527
2023-01-07 08:04:04,010 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -1.3110322952270508
2023-01-07 08:04:04,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,011 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,011 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,011 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,011 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:04,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.053949356079102
2023-01-07 08:04:04,012 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,012 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,013 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,013 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,013 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:04,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,013 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.052820682525635
2023-01-07 08:04:04,014 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 56.35940933227539
2023-01-07 08:04:04,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,015 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,015 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,015 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,015 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,015 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.94378662109375
2023-01-07 08:04:04,016 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,017 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,017 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,017 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.077699661254883
2023-01-07 08:04:04,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,017 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.022254943847656
2023-01-07 08:04:04,018 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.077699661254883
2023-01-07 08:04:04,018 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,019 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:04,019 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,019 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.620067596435547
2023-01-07 08:04:04,020 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,020 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,020 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -24.877731323242188
2023-01-07 08:04:04,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.261249542236328
2023-01-07 08:04:04,022 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.877731323242188
2023-01-07 08:04:04,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,022 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,022 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,022 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,022 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:04,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,023 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.878303050994873
2023-01-07 08:04:04,024 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,024 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,024 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,024 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -21.230371475219727
2023-01-07 08:04:04,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.024528980255127
2023-01-07 08:04:04,026 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: -21.230371475219727
2023-01-07 08:04:04,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,027 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,027 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,027 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,027 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,027 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,028 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.836666107177734
2023-01-07 08:04:04,028 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,029 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,029 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,029 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.277565002441406
2023-01-07 08:04:04,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.367401123046875
2023-01-07 08:04:04,030 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -19.277565002441406
2023-01-07 08:04:04,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,031 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:04,031 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,031 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,031 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:04,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,031 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.35006856918335
2023-01-07 08:04:04,032 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:04,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,033 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:04,033 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,033 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,033 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1.5568034648895264
2023-01-07 08:04:04,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.378364562988281
2023-01-07 08:04:04,034 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 1.5568034648895264
2023-01-07 08:04:04,034 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,035 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:04,035 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,035 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,035 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:04,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,035 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,035 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.8748762607574463
2023-01-07 08:04:04,036 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,036 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,037 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -11.064445495605469
2023-01-07 08:04:04,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1453969478607178
2023-01-07 08:04:04,038 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -11.064445495605469
2023-01-07 08:04:04,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,038 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:04,038 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,039 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:04,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,039 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.737056732177734
2023-01-07 08:04:04,040 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,040 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,041 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,041 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,041 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,041 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 14.501985549926758
2023-01-07 08:04:04,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,041 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3012233376502991
2023-01-07 08:04:04,042 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.501985549926758
2023-01-07 08:04:04,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,042 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,043 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,043 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,043 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:04,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4343485832214355
2023-01-07 08:04:04,044 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,044 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,044 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,044 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,044 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:04,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3779544830322266
2023-01-07 08:04:04,046 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 32.03540802001953
2023-01-07 08:04:04,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,046 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,046 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,046 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,046 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:04,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.515338897705078
2023-01-07 08:04:04,047 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,048 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,048 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,048 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,048 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:04,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,048 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.789299488067627
2023-01-07 08:04:04,049 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 1.161661148071289
2023-01-07 08:04:04,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,049 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:04,050 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,050 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:04,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.3012590408325195
2023-01-07 08:04:04,051 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,051 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,051 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,051 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,052 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:04,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.928495407104492
2023-01-07 08:04:04,053 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: 6.488774299621582
2023-01-07 08:04:04,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,053 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,053 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,053 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,054 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:04,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,054 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:04:04,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.087410926818848
2023-01-07 08:04:04,055 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,055 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,055 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,055 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,055 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -31.224315643310547
2023-01-07 08:04:04,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.608718156814575
2023-01-07 08:04:04,057 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -31.224315643310547
2023-01-07 08:04:04,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,057 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,057 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,057 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,057 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:04,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9681552648544312
2023-01-07 08:04:04,058 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,059 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,059 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,059 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,059 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:04,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,059 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18803177773952484
2023-01-07 08:04:04,060 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 24.167444229125977
2023-01-07 08:04:04,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,061 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,061 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,061 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:04,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,061 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9593843221664429
2023-01-07 08:04:04,062 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,062 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,063 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,063 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,063 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:04,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5619072914123535
2023-01-07 08:04:04,064 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -5.7928009033203125
2023-01-07 08:04:04,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,064 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,064 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,065 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:04,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.632724761962891
2023-01-07 08:04:04,066 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,066 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,066 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,066 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:04,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.586026191711426
2023-01-07 08:04:04,068 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 16.308856964111328
2023-01-07 08:04:04,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,068 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,068 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,068 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,068 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:04,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.537571668624878
2023-01-07 08:04:04,069 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,070 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,070 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,070 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:04,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0143489837646484
2023-01-07 08:04:04,071 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -6.959627151489258
2023-01-07 08:04:04,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,072 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,072 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,072 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:04,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0579757690429688
2023-01-07 08:04:04,073 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,073 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,074 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:04,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27855414152145386
2023-01-07 08:04:04,075 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 12.91061019897461
2023-01-07 08:04:04,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,075 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,075 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,076 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,076 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:04,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7340022325515747
2023-01-07 08:04:04,077 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,077 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,077 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,077 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:04,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01699596643447876
2023-01-07 08:04:04,079 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.9678564071655273
2023-01-07 08:04:04,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,079 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,079 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,079 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,079 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:04,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,080 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.812965393066406
2023-01-07 08:04:04,080 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,081 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,081 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,081 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,081 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:04,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8365665674209595
2023-01-07 08:04:04,082 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 19.857297897338867
2023-01-07 08:04:04,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,083 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,083 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,083 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:04,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,083 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5939082503318787
2023-01-07 08:04:04,084 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,085 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,085 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,085 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:04,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5077993869781494
2023-01-07 08:04:04,086 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -1.9362564086914062
2023-01-07 08:04:04,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,086 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,086 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,087 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,087 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:04,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,087 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2858433723449707
2023-01-07 08:04:04,088 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,088 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,088 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,088 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:04,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.999572992324829
2023-01-07 08:04:04,090 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: -15.344305038452148
2023-01-07 08:04:04,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,090 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,090 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:04,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1086188554763794
2023-01-07 08:04:04,091 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,092 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,092 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:04,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,092 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5662001967430115
2023-01-07 08:04:04,093 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.877996444702148
2023-01-07 08:04:04,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,094 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,094 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,094 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:04,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9490301609039307
2023-01-07 08:04:04,095 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,095 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,096 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:04,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,096 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.966895580291748
2023-01-07 08:04:04,097 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -22.828798294067383
2023-01-07 08:04:04,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,098 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,098 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,098 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,098 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:04,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4235872030258179
2023-01-07 08:04:04,099 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,099 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,099 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,099 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,100 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:04,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.503044605255127
2023-01-07 08:04:04,101 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -34.145172119140625
2023-01-07 08:04:04,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,101 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:04,101 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:04,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8991469144821167
2023-01-07 08:04:04,102 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:04,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,103 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:04,103 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:04,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30474117398262024
2023-01-07 08:04:04,104 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -11.372824668884277
2023-01-07 08:04:04,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,105 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:04,105 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,105 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:04,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4106789827346802
2023-01-07 08:04:04,106 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:04,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,106 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:04,107 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:04,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7785912752151489
2023-01-07 08:04:04,108 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -60.26964569091797
2023-01-07 08:04:04,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,108 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:04,108 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,108 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:04,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07208256423473358
2023-01-07 08:04:04,110 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,110 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,110 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,110 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,110 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,110 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:04,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3773577213287354
2023-01-07 08:04:04,112 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -24.857271194458008
2023-01-07 08:04:04,112 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,112 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:04,112 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,112 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,112 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:04,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7365119457244873
2023-01-07 08:04:04,113 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,114 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,114 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:04,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6599616408348083
2023-01-07 08:04:04,115 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 17.965049743652344
2023-01-07 08:04:04,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,116 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:04,116 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:04,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8315080404281616
2023-01-07 08:04:04,117 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:04,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,117 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:04,118 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:04,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2359987497329712
2023-01-07 08:04:04,119 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -34.256595611572266
2023-01-07 08:04:04,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,119 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:04:04,119 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:04,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16073717176914215
2023-01-07 08:04:04,120 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:04,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,121 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:04,121 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:04,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24946823716163635
2023-01-07 08:04:04,122 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.075042724609375
2023-01-07 08:04:04,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,123 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:04,123 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,123 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:04,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06265002489089966
2023-01-07 08:04:04,124 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,125 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,125 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:04,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5468940734863281
2023-01-07 08:04:04,126 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7992687225341797
2023-01-07 08:04:04,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,126 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:04,126 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:04,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7776662707328796
2023-01-07 08:04:04,128 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,128 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,128 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:04,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3328101336956024
2023-01-07 08:04:04,130 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -23.16168212890625
2023-01-07 08:04:04,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,130 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:04,130 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:04,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04464595764875412
2023-01-07 08:04:04,131 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:04,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,132 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:04,132 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:04,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.096151202917099
2023-01-07 08:04:04,133 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -62.17387390136719
2023-01-07 08:04:04,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,134 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:04,134 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,134 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,134 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:04,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2593792974948883
2023-01-07 08:04:04,135 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,135 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,135 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,136 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:04,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10125839710235596
2023-01-07 08:04:04,137 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -67.58294677734375
2023-01-07 08:04:04,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,137 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:04,137 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:04,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14510907232761383
2023-01-07 08:04:04,139 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:04,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,139 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:04,139 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:04,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20033684372901917
2023-01-07 08:04:04,140 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 42.423255920410156
2023-01-07 08:04:04,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,141 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:04,141 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:04,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7550872564315796
2023-01-07 08:04:04,142 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:04,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,143 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:04,143 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:04,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9679353833198547
2023-01-07 08:04:04,144 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.02621841430664
2023-01-07 08:04:04,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,145 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:04:04,145 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,145 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:04,146 > [DEBUG] 0 :: 7.185880184173584
2023-01-07 08:04:04,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,148 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -456.8349609375
2023-01-07 08:04:04,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,150 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -463.34552001953125
2023-01-07 08:04:04,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,152 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05622105672955513
2023-01-07 08:04:04,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,154 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8308617472648621
2023-01-07 08:04:04,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,156 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08246096223592758
2023-01-07 08:04:04,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,157 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20023134350776672
2023-01-07 08:04:04,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,159 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0157728660851717
2023-01-07 08:04:04,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,160 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7832888960838318
2023-01-07 08:04:04,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,162 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3664265275001526
2023-01-07 08:04:04,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,163 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4103531837463379
2023-01-07 08:04:04,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,165 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4047943353652954
2023-01-07 08:04:04,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,166 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0547356605529785
2023-01-07 08:04:04,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,167 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11891724914312363
2023-01-07 08:04:04,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,168 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4362642765045166
2023-01-07 08:04:04,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,170 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9746651649475098
2023-01-07 08:04:04,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,171 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3489024043083191
2023-01-07 08:04:04,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,173 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4821799397468567
2023-01-07 08:04:04,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,174 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2246267795562744
2023-01-07 08:04:04,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,175 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.138701319694519
2023-01-07 08:04:04,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,176 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,177 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23250427842140198
2023-01-07 08:04:04,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,178 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8551002740859985
2023-01-07 08:04:04,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,179 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.434007167816162
2023-01-07 08:04:04,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,180 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,180 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6767089366912842
2023-01-07 08:04:04,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,182 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6624466180801392
2023-01-07 08:04:04,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,183 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8545444011688232
2023-01-07 08:04:04,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,184 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13486051559448242
2023-01-07 08:04:04,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,186 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5467748641967773
2023-01-07 08:04:04,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,187 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,187 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26005125045776367
2023-01-07 08:04:04,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,188 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16694864630699158
2023-01-07 08:04:04,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,189 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,190 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7290023565292358
2023-01-07 08:04:04,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,191 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,191 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.852668046951294
2023-01-07 08:04:04,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,192 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8830428719520569
2023-01-07 08:04:04,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,193 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,194 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6009222269058228
2023-01-07 08:04:04,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,195 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8020883202552795
2023-01-07 08:04:04,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,196 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,196 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.044647399336099625
2023-01-07 08:04:04,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,197 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1155190467834473
2023-01-07 08:04:04,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,199 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3738762140274048
2023-01-07 08:04:04,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,200 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.47557714581489563
2023-01-07 08:04:04,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,201 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,201 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.150862216949463
2023-01-07 08:04:04,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,202 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6986982226371765
2023-01-07 08:04:04,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,204 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,204 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0300508737564087
2023-01-07 08:04:04,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,205 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6981778144836426
2023-01-07 08:04:04,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,206 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,206 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.693227529525757
2023-01-07 08:04:04,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,207 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8053462505340576
2023-01-07 08:04:04,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,209 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.807680606842041
2023-01-07 08:04:04,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,210 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5178954005241394
2023-01-07 08:04:04,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,211 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4759129285812378
2023-01-07 08:04:04,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,213 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13400648534297943
2023-01-07 08:04:04,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,214 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.455919027328491
2023-01-07 08:04:04,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,215 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.54670524597168
2023-01-07 08:04:04,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,216 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8104362487792969
2023-01-07 08:04:04,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,217 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.809440612792969
2023-01-07 08:04:04,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,219 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7413128614425659
2023-01-07 08:04:04,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,220 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.077981948852539
2023-01-07 08:04:04,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,222 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,222 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34243708848953247
2023-01-07 08:04:04,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,223 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,223 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.706485748291016
2023-01-07 08:04:04,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,224 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.005829811096191
2023-01-07 08:04:04,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.33639144897461
2023-01-07 08:04:04,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,227 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.46187973022461
2023-01-07 08:04:04,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,228 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.393585205078125
2023-01-07 08:04:04,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,229 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8239753246307373
2023-01-07 08:04:04,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,230 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8050066232681274
2023-01-07 08:04:04,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,232 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.395503044128418
2023-01-07 08:04:04,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,233 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.910274505615234
2023-01-07 08:04:04,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,234 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.431058883666992
2023-01-07 08:04:04,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.87350082397461
2023-01-07 08:04:04,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,237 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,237 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.738429546356201
2023-01-07 08:04:04,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.872773170471191
2023-01-07 08:04:04,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,239 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.53879165649414
2023-01-07 08:04:04,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.860992431640625
2023-01-07 08:04:04,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,242 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.946275234222412
2023-01-07 08:04:04,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,243 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,243 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.063589572906494
2023-01-07 08:04:04,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,244 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.290637969970703
2023-01-07 08:04:04,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.265531539916992
2023-01-07 08:04:04,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1343629360198975
2023-01-07 08:04:04,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.55010414123535
2023-01-07 08:04:04,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,249 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8230122327804565
2023-01-07 08:04:04,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,250 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.645092010498047
2023-01-07 08:04:04,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,251 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,252 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3792765140533447
2023-01-07 08:04:04,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,253 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.34994649887085
2023-01-07 08:04:04,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,254 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.216090202331543
2023-01-07 08:04:04,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.242629051208496
2023-01-07 08:04:04,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,256 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.899690628051758
2023-01-07 08:04:04,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.89029312133789
2023-01-07 08:04:04,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.596500396728516
2023-01-07 08:04:04,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,260 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,260 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.23625946044922
2023-01-07 08:04:04,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.264320373535156
2023-01-07 08:04:04,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.459758758544922
2023-01-07 08:04:04,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,264 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8456406593322754
2023-01-07 08:04:04,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,265 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.710153579711914
2023-01-07 08:04:04,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.62899398803711
2023-01-07 08:04:04,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,267 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,268 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.121204376220703
2023-01-07 08:04:04,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,269 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.883124351501465
2023-01-07 08:04:04,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,270 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.360273361206055
2023-01-07 08:04:04,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.742877960205078
2023-01-07 08:04:04,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,273 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 102.99728393554688
2023-01-07 08:04:04,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,274 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.2591047286987305
2023-01-07 08:04:04,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,275 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,275 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,276 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.50078582763672
2023-01-07 08:04:04,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.346784591674805
2023-01-07 08:04:04,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,278 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.843494415283203
2023-01-07 08:04:04,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.909280776977539
2023-01-07 08:04:04,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.94358825683594
2023-01-07 08:04:04,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,282 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -88.30654907226562
2023-01-07 08:04:04,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,283 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7644920349121094
2023-01-07 08:04:04,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -127.0407485961914
2023-01-07 08:04:04,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,285 > [DEBUG] 0 :: before allreduce fusion buffer :: -150.3294219970703
2023-01-07 08:04:04,289 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:04:04,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,290 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:04,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 1876.0921630859375
2023-01-07 08:04:04,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.124496459960938
2023-01-07 08:04:04,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.32903003692627
2023-01-07 08:04:04,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.48593521118164
2023-01-07 08:04:04,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:04,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:04,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 924.97265625
2023-01-07 08:04:04,305 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.6732504367828369
2023-01-07 08:04:04,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:04,305 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:04:04,305 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:04,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:05,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,151 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 347.24615478515625
2023-01-07 08:04:05,152 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,153 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,153 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 9.218955993652344
2023-01-07 08:04:05,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,153 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.1997184753418
2023-01-07 08:04:05,155 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.218955993652344
2023-01-07 08:04:05,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,155 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:05,155 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,155 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,155 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:05,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 91.23977661132812
2023-01-07 08:04:05,156 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,157 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,157 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:05,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 181.85377502441406
2023-01-07 08:04:05,158 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 7.042952537536621
2023-01-07 08:04:05,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,159 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:05,159 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,159 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:05,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,159 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,159 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9619407653808594
2023-01-07 08:04:05,160 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,161 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,161 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:05,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.894723892211914
2023-01-07 08:04:05,162 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.099423885345459
2023-01-07 08:04:05,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,163 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,163 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:05,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,163 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.3598747253418
2023-01-07 08:04:05,164 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,165 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,165 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:05,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.92180633544922
2023-01-07 08:04:05,166 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 5.568670272827148
2023-01-07 08:04:05,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,166 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,167 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,167 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,167 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:05,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -72.43075561523438
2023-01-07 08:04:05,168 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,168 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,168 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:05,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,169 > [DEBUG] 0 :: before allreduce fusion buffer :: -104.7798080444336
2023-01-07 08:04:05,170 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8228445053100586
2023-01-07 08:04:05,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,170 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,170 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:05,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,171 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -366.4366455078125
2023-01-07 08:04:05,172 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,172 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,172 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:05,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 64.77198791503906
2023-01-07 08:04:05,174 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -6.840078353881836
2023-01-07 08:04:05,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,174 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:05,174 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,174 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:05,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,175 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,175 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.042694091796875
2023-01-07 08:04:05,176 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,176 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,176 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,176 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,176 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:05,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,177 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.649024963378906
2023-01-07 08:04:05,178 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 11.602226257324219
2023-01-07 08:04:05,178 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,178 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,178 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,178 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,178 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:05,178 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,178 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.286015272140503
2023-01-07 08:04:05,179 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,180 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,180 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:05,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.770626068115234
2023-01-07 08:04:05,181 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -38.38706970214844
2023-01-07 08:04:05,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,182 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,182 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:05,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.57003402709961
2023-01-07 08:04:05,183 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,183 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,184 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:05,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.93426513671875
2023-01-07 08:04:05,185 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -10.686333656311035
2023-01-07 08:04:05,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,185 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:05,185 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:05,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,186 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,186 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.90472412109375
2023-01-07 08:04:05,187 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:05,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,187 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:05,188 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:05,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 61.99473571777344
2023-01-07 08:04:05,189 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -4.334873676300049
2023-01-07 08:04:05,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,189 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:05,189 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,190 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.012504577636719
2023-01-07 08:04:05,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.327724933624268
2023-01-07 08:04:05,191 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,191 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,191 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:05,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.779258728027344
2023-01-07 08:04:05,193 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -13.012504577636719
2023-01-07 08:04:05,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,193 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:04:05,193 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,193 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,193 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:05,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 94.70909118652344
2023-01-07 08:04:05,194 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,195 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,195 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:05,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.225481033325195
2023-01-07 08:04:05,196 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 2.7129430770874023
2023-01-07 08:04:05,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,197 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:05,197 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,197 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,197 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,197 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.93899154663086
2023-01-07 08:04:05,198 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,199 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,199 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.566534042358398
2023-01-07 08:04:05,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.680410385131836
2023-01-07 08:04:05,200 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 10.566534042358398
2023-01-07 08:04:05,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,201 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,201 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:05,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,201 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8464560508728027
2023-01-07 08:04:05,202 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,202 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,203 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,203 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.445682525634766
2023-01-07 08:04:05,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.846672058105469
2023-01-07 08:04:05,204 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.445682525634766
2023-01-07 08:04:05,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,204 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:05,204 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:05,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.746393203735352
2023-01-07 08:04:05,205 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,206 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,206 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:05,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.883846282958984
2023-01-07 08:04:05,207 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 18.37600326538086
2023-01-07 08:04:05,207 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,208 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,208 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,208 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5199270248413086
2023-01-07 08:04:05,209 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,210 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,210 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,210 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.85270881652832
2023-01-07 08:04:05,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,210 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2852389812469482
2023-01-07 08:04:05,211 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85270881652832
2023-01-07 08:04:05,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,211 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:05,212 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,212 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,212 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.41214048862457275
2023-01-07 08:04:05,213 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,213 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,213 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,214 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,214 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -1.3110322952270508
2023-01-07 08:04:05,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.09261417388916
2023-01-07 08:04:05,215 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -1.3110322952270508
2023-01-07 08:04:05,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,215 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,215 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,216 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:05,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.968156814575195
2023-01-07 08:04:05,217 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,217 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,217 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,217 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:05,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.651412963867188
2023-01-07 08:04:05,218 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 56.35940933227539
2023-01-07 08:04:05,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,219 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,219 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,219 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,220 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.91240692138672
2023-01-07 08:04:05,220 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,221 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,221 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.077699661254883
2023-01-07 08:04:05,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,221 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.753844261169434
2023-01-07 08:04:05,222 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.077699661254883
2023-01-07 08:04:05,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,223 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:05,223 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,223 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.25813102722168
2023-01-07 08:04:05,224 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,225 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,225 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -24.877731323242188
2023-01-07 08:04:05,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.499661445617676
2023-01-07 08:04:05,226 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.877731323242188
2023-01-07 08:04:05,226 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,227 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,227 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:05,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,227 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,227 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4309918880462646
2023-01-07 08:04:05,228 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,228 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,229 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,229 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -21.230371475219727
2023-01-07 08:04:05,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13349801301956177
2023-01-07 08:04:05,230 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: -21.230371475219727
2023-01-07 08:04:05,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,230 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,230 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,231 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.176861763000488
2023-01-07 08:04:05,232 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,232 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,232 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.277565002441406
2023-01-07 08:04:05,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8720719814300537
2023-01-07 08:04:05,234 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -19.277565002441406
2023-01-07 08:04:05,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,235 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:05,235 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:05,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,235 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6074206829071045
2023-01-07 08:04:05,236 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:05,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,237 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:05,237 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1.5568034648895264
2023-01-07 08:04:05,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7990221977233887
2023-01-07 08:04:05,238 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 1.5568034648895264
2023-01-07 08:04:05,238 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,238 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:05,238 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:05,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,239 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.202143669128418
2023-01-07 08:04:05,240 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,240 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,240 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -11.064445495605469
2023-01-07 08:04:05,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.011273384094238
2023-01-07 08:04:05,242 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -11.064445495605469
2023-01-07 08:04:05,242 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,242 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:05,242 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:05,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,243 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,243 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4027915000915527
2023-01-07 08:04:05,244 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,244 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,244 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 14.501985549926758
2023-01-07 08:04:05,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.095297813415527
2023-01-07 08:04:05,246 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.501985549926758
2023-01-07 08:04:05,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,246 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,246 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:05,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.993839263916016
2023-01-07 08:04:05,247 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,248 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,248 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,248 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:05,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.828634023666382
2023-01-07 08:04:05,249 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 32.03540802001953
2023-01-07 08:04:05,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,250 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,250 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:05,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.679337501525879
2023-01-07 08:04:05,251 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,251 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,251 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:05,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,252 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.567394256591797
2023-01-07 08:04:05,253 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 1.161661148071289
2023-01-07 08:04:05,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,253 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:05,253 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:05,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06679344177246094
2023-01-07 08:04:05,254 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,255 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,255 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:05,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.904216289520264
2023-01-07 08:04:05,256 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: 6.488774299621582
2023-01-07 08:04:05,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,257 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,257 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:05,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:04:05,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.985908031463623
2023-01-07 08:04:05,258 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,259 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,259 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -31.224315643310547
2023-01-07 08:04:05,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.730909824371338
2023-01-07 08:04:05,260 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -31.224315643310547
2023-01-07 08:04:05,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,261 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,261 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,261 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,261 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:05,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.811341643333435
2023-01-07 08:04:05,262 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,262 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,262 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:05,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5631885528564453
2023-01-07 08:04:05,264 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 24.167444229125977
2023-01-07 08:04:05,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,264 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,264 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:05,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.220463752746582
2023-01-07 08:04:05,266 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,266 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,266 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:05,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6682358980178833
2023-01-07 08:04:05,267 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -5.7928009033203125
2023-01-07 08:04:05,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,268 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,268 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:05,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1129164695739746
2023-01-07 08:04:05,269 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,270 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,270 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:05,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.588726043701172
2023-01-07 08:04:05,271 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 16.308856964111328
2023-01-07 08:04:05,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,272 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,272 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:05,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.5963945388793945
2023-01-07 08:04:05,273 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,273 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,273 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,274 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:05,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3197709619998932
2023-01-07 08:04:05,275 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -6.959627151489258
2023-01-07 08:04:05,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,275 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,275 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:05,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2972291707992554
2023-01-07 08:04:05,276 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,277 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,277 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,277 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:05,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8918788433074951
2023-01-07 08:04:05,278 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 12.91061019897461
2023-01-07 08:04:05,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,279 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,279 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:05,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2160266637802124
2023-01-07 08:04:05,280 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,281 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,281 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:05,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,281 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9266097545623779
2023-01-07 08:04:05,282 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.9678564071655273
2023-01-07 08:04:05,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,282 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,282 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:05,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7752642631530762
2023-01-07 08:04:05,284 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,284 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,284 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:05,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35607728362083435
2023-01-07 08:04:05,286 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 19.857297897338867
2023-01-07 08:04:05,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,286 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,286 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:05,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4112374782562256
2023-01-07 08:04:05,287 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,288 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,288 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:05,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01603473722934723
2023-01-07 08:04:05,290 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -1.9362564086914062
2023-01-07 08:04:05,290 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,291 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,291 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,291 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:05,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2108845710754395
2023-01-07 08:04:05,292 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,292 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,293 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:05,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,293 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9757509231567383
2023-01-07 08:04:05,294 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: -15.344305038452148
2023-01-07 08:04:05,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,294 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,294 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:05,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0785311684012413
2023-01-07 08:04:05,296 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,296 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,296 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:05,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2267378568649292
2023-01-07 08:04:05,298 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.877996444702148
2023-01-07 08:04:05,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,298 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,298 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:05,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8602819442749023
2023-01-07 08:04:05,299 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,300 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,300 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:05,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.449716329574585
2023-01-07 08:04:05,301 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -22.828798294067383
2023-01-07 08:04:05,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,302 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,302 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:05,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7115370035171509
2023-01-07 08:04:05,303 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,303 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,304 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:05,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3625068664550781
2023-01-07 08:04:05,305 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -34.145172119140625
2023-01-07 08:04:05,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,305 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:05,305 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:05,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03904583677649498
2023-01-07 08:04:05,307 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:05,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,307 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:05,307 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:05,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,308 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3707217276096344
2023-01-07 08:04:05,309 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -11.372824668884277
2023-01-07 08:04:05,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,309 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:05,309 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:05,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8459227085113525
2023-01-07 08:04:05,310 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:05,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,311 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:05,311 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:05,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,311 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.287947177886963
2023-01-07 08:04:05,312 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -60.26964569091797
2023-01-07 08:04:05,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,313 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:05,313 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:05,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.520550489425659
2023-01-07 08:04:05,314 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,314 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,314 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,314 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:05,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.545713424682617
2023-01-07 08:04:05,316 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -24.857271194458008
2023-01-07 08:04:05,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,316 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:05,316 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:05,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012118980288505554
2023-01-07 08:04:05,318 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,318 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,318 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:05,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35311824083328247
2023-01-07 08:04:05,320 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 17.965049743652344
2023-01-07 08:04:05,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,320 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:05,320 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:05,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,321 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6730539202690125
2023-01-07 08:04:05,321 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:05,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,322 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:05,322 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,322 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:05,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5076193809509277
2023-01-07 08:04:05,323 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -34.256595611572266
2023-01-07 08:04:05,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,324 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:04:05,324 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:05,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8104966282844543
2023-01-07 08:04:05,325 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:05,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,325 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:05,325 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,325 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:05,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6413631439208984
2023-01-07 08:04:05,327 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.075042724609375
2023-01-07 08:04:05,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,327 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:05,327 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:05,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1508541703224182
2023-01-07 08:04:05,328 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,329 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,329 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:05,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,329 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2529042959213257
2023-01-07 08:04:05,330 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7992687225341797
2023-01-07 08:04:05,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,331 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:05,331 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:05,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0699411928653717
2023-01-07 08:04:05,332 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,333 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,333 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:05,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6276434659957886
2023-01-07 08:04:05,334 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -23.16168212890625
2023-01-07 08:04:05,334 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,334 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:05,335 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:05,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2997920513153076
2023-01-07 08:04:05,336 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:05,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,336 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:05,336 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:05,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43284061551094055
2023-01-07 08:04:05,338 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -62.17387390136719
2023-01-07 08:04:05,338 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,338 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:05,338 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:05,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.374906063079834
2023-01-07 08:04:05,339 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,340 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,340 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:05,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03978724032640457
2023-01-07 08:04:05,341 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -67.58294677734375
2023-01-07 08:04:05,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,342 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:05,342 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:05,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14329282939434052
2023-01-07 08:04:05,343 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:05,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,343 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:05,343 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:05,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04348946362733841
2023-01-07 08:04:05,345 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 42.423255920410156
2023-01-07 08:04:05,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,345 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:05,345 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:05,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6047964096069336
2023-01-07 08:04:05,347 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:05,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,347 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:05,347 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:05,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3375766277313232
2023-01-07 08:04:05,349 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.02621841430664
2023-01-07 08:04:05,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,349 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:04:05,349 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:05,350 > [DEBUG] 0 :: 7.165858268737793
2023-01-07 08:04:05,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,354 > [DEBUG] 0 :: before allreduce fusion buffer :: -547.4719848632812
2023-01-07 08:04:05,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -601.90673828125
2023-01-07 08:04:05,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29881131649017334
2023-01-07 08:04:05,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,363 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.676810622215271
2023-01-07 08:04:05,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06007283180952072
2023-01-07 08:04:05,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,366 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27112463116645813
2023-01-07 08:04:05,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,367 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3614462614059448
2023-01-07 08:04:05,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,368 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5683247447013855
2023-01-07 08:04:05,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5179659128189087
2023-01-07 08:04:05,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9702930450439453
2023-01-07 08:04:05,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.042540229856967926
2023-01-07 08:04:05,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3593885004520416
2023-01-07 08:04:05,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14809006452560425
2023-01-07 08:04:05,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3527594804763794
2023-01-07 08:04:05,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35505038499832153
2023-01-07 08:04:05,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7493366599082947
2023-01-07 08:04:05,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3922910988330841
2023-01-07 08:04:05,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.847424030303955
2023-01-07 08:04:05,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4574145972728729
2023-01-07 08:04:05,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2111077308654785
2023-01-07 08:04:05,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.707699775695801
2023-01-07 08:04:05,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.510989189147949
2023-01-07 08:04:05,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,387 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.959566593170166
2023-01-07 08:04:05,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9969915151596069
2023-01-07 08:04:05,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5054739713668823
2023-01-07 08:04:05,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,391 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9206423759460449
2023-01-07 08:04:05,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8045448660850525
2023-01-07 08:04:05,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44268617033958435
2023-01-07 08:04:05,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4091138541698456
2023-01-07 08:04:05,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05665746331214905
2023-01-07 08:04:05,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,397 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8088221549987793
2023-01-07 08:04:05,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.003385066986084
2023-01-07 08:04:05,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18115723133087158
2023-01-07 08:04:05,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3788681030273438
2023-01-07 08:04:05,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9897930026054382
2023-01-07 08:04:05,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0183604955673218
2023-01-07 08:04:05,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7275226712226868
2023-01-07 08:04:05,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9419199228286743
2023-01-07 08:04:05,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6474078297615051
2023-01-07 08:04:05,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.928896963596344
2023-01-07 08:04:05,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7364713549613953
2023-01-07 08:04:05,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,410 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40643391013145447
2023-01-07 08:04:05,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.930768966674805
2023-01-07 08:04:05,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.444826126098633
2023-01-07 08:04:05,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.412785530090332
2023-01-07 08:04:05,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.931258201599121
2023-01-07 08:04:05,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5083160996437073
2023-01-07 08:04:05,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7176244258880615
2023-01-07 08:04:05,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.030622780323028564
2023-01-07 08:04:05,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.72994613647461
2023-01-07 08:04:05,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,422 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.802837371826172
2023-01-07 08:04:05,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.635719299316406
2023-01-07 08:04:05,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6737918853759766
2023-01-07 08:04:05,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.097446441650391
2023-01-07 08:04:05,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4606029987335205
2023-01-07 08:04:05,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.202539443969727
2023-01-07 08:04:05,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5484323501586914
2023-01-07 08:04:05,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.789024353027344
2023-01-07 08:04:05,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,431 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.545736789703369
2023-01-07 08:04:05,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,433 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.887993812561035
2023-01-07 08:04:05,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.751466751098633
2023-01-07 08:04:05,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.751781940460205
2023-01-07 08:04:05,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.918588638305664
2023-01-07 08:04:05,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.71059799194336
2023-01-07 08:04:05,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.617260456085205
2023-01-07 08:04:05,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.076690673828125
2023-01-07 08:04:05,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2042332887649536
2023-01-07 08:04:05,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1743855476379395
2023-01-07 08:04:05,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.16703987121582
2023-01-07 08:04:05,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.584352493286133
2023-01-07 08:04:05,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8995699882507324
2023-01-07 08:04:05,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9987704753875732
2023-01-07 08:04:05,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7545361518859863
2023-01-07 08:04:05,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.359909057617188
2023-01-07 08:04:05,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.240768432617188
2023-01-07 08:04:05,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.348284721374512
2023-01-07 08:04:05,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7959861755371094
2023-01-07 08:04:05,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.336606502532959
2023-01-07 08:04:05,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.638892650604248
2023-01-07 08:04:05,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.683356285095215
2023-01-07 08:04:05,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.373523235321045
2023-01-07 08:04:05,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.187487602233887
2023-01-07 08:04:05,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.33811569213867
2023-01-07 08:04:05,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.283050537109375
2023-01-07 08:04:05,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.518806457519531
2023-01-07 08:04:05,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.03899383544922
2023-01-07 08:04:05,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.298346519470215
2023-01-07 08:04:05,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.761234283447266
2023-01-07 08:04:05,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.400970458984375
2023-01-07 08:04:05,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,468 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.08710479736328
2023-01-07 08:04:05,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.93089294433594
2023-01-07 08:04:05,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.19902038574219
2023-01-07 08:04:05,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.468462944030762
2023-01-07 08:04:05,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.800235748291016
2023-01-07 08:04:05,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.649934768676758
2023-01-07 08:04:05,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.106842041015625
2023-01-07 08:04:05,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.858829498291016
2023-01-07 08:04:05,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,479 > [DEBUG] 0 :: before allreduce fusion buffer :: -71.32081604003906
2023-01-07 08:04:05,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.543799877166748
2023-01-07 08:04:05,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.518610954284668
2023-01-07 08:04:05,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.458086013793945
2023-01-07 08:04:05,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 114.09930419921875
2023-01-07 08:04:05,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.13965606689453
2023-01-07 08:04:05,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.18421173095703
2023-01-07 08:04:05,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -213.54611206054688
2023-01-07 08:04:05,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.455169677734375
2023-01-07 08:04:05,491 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:04:05,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 430.4598388671875
2023-01-07 08:04:05,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 190.24017333984375
2023-01-07 08:04:05,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.634981155395508
2023-01-07 08:04:05,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.718656539916992
2023-01-07 08:04:05,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:05,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:05,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 1160.4580078125
2023-01-07 08:04:05,507 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.6732504367828369
2023-01-07 08:04:05,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:05,509 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:04:05,509 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:05,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:06,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 236.72821044921875
2023-01-07 08:04:06,353 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,354 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,354 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 9.218955993652344
2023-01-07 08:04:06,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,355 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.473520278930664
2023-01-07 08:04:06,356 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.218955993652344
2023-01-07 08:04:06,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,356 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:06,356 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,356 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:06,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 128.5327911376953
2023-01-07 08:04:06,357 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,358 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,358 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:06,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.37171173095703
2023-01-07 08:04:06,359 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 7.042952537536621
2023-01-07 08:04:06,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,360 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:06,360 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:06,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 142.26153564453125
2023-01-07 08:04:06,362 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,362 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,362 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:06,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.50884246826172
2023-01-07 08:04:06,363 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.099423885345459
2023-01-07 08:04:06,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,364 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,364 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:06,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.220263957977295
2023-01-07 08:04:06,365 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,366 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,366 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:06,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.91347312927246
2023-01-07 08:04:06,367 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 5.568670272827148
2023-01-07 08:04:06,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,368 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,368 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:06,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 102.25076293945312
2023-01-07 08:04:06,369 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,369 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,369 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:06,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -157.94989013671875
2023-01-07 08:04:06,371 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8228445053100586
2023-01-07 08:04:06,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,371 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,371 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:06,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -236.80401611328125
2023-01-07 08:04:06,373 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,373 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,373 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:06,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,374 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.640937805175781
2023-01-07 08:04:06,375 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -6.840078353881836
2023-01-07 08:04:06,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,375 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:06,375 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:06,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.268043518066406
2023-01-07 08:04:06,377 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,377 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,377 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:06,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -90.16444396972656
2023-01-07 08:04:06,378 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 11.602226257324219
2023-01-07 08:04:06,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,379 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,379 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:06,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.34247970581055
2023-01-07 08:04:06,380 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,381 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,381 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:06,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.27743148803711
2023-01-07 08:04:06,382 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -38.38706970214844
2023-01-07 08:04:06,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,383 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,383 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:06,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.438724517822266
2023-01-07 08:04:06,384 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,384 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,385 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:06,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.03175735473633
2023-01-07 08:04:06,386 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -10.686333656311035
2023-01-07 08:04:06,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,386 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:06,386 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:06,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.41888427734375
2023-01-07 08:04:06,388 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:06,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,388 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:06,388 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:06,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.45355987548828
2023-01-07 08:04:06,390 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -4.334873676300049
2023-01-07 08:04:06,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,390 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:06,390 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.012504577636719
2023-01-07 08:04:06,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.28243637084961
2023-01-07 08:04:06,392 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,392 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,392 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:06,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.74992561340332
2023-01-07 08:04:06,394 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -13.012504577636719
2023-01-07 08:04:06,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,394 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:04:06,394 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:06,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.10306930541992
2023-01-07 08:04:06,395 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,396 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,396 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:06,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.695097923278809
2023-01-07 08:04:06,397 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 2.7129430770874023
2023-01-07 08:04:06,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,398 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:06,398 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,398 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,398 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.45918273925781
2023-01-07 08:04:06,399 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,399 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,400 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.566534042358398
2023-01-07 08:04:06,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.864818572998047
2023-01-07 08:04:06,401 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 10.566534042358398
2023-01-07 08:04:06,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,401 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,401 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:06,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,402 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.209235668182373
2023-01-07 08:04:06,403 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,403 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,403 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,403 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,403 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.445682525634766
2023-01-07 08:04:06,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.835411071777344
2023-01-07 08:04:06,404 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.445682525634766
2023-01-07 08:04:06,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,405 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:06,405 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:06,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.99972677230835
2023-01-07 08:04:06,406 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,407 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,407 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:06,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.294702529907227
2023-01-07 08:04:06,408 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 18.37600326538086
2023-01-07 08:04:06,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,409 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,409 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,409 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,409 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.001359939575195
2023-01-07 08:04:06,410 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,410 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,411 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.85270881652832
2023-01-07 08:04:06,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,411 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.632291316986084
2023-01-07 08:04:06,412 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85270881652832
2023-01-07 08:04:06,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,412 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:06,412 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9315075874328613
2023-01-07 08:04:06,414 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,414 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,414 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -1.3110322952270508
2023-01-07 08:04:06,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7617471218109131
2023-01-07 08:04:06,416 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -1.3110322952270508
2023-01-07 08:04:06,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,416 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,416 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:06,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.493477821350098
2023-01-07 08:04:06,417 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,418 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,418 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:06,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,418 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.909934997558594
2023-01-07 08:04:06,419 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 56.35940933227539
2023-01-07 08:04:06,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,420 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,420 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.26953887939453
2023-01-07 08:04:06,421 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,422 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,422 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.077699661254883
2023-01-07 08:04:06,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.808110237121582
2023-01-07 08:04:06,423 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.077699661254883
2023-01-07 08:04:06,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,424 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:06,424 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,424 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.174800872802734
2023-01-07 08:04:06,425 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,426 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,426 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,426 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,426 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -24.877731323242188
2023-01-07 08:04:06,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,426 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.037975311279297
2023-01-07 08:04:06,427 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.877731323242188
2023-01-07 08:04:06,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,427 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,427 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:06,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8699450492858887
2023-01-07 08:04:06,429 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,429 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,429 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,429 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -21.230371475219727
2023-01-07 08:04:06,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.573589563369751
2023-01-07 08:04:06,431 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: -21.230371475219727
2023-01-07 08:04:06,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,431 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,431 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,431 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,431 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.665599822998047
2023-01-07 08:04:06,433 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,433 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,433 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.277565002441406
2023-01-07 08:04:06,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.42588996887207
2023-01-07 08:04:06,435 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -19.277565002441406
2023-01-07 08:04:06,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,435 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:06,435 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:06,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.502573013305664
2023-01-07 08:04:06,437 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:06,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,437 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:06,437 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1.5568034648895264
2023-01-07 08:04:06,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.017194747924805
2023-01-07 08:04:06,439 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 1.5568034648895264
2023-01-07 08:04:06,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,439 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:06,439 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,439 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:06,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.991597652435303
2023-01-07 08:04:06,440 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,441 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,441 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -11.064445495605469
2023-01-07 08:04:06,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,441 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.06095027923584
2023-01-07 08:04:06,442 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -11.064445495605469
2023-01-07 08:04:06,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,443 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:06,443 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:06,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.608619689941406
2023-01-07 08:04:06,444 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,445 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,445 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 14.501985549926758
2023-01-07 08:04:06,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.190879821777344
2023-01-07 08:04:06,446 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.501985549926758
2023-01-07 08:04:06,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,447 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,447 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:06,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8248472213745117
2023-01-07 08:04:06,448 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,448 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,448 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:06,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37947148084640503
2023-01-07 08:04:06,450 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 32.03540802001953
2023-01-07 08:04:06,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,450 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,450 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,450 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:06,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3968536853790283
2023-01-07 08:04:06,452 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,452 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,452 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:06,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.128462076187134
2023-01-07 08:04:06,453 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 1.161661148071289
2023-01-07 08:04:06,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,454 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:06,454 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,454 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:06,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.681788921356201
2023-01-07 08:04:06,455 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,455 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,456 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:06,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4028016328811646
2023-01-07 08:04:06,457 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: 6.488774299621582
2023-01-07 08:04:06,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,457 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,457 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:06,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.5822114944458
2023-01-07 08:04:06,459 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,459 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,459 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,459 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,459 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -31.224315643310547
2023-01-07 08:04:06,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.052371978759766
2023-01-07 08:04:06,461 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -31.224315643310547
2023-01-07 08:04:06,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,461 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,461 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:06,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.627493143081665
2023-01-07 08:04:06,462 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,463 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,463 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:06,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6097213625907898
2023-01-07 08:04:06,464 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 24.167444229125977
2023-01-07 08:04:06,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,465 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,465 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,465 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,465 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:06,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6636364459991455
2023-01-07 08:04:06,466 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,466 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,467 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:06,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03588937222957611
2023-01-07 08:04:06,468 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -5.7928009033203125
2023-01-07 08:04:06,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,468 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,469 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:06,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5398006439208984
2023-01-07 08:04:06,470 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,470 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,470 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,470 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:06,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.448758602142334
2023-01-07 08:04:06,472 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 16.308856964111328
2023-01-07 08:04:06,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,472 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,472 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:06,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3859875202178955
2023-01-07 08:04:06,473 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,474 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,474 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:06,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3493456840515137
2023-01-07 08:04:06,475 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -6.959627151489258
2023-01-07 08:04:06,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,476 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,476 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:06,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2406244277954102
2023-01-07 08:04:06,477 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,477 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,477 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:06,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.336911201477051
2023-01-07 08:04:06,479 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 12.91061019897461
2023-01-07 08:04:06,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,479 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,479 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,479 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:06,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.152207136154175
2023-01-07 08:04:06,481 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,481 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,481 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:06,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.101930618286133
2023-01-07 08:04:06,483 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.9678564071655273
2023-01-07 08:04:06,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,483 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,483 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:06,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.832728624343872
2023-01-07 08:04:06,484 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,485 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,485 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,485 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,485 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:06,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4654829502105713
2023-01-07 08:04:06,486 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 19.857297897338867
2023-01-07 08:04:06,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,487 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,487 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:06,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6999402046203613
2023-01-07 08:04:06,488 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,488 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,488 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:06,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17030705511569977
2023-01-07 08:04:06,490 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -1.9362564086914062
2023-01-07 08:04:06,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,490 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,490 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,490 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:06,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4788563251495361
2023-01-07 08:04:06,492 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,492 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,492 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,492 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:06,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8164081573486328
2023-01-07 08:04:06,493 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: -15.344305038452148
2023-01-07 08:04:06,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,494 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,494 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:06,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33784985542297363
2023-01-07 08:04:06,495 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,496 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,496 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:06,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2852205038070679
2023-01-07 08:04:06,497 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.877996444702148
2023-01-07 08:04:06,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,497 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,498 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,498 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:06,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8190476894378662
2023-01-07 08:04:06,499 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,499 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,499 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:06,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,500 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8897972106933594
2023-01-07 08:04:06,501 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -22.828798294067383
2023-01-07 08:04:06,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,501 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,501 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:06,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,502 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.053614616394043
2023-01-07 08:04:06,502 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,503 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,503 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:06,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8299477100372314
2023-01-07 08:04:06,504 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -34.145172119140625
2023-01-07 08:04:06,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,505 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:06,505 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,505 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:06,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3222091794013977
2023-01-07 08:04:06,506 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:06,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,506 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:06,506 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:06,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20215561985969543
2023-01-07 08:04:06,508 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -11.372824668884277
2023-01-07 08:04:06,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,508 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:06,508 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,508 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:06,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6736927628517151
2023-01-07 08:04:06,510 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:06,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,510 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:06,510 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,510 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:06,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,511 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.964203834533691
2023-01-07 08:04:06,512 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -60.26964569091797
2023-01-07 08:04:06,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,512 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:06,512 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:06,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,512 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7993338108062744
2023-01-07 08:04:06,513 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,514 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,514 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:06,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.307966470718384
2023-01-07 08:04:06,515 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -24.857271194458008
2023-01-07 08:04:06,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,516 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:06,516 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:06,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3419010639190674
2023-01-07 08:04:06,517 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,517 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,517 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:06,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06986665725708008
2023-01-07 08:04:06,519 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 17.965049743652344
2023-01-07 08:04:06,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,519 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:06,519 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:06,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5575112104415894
2023-01-07 08:04:06,521 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:06,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,521 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:06,521 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:06,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,522 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1776219606399536
2023-01-07 08:04:06,522 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -34.256595611572266
2023-01-07 08:04:06,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,523 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:04:06,523 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:06,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.394839882850647
2023-01-07 08:04:06,524 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:06,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,524 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:06,525 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,525 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,525 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:06,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8558496832847595
2023-01-07 08:04:06,526 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.075042724609375
2023-01-07 08:04:06,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,526 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:06,526 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:06,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1966433823108673
2023-01-07 08:04:06,528 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,528 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,528 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,528 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:06,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15170887112617493
2023-01-07 08:04:06,530 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7992687225341797
2023-01-07 08:04:06,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,530 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:06,530 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:06,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.950492262840271
2023-01-07 08:04:06,531 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,532 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,532 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:06,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1667206585407257
2023-01-07 08:04:06,533 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -23.16168212890625
2023-01-07 08:04:06,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,534 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:06,534 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,534 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:06,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4617776572704315
2023-01-07 08:04:06,535 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:06,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,535 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:06,535 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:06,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,536 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32645970582962036
2023-01-07 08:04:06,537 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -62.17387390136719
2023-01-07 08:04:06,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,537 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:06,537 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:06,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,538 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15011733770370483
2023-01-07 08:04:06,538 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,539 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,539 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:06,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35566282272338867
2023-01-07 08:04:06,540 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -67.58294677734375
2023-01-07 08:04:06,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,541 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:06,541 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:06,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35279029607772827
2023-01-07 08:04:06,542 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:06,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,543 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:06,543 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:06,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19194674491882324
2023-01-07 08:04:06,544 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 42.423255920410156
2023-01-07 08:04:06,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,544 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:06,544 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:06,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3025403022766113
2023-01-07 08:04:06,546 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:06,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,546 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:06,546 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:06,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4787851572036743
2023-01-07 08:04:06,549 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.02621841430664
2023-01-07 08:04:06,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,549 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:04:06,549 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,549 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:06,550 > [DEBUG] 0 :: 7.420660972595215
2023-01-07 08:04:06,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -577.8134765625
2023-01-07 08:04:06,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -493.9644775390625
2023-01-07 08:04:06,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,560 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1961839199066162
2023-01-07 08:04:06,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45172810554504395
2023-01-07 08:04:06,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12446708977222443
2023-01-07 08:04:06,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6929591298103333
2023-01-07 08:04:06,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3729810416698456
2023-01-07 08:04:06,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8835235834121704
2023-01-07 08:04:06,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.047646842896938324
2023-01-07 08:04:06,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.925457775592804
2023-01-07 08:04:06,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34461432695388794
2023-01-07 08:04:06,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5524513125419617
2023-01-07 08:04:06,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.176337242126465
2023-01-07 08:04:06,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5369926691055298
2023-01-07 08:04:06,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1543810367584229
2023-01-07 08:04:06,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26473724842071533
2023-01-07 08:04:06,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1225779056549072
2023-01-07 08:04:06,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,583 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3983607292175293
2023-01-07 08:04:06,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5353381633758545
2023-01-07 08:04:06,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3566787242889404
2023-01-07 08:04:06,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7523981332778931
2023-01-07 08:04:06,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.079038381576538
2023-01-07 08:04:06,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7546236515045166
2023-01-07 08:04:06,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,591 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3611539602279663
2023-01-07 08:04:06,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7909526824951172
2023-01-07 08:04:06,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2820286750793457
2023-01-07 08:04:06,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.862165927886963
2023-01-07 08:04:06,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.048048734664917
2023-01-07 08:04:06,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0117006301879883
2023-01-07 08:04:06,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.65777587890625
2023-01-07 08:04:06,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2805360555648804
2023-01-07 08:04:06,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9313042163848877
2023-01-07 08:04:06,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.009340222924947739
2023-01-07 08:04:06,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.711544990539551
2023-01-07 08:04:06,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1530916690826416
2023-01-07 08:04:06,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.540870189666748
2023-01-07 08:04:06,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.29459142684936523
2023-01-07 08:04:06,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14641594886779785
2023-01-07 08:04:06,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6995701789855957
2023-01-07 08:04:06,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7401467561721802
2023-01-07 08:04:06,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9265862703323364
2023-01-07 08:04:06,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.368280410766602
2023-01-07 08:04:06,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0231236219406128
2023-01-07 08:04:06,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,616 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.644786834716797
2023-01-07 08:04:06,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.317683219909668
2023-01-07 08:04:06,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2351874113082886
2023-01-07 08:04:06,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17562073469161987
2023-01-07 08:04:06,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.056011199951172
2023-01-07 08:04:06,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.32779860496521
2023-01-07 08:04:06,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.420236587524414
2023-01-07 08:04:06,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.307922840118408
2023-01-07 08:04:06,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.384070873260498
2023-01-07 08:04:06,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3569881916046143
2023-01-07 08:04:06,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.717236518859863
2023-01-07 08:04:06,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.20713472366333
2023-01-07 08:04:06,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.196564674377441
2023-01-07 08:04:06,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.265636444091797
2023-01-07 08:04:06,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.866262435913086
2023-01-07 08:04:06,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.852890014648438
2023-01-07 08:04:06,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.887019157409668
2023-01-07 08:04:06,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,637 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.706316351890564
2023-01-07 08:04:06,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,638 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.547894477844238
2023-01-07 08:04:06,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9844239950180054
2023-01-07 08:04:06,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.205692291259766
2023-01-07 08:04:06,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.11378288269043
2023-01-07 08:04:06,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.717079162597656
2023-01-07 08:04:06,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8192665576934814
2023-01-07 08:04:06,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.608885765075684
2023-01-07 08:04:06,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.87210464477539
2023-01-07 08:04:06,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.14870548248291
2023-01-07 08:04:06,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.655672073364258
2023-01-07 08:04:06,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5337634086608887
2023-01-07 08:04:06,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.533604145050049
2023-01-07 08:04:06,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.237781524658203
2023-01-07 08:04:06,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.636451244354248
2023-01-07 08:04:06,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.03130531311035
2023-01-07 08:04:06,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.461212158203125
2023-01-07 08:04:06,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.276889801025391
2023-01-07 08:04:06,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1843215525150299
2023-01-07 08:04:06,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.496098518371582
2023-01-07 08:04:06,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.568276882171631
2023-01-07 08:04:06,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0959007740020752
2023-01-07 08:04:06,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.870759963989258
2023-01-07 08:04:06,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2860326766967773
2023-01-07 08:04:06,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.9050178527832
2023-01-07 08:04:06,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.241310119628906
2023-01-07 08:04:06,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.30907917022705
2023-01-07 08:04:06,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.93074417114258
2023-01-07 08:04:06,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.643606185913086
2023-01-07 08:04:06,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,672 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.0882568359375
2023-01-07 08:04:06,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.39396667480469
2023-01-07 08:04:06,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.861722946166992
2023-01-07 08:04:06,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.894109725952148
2023-01-07 08:04:06,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.91944122314453
2023-01-07 08:04:06,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.578887939453125
2023-01-07 08:04:06,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,679 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.366511344909668
2023-01-07 08:04:06,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.30921173095703
2023-01-07 08:04:06,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.249444961547852
2023-01-07 08:04:06,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.00041389465332
2023-01-07 08:04:06,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 179.7063751220703
2023-01-07 08:04:06,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.226356506347656
2023-01-07 08:04:06,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 147.993896484375
2023-01-07 08:04:06,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.909433364868164
2023-01-07 08:04:06,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.91775894165039
2023-01-07 08:04:06,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.32230567932129
2023-01-07 08:04:06,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.168966293334961
2023-01-07 08:04:06,695 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:04:06,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -1570.001708984375
2023-01-07 08:04:06,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,703 > [DEBUG] 0 :: before allreduce fusion buffer :: 255.14817810058594
2023-01-07 08:04:06,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.162580490112305
2023-01-07 08:04:06,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.05214500427246
2023-01-07 08:04:06,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:06,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:06,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 1069.06982421875
2023-01-07 08:04:06,707 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 sum :: -0.6732504367828369
2023-01-07 08:04:06,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:06,707 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:04:06,707 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:06,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:07,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -88.19236755371094
2023-01-07 08:04:07,554 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,554 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,554 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 9.218955993652344
2023-01-07 08:04:07,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.901634216308594
2023-01-07 08:04:07,556 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 sum :: 9.218955993652344
2023-01-07 08:04:07,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,556 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:07,556 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:07,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,557 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.60694122314453
2023-01-07 08:04:07,558 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,558 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,558 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 7.042952537536621
2023-01-07 08:04:07,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.99455261230469
2023-01-07 08:04:07,559 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 sum :: 7.042952537536621
2023-01-07 08:04:07,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,560 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:07,560 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:07,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 127.56974792480469
2023-01-07 08:04:07,562 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,562 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,562 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:07,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.03091049194336
2023-01-07 08:04:07,563 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 sum :: -2.099423885345459
2023-01-07 08:04:07,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,564 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,564 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:07,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.72278594970703
2023-01-07 08:04:07,565 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,566 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,566 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,566 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:07,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.777132034301758
2023-01-07 08:04:07,567 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 5.568670272827148
2023-01-07 08:04:07,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,568 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,568 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:07,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.722026824951172
2023-01-07 08:04:07,569 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,569 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,569 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:07,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -84.67033386230469
2023-01-07 08:04:07,571 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 sum :: 0.8228445053100586
2023-01-07 08:04:07,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,571 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,571 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:07,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.86073303222656
2023-01-07 08:04:07,573 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,573 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,573 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -6.840078353881836
2023-01-07 08:04:07,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -106.95332336425781
2023-01-07 08:04:07,575 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 sum :: -6.840078353881836
2023-01-07 08:04:07,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,575 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:07,575 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:07,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6478910446166992
2023-01-07 08:04:07,577 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,577 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,577 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:07,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3751378059387207
2023-01-07 08:04:07,578 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 sum :: 11.602226257324219
2023-01-07 08:04:07,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,579 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,579 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,579 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,579 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:07,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.794185638427734
2023-01-07 08:04:07,580 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,581 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,581 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,581 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: -38.38706970214844
2023-01-07 08:04:07,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.520923614501953
2023-01-07 08:04:07,582 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 sum :: -38.38706970214844
2023-01-07 08:04:07,582 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,583 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,583 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:07,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.97459411621094
2023-01-07 08:04:07,584 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,584 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,585 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.686333656311035
2023-01-07 08:04:07,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.256446838378906
2023-01-07 08:04:07,586 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 sum :: -10.686333656311035
2023-01-07 08:04:07,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,586 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:04:07,586 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:04:07,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.600053787231445
2023-01-07 08:04:07,588 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 sum :: 64.0
2023-01-07 08:04:07,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,588 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:04:07,588 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,589 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,589 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:07,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.435197830200195
2023-01-07 08:04:07,590 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 sum :: -4.334873676300049
2023-01-07 08:04:07,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,590 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:04:07,590 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -13.012504577636719
2023-01-07 08:04:07,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.24099349975586
2023-01-07 08:04:07,592 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,592 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,592 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,592 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,592 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:07,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 90.3128433227539
2023-01-07 08:04:07,594 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 sum :: -13.012504577636719
2023-01-07 08:04:07,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,594 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:04:07,594 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,594 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,594 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:07,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.84423828125
2023-01-07 08:04:07,595 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,596 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,596 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 2.7129430770874023
2023-01-07 08:04:07,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.586817741394043
2023-01-07 08:04:07,597 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 sum :: 2.7129430770874023
2023-01-07 08:04:07,597 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,598 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:07,598 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,598 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,598 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.859186172485352
2023-01-07 08:04:07,599 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,600 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,600 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,600 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 10.566534042358398
2023-01-07 08:04:07,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.836103439331055
2023-01-07 08:04:07,601 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 sum :: 10.566534042358398
2023-01-07 08:04:07,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,601 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,601 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,602 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,602 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:07,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.114568710327148
2023-01-07 08:04:07,603 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,603 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,603 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,604 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.445682525634766
2023-01-07 08:04:07,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.146615982055664
2023-01-07 08:04:07,605 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 17.445682525634766
2023-01-07 08:04:07,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,605 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:07,605 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:07,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.454387187957764
2023-01-07 08:04:07,606 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,607 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,607 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,607 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,607 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 18.37600326538086
2023-01-07 08:04:07,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.412030220031738
2023-01-07 08:04:07,608 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 sum :: 18.37600326538086
2023-01-07 08:04:07,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,609 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,609 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.292240142822266
2023-01-07 08:04:07,610 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,611 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,611 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.85270881652832
2023-01-07 08:04:07,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.18273162841797
2023-01-07 08:04:07,612 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 sum :: -15.85270881652832
2023-01-07 08:04:07,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,613 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:07,613 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,613 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6925437450408936
2023-01-07 08:04:07,615 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,615 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,615 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -1.3110322952270508
2023-01-07 08:04:07,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1012768745422363
2023-01-07 08:04:07,617 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 sum :: -1.3110322952270508
2023-01-07 08:04:07,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,617 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,617 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,617 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:07,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.3714776039123535
2023-01-07 08:04:07,619 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,619 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,619 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,619 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,619 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 56.35940933227539
2023-01-07 08:04:07,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7079880237579346
2023-01-07 08:04:07,621 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 sum :: 56.35940933227539
2023-01-07 08:04:07,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,621 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,621 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,621 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.2020206451416
2023-01-07 08:04:07,622 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,623 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,623 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 2.077699661254883
2023-01-07 08:04:07,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.83359146118164
2023-01-07 08:04:07,624 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 sum :: 2.077699661254883
2023-01-07 08:04:07,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,625 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:07,625 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4518260955810547
2023-01-07 08:04:07,626 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,627 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,627 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -24.877731323242188
2023-01-07 08:04:07,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.528804779052734
2023-01-07 08:04:07,628 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 sum :: -24.877731323242188
2023-01-07 08:04:07,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,629 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,629 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,629 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:07,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7648110389709473
2023-01-07 08:04:07,630 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,630 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,630 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -21.230371475219727
2023-01-07 08:04:07,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.378643035888672
2023-01-07 08:04:07,632 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 sum :: -21.230371475219727
2023-01-07 08:04:07,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,632 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,632 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.92868995666504
2023-01-07 08:04:07,634 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,634 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,634 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -19.277565002441406
2023-01-07 08:04:07,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.895343780517578
2023-01-07 08:04:07,636 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 sum :: -19.277565002441406
2023-01-07 08:04:07,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,636 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:04:07,637 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:04:07,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3863004446029663
2023-01-07 08:04:07,638 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 sum :: 128.0
2023-01-07 08:04:07,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,638 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:04:07,638 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,639 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1.5568034648895264
2023-01-07 08:04:07,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.452850341796875
2023-01-07 08:04:07,640 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 sum :: 1.5568034648895264
2023-01-07 08:04:07,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,640 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:04:07,640 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:04:07,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.022497177124023
2023-01-07 08:04:07,642 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,642 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,642 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -11.064445495605469
2023-01-07 08:04:07,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.21023178100586
2023-01-07 08:04:07,643 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 sum :: -11.064445495605469
2023-01-07 08:04:07,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,644 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:04:07,644 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:07,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9832262992858887
2023-01-07 08:04:07,645 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,646 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,646 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 14.501985549926758
2023-01-07 08:04:07,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.263492226600647
2023-01-07 08:04:07,647 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 sum :: 14.501985549926758
2023-01-07 08:04:07,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,648 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,648 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:07,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2166152000427246
2023-01-07 08:04:07,649 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,649 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,649 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 32.03540802001953
2023-01-07 08:04:07,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.157512664794922
2023-01-07 08:04:07,651 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 sum :: 32.03540802001953
2023-01-07 08:04:07,651 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,651 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,651 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:07,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.006103992462158
2023-01-07 08:04:07,652 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,653 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,653 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 1.161661148071289
2023-01-07 08:04:07,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,653 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.961633682250977
2023-01-07 08:04:07,654 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: 1.161661148071289
2023-01-07 08:04:07,654 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,654 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:07,655 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:07,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2516038417816162
2023-01-07 08:04:07,656 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,656 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,656 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 6.488774299621582
2023-01-07 08:04:07,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.230169296264648
2023-01-07 08:04:07,658 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 sum :: 6.488774299621582
2023-01-07 08:04:07,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,658 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,658 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:04:07,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.316286087036133
2023-01-07 08:04:07,660 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,660 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,660 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -31.224315643310547
2023-01-07 08:04:07,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3173021674156189
2023-01-07 08:04:07,661 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 sum :: -31.224315643310547
2023-01-07 08:04:07,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,662 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,662 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:07,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.19426155090332
2023-01-07 08:04:07,663 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,664 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,664 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,664 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,664 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 24.167444229125977
2023-01-07 08:04:07,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6894075274467468
2023-01-07 08:04:07,665 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 sum :: 24.167444229125977
2023-01-07 08:04:07,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,665 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,665 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:07,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0801358222961426
2023-01-07 08:04:07,667 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,667 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,667 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,667 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,667 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,667 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -5.7928009033203125
2023-01-07 08:04:07,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.438277721405029
2023-01-07 08:04:07,669 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 sum :: -5.7928009033203125
2023-01-07 08:04:07,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,669 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,669 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:07,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.045910358428955
2023-01-07 08:04:07,670 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,671 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,671 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 16.308856964111328
2023-01-07 08:04:07,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9385037422180176
2023-01-07 08:04:07,672 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 sum :: 16.308856964111328
2023-01-07 08:04:07,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,673 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,673 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:07,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.016462326049805
2023-01-07 08:04:07,674 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,674 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,674 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,675 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: -6.959627151489258
2023-01-07 08:04:07,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,675 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.499321937561035
2023-01-07 08:04:07,676 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 sum :: -6.959627151489258
2023-01-07 08:04:07,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,676 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,676 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,677 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:07,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3061444163322449
2023-01-07 08:04:07,678 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,678 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,678 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,678 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 12.91061019897461
2023-01-07 08:04:07,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,678 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.045828342437744
2023-01-07 08:04:07,679 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 sum :: 12.91061019897461
2023-01-07 08:04:07,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,680 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,680 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:07,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,680 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3031604290008545
2023-01-07 08:04:07,681 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,682 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,682 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: -0.9678564071655273
2023-01-07 08:04:07,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.614896297454834
2023-01-07 08:04:07,683 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 sum :: -0.9678564071655273
2023-01-07 08:04:07,683 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,684 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,684 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:07,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,684 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7109620571136475
2023-01-07 08:04:07,685 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,685 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,685 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,685 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,686 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 19.857297897338867
2023-01-07 08:04:07,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.629605293273926
2023-01-07 08:04:07,687 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 sum :: 19.857297897338867
2023-01-07 08:04:07,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,687 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,687 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:07,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.349596619606018
2023-01-07 08:04:07,688 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,689 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,689 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,689 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -1.9362564086914062
2023-01-07 08:04:07,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.098647117614746
2023-01-07 08:04:07,690 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 sum :: -1.9362564086914062
2023-01-07 08:04:07,691 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,691 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,691 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:07,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.866703987121582
2023-01-07 08:04:07,692 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,693 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,693 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,693 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -15.344305038452148
2023-01-07 08:04:07,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,693 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7319884300231934
2023-01-07 08:04:07,694 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 sum :: -15.344305038452148
2023-01-07 08:04:07,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,694 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,695 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:07,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8862472772598267
2023-01-07 08:04:07,696 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,696 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,696 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,696 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -7.877996444702148
2023-01-07 08:04:07,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6707838773727417
2023-01-07 08:04:07,698 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 sum :: -7.877996444702148
2023-01-07 08:04:07,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,698 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,698 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,698 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:07,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1878504753112793
2023-01-07 08:04:07,699 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,700 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,700 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,700 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -22.828798294067383
2023-01-07 08:04:07,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9576606750488281
2023-01-07 08:04:07,701 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 sum :: -22.828798294067383
2023-01-07 08:04:07,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,702 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,702 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,702 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:07,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6909725666046143
2023-01-07 08:04:07,703 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,703 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,704 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,704 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -34.145172119140625
2023-01-07 08:04:07,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.618112802505493
2023-01-07 08:04:07,705 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 sum :: -34.145172119140625
2023-01-07 08:04:07,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,705 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:04:07,705 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:07,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6141844987869263
2023-01-07 08:04:07,707 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 sum :: 256.0
2023-01-07 08:04:07,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,707 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:04:07,707 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,707 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -11.372824668884277
2023-01-07 08:04:07,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4715144634246826
2023-01-07 08:04:07,709 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 sum :: -11.372824668884277
2023-01-07 08:04:07,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,709 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:04:07,709 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:07,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.652697563171387
2023-01-07 08:04:07,710 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 sum :: 1024.0
2023-01-07 08:04:07,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,711 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:04:07,711 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -60.26964569091797
2023-01-07 08:04:07,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.760922431945801
2023-01-07 08:04:07,712 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 sum :: -60.26964569091797
2023-01-07 08:04:07,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,713 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:04:07,713 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:07,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.90482497215271
2023-01-07 08:04:07,714 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,714 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,714 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.857271194458008
2023-01-07 08:04:07,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.314165472984314
2023-01-07 08:04:07,716 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 sum :: -24.857271194458008
2023-01-07 08:04:07,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,716 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:07,716 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:07,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26719093322753906
2023-01-07 08:04:07,718 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,718 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,718 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 17.965049743652344
2023-01-07 08:04:07,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4595475494861603
2023-01-07 08:04:07,720 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 sum :: 17.965049743652344
2023-01-07 08:04:07,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,720 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:07,720 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:07,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3032805919647217
2023-01-07 08:04:07,721 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:07,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,722 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:07,722 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -34.256595611572266
2023-01-07 08:04:07,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,722 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07060804963111877
2023-01-07 08:04:07,723 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 sum :: -34.256595611572266
2023-01-07 08:04:07,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,723 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:04:07,724 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:07,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.30457305908203125
2023-01-07 08:04:07,725 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:07,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,725 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:07,725 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,725 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,725 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -19.075042724609375
2023-01-07 08:04:07,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1103469580411911
2023-01-07 08:04:07,727 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 sum :: -19.075042724609375
2023-01-07 08:04:07,727 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,727 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:07,727 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,727 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:07,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2055625468492508
2023-01-07 08:04:07,728 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,729 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,729 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,729 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.7992687225341797
2023-01-07 08:04:07,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16667915880680084
2023-01-07 08:04:07,730 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 sum :: -1.7992687225341797
2023-01-07 08:04:07,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,731 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:07,731 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,731 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:07,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1869845688343048
2023-01-07 08:04:07,732 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,733 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,733 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -23.16168212890625
2023-01-07 08:04:07,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05426841229200363
2023-01-07 08:04:07,734 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 sum :: -23.16168212890625
2023-01-07 08:04:07,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,734 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:07,734 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,735 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:07,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.027179572731256485
2023-01-07 08:04:07,736 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:07,736 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,736 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:07,736 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,736 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.17387390136719
2023-01-07 08:04:07,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07122396677732468
2023-01-07 08:04:07,738 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 sum :: -62.17387390136719
2023-01-07 08:04:07,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,738 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:07,738 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:07,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1135643720626831
2023-01-07 08:04:07,739 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,740 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,740 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -67.58294677734375
2023-01-07 08:04:07,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17500491440296173
2023-01-07 08:04:07,741 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 sum :: -67.58294677734375
2023-01-07 08:04:07,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,742 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:04:07,742 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:07,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5024335384368896
2023-01-07 08:04:07,743 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 sum :: 512.0
2023-01-07 08:04:07,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,743 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:04:07,743 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 42.423255920410156
2023-01-07 08:04:07,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05752945691347122
2023-01-07 08:04:07,745 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 sum :: 42.423255920410156
2023-01-07 08:04:07,745 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,745 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:04:07,745 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:07,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5320870876312256
2023-01-07 08:04:07,747 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 sum :: 2048.0
2023-01-07 08:04:07,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,747 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:04:07,747 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: 12.02621841430664
2023-01-07 08:04:07,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,747 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4701710939407349
2023-01-07 08:04:07,749 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 sum :: 12.02621841430664
2023-01-07 08:04:07,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:04:07,749 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:04:07,749 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:04:07,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:04:07,750 > [DEBUG] 0 :: 7.071974277496338
2023-01-07 08:04:07,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -620.9021606445312
2023-01-07 08:04:07,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -574.0053100585938
2023-01-07 08:04:07,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25413939356803894
2023-01-07 08:04:07,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,763 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20151539146900177
2023-01-07 08:04:07,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13386483490467072
2023-01-07 08:04:07,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28651344776153564
2023-01-07 08:04:07,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43613332509994507
2023-01-07 08:04:07,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,768 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.959081768989563
2023-01-07 08:04:07,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2079864740371704
2023-01-07 08:04:07,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5423911809921265
2023-01-07 08:04:07,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5576378107070923
2023-01-07 08:04:07,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.49087804555892944
2023-01-07 08:04:07,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.41725754737854
2023-01-07 08:04:07,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.219642162322998
2023-01-07 08:04:07,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,777 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6242234706878662
2023-01-07 08:04:07,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3218700885772705
2023-01-07 08:04:07,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8868440985679626
2023-01-07 08:04:07,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.284519672393799
2023-01-07 08:04:07,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.47800612449646
2023-01-07 08:04:07,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5180208683013916
2023-01-07 08:04:07,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4643489420413971
2023-01-07 08:04:07,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.054758071899414
2023-01-07 08:04:07,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3121366500854492
2023-01-07 08:04:07,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8205394744873047
2023-01-07 08:04:07,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3029074668884277
2023-01-07 08:04:07,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7501657605171204
2023-01-07 08:04:07,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3438350260257721
2023-01-07 08:04:07,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2464154958724976
2023-01-07 08:04:07,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,795 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9750012159347534
2023-01-07 08:04:07,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,797 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1477335691452026
2023-01-07 08:04:07,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.547910451889038
2023-01-07 08:04:07,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,799 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3820319175720215
2023-01-07 08:04:07,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,800 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9569715857505798
2023-01-07 08:04:07,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1929905414581299
2023-01-07 08:04:07,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8567523956298828
2023-01-07 08:04:07,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19618269801139832
2023-01-07 08:04:07,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8517526388168335
2023-01-07 08:04:07,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,807 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07764224708080292
2023-01-07 08:04:07,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7328596115112305
2023-01-07 08:04:07,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,809 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.575488090515137
2023-01-07 08:04:07,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9738373756408691
2023-01-07 08:04:07,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8418397903442383
2023-01-07 08:04:07,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.115959167480469
2023-01-07 08:04:07,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4471911191940308
2023-01-07 08:04:07,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5397502779960632
2023-01-07 08:04:07,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,817 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.361623287200928
2023-01-07 08:04:07,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,818 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0190287828445435
2023-01-07 08:04:07,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.171478271484375
2023-01-07 08:04:07,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9560747146606445
2023-01-07 08:04:07,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,823 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7120144367218018
2023-01-07 08:04:07,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,824 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9063661098480225
2023-01-07 08:04:07,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,825 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.868616104125977
2023-01-07 08:04:07,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,826 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0029304325580596924
2023-01-07 08:04:07,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5862668752670288
2023-01-07 08:04:07,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,829 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.36929476261138916
2023-01-07 08:04:07,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.431785583496094
2023-01-07 08:04:07,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.480873107910156
2023-01-07 08:04:07,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.100781440734863
2023-01-07 08:04:07,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6598055362701416
2023-01-07 08:04:07,835 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,835 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.34066390991211
2023-01-07 08:04:07,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6713966131210327
2023-01-07 08:04:07,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,837 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.087501525878906
2023-01-07 08:04:07,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33018600940704346
2023-01-07 08:04:07,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.792459011077881
2023-01-07 08:04:07,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2992798089981079
2023-01-07 08:04:07,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,842 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21461749076843262
2023-01-07 08:04:07,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0639519691467285
2023-01-07 08:04:07,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,844 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.504922866821289
2023-01-07 08:04:07,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.00871467590332
2023-01-07 08:04:07,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.22264099121094
2023-01-07 08:04:07,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.878407955169678
2023-01-07 08:04:07,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.258327484130859
2023-01-07 08:04:07,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,851 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.33139181137085
2023-01-07 08:04:07,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.94144058227539
2023-01-07 08:04:07,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.803032159805298
2023-01-07 08:04:07,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.08085632324219
2023-01-07 08:04:07,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9080965518951416
2023-01-07 08:04:07,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,856 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.573137283325195
2023-01-07 08:04:07,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.724849224090576
2023-01-07 08:04:07,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.606462478637695
2023-01-07 08:04:07,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,860 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.629241943359375
2023-01-07 08:04:07,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.6052141189575195
2023-01-07 08:04:07,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.367195129394531
2023-01-07 08:04:07,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.800484657287598
2023-01-07 08:04:07,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,865 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.89451599121094
2023-01-07 08:04:07,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.1036148071289
2023-01-07 08:04:07,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.318782806396484
2023-01-07 08:04:07,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.01995849609375
2023-01-07 08:04:07,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.720401763916016
2023-01-07 08:04:07,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -126.53629302978516
2023-01-07 08:04:07,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.98127746582031
2023-01-07 08:04:07,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.601905822753906
2023-01-07 08:04:07,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,875 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.38715171813965
2023-01-07 08:04:07,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 96.66586303710938
2023-01-07 08:04:07,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.206392288208008
2023-01-07 08:04:07,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.544219970703125
2023-01-07 08:04:07,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.98277282714844
2023-01-07 08:04:07,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3492698669433594
2023-01-07 08:04:07,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.19490432739258
2023-01-07 08:04:07,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.1485366821289
2023-01-07 08:04:07,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.501541137695312
2023-01-07 08:04:07,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,887 > [DEBUG] 0 :: before allreduce fusion buffer :: -111.07688903808594
2023-01-07 08:04:07,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,888 > [DEBUG] 0 :: before allreduce fusion buffer :: -53.429569244384766
2023-01-07 08:04:07,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -161.86819458007812
2023-01-07 08:04:07,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,891 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.051851272583008
2023-01-07 08:04:07,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:04:07,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:04:07,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -36.02198791503906
