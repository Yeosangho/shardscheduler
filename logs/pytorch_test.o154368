==========================================
SLURM_JOB_ID = 154368
SLURM_NODELIST = gpu10
==========================================
NODELIST=gpu10
MASTER_ADDR=gpu10
world_size : 4
rank : 3
before init dataset  0.0
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
after init dataset  0.0
before init model  0.0
after init model  487.46875
{'FSDP': 0, 'DP': 125, 'SDP': 0}
{'FSDP': 0, 'DP': 124, 'SDP': 0}
{'FSDP': 0, 'DP': 123, 'SDP': 0}
{'FSDP': 0, 'DP': 122, 'SDP': 0}
{'FSDP': 0, 'DP': 121, 'SDP': 0}
{'FSDP': 0, 'DP': 120, 'SDP': 0}
{'FSDP': 0, 'DP': 119, 'SDP': 0}
{'FSDP': 0, 'DP': 118, 'SDP': 0}
{'FSDP': 0, 'DP': 117, 'SDP': 0}
{'FSDP': 0, 'DP': 116, 'SDP': 0}
{'FSDP': 0, 'DP': 115, 'SDP': 0}
{'FSDP': 0, 'DP': 114, 'SDP': 0}
{'FSDP': 0, 'DP': 113, 'SDP': 0}
{'FSDP': 0, 'DP': 112, 'SDP': 0}
{'FSDP': 0, 'DP': 111, 'SDP': 0}
{'FSDP': 0, 'DP': 110, 'SDP': 0}
{'FSDP': 0, 'DP': 109, 'SDP': 0}
{'FSDP': 0, 'DP': 108, 'SDP': 0}
{'FSDP': 0, 'DP': 107, 'SDP': 0}
{'FSDP': 0, 'DP': 106, 'SDP': 0}
{'FSDP': 0, 'DP': 105, 'SDP': 0}
{'FSDP': 0, 'DP': 104, 'SDP': 0}
{'FSDP': 0, 'DP': 103, 'SDP': 0}
{'FSDP': 0, 'DP': 102, 'SDP': 0}
{'FSDP': 0, 'DP': 101, 'SDP': 0}
{'FSDP': 0, 'DP': 100, 'SDP': 0}
{'FSDP': 0, 'DP': 99, 'SDP': 0}
{'FSDP': 0, 'DP': 98, 'SDP': 0}
{'FSDP': 0, 'DP': 97, 'SDP': 0}
{'FSDP': 0, 'DP': 96, 'SDP': 0}
{'FSDP': 0, 'DP': 95, 'SDP': 0}
{'FSDP': 0, 'DP': 94, 'SDP': 0}
{'FSDP': 0, 'DP': 93, 'SDP': 0}
{'FSDP': 0, 'DP': 92, 'SDP': 0}
{'FSDP': 0, 'DP': 91, 'SDP': 0}
{'FSDP': 0, 'DP': 90, 'SDP': 0}
{'FSDP': 0, 'DP': 89, 'SDP': 0}
{'FSDP': 0, 'DP': 88, 'SDP': 0}
{'FSDP': 0, 'DP': 87, 'SDP': 0}
{'FSDP': 0, 'DP': 86, 'SDP': 0}
{'FSDP': 0, 'DP': 85, 'SDP': 0}
{'FSDP': 0, 'DP': 84, 'SDP': 0}
{'FSDP': 0, 'DP': 83, 'SDP': 0}
{'FSDP': 0, 'DP': 82, 'SDP': 0}
{'FSDP': 0, 'DP': 81, 'SDP': 0}
{'FSDP': 0, 'DP': 80, 'SDP': 0}
{'FSDP': 0, 'DP': 79, 'SDP': 0}
{'FSDP': 0, 'DP': 78, 'SDP': 0}
{'FSDP': 0, 'DP': 77, 'SDP': 0}
{'FSDP': 0, 'DP': 76, 'SDP': 0}
{'FSDP': 0, 'DP': 75, 'SDP': 0}
{'FSDP': 0, 'DP': 74, 'SDP': 0}
{'FSDP': 0, 'DP': 73, 'SDP': 0}
{'FSDP': 0, 'DP': 72, 'SDP': 0}
{'FSDP': 0, 'DP': 71, 'SDP': 0}
{'FSDP': 0, 'DP': 70, 'SDP': 0}
{'FSDP': 0, 'DP': 69, 'SDP': 0}
{'FSDP': 0, 'DP': 68, 'SDP': 0}
{'FSDP': 0, 'DP': 67, 'SDP': 0}
{'FSDP': 0, 'DP': 66, 'SDP': 0}
{'FSDP': 0, 'DP': 65, 'SDP': 0}
{'FSDP': 0, 'DP': 64, 'SDP': 0}
{'FSDP': 0, 'DP': 63, 'SDP': 0}
{'FSDP': 0, 'DP': 62, 'SDP': 0}
{'FSDP': 0, 'DP': 61, 'SDP': 0}
{'FSDP': 0, 'DP': 60, 'SDP': 0}
{'FSDP': 0, 'DP': 59, 'SDP': 0}
{'FSDP': 0, 'DP': 58, 'SDP': 0}
{'FSDP': 0, 'DP': 57, 'SDP': 0}
{'FSDP': 0, 'DP': 56, 'SDP': 0}
{'FSDP': 0, 'DP': 55, 'SDP': 0}
{'FSDP': 0, 'DP': 54, 'SDP': 0}
{'FSDP': 0, 'DP': 53, 'SDP': 0}
{'FSDP': 0, 'DP': 52, 'SDP': 0}
{'FSDP': 0, 'DP': 51, 'SDP': 0}
{'FSDP': 0, 'DP': 50, 'SDP': 0}
{'FSDP': 0, 'DP': 49, 'SDP': 0}
{'FSDP': 0, 'DP': 48, 'SDP': 0}
{'FSDP': 0, 'DP': 47, 'SDP': 0}
{'FSDP': 0, 'DP': 46, 'SDP': 0}
{'FSDP': 0, 'DP': 45, 'SDP': 0}
{'FSDP': 0, 'DP': 44, 'SDP': 0}
{'FSDP': 0, 'DP': 43, 'SDP': 0}
{'FSDP': 0, 'DP': 42, 'SDP': 0}
{'FSDP': 0, 'DP': 41, 'SDP': 0}
{'FSDP': 0, 'DP': 40, 'SDP': 0}
{'FSDP': 0, 'DP': 39, 'SDP': 0}
{'FSDP': 0, 'DP': 38, 'SDP': 0}
{'FSDP': 0, 'DP': 37, 'SDP': 0}
{'FSDP': 0, 'DP': 36, 'SDP': 0}
{'FSDP': 0, 'DP': 35, 'SDP': 0}
{'FSDP': 0, 'DP': 34, 'SDP': 0}
{'FSDP': 0, 'DP': 33, 'SDP': 0}
{'FSDP': 0, 'DP': 32, 'SDP': 0}
{'FSDP': 0, 'DP': 31, 'SDP': 0}
{'FSDP': 0, 'DP': 30, 'SDP': 0}
{'FSDP': 0, 'DP': 29, 'SDP': 0}
{'FSDP': 0, 'DP': 28, 'SDP': 0}
{'FSDP': 0, 'DP': 27, 'SDP': 0}
{'FSDP': 0, 'DP': 26, 'SDP': 0}
{'FSDP': 0, 'DP': 25, 'SDP': 0}
{'FSDP': 0, 'DP': 24, 'SDP': 0}
{'FSDP': 0, 'DP': 23, 'SDP': 0}
{'FSDP': 0, 'DP': 22, 'SDP': 0}
{'FSDP': 0, 'DP': 21, 'SDP': 0}
{'FSDP': 0, 'DP': 20, 'SDP': 0}
{'FSDP': 0, 'DP': 19, 'SDP': 0}
{'FSDP': 0, 'DP': 18, 'SDP': 0}
{'FSDP': 0, 'DP': 17, 'SDP': 0}
{'FSDP': 0, 'DP': 16, 'SDP': 0}
{'FSDP': 0, 'DP': 15, 'SDP': 0}
{'FSDP': 0, 'DP': 14, 'SDP': 0}
{'FSDP': 0, 'DP': 13, 'SDP': 0}
{'FSDP': 0, 'DP': 12, 'SDP': 0}
{'FSDP': 0, 'DP': 11, 'SDP': 0}
{'FSDP': 0, 'DP': 10, 'SDP': 0}
{'FSDP': 0, 'DP': 9, 'SDP': 0}
{'FSDP': 0, 'DP': 8, 'SDP': 0}
{'FSDP': 0, 'DP': 7, 'SDP': 0}
{'FSDP': 0, 'DP': 6, 'SDP': 0}
{'FSDP': 0, 'DP': 5, 'SDP': 0}
{'FSDP': 0, 'DP': 4, 'SDP': 0}
{'FSDP': 0, 'DP': 3, 'SDP': 0}
{'FSDP': 0, 'DP': 2, 'SDP': 0}
{'FSDP': 0, 'DP': 1, 'SDP': 0}
_dp_wrapped_module._fpw_module.transformer.wte._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1101, -0.0393,  0.0331,  ..., -0.0165,  0.0197,  0.0345],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.wpe._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0188, -0.1974,  0.0040,  ..., -0.0047, -0.0024, -0.0058],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2232,  0.1820,  0.1534,  ..., -0.0111, -0.0142, -0.0157],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.4738, -0.2614, -0.0978,  ...,  0.0126, -0.0499,  0.0032],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.3127, -0.1874,  0.0980,  ...,  0.1222, -0.0132,  0.0283],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1310,  0.2093,  0.2066,  ..., -0.0179,  0.0063, -0.0016],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0942,  0.0982, -0.0321,  ..., -0.2490, -0.0768,  0.0143],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1066,  0.1528,  0.0331,  ...,  0.0100,  0.0968,  0.0296],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2086,  0.2185,  0.2183,  ..., -0.0075, -0.0014, -0.0038],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2906,  0.3057,  0.0302,  ...,  0.0404,  0.0494, -0.0038],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0971, -0.0016,  0.1122,  ...,  0.0633,  0.0618,  0.1215],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2241,  0.2531,  0.2446,  ...,  0.0008, -0.0142,  0.0174],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0078, -0.2255, -0.0207,  ..., -0.1943, -0.1426, -0.0668],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1112, -0.0263, -0.0332,  ..., -0.0128,  0.0883,  0.0801],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1871,  0.2227,  0.2366,  ...,  0.0054, -0.0074,  0.0045],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2659,  0.0279,  0.0728,  ...,  0.0015, -0.0427,  0.0059],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0131, -0.0666, -0.0697,  ..., -0.0914,  0.0728, -0.0527],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2646, 0.3057, 0.2836,  ..., 0.0008, 0.0253, 0.0337], device='cuda:3',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0137,  0.3680, -0.1068,  ..., -0.1723, -0.1186, -0.1458],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0240, -0.2091, -0.0291,  ..., -0.0054,  0.0097,  0.0094],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2725, 0.3078, 0.3174,  ..., 0.0061, 0.0003, 0.0134], device='cuda:3',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0180, -0.1333,  0.1339,  ...,  0.0205, -0.0017, -0.0044],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0660, -0.0059, -0.0359,  ..., -0.0921, -0.0232, -0.0359],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2822,  0.3131,  0.3034,  ..., -0.0040,  0.0011,  0.0230],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0354, -0.0083, -0.0900,  ..., -0.0739,  0.0215, -0.1412],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0766,  0.0050, -0.0819,  ...,  0.1011, -0.0088, -0.0448],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2971,  0.3330,  0.3428,  ...,  0.0111, -0.0019,  0.0193],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0542,  0.1037, -0.1203,  ..., -0.0185, -0.0097,  0.0927],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1220, -0.0009,  0.0201,  ..., -0.0997, -0.1438,  0.0421],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2607,  0.2783,  ..., -0.0063, -0.0006,  0.0159],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0004, -0.1200, -0.0123,  ..., -0.1039, -0.1219, -0.0806],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0166,  0.0334, -0.0259,  ...,  0.1178, -0.0939,  0.0416],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3461, 0.3910, 0.4093,  ..., 0.0236, 0.0016, 0.0233], device='cuda:3',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1861, -0.1976,  0.0349,  ...,  0.0089, -0.0007,  0.0082],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0318,  0.1413, -0.0693,  ..., -0.1165, -0.1083, -0.0404],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2686,  0.2705,  0.2811,  ..., -0.0061, -0.0071,  0.0144],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0246,  0.1288,  0.0653,  ...,  0.0421, -0.0274, -0.1130],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0648,  0.1070,  0.1007,  ...,  0.1291, -0.0245,  0.0073],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3310, 0.3408, 0.3612,  ..., 0.0136, 0.0081, 0.0195], device='cuda:3',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1109, -0.0750,  0.0721,  ..., -0.0441,  0.0544,  0.0041],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463, -0.1530,  0.0253,  ..., -0.0226, -0.1069, -0.0019],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2588,  0.2482,  0.2724,  ..., -0.0119, -0.0073, -0.0036],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1218,  0.0713,  0.0619,  ...,  0.0448, -0.1546, -0.0703],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0970, -0.0954,  0.0650,  ...,  0.0910,  0.0212,  0.0274],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3551, 0.3564, 0.3759,  ..., 0.0197, 0.0094, 0.0280], device='cuda:3',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0951,  0.2815,  0.0759,  ...,  0.0123, -0.0721,  0.0015],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0780, -0.0183,  0.1131,  ..., -0.0387, -0.2163, -0.1506],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2529,  0.2646,  ..., -0.0250, -0.0093,  0.0164],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463,  0.0148,  0.0833,  ..., -0.0783, -0.1670, -0.0382],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0227, -0.0343,  0.1083,  ...,  0.0295, -0.0121, -0.0167],
       device='cuda:3', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.8.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3518, 0.3381, 0.3564,  ..., 0.0100, 0.0037, 0.0385], device='cuda:3',
       requires_grad=True)
world_size : 4
rank : 0
before init dataset  0.0
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
after init dataset  0.0
before init model  0.0
after init model  487.46875
{'FSDP': 0, 'DP': 125, 'SDP': 0}
{'FSDP': 0, 'DP': 124, 'SDP': 0}
{'FSDP': 0, 'DP': 123, 'SDP': 0}
{'FSDP': 0, 'DP': 122, 'SDP': 0}
{'FSDP': 0, 'DP': 121, 'SDP': 0}
{'FSDP': 0, 'DP': 120, 'SDP': 0}
{'FSDP': 0, 'DP': 119, 'SDP': 0}
{'FSDP': 0, 'DP': 118, 'SDP': 0}
{'FSDP': 0, 'DP': 117, 'SDP': 0}
{'FSDP': 0, 'DP': 116, 'SDP': 0}
{'FSDP': 0, 'DP': 115, 'SDP': 0}
{'FSDP': 0, 'DP': 114, 'SDP': 0}
{'FSDP': 0, 'DP': 113, 'SDP': 0}
{'FSDP': 0, 'DP': 112, 'SDP': 0}
{'FSDP': 0, 'DP': 111, 'SDP': 0}
{'FSDP': 0, 'DP': 110, 'SDP': 0}
{'FSDP': 0, 'DP': 109, 'SDP': 0}
{'FSDP': 0, 'DP': 108, 'SDP': 0}
{'FSDP': 0, 'DP': 107, 'SDP': 0}
{'FSDP': 0, 'DP': 106, 'SDP': 0}
{'FSDP': 0, 'DP': 105, 'SDP': 0}
{'FSDP': 0, 'DP': 104, 'SDP': 0}
{'FSDP': 0, 'DP': 103, 'SDP': 0}
{'FSDP': 0, 'DP': 102, 'SDP': 0}
{'FSDP': 0, 'DP': 101, 'SDP': 0}
{'FSDP': 0, 'DP': 100, 'SDP': 0}
{'FSDP': 0, 'DP': 99, 'SDP': 0}
{'FSDP': 0, 'DP': 98, 'SDP': 0}
{'FSDP': 0, 'DP': 97, 'SDP': 0}
{'FSDP': 0, 'DP': 96, 'SDP': 0}
{'FSDP': 0, 'DP': 95, 'SDP': 0}
{'FSDP': 0, 'DP': 94, 'SDP': 0}
{'FSDP': 0, 'DP': 93, 'SDP': 0}
{'FSDP': 0, 'DP': 92, 'SDP': 0}
{'FSDP': 0, 'DP': 91, 'SDP': 0}
{'FSDP': 0, 'DP': 90, 'SDP': 0}
{'FSDP': 0, 'DP': 89, 'SDP': 0}
{'FSDP': 0, 'DP': 88, 'SDP': 0}
{'FSDP': 0, 'DP': 87, 'SDP': 0}
{'FSDP': 0, 'DP': 86, 'SDP': 0}
{'FSDP': 0, 'DP': 85, 'SDP': 0}
{'FSDP': 0, 'DP': 84, 'SDP': 0}
{'FSDP': 0, 'DP': 83, 'SDP': 0}
{'FSDP': 0, 'DP': 82, 'SDP': 0}
{'FSDP': 0, 'DP': 81, 'SDP': 0}
{'FSDP': 0, 'DP': 80, 'SDP': 0}
{'FSDP': 0, 'DP': 79, 'SDP': 0}
{'FSDP': 0, 'DP': 78, 'SDP': 0}
{'FSDP': 0, 'DP': 77, 'SDP': 0}
{'FSDP': 0, 'DP': 76, 'SDP': 0}
{'FSDP': 0, 'DP': 75, 'SDP': 0}
{'FSDP': 0, 'DP': 74, 'SDP': 0}
{'FSDP': 0, 'DP': 73, 'SDP': 0}
{'FSDP': 0, 'DP': 72, 'SDP': 0}
{'FSDP': 0, 'DP': 71, 'SDP': 0}
{'FSDP': 0, 'DP': 70, 'SDP': 0}
{'FSDP': 0, 'DP': 69, 'SDP': 0}
{'FSDP': 0, 'DP': 68, 'SDP': 0}
{'FSDP': 0, 'DP': 67, 'SDP': 0}
{'FSDP': 0, 'DP': 66, 'SDP': 0}
{'FSDP': 0, 'DP': 65, 'SDP': 0}
{'FSDP': 0, 'DP': 64, 'SDP': 0}
{'FSDP': 0, 'DP': 63, 'SDP': 0}
{'FSDP': 0, 'DP': 62, 'SDP': 0}
{'FSDP': 0, 'DP': 61, 'SDP': 0}
{'FSDP': 0, 'DP': 60, 'SDP': 0}
{'FSDP': 0, 'DP': 59, 'SDP': 0}
{'FSDP': 0, 'DP': 58, 'SDP': 0}
{'FSDP': 0, 'DP': 57, 'SDP': 0}
{'FSDP': 0, 'DP': 56, 'SDP': 0}
{'FSDP': 0, 'DP': 55, 'SDP': 0}
{'FSDP': 0, 'DP': 54, 'SDP': 0}
{'FSDP': 0, 'DP': 53, 'SDP': 0}
{'FSDP': 0, 'DP': 52, 'SDP': 0}
{'FSDP': 0, 'DP': 51, 'SDP': 0}
{'FSDP': 0, 'DP': 50, 'SDP': 0}
{'FSDP': 0, 'DP': 49, 'SDP': 0}
{'FSDP': 0, 'DP': 48, 'SDP': 0}
{'FSDP': 0, 'DP': 47, 'SDP': 0}
{'FSDP': 0, 'DP': 46, 'SDP': 0}
{'FSDP': 0, 'DP': 45, 'SDP': 0}
{'FSDP': 0, 'DP': 44, 'SDP': 0}
{'FSDP': 0, 'DP': 43, 'SDP': 0}
{'FSDP': 0, 'DP': 42, 'SDP': 0}
{'FSDP': 0, 'DP': 41, 'SDP': 0}
{'FSDP': 0, 'DP': 40, 'SDP': 0}
{'FSDP': 0, 'DP': 39, 'SDP': 0}
{'FSDP': 0, 'DP': 38, 'SDP': 0}
{'FSDP': 0, 'DP': 37, 'SDP': 0}
{'FSDP': 0, 'DP': 36, 'SDP': 0}
{'FSDP': 0, 'DP': 35, 'SDP': 0}
{'FSDP': 0, 'DP': 34, 'SDP': 0}
{'FSDP': 0, 'DP': 33, 'SDP': 0}
{'FSDP': 0, 'DP': 32, 'SDP': 0}
{'FSDP': 0, 'DP': 31, 'SDP': 0}
{'FSDP': 0, 'DP': 30, 'SDP': 0}
{'FSDP': 0, 'DP': 29, 'SDP': 0}
{'FSDP': 0, 'DP': 28, 'SDP': 0}
{'FSDP': 0, 'DP': 27, 'SDP': 0}
{'FSDP': 0, 'DP': 26, 'SDP': 0}
{'FSDP': 0, 'DP': 25, 'SDP': 0}
{'FSDP': 0, 'DP': 24, 'SDP': 0}
{'FSDP': 0, 'DP': 23, 'SDP': 0}
{'FSDP': 0, 'DP': 22, 'SDP': 0}
{'FSDP': 0, 'DP': 21, 'SDP': 0}
{'FSDP': 0, 'DP': 20, 'SDP': 0}
{'FSDP': 0, 'DP': 19, 'SDP': 0}
{'FSDP': 0, 'DP': 18, 'SDP': 0}
{'FSDP': 0, 'DP': 17, 'SDP': 0}
{'FSDP': 0, 'DP': 16, 'SDP': 0}
{'FSDP': 0, 'DP': 15, 'SDP': 0}
{'FSDP': 0, 'DP': 14, 'SDP': 0}
{'FSDP': 0, 'DP': 13, 'SDP': 0}
{'FSDP': 0, 'DP': 12, 'SDP': 0}
{'FSDP': 0, 'DP': 11, 'SDP': 0}
{'FSDP': 0, 'DP': 10, 'SDP': 0}
{'FSDP': 0, 'DP': 9, 'SDP': 0}
{'FSDP': 0, 'DP': 8, 'SDP': 0}
{'FSDP': 0, 'DP': 7, 'SDP': 0}
{'FSDP': 0, 'DP': 6, 'SDP': 0}
{'FSDP': 0, 'DP': 5, 'SDP': 0}
{'FSDP': 0, 'DP': 4, 'SDP': 0}
{'FSDP': 0, 'DP': 3, 'SDP': 0}
{'FSDP': 0, 'DP': 2, 'SDP': 0}
{'FSDP': 0, 'DP': 1, 'SDP': 0}
_dp_wrapped_module._fpw_module.transformer.wte._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1101, -0.0393,  0.0331,  ...,  0.0163, -0.0116, -0.0297],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.wpe._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0188, -0.1974,  0.0040,  ..., -0.0047, -0.0024, -0.0058],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2232,  0.1820,  0.1534,  ..., -0.0111, -0.0142, -0.0157],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.4738, -0.2614, -0.0978,  ...,  0.0126, -0.0499,  0.0032],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.3127, -0.1874,  0.0980,  ...,  0.1222, -0.0132,  0.0283],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1310,  0.2093,  0.2066,  ..., -0.0179,  0.0063, -0.0016],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0942,  0.0982, -0.0321,  ..., -0.2490, -0.0768,  0.0143],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1066,  0.1528,  0.0331,  ...,  0.0100,  0.0968,  0.0296],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2086,  0.2185,  0.2183,  ..., -0.0075, -0.0014, -0.0038],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2906,  0.3057,  0.0302,  ...,  0.0404,  0.0494, -0.0038],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0971, -0.0016,  0.1122,  ...,  0.0633,  0.0618,  0.1215],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2241,  0.2531,  0.2446,  ...,  0.0008, -0.0142,  0.0174],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0078, -0.2255, -0.0207,  ..., -0.1943, -0.1426, -0.0668],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1112, -0.0263, -0.0332,  ..., -0.0128,  0.0883,  0.0801],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1871,  0.2227,  0.2366,  ...,  0.0054, -0.0074,  0.0045],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2659,  0.0279,  0.0728,  ...,  0.0015, -0.0427,  0.0059],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0131, -0.0666, -0.0697,  ..., -0.0914,  0.0728, -0.0527],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2646, 0.3057, 0.2836,  ..., 0.0008, 0.0253, 0.0337], device='cuda:0',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0137,  0.3680, -0.1068,  ..., -0.1723, -0.1186, -0.1458],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0240, -0.2091, -0.0291,  ..., -0.0054,  0.0097,  0.0094],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2725, 0.3078, 0.3174,  ..., 0.0061, 0.0003, 0.0134], device='cuda:0',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0180, -0.1333,  0.1339,  ...,  0.0205, -0.0017, -0.0044],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0660, -0.0059, -0.0359,  ..., -0.0921, -0.0232, -0.0359],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2822,  0.3131,  0.3034,  ..., -0.0040,  0.0011,  0.0230],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0354, -0.0083, -0.0900,  ..., -0.0739,  0.0215, -0.1412],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0766,  0.0050, -0.0819,  ...,  0.1011, -0.0088, -0.0448],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2971,  0.3330,  0.3428,  ...,  0.0111, -0.0019,  0.0193],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0542,  0.1037, -0.1203,  ..., -0.0185, -0.0097,  0.0927],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1220, -0.0009,  0.0201,  ..., -0.0997, -0.1438,  0.0421],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2607,  0.2783,  ..., -0.0063, -0.0006,  0.0159],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0004, -0.1200, -0.0123,  ..., -0.1039, -0.1219, -0.0806],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0166,  0.0334, -0.0259,  ...,  0.1178, -0.0939,  0.0416],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3461, 0.3910, 0.4093,  ..., 0.0236, 0.0016, 0.0233], device='cuda:0',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1861, -0.1976,  0.0349,  ...,  0.0089, -0.0007,  0.0082],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0318,  0.1413, -0.0693,  ..., -0.1165, -0.1083, -0.0404],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2686,  0.2705,  0.2811,  ..., -0.0061, -0.0071,  0.0144],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0246,  0.1288,  0.0653,  ...,  0.0421, -0.0274, -0.1130],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0648,  0.1070,  0.1007,  ...,  0.1291, -0.0245,  0.0073],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3310, 0.3408, 0.3612,  ..., 0.0136, 0.0081, 0.0195], device='cuda:0',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1109, -0.0750,  0.0721,  ..., -0.0441,  0.0544,  0.0041],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463, -0.1530,  0.0253,  ..., -0.0226, -0.1069, -0.0019],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2588,  0.2482,  0.2724,  ..., -0.0119, -0.0073, -0.0036],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1218,  0.0713,  0.0619,  ...,  0.0448, -0.1546, -0.0703],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0970, -0.0954,  0.0650,  ...,  0.0910,  0.0212,  0.0274],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3551, 0.3564, 0.3759,  ..., 0.0197, 0.0094, 0.0280], device='cuda:0',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0951,  0.2815,  0.0759,  ...,  0.0123, -0.0721,  0.0015],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0780, -0.0183,  0.1131,  ..., -0.0387, -0.2163, -0.1506],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2529,  0.2646,  ..., -0.0250, -0.0093,  0.0164],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463,  0.0148,  0.0833,  ..., -0.0783, -0.1670, -0.0382],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0227, -0.0343,  0.1083,  ...,  0.0295, -0.0121, -0.0167],
       device='cuda:0', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.8.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3518, 0.3381, 0.3564,  ..., 0.0100, 0.0037, 0.0385], device='cuda:0',
       requires_grad=True)
world_size : 4
rank : 2
before init dataset  0.0
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
after init dataset  0.0
before init model  0.0
after init model  487.46875
{'FSDP': 0, 'DP': 125, 'SDP': 0}
{'FSDP': 0, 'DP': 124, 'SDP': 0}
{'FSDP': 0, 'DP': 123, 'SDP': 0}
{'FSDP': 0, 'DP': 122, 'SDP': 0}
{'FSDP': 0, 'DP': 121, 'SDP': 0}
{'FSDP': 0, 'DP': 120, 'SDP': 0}
{'FSDP': 0, 'DP': 119, 'SDP': 0}
{'FSDP': 0, 'DP': 118, 'SDP': 0}
{'FSDP': 0, 'DP': 117, 'SDP': 0}
{'FSDP': 0, 'DP': 116, 'SDP': 0}
{'FSDP': 0, 'DP': 115, 'SDP': 0}
{'FSDP': 0, 'DP': 114, 'SDP': 0}
{'FSDP': 0, 'DP': 113, 'SDP': 0}
{'FSDP': 0, 'DP': 112, 'SDP': 0}
{'FSDP': 0, 'DP': 111, 'SDP': 0}
{'FSDP': 0, 'DP': 110, 'SDP': 0}
{'FSDP': 0, 'DP': 109, 'SDP': 0}
{'FSDP': 0, 'DP': 108, 'SDP': 0}
{'FSDP': 0, 'DP': 107, 'SDP': 0}
{'FSDP': 0, 'DP': 106, 'SDP': 0}
{'FSDP': 0, 'DP': 105, 'SDP': 0}
{'FSDP': 0, 'DP': 104, 'SDP': 0}
{'FSDP': 0, 'DP': 103, 'SDP': 0}
{'FSDP': 0, 'DP': 102, 'SDP': 0}
{'FSDP': 0, 'DP': 101, 'SDP': 0}
{'FSDP': 0, 'DP': 100, 'SDP': 0}
{'FSDP': 0, 'DP': 99, 'SDP': 0}
{'FSDP': 0, 'DP': 98, 'SDP': 0}
{'FSDP': 0, 'DP': 97, 'SDP': 0}
{'FSDP': 0, 'DP': 96, 'SDP': 0}
{'FSDP': 0, 'DP': 95, 'SDP': 0}
{'FSDP': 0, 'DP': 94, 'SDP': 0}
{'FSDP': 0, 'DP': 93, 'SDP': 0}
{'FSDP': 0, 'DP': 92, 'SDP': 0}
{'FSDP': 0, 'DP': 91, 'SDP': 0}
{'FSDP': 0, 'DP': 90, 'SDP': 0}
{'FSDP': 0, 'DP': 89, 'SDP': 0}
{'FSDP': 0, 'DP': 88, 'SDP': 0}
{'FSDP': 0, 'DP': 87, 'SDP': 0}
{'FSDP': 0, 'DP': 86, 'SDP': 0}
{'FSDP': 0, 'DP': 85, 'SDP': 0}
{'FSDP': 0, 'DP': 84, 'SDP': 0}
{'FSDP': 0, 'DP': 83, 'SDP': 0}
{'FSDP': 0, 'DP': 82, 'SDP': 0}
{'FSDP': 0, 'DP': 81, 'SDP': 0}
{'FSDP': 0, 'DP': 80, 'SDP': 0}
{'FSDP': 0, 'DP': 79, 'SDP': 0}
{'FSDP': 0, 'DP': 78, 'SDP': 0}
{'FSDP': 0, 'DP': 77, 'SDP': 0}
{'FSDP': 0, 'DP': 76, 'SDP': 0}
{'FSDP': 0, 'DP': 75, 'SDP': 0}
{'FSDP': 0, 'DP': 74, 'SDP': 0}
{'FSDP': 0, 'DP': 73, 'SDP': 0}
{'FSDP': 0, 'DP': 72, 'SDP': 0}
{'FSDP': 0, 'DP': 71, 'SDP': 0}
{'FSDP': 0, 'DP': 70, 'SDP': 0}
{'FSDP': 0, 'DP': 69, 'SDP': 0}
{'FSDP': 0, 'DP': 68, 'SDP': 0}
{'FSDP': 0, 'DP': 67, 'SDP': 0}
{'FSDP': 0, 'DP': 66, 'SDP': 0}
{'FSDP': 0, 'DP': 65, 'SDP': 0}
{'FSDP': 0, 'DP': 64, 'SDP': 0}
{'FSDP': 0, 'DP': 63, 'SDP': 0}
{'FSDP': 0, 'DP': 62, 'SDP': 0}
{'FSDP': 0, 'DP': 61, 'SDP': 0}
{'FSDP': 0, 'DP': 60, 'SDP': 0}
{'FSDP': 0, 'DP': 59, 'SDP': 0}
{'FSDP': 0, 'DP': 58, 'SDP': 0}
{'FSDP': 0, 'DP': 57, 'SDP': 0}
{'FSDP': 0, 'DP': 56, 'SDP': 0}
{'FSDP': 0, 'DP': 55, 'SDP': 0}
{'FSDP': 0, 'DP': 54, 'SDP': 0}
{'FSDP': 0, 'DP': 53, 'SDP': 0}
{'FSDP': 0, 'DP': 52, 'SDP': 0}
{'FSDP': 0, 'DP': 51, 'SDP': 0}
{'FSDP': 0, 'DP': 50, 'SDP': 0}
{'FSDP': 0, 'DP': 49, 'SDP': 0}
{'FSDP': 0, 'DP': 48, 'SDP': 0}
{'FSDP': 0, 'DP': 47, 'SDP': 0}
{'FSDP': 0, 'DP': 46, 'SDP': 0}
{'FSDP': 0, 'DP': 45, 'SDP': 0}
{'FSDP': 0, 'DP': 44, 'SDP': 0}
{'FSDP': 0, 'DP': 43, 'SDP': 0}
{'FSDP': 0, 'DP': 42, 'SDP': 0}
{'FSDP': 0, 'DP': 41, 'SDP': 0}
{'FSDP': 0, 'DP': 40, 'SDP': 0}
{'FSDP': 0, 'DP': 39, 'SDP': 0}
{'FSDP': 0, 'DP': 38, 'SDP': 0}
{'FSDP': 0, 'DP': 37, 'SDP': 0}
{'FSDP': 0, 'DP': 36, 'SDP': 0}
{'FSDP': 0, 'DP': 35, 'SDP': 0}
{'FSDP': 0, 'DP': 34, 'SDP': 0}
{'FSDP': 0, 'DP': 33, 'SDP': 0}
{'FSDP': 0, 'DP': 32, 'SDP': 0}
{'FSDP': 0, 'DP': 31, 'SDP': 0}
{'FSDP': 0, 'DP': 30, 'SDP': 0}
{'FSDP': 0, 'DP': 29, 'SDP': 0}
{'FSDP': 0, 'DP': 28, 'SDP': 0}
{'FSDP': 0, 'DP': 27, 'SDP': 0}
{'FSDP': 0, 'DP': 26, 'SDP': 0}
{'FSDP': 0, 'DP': 25, 'SDP': 0}
{'FSDP': 0, 'DP': 24, 'SDP': 0}
{'FSDP': 0, 'DP': 23, 'SDP': 0}
{'FSDP': 0, 'DP': 22, 'SDP': 0}
{'FSDP': 0, 'DP': 21, 'SDP': 0}
{'FSDP': 0, 'DP': 20, 'SDP': 0}
{'FSDP': 0, 'DP': 19, 'SDP': 0}
{'FSDP': 0, 'DP': 18, 'SDP': 0}
{'FSDP': 0, 'DP': 17, 'SDP': 0}
{'FSDP': 0, 'DP': 16, 'SDP': 0}
{'FSDP': 0, 'DP': 15, 'SDP': 0}
{'FSDP': 0, 'DP': 14, 'SDP': 0}
{'FSDP': 0, 'DP': 13, 'SDP': 0}
{'FSDP': 0, 'DP': 12, 'SDP': 0}
{'FSDP': 0, 'DP': 11, 'SDP': 0}
{'FSDP': 0, 'DP': 10, 'SDP': 0}
{'FSDP': 0, 'DP': 9, 'SDP': 0}
{'FSDP': 0, 'DP': 8, 'SDP': 0}
{'FSDP': 0, 'DP': 7, 'SDP': 0}
{'FSDP': 0, 'DP': 6, 'SDP': 0}
{'FSDP': 0, 'DP': 5, 'SDP': 0}
{'FSDP': 0, 'DP': 4, 'SDP': 0}
{'FSDP': 0, 'DP': 3, 'SDP': 0}
{'FSDP': 0, 'DP': 2, 'SDP': 0}
{'FSDP': 0, 'DP': 1, 'SDP': 0}
_dp_wrapped_module._fpw_module.transformer.wte._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1101, -0.0393,  0.0331,  ..., -0.0040,  0.0114, -0.0030],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.wpe._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0188, -0.1974,  0.0040,  ..., -0.0047, -0.0024, -0.0058],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2232,  0.1820,  0.1534,  ..., -0.0111, -0.0142, -0.0157],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.4738, -0.2614, -0.0978,  ...,  0.0126, -0.0499,  0.0032],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.3127, -0.1874,  0.0980,  ...,  0.1222, -0.0132,  0.0283],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1310,  0.2093,  0.2066,  ..., -0.0179,  0.0063, -0.0016],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0942,  0.0982, -0.0321,  ..., -0.2490, -0.0768,  0.0143],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1066,  0.1528,  0.0331,  ...,  0.0100,  0.0968,  0.0296],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2086,  0.2185,  0.2183,  ..., -0.0075, -0.0014, -0.0038],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2906,  0.3057,  0.0302,  ...,  0.0404,  0.0494, -0.0038],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0971, -0.0016,  0.1122,  ...,  0.0633,  0.0618,  0.1215],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2241,  0.2531,  0.2446,  ...,  0.0008, -0.0142,  0.0174],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0078, -0.2255, -0.0207,  ..., -0.1943, -0.1426, -0.0668],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1112, -0.0263, -0.0332,  ..., -0.0128,  0.0883,  0.0801],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1871,  0.2227,  0.2366,  ...,  0.0054, -0.0074,  0.0045],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2659,  0.0279,  0.0728,  ...,  0.0015, -0.0427,  0.0059],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0131, -0.0666, -0.0697,  ..., -0.0914,  0.0728, -0.0527],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2646, 0.3057, 0.2836,  ..., 0.0008, 0.0253, 0.0337], device='cuda:2',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0137,  0.3680, -0.1068,  ..., -0.1723, -0.1186, -0.1458],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0240, -0.2091, -0.0291,  ..., -0.0054,  0.0097,  0.0094],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2725, 0.3078, 0.3174,  ..., 0.0061, 0.0003, 0.0134], device='cuda:2',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0180, -0.1333,  0.1339,  ...,  0.0205, -0.0017, -0.0044],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0660, -0.0059, -0.0359,  ..., -0.0921, -0.0232, -0.0359],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2822,  0.3131,  0.3034,  ..., -0.0040,  0.0011,  0.0230],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0354, -0.0083, -0.0900,  ..., -0.0739,  0.0215, -0.1412],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0766,  0.0050, -0.0819,  ...,  0.1011, -0.0088, -0.0448],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2971,  0.3330,  0.3428,  ...,  0.0111, -0.0019,  0.0193],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0542,  0.1037, -0.1203,  ..., -0.0185, -0.0097,  0.0927],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1220, -0.0009,  0.0201,  ..., -0.0997, -0.1438,  0.0421],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2607,  0.2783,  ..., -0.0063, -0.0006,  0.0159],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0004, -0.1200, -0.0123,  ..., -0.1039, -0.1219, -0.0806],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0166,  0.0334, -0.0259,  ...,  0.1178, -0.0939,  0.0416],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3461, 0.3910, 0.4093,  ..., 0.0236, 0.0016, 0.0233], device='cuda:2',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1861, -0.1976,  0.0349,  ...,  0.0089, -0.0007,  0.0082],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0318,  0.1413, -0.0693,  ..., -0.1165, -0.1083, -0.0404],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2686,  0.2705,  0.2811,  ..., -0.0061, -0.0071,  0.0144],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0246,  0.1288,  0.0653,  ...,  0.0421, -0.0274, -0.1130],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0648,  0.1070,  0.1007,  ...,  0.1291, -0.0245,  0.0073],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3310, 0.3408, 0.3612,  ..., 0.0136, 0.0081, 0.0195], device='cuda:2',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1109, -0.0750,  0.0721,  ..., -0.0441,  0.0544,  0.0041],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463, -0.1530,  0.0253,  ..., -0.0226, -0.1069, -0.0019],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2588,  0.2482,  0.2724,  ..., -0.0119, -0.0073, -0.0036],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1218,  0.0713,  0.0619,  ...,  0.0448, -0.1546, -0.0703],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0970, -0.0954,  0.0650,  ...,  0.0910,  0.0212,  0.0274],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3551, 0.3564, 0.3759,  ..., 0.0197, 0.0094, 0.0280], device='cuda:2',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0951,  0.2815,  0.0759,  ...,  0.0123, -0.0721,  0.0015],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0780, -0.0183,  0.1131,  ..., -0.0387, -0.2163, -0.1506],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2529,  0.2646,  ..., -0.0250, -0.0093,  0.0164],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463,  0.0148,  0.0833,  ..., -0.0783, -0.1670, -0.0382],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0227, -0.0343,  0.1083,  ...,  0.0295, -0.0121, -0.0167],
       device='cuda:2', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.8.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3518, 0.3381, 0.3564,  ..., 0.0100, 0.0037, 0.0385], device='cuda:2',
       requires_grad=True)
world_size : 4
rank : 1
before init dataset  0.0
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
after init dataset  0.0
before init model  0.0
after init model  487.46875
{'FSDP': 0, 'DP': 125, 'SDP': 0}
{'FSDP': 0, 'DP': 124, 'SDP': 0}
{'FSDP': 0, 'DP': 123, 'SDP': 0}
{'FSDP': 0, 'DP': 122, 'SDP': 0}
{'FSDP': 0, 'DP': 121, 'SDP': 0}
{'FSDP': 0, 'DP': 120, 'SDP': 0}
{'FSDP': 0, 'DP': 119, 'SDP': 0}
{'FSDP': 0, 'DP': 118, 'SDP': 0}
{'FSDP': 0, 'DP': 117, 'SDP': 0}
{'FSDP': 0, 'DP': 116, 'SDP': 0}
{'FSDP': 0, 'DP': 115, 'SDP': 0}
{'FSDP': 0, 'DP': 114, 'SDP': 0}
{'FSDP': 0, 'DP': 113, 'SDP': 0}
{'FSDP': 0, 'DP': 112, 'SDP': 0}
{'FSDP': 0, 'DP': 111, 'SDP': 0}
{'FSDP': 0, 'DP': 110, 'SDP': 0}
{'FSDP': 0, 'DP': 109, 'SDP': 0}
{'FSDP': 0, 'DP': 108, 'SDP': 0}
{'FSDP': 0, 'DP': 107, 'SDP': 0}
{'FSDP': 0, 'DP': 106, 'SDP': 0}
{'FSDP': 0, 'DP': 105, 'SDP': 0}
{'FSDP': 0, 'DP': 104, 'SDP': 0}
{'FSDP': 0, 'DP': 103, 'SDP': 0}
{'FSDP': 0, 'DP': 102, 'SDP': 0}
{'FSDP': 0, 'DP': 101, 'SDP': 0}
{'FSDP': 0, 'DP': 100, 'SDP': 0}
{'FSDP': 0, 'DP': 99, 'SDP': 0}
{'FSDP': 0, 'DP': 98, 'SDP': 0}
{'FSDP': 0, 'DP': 97, 'SDP': 0}
{'FSDP': 0, 'DP': 96, 'SDP': 0}
{'FSDP': 0, 'DP': 95, 'SDP': 0}
{'FSDP': 0, 'DP': 94, 'SDP': 0}
{'FSDP': 0, 'DP': 93, 'SDP': 0}
{'FSDP': 0, 'DP': 92, 'SDP': 0}
{'FSDP': 0, 'DP': 91, 'SDP': 0}
{'FSDP': 0, 'DP': 90, 'SDP': 0}
{'FSDP': 0, 'DP': 89, 'SDP': 0}
{'FSDP': 0, 'DP': 88, 'SDP': 0}
{'FSDP': 0, 'DP': 87, 'SDP': 0}
{'FSDP': 0, 'DP': 86, 'SDP': 0}
{'FSDP': 0, 'DP': 85, 'SDP': 0}
{'FSDP': 0, 'DP': 84, 'SDP': 0}
{'FSDP': 0, 'DP': 83, 'SDP': 0}
{'FSDP': 0, 'DP': 82, 'SDP': 0}
{'FSDP': 0, 'DP': 81, 'SDP': 0}
{'FSDP': 0, 'DP': 80, 'SDP': 0}
{'FSDP': 0, 'DP': 79, 'SDP': 0}
{'FSDP': 0, 'DP': 78, 'SDP': 0}
{'FSDP': 0, 'DP': 77, 'SDP': 0}
{'FSDP': 0, 'DP': 76, 'SDP': 0}
{'FSDP': 0, 'DP': 75, 'SDP': 0}
{'FSDP': 0, 'DP': 74, 'SDP': 0}
{'FSDP': 0, 'DP': 73, 'SDP': 0}
{'FSDP': 0, 'DP': 72, 'SDP': 0}
{'FSDP': 0, 'DP': 71, 'SDP': 0}
{'FSDP': 0, 'DP': 70, 'SDP': 0}
{'FSDP': 0, 'DP': 69, 'SDP': 0}
{'FSDP': 0, 'DP': 68, 'SDP': 0}
{'FSDP': 0, 'DP': 67, 'SDP': 0}
{'FSDP': 0, 'DP': 66, 'SDP': 0}
{'FSDP': 0, 'DP': 65, 'SDP': 0}
{'FSDP': 0, 'DP': 64, 'SDP': 0}
{'FSDP': 0, 'DP': 63, 'SDP': 0}
{'FSDP': 0, 'DP': 62, 'SDP': 0}
{'FSDP': 0, 'DP': 61, 'SDP': 0}
{'FSDP': 0, 'DP': 60, 'SDP': 0}
{'FSDP': 0, 'DP': 59, 'SDP': 0}
{'FSDP': 0, 'DP': 58, 'SDP': 0}
{'FSDP': 0, 'DP': 57, 'SDP': 0}
{'FSDP': 0, 'DP': 56, 'SDP': 0}
{'FSDP': 0, 'DP': 55, 'SDP': 0}
{'FSDP': 0, 'DP': 54, 'SDP': 0}
{'FSDP': 0, 'DP': 53, 'SDP': 0}
{'FSDP': 0, 'DP': 52, 'SDP': 0}
{'FSDP': 0, 'DP': 51, 'SDP': 0}
{'FSDP': 0, 'DP': 50, 'SDP': 0}
{'FSDP': 0, 'DP': 49, 'SDP': 0}
{'FSDP': 0, 'DP': 48, 'SDP': 0}
{'FSDP': 0, 'DP': 47, 'SDP': 0}
{'FSDP': 0, 'DP': 46, 'SDP': 0}
{'FSDP': 0, 'DP': 45, 'SDP': 0}
{'FSDP': 0, 'DP': 44, 'SDP': 0}
{'FSDP': 0, 'DP': 43, 'SDP': 0}
{'FSDP': 0, 'DP': 42, 'SDP': 0}
{'FSDP': 0, 'DP': 41, 'SDP': 0}
{'FSDP': 0, 'DP': 40, 'SDP': 0}
{'FSDP': 0, 'DP': 39, 'SDP': 0}
{'FSDP': 0, 'DP': 38, 'SDP': 0}
{'FSDP': 0, 'DP': 37, 'SDP': 0}
{'FSDP': 0, 'DP': 36, 'SDP': 0}
{'FSDP': 0, 'DP': 35, 'SDP': 0}
{'FSDP': 0, 'DP': 34, 'SDP': 0}
{'FSDP': 0, 'DP': 33, 'SDP': 0}
{'FSDP': 0, 'DP': 32, 'SDP': 0}
{'FSDP': 0, 'DP': 31, 'SDP': 0}
{'FSDP': 0, 'DP': 30, 'SDP': 0}
{'FSDP': 0, 'DP': 29, 'SDP': 0}
{'FSDP': 0, 'DP': 28, 'SDP': 0}
{'FSDP': 0, 'DP': 27, 'SDP': 0}
{'FSDP': 0, 'DP': 26, 'SDP': 0}
{'FSDP': 0, 'DP': 25, 'SDP': 0}
{'FSDP': 0, 'DP': 24, 'SDP': 0}
{'FSDP': 0, 'DP': 23, 'SDP': 0}
{'FSDP': 0, 'DP': 22, 'SDP': 0}
{'FSDP': 0, 'DP': 21, 'SDP': 0}
{'FSDP': 0, 'DP': 20, 'SDP': 0}
{'FSDP': 0, 'DP': 19, 'SDP': 0}
{'FSDP': 0, 'DP': 18, 'SDP': 0}
{'FSDP': 0, 'DP': 17, 'SDP': 0}
{'FSDP': 0, 'DP': 16, 'SDP': 0}
{'FSDP': 0, 'DP': 15, 'SDP': 0}
{'FSDP': 0, 'DP': 14, 'SDP': 0}
{'FSDP': 0, 'DP': 13, 'SDP': 0}
{'FSDP': 0, 'DP': 12, 'SDP': 0}
{'FSDP': 0, 'DP': 11, 'SDP': 0}
{'FSDP': 0, 'DP': 10, 'SDP': 0}
{'FSDP': 0, 'DP': 9, 'SDP': 0}
{'FSDP': 0, 'DP': 8, 'SDP': 0}
{'FSDP': 0, 'DP': 7, 'SDP': 0}
{'FSDP': 0, 'DP': 6, 'SDP': 0}
{'FSDP': 0, 'DP': 5, 'SDP': 0}
{'FSDP': 0, 'DP': 4, 'SDP': 0}
{'FSDP': 0, 'DP': 3, 'SDP': 0}
{'FSDP': 0, 'DP': 2, 'SDP': 0}
{'FSDP': 0, 'DP': 1, 'SDP': 0}
_dp_wrapped_module._fpw_module.transformer.wte._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1101, -0.0393,  0.0331,  ..., -0.0130, -0.0056, -0.0017],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.wpe._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0188, -0.1974,  0.0040,  ..., -0.0047, -0.0024, -0.0058],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2232,  0.1820,  0.1534,  ..., -0.0111, -0.0142, -0.0157],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.4738, -0.2614, -0.0978,  ...,  0.0126, -0.0499,  0.0032],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.3127, -0.1874,  0.0980,  ...,  0.1222, -0.0132,  0.0283],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1310,  0.2093,  0.2066,  ..., -0.0179,  0.0063, -0.0016],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0942,  0.0982, -0.0321,  ..., -0.2490, -0.0768,  0.0143],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.0.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1066,  0.1528,  0.0331,  ...,  0.0100,  0.0968,  0.0296],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2086,  0.2185,  0.2183,  ..., -0.0075, -0.0014, -0.0038],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2906,  0.3057,  0.0302,  ...,  0.0404,  0.0494, -0.0038],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0971, -0.0016,  0.1122,  ...,  0.0633,  0.0618,  0.1215],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2241,  0.2531,  0.2446,  ...,  0.0008, -0.0142,  0.0174],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0078, -0.2255, -0.0207,  ..., -0.1943, -0.1426, -0.0668],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.1.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1112, -0.0263, -0.0332,  ..., -0.0128,  0.0883,  0.0801],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1871,  0.2227,  0.2366,  ...,  0.0054, -0.0074,  0.0045],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.2659,  0.0279,  0.0728,  ...,  0.0015, -0.0427,  0.0059],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0131, -0.0666, -0.0697,  ..., -0.0914,  0.0728, -0.0527],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2646, 0.3057, 0.2836,  ..., 0.0008, 0.0253, 0.0337], device='cuda:1',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0137,  0.3680, -0.1068,  ..., -0.1723, -0.1186, -0.1458],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.2.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0240, -0.2091, -0.0291,  ..., -0.0054,  0.0097,  0.0094],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.2725, 0.3078, 0.3174,  ..., 0.0061, 0.0003, 0.0134], device='cuda:1',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0180, -0.1333,  0.1339,  ...,  0.0205, -0.0017, -0.0044],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0660, -0.0059, -0.0359,  ..., -0.0921, -0.0232, -0.0359],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2822,  0.3131,  0.3034,  ..., -0.0040,  0.0011,  0.0230],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0354, -0.0083, -0.0900,  ..., -0.0739,  0.0215, -0.1412],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.3.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0766,  0.0050, -0.0819,  ...,  0.1011, -0.0088, -0.0448],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2971,  0.3330,  0.3428,  ...,  0.0111, -0.0019,  0.0193],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0542,  0.1037, -0.1203,  ..., -0.0185, -0.0097,  0.0927],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1220, -0.0009,  0.0201,  ..., -0.0997, -0.1438,  0.0421],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2607,  0.2783,  ..., -0.0063, -0.0006,  0.0159],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0004, -0.1200, -0.0123,  ..., -0.1039, -0.1219, -0.0806],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.4.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0166,  0.0334, -0.0259,  ...,  0.1178, -0.0939,  0.0416],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3461, 0.3910, 0.4093,  ..., 0.0236, 0.0016, 0.0233], device='cuda:1',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1861, -0.1976,  0.0349,  ...,  0.0089, -0.0007,  0.0082],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0318,  0.1413, -0.0693,  ..., -0.1165, -0.1083, -0.0404],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2686,  0.2705,  0.2811,  ..., -0.0061, -0.0071,  0.0144],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0246,  0.1288,  0.0653,  ...,  0.0421, -0.0274, -0.1130],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.5.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0648,  0.1070,  0.1007,  ...,  0.1291, -0.0245,  0.0073],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3310, 0.3408, 0.3612,  ..., 0.0136, 0.0081, 0.0195], device='cuda:1',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.1109, -0.0750,  0.0721,  ..., -0.0441,  0.0544,  0.0041],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463, -0.1530,  0.0253,  ..., -0.0226, -0.1069, -0.0019],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2588,  0.2482,  0.2724,  ..., -0.0119, -0.0073, -0.0036],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.1218,  0.0713,  0.0619,  ...,  0.0448, -0.1546, -0.0703],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.6.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0970, -0.0954,  0.0650,  ...,  0.0910,  0.0212,  0.0274],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3551, 0.3564, 0.3759,  ..., 0.0197, 0.0094, 0.0280], device='cuda:1',
       requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_attn._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0951,  0.2815,  0.0759,  ...,  0.0123, -0.0721,  0.0015],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.attn.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0780, -0.0183,  0.1131,  ..., -0.0387, -0.2163, -0.1506],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.ln_2._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.2568,  0.2529,  0.2646,  ..., -0.0250, -0.0093,  0.0164],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_fc._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([-0.0463,  0.0148,  0.0833,  ..., -0.0783, -0.1670, -0.0382],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.7.mlp.c_proj._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([ 0.0227, -0.0343,  0.1083,  ...,  0.0295, -0.0121, -0.0167],
       device='cuda:1', requires_grad=True)
_dp_wrapped_module._fpw_module.transformer.h.8.ln_1._dp_wrapped_module.flat_param_0
Parameter containing:
tensor([0.3518, 0.3381, 0.3564,  ..., 0.0100, 0.0037, 0.0385], device='cuda:1',
       requires_grad=True)
