2023-01-07 08:47:51,502 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:47:51,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:51,537 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:51,537 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:51,537 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 08:47:51,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,380 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,381 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,381 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 08:47:52,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,383 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,383 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,383 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 08:47:52,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,384 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,384 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,384 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 08:47:52,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,385 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,385 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,385 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 08:47:52,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,423 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,423 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,423 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 08:47:52,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,424 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,424 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,424 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 08:47:52,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,426 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,426 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,426 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 08:47:52,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,427 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,427 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,427 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 08:47:52,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,428 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,428 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,428 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 08:47:52,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,429 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,429 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,429 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 08:47:52,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,430 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,430 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,430 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 08:47:52,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,431 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,431 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,432 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 08:47:52,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,432 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,433 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,433 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 08:47:52,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,434 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,434 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,434 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 08:47:52,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,435 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,435 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,435 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 08:47:52,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,436 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,436 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,436 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 08:47:52,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,437 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,437 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,437 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 08:47:52,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,438 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,438 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,439 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 08:47:52,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,439 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,440 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,440 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 08:47:52,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,441 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,441 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,441 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 08:47:52,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,442 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,442 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,442 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 08:47:52,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,443 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,443 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,443 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 08:47:52,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,444 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,444 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,444 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 08:47:52,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,446 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,446 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,446 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 08:47:52,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,447 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,447 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,447 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 08:47:52,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,448 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,448 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,448 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 08:47:52,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,449 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,449 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,449 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 08:47:52,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,450 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,450 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,450 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 08:47:52,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,452 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,452 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,452 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 08:47:52,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,453 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,453 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,453 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 08:47:52,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,454 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,454 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,454 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 08:47:52,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,455 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,455 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,455 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 08:47:52,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,457 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,457 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,457 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 08:47:52,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,458 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,458 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,458 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 08:47:52,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,459 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,459 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,459 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 08:47:52,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,460 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,460 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,460 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 08:47:52,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,461 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,461 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,461 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 08:47:52,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,462 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,463 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,463 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 08:47:52,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,464 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,464 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,464 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 08:47:52,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,465 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,465 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,465 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 08:47:52,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,466 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,466 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,466 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 08:47:52,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,467 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,467 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,467 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 08:47:52,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,468 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,468 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,468 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 08:47:52,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,469 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,469 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,470 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 08:47:52,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,470 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,471 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,471 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 08:47:52,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,472 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,472 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,472 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 08:47:52,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,473 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,473 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,473 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 08:47:52,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,474 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,474 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,474 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 08:47:52,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,475 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,475 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,475 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 08:47:52,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,477 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,477 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,477 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 08:47:52,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,478 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,478 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,478 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 08:47:52,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,479 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,479 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,479 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 08:47:52,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,480 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,480 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,480 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 08:47:52,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,481 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,481 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,481 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 08:47:52,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,482 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,482 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,482 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 08:47:52,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,483 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,483 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,484 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 08:47:52,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,485 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,485 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,485 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 08:47:52,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,486 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,486 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,486 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 08:47:52,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,487 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,487 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,487 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 08:47:52,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,488 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,488 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,488 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 08:47:52,489 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,489 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,489 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,489 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 08:47:52,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,491 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,491 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,491 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 08:47:52,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,492 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,492 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,492 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 08:47:52,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,493 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,493 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,493 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 08:47:52,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,494 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,494 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,494 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 08:47:52,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,495 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,495 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,495 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 08:47:52,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,496 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,496 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,496 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 08:47:52,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,497 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,497 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,498 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 08:47:52,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,499 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,499 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,499 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 08:47:52,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,500 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,500 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,500 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 08:47:52,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,501 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,501 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,501 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 08:47:52,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,502 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,502 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,502 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 08:47:52,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,503 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,503 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,503 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 08:47:52,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,504 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,504 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,504 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 08:47:52,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,505 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,505 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,505 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 08:47:52,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,507 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,507 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,507 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 08:47:52,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,508 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,508 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,508 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 08:47:52,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,509 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,509 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,509 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 08:47:52,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,510 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,510 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,510 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 08:47:52,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,511 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,511 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,511 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 08:47:52,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,512 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,512 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,512 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 08:47:52,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,513 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,513 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,514 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 08:47:52,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,514 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,514 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,515 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 08:47:52,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,516 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,516 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,516 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 08:47:52,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,517 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,517 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,517 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 08:47:52,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,518 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,518 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,518 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 08:47:52,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,519 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,519 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,519 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 08:47:52,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,520 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,520 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,520 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 08:47:52,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,521 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,522 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,522 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 08:47:52,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,523 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,523 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,523 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 08:47:52,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,524 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,524 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,524 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 08:47:52,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,525 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,525 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,525 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 08:47:52,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,526 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,526 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,526 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 08:47:52,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,527 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,527 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,527 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 08:47:52,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,528 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,528 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,528 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 08:47:52,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,529 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,529 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,530 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 08:47:52,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,531 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,531 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,531 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 08:47:52,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,532 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,532 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,532 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 08:47:52,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,533 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,533 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,533 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 08:47:52,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,534 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,534 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,534 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 08:47:52,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,535 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,535 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,535 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 08:47:52,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,536 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,536 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,536 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 08:47:52,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,537 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,537 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,538 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 08:47:52,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,539 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,539 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,539 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 08:47:52,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,540 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,540 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,540 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 08:47:52,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:47:52,541 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:52,541 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:47:52,541 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:47:52,543 > [DEBUG] 0 :: 7.345709800720215
2023-01-07 08:47:52,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,548 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,548 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00225830078125
2023-01-07 08:47:52,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -374.4405212402344
2023-01-07 08:47:52,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,552 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,553 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.21060949563980103
2023-01-07 08:47:52,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,553 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -188.11468505859375
2023-01-07 08:47:52,554 > [DEBUG] 0 :: before allreduce fusion buffer :: -346.28887939453125
2023-01-07 08:47:52,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,565 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,565 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 3.1336584091186523
2023-01-07 08:47:52,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18263700604438782
2023-01-07 08:47:52,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,566 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,566 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.029374726116657257
2023-01-07 08:47:52,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,566 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -139.8834228515625
2023-01-07 08:47:52,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5745304822921753
2023-01-07 08:47:52,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,569 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,569 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 18.987136840820312
2023-01-07 08:47:52,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18815290927886963
2023-01-07 08:47:52,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,570 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,570 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.020616067573428154
2023-01-07 08:47:52,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,570 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -328.82122802734375
2023-01-07 08:47:52,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05879712104797363
2023-01-07 08:47:52,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,572 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,572 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 12.842772483825684
2023-01-07 08:47:52,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15170946717262268
2023-01-07 08:47:52,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,573 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.29188305139541626
2023-01-07 08:47:52,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -385.0865478515625
2023-01-07 08:47:52,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4000506401062012
2023-01-07 08:47:52,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,575 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 17.2366886138916
2023-01-07 08:47:52,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.534026563167572
2023-01-07 08:47:52,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,577 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.02862424962222576
2023-01-07 08:47:52,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -169.76907348632812
2023-01-07 08:47:52,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5338387489318848
2023-01-07 08:47:52,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,579 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,579 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -5.340951442718506
2023-01-07 08:47:52,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2898640036582947
2023-01-07 08:47:52,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,580 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,580 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.007632389664649963
2023-01-07 08:47:52,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,580 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -367.0451965332031
2023-01-07 08:47:52,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7012833952903748
2023-01-07 08:47:52,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,582 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,582 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.886725902557373
2023-01-07 08:47:52,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46699365973472595
2023-01-07 08:47:52,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,583 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,583 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.17110568284988403
2023-01-07 08:47:52,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,584 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -501.80279541015625
2023-01-07 08:47:52,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.36341971158981323
2023-01-07 08:47:52,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,585 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,585 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 18.002140045166016
2023-01-07 08:47:52,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15056486427783966
2023-01-07 08:47:52,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,587 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,587 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.14967957139015198
2023-01-07 08:47:52,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,587 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -489.94049072265625
2023-01-07 08:47:52,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2413382530212402
2023-01-07 08:47:52,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,588 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,589 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 10.60403823852539
2023-01-07 08:47:52,589 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8295003175735474
2023-01-07 08:47:52,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,590 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,590 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.02712409570813179
2023-01-07 08:47:52,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,590 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -245.3701629638672
2023-01-07 08:47:52,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4392226934432983
2023-01-07 08:47:52,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,592 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,592 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -6.668849945068359
2023-01-07 08:47:52,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0378077030181885
2023-01-07 08:47:52,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,594 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,594 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.13994409143924713
2023-01-07 08:47:52,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,594 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -378.50653076171875
2023-01-07 08:47:52,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.269514560699463
2023-01-07 08:47:52,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,596 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,596 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 8.56442642211914
2023-01-07 08:47:52,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0581603050231934
2023-01-07 08:47:52,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,597 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,597 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.01998869702219963
2023-01-07 08:47:52,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,598 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -355.915771484375
2023-01-07 08:47:52,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3826050758361816
2023-01-07 08:47:52,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,599 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.751022815704346
2023-01-07 08:47:52,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2367466390132904
2023-01-07 08:47:52,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,601 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.02621828019618988
2023-01-07 08:47:52,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -243.91134643554688
2023-01-07 08:47:52,601 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13085415959358215
2023-01-07 08:47:52,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,603 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -7.443538665771484
2023-01-07 08:47:52,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.024973034858703613
2023-01-07 08:47:52,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,605 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.013508856296539307
2023-01-07 08:47:52,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -252.85873413085938
2023-01-07 08:47:52,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9164247512817383
2023-01-07 08:47:52,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,607 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,607 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -13.085214614868164
2023-01-07 08:47:52,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7411860227584839
2023-01-07 08:47:52,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,608 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.08907723426818848
2023-01-07 08:47:52,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -369.2143249511719
2023-01-07 08:47:52,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2446587085723877
2023-01-07 08:47:52,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,610 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,610 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -8.153365135192871
2023-01-07 08:47:52,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.583024799823761
2023-01-07 08:47:52,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,611 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.12391085922718048
2023-01-07 08:47:52,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -253.6625518798828
2023-01-07 08:47:52,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22832821309566498
2023-01-07 08:47:52,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,613 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 21.649686813354492
2023-01-07 08:47:52,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3278415203094482
2023-01-07 08:47:52,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,614 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.17431513965129852
2023-01-07 08:47:52,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -215.5770263671875
2023-01-07 08:47:52,615 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7741113901138306
2023-01-07 08:47:52,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,616 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 10.655746459960938
2023-01-07 08:47:52,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0595555305480957
2023-01-07 08:47:52,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,618 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.16068969666957855
2023-01-07 08:47:52,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,618 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -337.611572265625
2023-01-07 08:47:52,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30473029613494873
2023-01-07 08:47:52,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,620 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.698480129241943
2023-01-07 08:47:52,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.674872875213623
2023-01-07 08:47:52,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,621 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,621 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.094012051820755
2023-01-07 08:47:52,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,621 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -242.38075256347656
2023-01-07 08:47:52,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0153313875198364
2023-01-07 08:47:52,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,623 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -33.7567138671875
2023-01-07 08:47:52,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7143688201904297
2023-01-07 08:47:52,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,624 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,624 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.05759158730506897
2023-01-07 08:47:52,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -269.8492126464844
2023-01-07 08:47:52,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8431787490844727
2023-01-07 08:47:52,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,626 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,626 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1.272756576538086
2023-01-07 08:47:52,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.662254810333252
2023-01-07 08:47:52,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,627 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.22920092940330505
2023-01-07 08:47:52,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -355.0743103027344
2023-01-07 08:47:52,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0517739057540894
2023-01-07 08:47:52,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,629 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 5.130815505981445
2023-01-07 08:47:52,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2359165996313095
2023-01-07 08:47:52,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,631 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.03771594166755676
2023-01-07 08:47:52,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -218.4306182861328
2023-01-07 08:47:52,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3838924169540405
2023-01-07 08:47:52,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,633 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -43.645118713378906
2023-01-07 08:47:52,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1635961532592773
2023-01-07 08:47:52,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,634 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.05902475863695145
2023-01-07 08:47:52,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -252.23287963867188
2023-01-07 08:47:52,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.665947437286377
2023-01-07 08:47:52,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,636 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -21.385822296142578
2023-01-07 08:47:52,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0030239224433898926
2023-01-07 08:47:52,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,638 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.33668842911720276
2023-01-07 08:47:52,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -344.48931884765625
2023-01-07 08:47:52,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2935123443603516
2023-01-07 08:47:52,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,640 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 8.841272354125977
2023-01-07 08:47:52,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6904296875
2023-01-07 08:47:52,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,641 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.06491774320602417
2023-01-07 08:47:52,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -165.99380493164062
2023-01-07 08:47:52,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0014106035232543945
2023-01-07 08:47:52,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,643 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -65.06156921386719
2023-01-07 08:47:52,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9281182289123535
2023-01-07 08:47:52,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -238.67373657226562
2023-01-07 08:47:52,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.401338577270508
2023-01-07 08:47:52,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,646 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,646 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -37.970245361328125
2023-01-07 08:47:52,646 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.819099187850952
2023-01-07 08:47:52,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,647 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,648 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.04636052995920181
2023-01-07 08:47:52,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,648 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -329.1301574707031
2023-01-07 08:47:52,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.493230819702148
2023-01-07 08:47:52,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,650 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,651 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -20.810396194458008
2023-01-07 08:47:52,651 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6917831897735596
2023-01-07 08:47:52,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,652 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,652 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.5843318104743958
2023-01-07 08:47:52,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -129.60546875
2023-01-07 08:47:52,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.55211067199707
2023-01-07 08:47:52,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,654 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,654 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -16.94778060913086
2023-01-07 08:47:52,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7141520977020264
2023-01-07 08:47:52,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,656 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.027053087949752808
2023-01-07 08:47:52,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,656 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -149.5579833984375
2023-01-07 08:47:52,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5736076831817627
2023-01-07 08:47:52,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,658 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 106.98517608642578
2023-01-07 08:47:52,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9861389994621277
2023-01-07 08:47:52,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -24.911890029907227
2023-01-07 08:47:52,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.8372602462768555
2023-01-07 08:47:52,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,661 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 2.645723342895508
2023-01-07 08:47:52,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3902605772018433
2023-01-07 08:47:52,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,663 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -182.32887268066406
2023-01-07 08:47:52,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1935675144195557
2023-01-07 08:47:52,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,665 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,665 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.4041800498962402
2023-01-07 08:47:52,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21548861265182495
2023-01-07 08:47:52,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,666 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -126.80973815917969
2023-01-07 08:47:52,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.13004207611084
2023-01-07 08:47:52,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,668 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,668 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -53.077613830566406
2023-01-07 08:47:52,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.921777725219727
2023-01-07 08:47:52,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,669 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -176.16038513183594
2023-01-07 08:47:52,669 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3308959007263184
2023-01-07 08:47:52,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,671 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,671 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 1.6073079109191895
2023-01-07 08:47:52,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1438214778900146
2023-01-07 08:47:52,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,672 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -170.18161010742188
2023-01-07 08:47:52,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10345113277435303
2023-01-07 08:47:52,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,674 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,674 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -19.009292602539062
2023-01-07 08:47:52,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6048863530158997
2023-01-07 08:47:52,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,676 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -125.80500030517578
2023-01-07 08:47:52,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.997543215751648
2023-01-07 08:47:52,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,677 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -64.31318664550781
2023-01-07 08:47:52,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7058372497558594
2023-01-07 08:47:52,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,678 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -174.30218505859375
2023-01-07 08:47:52,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.58278465270996
2023-01-07 08:47:52,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,680 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 21.4540958404541
2023-01-07 08:47:52,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.274172306060791
2023-01-07 08:47:52,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,681 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,681 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.823206901550293
2023-01-07 08:47:52,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -171.42237854003906
2023-01-07 08:47:52,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.971335411071777
2023-01-07 08:47:52,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,683 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,683 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -15.27513599395752
2023-01-07 08:47:52,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2070717811584473
2023-01-07 08:47:52,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -130.5587158203125
2023-01-07 08:47:52,685 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.4759416580200195
2023-01-07 08:47:52,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,686 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 56.94886016845703
2023-01-07 08:47:52,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.920400619506836
2023-01-07 08:47:52,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -62.85308074951172
2023-01-07 08:47:52,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.48896026611328
2023-01-07 08:47:52,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,689 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -14.345418930053711
2023-01-07 08:47:52,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1744683980941772
2023-01-07 08:47:52,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,690 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.05645108222961426
2023-01-07 08:47:52,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -179.06582641601562
2023-01-07 08:47:52,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.487239837646484
2023-01-07 08:47:52,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,693 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -43.21414566040039
2023-01-07 08:47:52,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.063451766967773
2023-01-07 08:47:52,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -80.30387115478516
2023-01-07 08:47:52,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.990897178649902
2023-01-07 08:47:52,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,695 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -25.12081527709961
2023-01-07 08:47:52,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.449497222900391
2023-01-07 08:47:52,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -109.7193832397461
2023-01-07 08:47:52,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.858736038208008
2023-01-07 08:47:52,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,699 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -304.5715637207031
2023-01-07 08:47:52,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.421446800231934
2023-01-07 08:47:52,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,700 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,700 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 3.1253044605255127
2023-01-07 08:47:52,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -387.94915771484375
2023-01-07 08:47:52,701 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.2773208618164
2023-01-07 08:47:52,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,703 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1018.0140380859375
2023-01-07 08:47:52,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.890157699584961
2023-01-07 08:47:52,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,704 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,704 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.4416241645812988
2023-01-07 08:47:52,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,704 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,704 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -40.63841247558594
2023-01-07 08:47:52,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1192.185791015625
2023-01-07 08:47:52,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.882583618164062
2023-01-07 08:47:52,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,707 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1286.7945556640625
2023-01-07 08:47:52,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.7191619873046875
2023-01-07 08:47:52,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -652.27490234375
2023-01-07 08:47:52,708 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.393324851989746
2023-01-07 08:47:52,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,710 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -289.9905700683594
2023-01-07 08:47:52,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.756166458129883
2023-01-07 08:47:52,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,711 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,712 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.931682825088501
2023-01-07 08:47:52,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,712 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -357.12176513671875
2023-01-07 08:47:52,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.80042266845703
2023-01-07 08:47:52,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,714 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -885.3117065429688
2023-01-07 08:47:52,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0396080017089844
2023-01-07 08:47:52,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,715 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,715 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.834398627281189
2023-01-07 08:47:52,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,715 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,715 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -100.51213073730469
2023-01-07 08:47:52,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.31977081298828
2023-01-07 08:47:52,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,718 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1168.865478515625
2023-01-07 08:47:52,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.56103515625
2023-01-07 08:47:52,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,719 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,719 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -23.019420623779297
2023-01-07 08:47:52,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,720 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1355.9776611328125
2023-01-07 08:47:52,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.75437927246094
2023-01-07 08:47:52,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,721 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1273.381103515625
2023-01-07 08:47:52,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.602579116821289
2023-01-07 08:47:52,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,723 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,723 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 21.914987564086914
2023-01-07 08:47:52,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.43719482421875
2023-01-07 08:47:52,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,724 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1364.9453125
2023-01-07 08:47:52,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.039054870605469
2023-01-07 08:47:52,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,725 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,726 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.7289173603057861
2023-01-07 08:47:52,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,726 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,726 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -31.50103759765625
2023-01-07 08:47:52,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,726 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1579.999755859375
2023-01-07 08:47:52,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,727 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1471.0985107421875
2023-01-07 08:47:52,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -86.76802825927734
2023-01-07 08:47:52,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,728 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1634.07666015625
2023-01-07 08:47:52,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9459881782531738
2023-01-07 08:47:52,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,729 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,729 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 73.83601379394531
2023-01-07 08:47:52,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 118.353515625
2023-01-07 08:47:52,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,731 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1644.597900390625
2023-01-07 08:47:52,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.503011703491211
2023-01-07 08:47:52,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,732 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -823.1611328125
2023-01-07 08:47:52,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1505.5616455078125
2023-01-07 08:47:52,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.11762237548828
2023-01-07 08:47:52,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,734 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,734 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 228.72866821289062
2023-01-07 08:47:52,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.46589660644531
2023-01-07 08:47:52,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,735 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,735 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.4717769622802734
2023-01-07 08:47:52,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,736 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 222.85333251953125
2023-01-07 08:47:52,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -116.4529800415039
2023-01-07 08:47:52,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,738 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -371.00323486328125
2023-01-07 08:47:52,738 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.34359359741211
2023-01-07 08:47:52,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,739 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -907.1220703125
2023-01-07 08:47:52,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 203.21563720703125
2023-01-07 08:47:52,741 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:47:52,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,741 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:52,741 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 625.5638427734375
2023-01-07 08:47:52,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,742 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1438.21826171875
2023-01-07 08:47:52,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1486.649658203125
2023-01-07 08:47:52,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -756.58251953125
2023-01-07 08:47:52,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -601.7095336914062
2023-01-07 08:47:52,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -767.0982666015625
2023-01-07 08:47:52,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,743 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -807.3832397460938
2023-01-07 08:47:52,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -754.523193359375
2023-01-07 08:47:52,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -645.0193481445312
2023-01-07 08:47:52,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -890.7447509765625
2023-01-07 08:47:52,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -727.5534057617188
2023-01-07 08:47:52,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -711.79736328125
2023-01-07 08:47:52,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,745 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -887.6875
2023-01-07 08:47:52,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -733.1810302734375
2023-01-07 08:47:52,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -710.0880126953125
2023-01-07 08:47:52,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -760.4216918945312
2023-01-07 08:47:52,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -1034.2713623046875
2023-01-07 08:47:52,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -727.1326904296875
2023-01-07 08:47:52,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1071.0164794921875
2023-01-07 08:47:52,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -952.249267578125
2023-01-07 08:47:52,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -978.49365234375
2023-01-07 08:47:52,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -994.96240234375
2023-01-07 08:47:52,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -950.8937377929688
2023-01-07 08:47:52,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1037.22216796875
2023-01-07 08:47:52,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1019.4159545898438
2023-01-07 08:47:52,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1013.3110961914062
2023-01-07 08:47:52,750 > [DEBUG] 0 :: before allreduce fusion buffer :: 121.04464721679688
2023-01-07 08:47:52,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1064.02685546875
2023-01-07 08:47:52,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1002.9671020507812
2023-01-07 08:47:52,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -955.226806640625
2023-01-07 08:47:52,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1074.329833984375
2023-01-07 08:47:52,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1032.9925537109375
2023-01-07 08:47:52,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -992.8524169921875
2023-01-07 08:47:52,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,754 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1061.2637939453125
2023-01-07 08:47:52,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,754 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1014.710205078125
2023-01-07 08:47:52,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,754 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1126.5162353515625
2023-01-07 08:47:52,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.653543472290039
2023-01-07 08:47:52,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,756 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -785.4763793945312
2023-01-07 08:47:52,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,756 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1000.4036254882812
2023-01-07 08:47:52,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,756 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1013.8789672851562
2023-01-07 08:47:52,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,757 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1088.397216796875
2023-01-07 08:47:52,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.586626052856445
2023-01-07 08:47:52,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,758 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -717.8229370117188
2023-01-07 08:47:52,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,758 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -896.0146484375
2023-01-07 08:47:52,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,758 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -994.8763427734375
2023-01-07 08:47:52,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.71030044555664
2023-01-07 08:47:52,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,760 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -962.578125
2023-01-07 08:47:52,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:52,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:52,761 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -352.1121826171875
2023-01-07 08:47:52,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 719.0416870117188
2023-01-07 08:47:53,616 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -828.1668090820312
2023-01-07 08:47:53,616 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:53,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,617 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,617 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -9.158035278320312
2023-01-07 08:47:53,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,617 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1558.179443359375
2023-01-07 08:47:53,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.06407928466797
2023-01-07 08:47:53,618 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -8.820928573608398
2023-01-07 08:47:53,618 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,619 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.6681442260742188
2023-01-07 08:47:53,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,619 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -43.78860092163086
2023-01-07 08:47:53,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1738.7611083984375
2023-01-07 08:47:53,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.63792419433594
2023-01-07 08:47:53,621 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 160.40457153320312
2023-01-07 08:47:53,621 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,621 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:53,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,621 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1750.618896484375
2023-01-07 08:47:53,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 139.67861938476562
2023-01-07 08:47:53,622 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -0.10420632362365723
2023-01-07 08:47:53,622 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,622 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:53,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,623 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1788.319580078125
2023-01-07 08:47:53,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.74185180664062
2023-01-07 08:47:53,624 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -1721.0330810546875
2023-01-07 08:47:53,624 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,624 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:53,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,624 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,624 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.645012378692627
2023-01-07 08:47:53,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,625 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,625 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 33.29012680053711
2023-01-07 08:47:53,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,625 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1623.286865234375
2023-01-07 08:47:53,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.53150177001953
2023-01-07 08:47:53,627 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.3545384407043457
2023-01-07 08:47:53,627 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,627 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,627 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:53,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,627 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1611.6011962890625
2023-01-07 08:47:53,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.15731048583984
2023-01-07 08:47:53,628 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -40.95613098144531
2023-01-07 08:47:53,628 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:53,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,629 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 3.697807788848877
2023-01-07 08:47:53,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,629 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -738.0610961914062
2023-01-07 08:47:53,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.974555969238281
2023-01-07 08:47:53,630 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 3.2569499015808105
2023-01-07 08:47:53,630 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:53,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,631 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -629.6382446289062
2023-01-07 08:47:53,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -87.53358459472656
2023-01-07 08:47:53,632 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1543.739501953125
2023-01-07 08:47:53,632 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,632 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:53,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,632 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -691.0098876953125
2023-01-07 08:47:53,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.365325927734375
2023-01-07 08:47:53,634 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -16.607139587402344
2023-01-07 08:47:53,634 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:53,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,634 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -827.122314453125
2023-01-07 08:47:53,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.04815673828125
2023-01-07 08:47:53,635 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -754.661376953125
2023-01-07 08:47:53,635 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,636 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:53,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,636 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,636 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.10336250066757202
2023-01-07 08:47:53,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,636 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -808.4758911132812
2023-01-07 08:47:53,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.65394973754883
2023-01-07 08:47:53,638 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.5411463975906372
2023-01-07 08:47:53,638 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:53,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,638 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -941.04541015625
2023-01-07 08:47:53,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,638 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1184.95751953125
2023-01-07 08:47:53,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.222537994384766
2023-01-07 08:47:53,640 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -961.1605224609375
2023-01-07 08:47:53,640 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:53,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,640 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,640 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.5334471464157104
2023-01-07 08:47:53,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,641 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -717.544189453125
2023-01-07 08:47:53,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.635438919067383
2023-01-07 08:47:53,642 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.5611450672149658
2023-01-07 08:47:53,642 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:53,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,642 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1597.1466064453125
2023-01-07 08:47:53,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.377262592315674
2023-01-07 08:47:53,643 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -687.7289428710938
2023-01-07 08:47:53,643 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:53,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -931.973388671875
2023-01-07 08:47:53,644 > [DEBUG] 0 :: before allreduce fusion buffer :: -67.67693328857422
2023-01-07 08:47:53,645 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -12.389192581176758
2023-01-07 08:47:53,645 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:53,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -911.8787841796875
2023-01-07 08:47:53,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1583.61669921875
2023-01-07 08:47:53,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.2996826171875
2023-01-07 08:47:53,647 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -914.944580078125
2023-01-07 08:47:53,647 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:53,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,648 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1587.9998779296875
2023-01-07 08:47:53,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -70.48979187011719
2023-01-07 08:47:53,649 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -8.126004219055176
2023-01-07 08:47:53,649 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:53,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1563.864990234375
2023-01-07 08:47:53,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -45.38937759399414
2023-01-07 08:47:53,650 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -1575.66796875
2023-01-07 08:47:53,651 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:53,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,651 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,651 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.9211347103118896
2023-01-07 08:47:53,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,651 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,651 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -10.339362144470215
2023-01-07 08:47:53,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,652 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1320.224853515625
2023-01-07 08:47:53,652 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.956401824951172
2023-01-07 08:47:53,653 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 2.4867801666259766
2023-01-07 08:47:53,653 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:53,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1199.7149658203125
2023-01-07 08:47:53,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.461868286132812
2023-01-07 08:47:53,655 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -19.052753448486328
2023-01-07 08:47:53,655 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -32.44528579711914
2023-01-07 08:47:53,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1287.925048828125
2023-01-07 08:47:53,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2230730056762695
2023-01-07 08:47:53,657 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -29.53507423400879
2023-01-07 08:47:53,657 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,657 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,657 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:53,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,657 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1201.271484375
2023-01-07 08:47:53,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -60.1691780090332
2023-01-07 08:47:53,658 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1316.028076171875
2023-01-07 08:47:53,659 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:53,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,659 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1225.3739013671875
2023-01-07 08:47:53,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9681260585784912
2023-01-07 08:47:53,660 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -17.065824508666992
2023-01-07 08:47:53,660 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:53,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,661 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1225.887939453125
2023-01-07 08:47:53,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.912479400634766
2023-01-07 08:47:53,662 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -1187.4453125
2023-01-07 08:47:53,662 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,662 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,662 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.08599674701690674
2023-01-07 08:47:53,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,663 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -742.2744140625
2023-01-07 08:47:53,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0061120986938477
2023-01-07 08:47:53,664 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.3933585286140442
2023-01-07 08:47:53,664 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,664 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,664 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -13.1309175491333
2023-01-07 08:47:53,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,664 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -722.7291870117188
2023-01-07 08:47:53,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.302727699279785
2023-01-07 08:47:53,666 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -720.879150390625
2023-01-07 08:47:53,666 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:53,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,666 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,666 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.34963178634643555
2023-01-07 08:47:53,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,666 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -573.2882080078125
2023-01-07 08:47:53,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.552285194396973
2023-01-07 08:47:53,667 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.6697848439216614
2023-01-07 08:47:53,668 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.17344856262207
2023-01-07 08:47:53,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,668 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -560.8967895507812
2023-01-07 08:47:53,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11584091186523438
2023-01-07 08:47:53,669 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -569.4822387695312
2023-01-07 08:47:53,669 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:53,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,669 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -760.0731201171875
2023-01-07 08:47:53,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.660713195800781
2023-01-07 08:47:53,671 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -60.08818054199219
2023-01-07 08:47:53,671 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:53,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,671 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -769.3070068359375
2023-01-07 08:47:53,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.28742790222168
2023-01-07 08:47:53,672 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -766.9879760742188
2023-01-07 08:47:53,672 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,673 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.9077130556106567
2023-01-07 08:47:53,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -738.177490234375
2023-01-07 08:47:53,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.67009735107422
2023-01-07 08:47:53,674 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -2.5186777114868164
2023-01-07 08:47:53,674 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,675 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.533244132995605
2023-01-07 08:47:53,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -680.420166015625
2023-01-07 08:47:53,675 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.878360748291016
2023-01-07 08:47:53,676 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -694.2918090820312
2023-01-07 08:47:53,676 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,676 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.9234758615493774
2023-01-07 08:47:53,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -752.5377807617188
2023-01-07 08:47:53,677 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.251074314117432
2023-01-07 08:47:53,678 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 1.7442203760147095
2023-01-07 08:47:53,678 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 31.334651947021484
2023-01-07 08:47:53,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,679 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -758.2733154296875
2023-01-07 08:47:53,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8064708709716797
2023-01-07 08:47:53,680 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -765.391845703125
2023-01-07 08:47:53,680 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:53,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -644.9769287109375
2023-01-07 08:47:53,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.29844093322754
2023-01-07 08:47:53,681 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -85.95201873779297
2023-01-07 08:47:53,681 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:53,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -637.2490844726562
2023-01-07 08:47:53,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4688520431518555
2023-01-07 08:47:53,683 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -634.4691772460938
2023-01-07 08:47:53,684 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,684 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,684 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.4021720290184021
2023-01-07 08:47:53,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,684 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -888.0244750976562
2023-01-07 08:47:53,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.440549850463867
2023-01-07 08:47:53,685 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.8189719915390015
2023-01-07 08:47:53,686 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,686 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,686 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.319789171218872
2023-01-07 08:47:53,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -926.9304809570312
2023-01-07 08:47:53,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.383604049682617
2023-01-07 08:47:53,687 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -937.628662109375
2023-01-07 08:47:53,687 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,688 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.08696168661117554
2023-01-07 08:47:53,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -727.0185546875
2023-01-07 08:47:53,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.15084457397461
2023-01-07 08:47:53,689 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.053864240646362305
2023-01-07 08:47:53,689 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,689 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 6.020914077758789
2023-01-07 08:47:53,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -734.9202270507812
2023-01-07 08:47:53,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.737757682800293
2023-01-07 08:47:53,691 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -739.426025390625
2023-01-07 08:47:53,691 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:53,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,691 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,691 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.04357224702835083
2023-01-07 08:47:53,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -711.0836181640625
2023-01-07 08:47:53,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1922457218170166
2023-01-07 08:47:53,693 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.1422736793756485
2023-01-07 08:47:53,693 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,693 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 29.095399856567383
2023-01-07 08:47:53,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -711.9005737304688
2023-01-07 08:47:53,694 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8094515800476074
2023-01-07 08:47:53,695 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -719.0256958007812
2023-01-07 08:47:53,695 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,695 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,695 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.20035529136657715
2023-01-07 08:47:53,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,695 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -886.9388427734375
2023-01-07 08:47:53,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.68892288208008
2023-01-07 08:47:53,697 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.33313941955566406
2023-01-07 08:47:53,697 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -7.9246392250061035
2023-01-07 08:47:53,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -847.7228393554688
2023-01-07 08:47:53,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1916096210479736
2023-01-07 08:47:53,698 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -849.2408447265625
2023-01-07 08:47:53,698 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:53,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,699 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.14705753326416016
2023-01-07 08:47:53,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -734.2335205078125
2023-01-07 08:47:53,699 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.778545618057251
2023-01-07 08:47:53,700 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.005314946174621582
2023-01-07 08:47:53,700 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,701 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.432145118713379
2023-01-07 08:47:53,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -737.8927001953125
2023-01-07 08:47:53,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8847293853759766
2023-01-07 08:47:53,702 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -734.553466796875
2023-01-07 08:47:53,702 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,702 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:53,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,702 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,702 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.2976993918418884
2023-01-07 08:47:53,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,703 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -717.662109375
2023-01-07 08:47:53,703 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.911837577819824
2023-01-07 08:47:53,704 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.13721421360969543
2023-01-07 08:47:53,704 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -22.02167510986328
2023-01-07 08:47:53,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,704 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -714.6473388671875
2023-01-07 08:47:53,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.1101298332214355
2023-01-07 08:47:53,706 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -706.3436279296875
2023-01-07 08:47:53,706 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:53,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,706 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,706 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.27312391996383667
2023-01-07 08:47:53,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,706 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1156.225341796875
2023-01-07 08:47:53,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.727773666381836
2023-01-07 08:47:53,708 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.5798161625862122
2023-01-07 08:47:53,708 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.97386360168457
2023-01-07 08:47:53,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1124.8023681640625
2023-01-07 08:47:53,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3464508056640625
2023-01-07 08:47:53,710 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1138.6251220703125
2023-01-07 08:47:53,710 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,710 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,710 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:53,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,710 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -925.3562622070312
2023-01-07 08:47:53,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4637253284454346
2023-01-07 08:47:53,711 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -49.446075439453125
2023-01-07 08:47:53,711 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:53,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -922.8413696289062
2023-01-07 08:47:53,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.963283061981201
2023-01-07 08:47:53,713 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -920.4818725585938
2023-01-07 08:47:53,713 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:53,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,713 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -313.0115051269531
2023-01-07 08:47:53,714 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.7131242752075195
2023-01-07 08:47:53,715 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -188.1078643798828
2023-01-07 08:47:53,715 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:53,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -307.2188415527344
2023-01-07 08:47:53,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7698420286178589
2023-01-07 08:47:53,716 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -303.4224853515625
2023-01-07 08:47:53,716 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:53,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -768.5402221679688
2023-01-07 08:47:53,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.612422943115234
2023-01-07 08:47:53,718 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -190.38397216796875
2023-01-07 08:47:53,718 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:53,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,718 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -774.54150390625
2023-01-07 08:47:53,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9705080986022949
2023-01-07 08:47:53,720 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -774.5986328125
2023-01-07 08:47:53,720 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:53,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,720 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:47:53,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.28120169043540955
2023-01-07 08:47:53,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,721 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -302.45050048828125
2023-01-07 08:47:53,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.449609756469727
2023-01-07 08:47:53,722 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.040508925914764404
2023-01-07 08:47:53,722 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,722 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.42170524597168
2023-01-07 08:47:53,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -298.23687744140625
2023-01-07 08:47:53,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.441985130310059
2023-01-07 08:47:53,724 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -294.79931640625
2023-01-07 08:47:53,724 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:53,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,724 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -847.9029541015625
2023-01-07 08:47:53,724 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3298561573028564
2023-01-07 08:47:53,725 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -44.82316589355469
2023-01-07 08:47:53,725 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,725 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,725 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:53,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,726 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -844.7811279296875
2023-01-07 08:47:53,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5476682186126709
2023-01-07 08:47:53,728 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -845.6249389648438
2023-01-07 08:47:53,728 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,728 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:53,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,728 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -970.2820434570312
2023-01-07 08:47:53,729 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.097402811050415
2023-01-07 08:47:53,730 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -179.24136352539062
2023-01-07 08:47:53,730 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,730 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:53,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -967.3236083984375
2023-01-07 08:47:53,730 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.259856700897217
2023-01-07 08:47:53,731 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -964.6140747070312
2023-01-07 08:47:53,731 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:53,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -912.0050048828125
2023-01-07 08:47:53,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.171391010284424
2023-01-07 08:47:53,733 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -44.25251007080078
2023-01-07 08:47:53,733 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,733 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:53,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -916.9923706054688
2023-01-07 08:47:53,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.039754122495651245
2023-01-07 08:47:53,735 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -914.2380981445312
2023-01-07 08:47:53,735 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,735 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:53,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1041.4434814453125
2023-01-07 08:47:53,736 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3885257244110107
2023-01-07 08:47:53,736 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -44.15374755859375
2023-01-07 08:47:53,737 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,737 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:53,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,737 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1039.92919921875
2023-01-07 08:47:53,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43342381715774536
2023-01-07 08:47:53,738 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1038.663330078125
2023-01-07 08:47:53,738 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,739 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:53,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1013.1639404296875
2023-01-07 08:47:53,739 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.856217622756958
2023-01-07 08:47:53,740 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -177.84039306640625
2023-01-07 08:47:53,740 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:53,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1018.6627197265625
2023-01-07 08:47:53,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.312699317932129
2023-01-07 08:47:53,742 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -1017.5111694335938
2023-01-07 08:47:53,742 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,742 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:53,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2762.927490234375
2023-01-07 08:47:53,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11261793971061707
2023-01-07 08:47:53,743 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -43.64105224609375
2023-01-07 08:47:53,743 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:53,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,744 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2765.6015625
2023-01-07 08:47:53,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2070249319076538
2023-01-07 08:47:53,745 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -2768.004638671875
2023-01-07 08:47:53,745 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,745 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:53,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,746 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1057.2178955078125
2023-01-07 08:47:53,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7998523712158203
2023-01-07 08:47:53,747 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -43.029788970947266
2023-01-07 08:47:53,747 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:53,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1057.2476806640625
2023-01-07 08:47:53,748 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04755866527557373
2023-01-07 08:47:53,749 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1057.38623046875
2023-01-07 08:47:53,749 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:53,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1015.9570922851562
2023-01-07 08:47:53,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1366496086120605
2023-01-07 08:47:53,750 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -173.1231689453125
2023-01-07 08:47:53,750 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,750 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,751 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:53,751 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,751 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,751 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1015.4812622070312
2023-01-07 08:47:53,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6745822429656982
2023-01-07 08:47:53,752 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -1015.2015991210938
2023-01-07 08:47:53,752 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,752 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,752 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:53,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1057.1220703125
2023-01-07 08:47:53,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.001204490661621
2023-01-07 08:47:53,754 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -42.38236999511719
2023-01-07 08:47:53,754 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:53,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,754 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1059.515625
2023-01-07 08:47:53,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.08918833732605
2023-01-07 08:47:53,755 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1058.6455078125
2023-01-07 08:47:53,756 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:53,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,756 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2222.966064453125
2023-01-07 08:47:53,756 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24392393231391907
2023-01-07 08:47:53,757 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -42.9332275390625
2023-01-07 08:47:53,757 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,757 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:53,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2222.820556640625
2023-01-07 08:47:53,758 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13137681782245636
2023-01-07 08:47:53,759 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2222.6357421875
2023-01-07 08:47:53,759 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,759 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,759 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:53,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -535.9639282226562
2023-01-07 08:47:53,760 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4644125998020172
2023-01-07 08:47:53,761 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -171.99990844726562
2023-01-07 08:47:53,761 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,761 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:53,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -536.3682250976562
2023-01-07 08:47:53,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5057236552238464
2023-01-07 08:47:53,763 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -537.0682373046875
2023-01-07 08:47:53,763 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,763 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,763 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:53,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,763 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -399.522705078125
2023-01-07 08:47:53,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6609795093536377
2023-01-07 08:47:53,764 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -42.92823028564453
2023-01-07 08:47:53,765 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,765 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,765 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:53,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,765 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -398.771240234375
2023-01-07 08:47:53,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.022904619574546814
2023-01-07 08:47:53,766 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -397.8460693359375
2023-01-07 08:47:53,766 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,766 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,766 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:53,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -593.4930419921875
2023-01-07 08:47:53,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.135049819946289
2023-01-07 08:47:53,768 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -42.842044830322266
2023-01-07 08:47:53,768 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:53,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,769 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -593.010986328125
2023-01-07 08:47:53,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2925708293914795
2023-01-07 08:47:53,770 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -592.2341918945312
2023-01-07 08:47:53,770 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:53,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,770 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 88.81768798828125
2023-01-07 08:47:53,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4269044399261475
2023-01-07 08:47:53,771 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -172.73472595214844
2023-01-07 08:47:53,772 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,772 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,772 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:53,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,772 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 89.88189697265625
2023-01-07 08:47:53,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.196508526802063
2023-01-07 08:47:53,773 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 90.68829345703125
2023-01-07 08:47:53,773 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:53,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,774 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3712.2744140625
2023-01-07 08:47:53,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8132591247558594
2023-01-07 08:47:53,775 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -86.82447814941406
2023-01-07 08:47:53,775 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,775 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,775 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:53,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,775 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3710.21240234375
2023-01-07 08:47:53,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31284403800964355
2023-01-07 08:47:53,777 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -3712.76220703125
2023-01-07 08:47:53,777 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,777 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:53,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,777 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -975.9879760742188
2023-01-07 08:47:53,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6169103980064392
2023-01-07 08:47:53,778 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.4934835135936737
2023-01-07 08:47:53,778 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:53,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,779 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -975.2942504882812
2023-01-07 08:47:53,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.130948781967163
2023-01-07 08:47:53,780 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -975.683837890625
2023-01-07 08:47:53,780 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,780 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,780 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:53,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,781 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2571.147705078125
2023-01-07 08:47:53,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8040383458137512
2023-01-07 08:47:53,782 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -398.36383056640625
2023-01-07 08:47:53,782 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,782 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:53,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,782 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2571.06640625
2023-01-07 08:47:53,782 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9417939186096191
2023-01-07 08:47:53,783 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2570.99853515625
2023-01-07 08:47:53,783 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:53,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,784 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 166.28582763671875
2023-01-07 08:47:53,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.00682464987039566
2023-01-07 08:47:53,785 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -398.454833984375
2023-01-07 08:47:53,785 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:53,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,785 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 166.1788330078125
2023-01-07 08:47:53,786 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7673287391662598
2023-01-07 08:47:53,787 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 167.01226806640625
2023-01-07 08:47:53,787 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,787 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,787 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:53,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,787 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8499.447265625
2023-01-07 08:47:53,787 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9790894985198975
2023-01-07 08:47:53,788 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -87.12498474121094
2023-01-07 08:47:53,789 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:53,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,789 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8499.509765625
2023-01-07 08:47:53,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9455454349517822
2023-01-07 08:47:53,790 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -8499.8798828125
2023-01-07 08:47:53,790 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,790 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,790 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:53,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,791 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1013.5234375
2023-01-07 08:47:53,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1098075732588768
2023-01-07 08:47:53,792 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.03493183106184006
2023-01-07 08:47:53,792 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,792 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:53,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,792 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1014.235107421875
2023-01-07 08:47:53,793 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30233439803123474
2023-01-07 08:47:53,794 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1014.214599609375
2023-01-07 08:47:53,794 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:53,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,794 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7828.01220703125
2023-01-07 08:47:53,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07683896273374557
2023-01-07 08:47:53,795 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -398.0890197753906
2023-01-07 08:47:53,795 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,795 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,796 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:53,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,796 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7827.72119140625
2023-01-07 08:47:53,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23868586122989655
2023-01-07 08:47:53,797 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 7827.72998046875
2023-01-07 08:47:53,797 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,797 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:53,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,798 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -14059.7080078125
2023-01-07 08:47:53,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15626798570156097
2023-01-07 08:47:53,799 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -86.68099975585938
2023-01-07 08:47:53,799 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,799 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:53,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,799 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -14059.73828125
2023-01-07 08:47:53,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24666154384613037
2023-01-07 08:47:53,800 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -14059.6474609375
2023-01-07 08:47:53,800 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,801 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:53,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,801 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1595.424072265625
2023-01-07 08:47:53,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05941019952297211
2023-01-07 08:47:53,802 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -86.68753051757812
2023-01-07 08:47:53,802 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,802 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:53,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,803 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1595.556884765625
2023-01-07 08:47:53,803 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13619489967823029
2023-01-07 08:47:53,804 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -1595.5185546875
2023-01-07 08:47:53,804 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:53,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,804 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 18089.703125
2023-01-07 08:47:53,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.294466018676758
2023-01-07 08:47:53,805 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -398.1531066894531
2023-01-07 08:47:53,806 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:53,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,806 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 18091.07421875
2023-01-07 08:47:53,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1448733806610107
2023-01-07 08:47:53,807 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 18091.90625
2023-01-07 08:47:53,808 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:53,808 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:53,809 > [DEBUG] 0 :: 7.343299865722656
2023-01-07 08:47:53,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,812 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.027496337890625
2023-01-07 08:47:53,812 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,813 > [DEBUG] 0 :: before allreduce fusion buffer :: -275.28387451171875
2023-01-07 08:47:53,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,815 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.34344640374183655
2023-01-07 08:47:53,816 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,816 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -275.9889831542969
2023-01-07 08:47:53,817 > [DEBUG] 0 :: before allreduce fusion buffer :: -301.4296569824219
2023-01-07 08:47:53,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,820 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.1120619773864746
2023-01-07 08:47:53,820 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,821 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06078470125794411
2023-01-07 08:47:53,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,823 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.029469121247529984
2023-01-07 08:47:53,823 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,823 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -59.352169036865234
2023-01-07 08:47:53,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43695202469825745
2023-01-07 08:47:53,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,825 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 0.1282663345336914
2023-01-07 08:47:53,825 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23745214939117432
2023-01-07 08:47:53,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,826 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.02888910286128521
2023-01-07 08:47:53,826 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,827 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -57.99082565307617
2023-01-07 08:47:53,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41461318731307983
2023-01-07 08:47:53,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,828 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.3448200225830078
2023-01-07 08:47:53,828 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.434231162071228
2023-01-07 08:47:53,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,829 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.1926344335079193
2023-01-07 08:47:53,829 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,830 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -58.999534606933594
2023-01-07 08:47:53,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18407325446605682
2023-01-07 08:47:53,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,831 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2.3289170265197754
2023-01-07 08:47:53,831 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05621965229511261
2023-01-07 08:47:53,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,832 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.00864182785153389
2023-01-07 08:47:53,833 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,833 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -56.355648040771484
2023-01-07 08:47:53,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5967090129852295
2023-01-07 08:47:53,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,834 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 18.842853546142578
2023-01-07 08:47:53,835 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,835 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38216736912727356
2023-01-07 08:47:53,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,836 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.04875095188617706
2023-01-07 08:47:53,836 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,836 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -39.27324676513672
2023-01-07 08:47:53,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31647858023643494
2023-01-07 08:47:53,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,838 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 6.70653772354126
2023-01-07 08:47:53,838 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5670837759971619
2023-01-07 08:47:53,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.15617696940898895
2023-01-07 08:47:53,839 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,839 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -51.99512481689453
2023-01-07 08:47:53,839 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29295170307159424
2023-01-07 08:47:53,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,841 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 10.631277084350586
2023-01-07 08:47:53,841 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,841 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6954823732376099
2023-01-07 08:47:53,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,842 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.2283843606710434
2023-01-07 08:47:53,842 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,842 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.687255859375
2023-01-07 08:47:53,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0233843326568604
2023-01-07 08:47:53,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,844 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 2.880690574645996
2023-01-07 08:47:53,844 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,844 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.918593168258667
2023-01-07 08:47:53,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,845 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.04066969081759453
2023-01-07 08:47:53,845 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,846 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -56.686248779296875
2023-01-07 08:47:53,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2606306076049805
2023-01-07 08:47:53,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -10.801788330078125
2023-01-07 08:47:53,847 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.261074423789978
2023-01-07 08:47:53,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.14949119091033936
2023-01-07 08:47:53,849 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,849 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -10.606168746948242
2023-01-07 08:47:53,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34563130140304565
2023-01-07 08:47:53,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -2.5100035667419434
2023-01-07 08:47:53,851 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,851 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4065768718719482
2023-01-07 08:47:53,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,852 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.14989012479782104
2023-01-07 08:47:53,852 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.7492995262145996
2023-01-07 08:47:53,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1297467947006226
2023-01-07 08:47:53,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,854 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.1106786727905273
2023-01-07 08:47:53,854 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8981636166572571
2023-01-07 08:47:53,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,855 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.05774703621864319
2023-01-07 08:47:53,855 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,856 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.6846100091934204
2023-01-07 08:47:53,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35230720043182373
2023-01-07 08:47:53,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,857 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 16.495689392089844
2023-01-07 08:47:53,857 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0757858753204346
2023-01-07 08:47:53,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,858 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.022072337567806244
2023-01-07 08:47:53,859 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,859 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 17.56012725830078
2023-01-07 08:47:53,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.238092303276062
2023-01-07 08:47:53,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,860 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 7.0866498947143555
2023-01-07 08:47:53,860 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.115114450454712
2023-01-07 08:47:53,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.23904183506965637
2023-01-07 08:47:53,862 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.560301780700684
2023-01-07 08:47:53,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3158841133117676
2023-01-07 08:47:53,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,863 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -0.35308218002319336
2023-01-07 08:47:53,864 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,864 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7234700322151184
2023-01-07 08:47:53,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.06529001891613007
2023-01-07 08:47:53,865 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 3.1203603744506836
2023-01-07 08:47:53,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0689321756362915
2023-01-07 08:47:53,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,867 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 18.68642807006836
2023-01-07 08:47:53,867 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,867 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.255235880613327
2023-01-07 08:47:53,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,868 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.13828375935554504
2023-01-07 08:47:53,868 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,868 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 23.85718536376953
2023-01-07 08:47:53,869 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7226149439811707
2023-01-07 08:47:53,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,870 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -7.0905866622924805
2023-01-07 08:47:53,870 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0524587631225586
2023-01-07 08:47:53,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,871 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.1504000574350357
2023-01-07 08:47:53,871 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,871 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -4.5295610427856445
2023-01-07 08:47:53,872 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4277114868164062
2023-01-07 08:47:53,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,873 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -3.4969677925109863
2023-01-07 08:47:53,873 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5270325541496277
2023-01-07 08:47:53,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.08612002432346344
2023-01-07 08:47:53,874 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -4.1808319091796875
2023-01-07 08:47:53,875 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.194877028465271
2023-01-07 08:47:53,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -24.042564392089844
2023-01-07 08:47:53,876 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5702328681945801
2023-01-07 08:47:53,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,877 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.14583644270896912
2023-01-07 08:47:53,878 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -23.057464599609375
2023-01-07 08:47:53,878 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19641533493995667
2023-01-07 08:47:53,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,879 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 13.609360694885254
2023-01-07 08:47:53,879 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,880 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9942061305046082
2023-01-07 08:47:53,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.10328677296638489
2023-01-07 08:47:53,881 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 15.066839218139648
2023-01-07 08:47:53,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7589781284332275
2023-01-07 08:47:53,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 5.382565021514893
2023-01-07 08:47:53,883 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6856703758239746
2023-01-07 08:47:53,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.02739506959915161
2023-01-07 08:47:53,884 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 4.592630863189697
2023-01-07 08:47:53,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1827141046524048
2023-01-07 08:47:53,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,886 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 62.389366149902344
2023-01-07 08:47:53,886 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.960908889770508
2023-01-07 08:47:53,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.0670333206653595
2023-01-07 08:47:53,887 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 62.09156036376953
2023-01-07 08:47:53,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5726087093353271
2023-01-07 08:47:53,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,889 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 23.710697174072266
2023-01-07 08:47:53,889 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3125344514846802
2023-01-07 08:47:53,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.29905781149864197
2023-01-07 08:47:53,890 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,890 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 39.65275573730469
2023-01-07 08:47:53,891 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.633828639984131
2023-01-07 08:47:53,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,892 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -5.576811790466309
2023-01-07 08:47:53,892 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8428170680999756
2023-01-07 08:47:53,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,894 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.015317738056182861
2023-01-07 08:47:53,894 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,894 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.985136985778809
2023-01-07 08:47:53,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8384294509887695
2023-01-07 08:47:53,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,895 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 67.93057250976562
2023-01-07 08:47:53,896 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,896 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4855656921863556
2023-01-07 08:47:53,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,897 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 66.99147033691406
2023-01-07 08:47:53,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1130712032318115
2023-01-07 08:47:53,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,898 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.535327911376953
2023-01-07 08:47:53,898 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,898 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.20922589302063
2023-01-07 08:47:53,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.05661493539810181
2023-01-07 08:47:53,900 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,900 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -9.689529418945312
2023-01-07 08:47:53,900 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.27071475982666
2023-01-07 08:47:53,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,901 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 83.14808654785156
2023-01-07 08:47:53,901 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,902 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1889632940292358
2023-01-07 08:47:53,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.42237788438796997
2023-01-07 08:47:53,903 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 91.8803939819336
2023-01-07 08:47:53,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.206187725067139
2023-01-07 08:47:53,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 40.86419677734375
2023-01-07 08:47:53,905 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5307954549789429
2023-01-07 08:47:53,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,906 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.49253523349761963
2023-01-07 08:47:53,906 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,906 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 40.427162170410156
2023-01-07 08:47:53,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.302359104156494
2023-01-07 08:47:53,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 240.6049346923828
2023-01-07 08:47:53,908 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.295348882675171
2023-01-07 08:47:53,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,909 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 238.6622772216797
2023-01-07 08:47:53,909 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.651163101196289
2023-01-07 08:47:53,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,911 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 37.649837493896484
2023-01-07 08:47:53,911 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.378457546234131
2023-01-07 08:47:53,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,912 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 23.05858612060547
2023-01-07 08:47:53,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.024290084838867
2023-01-07 08:47:53,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,913 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 1.3985567092895508
2023-01-07 08:47:53,913 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,914 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4439384937286377
2023-01-07 08:47:53,914 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,914 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,915 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.7682218551635742
2023-01-07 08:47:53,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.465867042541504
2023-01-07 08:47:53,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,916 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 76.64332580566406
2023-01-07 08:47:53,916 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,916 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7225346565246582
2023-01-07 08:47:53,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,917 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 64.56497192382812
2023-01-07 08:47:53,918 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.595804214477539
2023-01-07 08:47:53,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,919 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 2.69474458694458
2023-01-07 08:47:53,919 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.732649803161621
2023-01-07 08:47:53,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,920 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -31.259063720703125
2023-01-07 08:47:53,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2467998266220093
2023-01-07 08:47:53,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,921 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 32.958473205566406
2023-01-07 08:47:53,922 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,922 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8758472204208374
2023-01-07 08:47:53,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,923 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 31.336864471435547
2023-01-07 08:47:53,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8457707762718201
2023-01-07 08:47:53,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,924 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -2.118633270263672
2023-01-07 08:47:53,924 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,924 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.634618759155273
2023-01-07 08:47:53,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,925 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,926 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -7.8977508544921875
2023-01-07 08:47:53,926 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.176300048828125
2023-01-07 08:47:53,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,927 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 11.174492835998535
2023-01-07 08:47:53,927 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.246570110321045
2023-01-07 08:47:53,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,928 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.7591583728790283
2023-01-07 08:47:53,928 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,929 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 4.846624851226807
2023-01-07 08:47:53,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6590580940246582
2023-01-07 08:47:53,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,930 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 25.170604705810547
2023-01-07 08:47:53,931 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44105035066604614
2023-01-07 08:47:53,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,932 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 11.025577545166016
2023-01-07 08:47:53,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.090393304824829
2023-01-07 08:47:53,933 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,933 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,933 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -303.2428283691406
2023-01-07 08:47:53,933 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.445919990539551
2023-01-07 08:47:53,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,934 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -313.7088317871094
2023-01-07 08:47:53,935 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.195384979248047
2023-01-07 08:47:53,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,936 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 10.623010635375977
2023-01-07 08:47:53,936 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,936 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6278700828552246
2023-01-07 08:47:53,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,937 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.0395264625549316
2023-01-07 08:47:53,937 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,937 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,937 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,938 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1.3415288925170898
2023-01-07 08:47:53,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.058028221130371
2023-01-07 08:47:53,939 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,940 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 107.02875518798828
2023-01-07 08:47:53,940 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,940 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.541314125061035
2023-01-07 08:47:53,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,942 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 221.23350524902344
2023-01-07 08:47:53,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.405267715454102
2023-01-07 08:47:53,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,943 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 23.191791534423828
2023-01-07 08:47:53,943 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,943 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.364994049072266
2023-01-07 08:47:53,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,944 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -812.6748657226562
2023-01-07 08:47:53,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7374473810195923
2023-01-07 08:47:53,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,946 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1.6787948608398438
2023-01-07 08:47:53,946 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.646283149719238
2023-01-07 08:47:53,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,947 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.0456540584564209
2023-01-07 08:47:53,947 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,947 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -745.8222045898438
2023-01-07 08:47:53,948 > [DEBUG] 0 :: before allreduce fusion buffer :: -73.95153045654297
2023-01-07 08:47:53,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,949 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1041.214599609375
2023-01-07 08:47:53,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.834924697875977
2023-01-07 08:47:53,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,950 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.03878617286682129
2023-01-07 08:47:53,950 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,950 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -29.93264389038086
2023-01-07 08:47:53,951 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,951 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,951 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,951 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2140.32080078125
2023-01-07 08:47:53,951 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.587953567504883
2023-01-07 08:47:53,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,952 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2139.899658203125
2023-01-07 08:47:53,953 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.101428985595703
2023-01-07 08:47:53,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,954 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -574.41943359375
2023-01-07 08:47:53,954 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.2360954284668
2023-01-07 08:47:53,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,955 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 30.711423873901367
2023-01-07 08:47:53,955 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,955 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.48187828063965
2023-01-07 08:47:53,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,956 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.3752257823944092
2023-01-07 08:47:53,956 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,957 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 176.27093505859375
2023-01-07 08:47:53,957 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.874250411987305
2023-01-07 08:47:53,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,958 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 288.725830078125
2023-01-07 08:47:53,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.48272705078125
2023-01-07 08:47:53,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,959 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 3.1114978790283203
2023-01-07 08:47:53,960 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,960 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 9.102248191833496
2023-01-07 08:47:53,960 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,960 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.43101119995117
2023-01-07 08:47:53,961 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,961 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,961 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 429.0499572753906
2023-01-07 08:47:53,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.841268539428711
2023-01-07 08:47:53,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,962 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -1.3860664367675781
2023-01-07 08:47:53,963 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,963 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1965.685546875
2023-01-07 08:47:53,963 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.916389465332031
2023-01-07 08:47:53,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,964 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 591.9488525390625
2023-01-07 08:47:53,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.040754318237305
2023-01-07 08:47:53,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,966 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -85.67860412597656
2023-01-07 08:47:53,966 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.019813537597656
2023-01-07 08:47:53,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,967 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 762.9248046875
2023-01-07 08:47:53,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.74034881591797
2023-01-07 08:47:53,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.4408440589904785
2023-01-07 08:47:53,969 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -71.0851058959961
2023-01-07 08:47:53,969 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1043.2718505859375
2023-01-07 08:47:53,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,970 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2003.238525390625
2023-01-07 08:47:53,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.185551881790161
2023-01-07 08:47:53,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,971 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1025.1455078125
2023-01-07 08:47:53,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.528234481811523
2023-01-07 08:47:53,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -278.74591064453125
2023-01-07 08:47:53,972 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,973 > [DEBUG] 0 :: before allreduce fusion buffer :: -274.6495666503906
2023-01-07 08:47:53,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,974 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1182.0565185546875
2023-01-07 08:47:53,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.405807495117188
2023-01-07 08:47:53,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,975 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -169.8757781982422
2023-01-07 08:47:53,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,975 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2004.83740234375
2023-01-07 08:47:53,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.097443103790283
2023-01-07 08:47:53,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,977 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 792.015869140625
2023-01-07 08:47:53,977 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 171.73182678222656
2023-01-07 08:47:53,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,978 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -4.1400299072265625
2023-01-07 08:47:53,978 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,978 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1039.619384765625
2023-01-07 08:47:53,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 92.55509185791016
2023-01-07 08:47:53,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,980 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1134.59814453125
2023-01-07 08:47:53,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 116.29166412353516
2023-01-07 08:47:53,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,981 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1580.9921875
2023-01-07 08:47:53,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 97.10608673095703
2023-01-07 08:47:53,984 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:47:53,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,984 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -4159.79443359375
2023-01-07 08:47:53,985 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:53,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,985 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 2126.89306640625
2023-01-07 08:47:53,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,985 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -2011.1455078125
2023-01-07 08:47:53,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,985 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -448.608642578125
2023-01-07 08:47:53,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -959.39013671875
2023-01-07 08:47:53,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -74.40152740478516
2023-01-07 08:47:53,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -388.96441650390625
2023-01-07 08:47:53,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,986 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 121.39221954345703
2023-01-07 08:47:53,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,987 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -31.449199676513672
2023-01-07 08:47:53,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,987 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -84.24829864501953
2023-01-07 08:47:53,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,987 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 26.731155395507812
2023-01-07 08:47:53,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,988 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.31475257873535
2023-01-07 08:47:53,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,988 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 5.345632553100586
2023-01-07 08:47:53,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,988 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -38.020042419433594
2023-01-07 08:47:53,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 18.120914459228516
2023-01-07 08:47:53,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 248.6184539794922
2023-01-07 08:47:53,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.78623962402344
2023-01-07 08:47:53,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 115.34992218017578
2023-01-07 08:47:53,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.638654708862305
2023-01-07 08:47:53,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 53.947444915771484
2023-01-07 08:47:53,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.623753547668457
2023-01-07 08:47:53,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 38.630043029785156
2023-01-07 08:47:53,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 64.33621978759766
2023-01-07 08:47:53,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 12.893896102905273
2023-01-07 08:47:53,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 19.26849937438965
2023-01-07 08:47:53,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -17.973644256591797
2023-01-07 08:47:53,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -3361.175048828125
2023-01-07 08:47:53,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,994 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -11.502408027648926
2023-01-07 08:47:53,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,994 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -4.420588493347168
2023-01-07 08:47:53,994 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,994 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 26.795257568359375
2023-01-07 08:47:53,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 4.1475677490234375
2023-01-07 08:47:53,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.922591209411621
2023-01-07 08:47:53,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 19.770357131958008
2023-01-07 08:47:53,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,996 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.8793153762817383
2023-01-07 08:47:53,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,996 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.8911914825439453
2023-01-07 08:47:53,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,996 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -9.910167694091797
2023-01-07 08:47:53,997 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.277702331542969
2023-01-07 08:47:53,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,998 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -56.55152893066406
2023-01-07 08:47:53,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,998 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -47.14209747314453
2023-01-07 08:47:53,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,998 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -53.36372375488281
2023-01-07 08:47:53,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:53,999 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -39.548057556152344
2023-01-07 08:47:53,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.272552490234375
2023-01-07 08:47:53,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:53,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,000 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -55.99451446533203
2023-01-07 08:47:54,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,000 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -58.48515701293945
2023-01-07 08:47:54,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,000 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -57.92558288574219
2023-01-07 08:47:54,000 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.151878356933594
2023-01-07 08:47:54,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,001 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -59.59217834472656
2023-01-07 08:47:54,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,001 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -612.760986328125
2023-01-07 08:47:54,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 569.992919921875
2023-01-07 08:47:54,843 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 322.9830322265625
2023-01-07 08:47:54,843 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,844 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,844 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:54,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,844 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 3.009243965148926
2023-01-07 08:47:54,844 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,844 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1853.9969482421875
2023-01-07 08:47:54,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.423744201660156
2023-01-07 08:47:54,846 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 2.1792633533477783
2023-01-07 08:47:54,846 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,846 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.6681442260742188
2023-01-07 08:47:54,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,846 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -153.8416748046875
2023-01-07 08:47:54,846 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,847 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1757.162109375
2023-01-07 08:47:54,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -168.93197631835938
2023-01-07 08:47:54,848 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -234.85134887695312
2023-01-07 08:47:54,848 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,848 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,848 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:54,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,849 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1772.176513671875
2023-01-07 08:47:54,849 > [DEBUG] 0 :: before allreduce fusion buffer :: -101.02640533447266
2023-01-07 08:47:54,850 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 38.08197021484375
2023-01-07 08:47:54,850 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,850 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,850 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:54,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,850 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1713.4351806640625
2023-01-07 08:47:54,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 191.4892120361328
2023-01-07 08:47:54,851 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 1796.51123046875
2023-01-07 08:47:54,851 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,851 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,852 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:54,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,852 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 2.4951894283294678
2023-01-07 08:47:54,852 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,852 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -26.318744659423828
2023-01-07 08:47:54,852 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,852 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1380.8885498046875
2023-01-07 08:47:54,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.306922912597656
2023-01-07 08:47:54,854 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 1.7492461204528809
2023-01-07 08:47:54,854 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,854 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,854 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:54,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,854 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1392.597900390625
2023-01-07 08:47:54,855 > [DEBUG] 0 :: before allreduce fusion buffer :: 99.4796142578125
2023-01-07 08:47:54,856 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -79.24604797363281
2023-01-07 08:47:54,856 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,856 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:54,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,856 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.5732295513153076
2023-01-07 08:47:54,856 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,856 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 105.290771484375
2023-01-07 08:47:54,857 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.374813079833984
2023-01-07 08:47:54,858 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.75058913230896
2023-01-07 08:47:54,858 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,858 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,858 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:54,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,858 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 32.74191665649414
2023-01-07 08:47:54,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -182.40150451660156
2023-01-07 08:47:54,859 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -295.7874755859375
2023-01-07 08:47:54,859 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,859 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,859 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:54,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,860 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 82.64373779296875
2023-01-07 08:47:54,860 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.677544116973877
2023-01-07 08:47:54,861 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -39.56736373901367
2023-01-07 08:47:54,861 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,861 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,861 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:54,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,861 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 83.80282592773438
2023-01-07 08:47:54,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.946070671081543
2023-01-07 08:47:54,863 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 91.9259262084961
2023-01-07 08:47:54,863 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:54,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,863 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.8835667371749878
2023-01-07 08:47:54,863 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,863 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 172.02505493164062
2023-01-07 08:47:54,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.893539428710938
2023-01-07 08:47:54,865 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.19855785369873047
2023-01-07 08:47:54,865 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:54,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,865 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 295.2999267578125
2023-01-07 08:47:54,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,866 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1978.98291015625
2023-01-07 08:47:54,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 89.6969223022461
2023-01-07 08:47:54,867 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 291.6929931640625
2023-01-07 08:47:54,867 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,867 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,867 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:54,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,867 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -3.0594449043273926
2023-01-07 08:47:54,868 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,868 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 125.42784118652344
2023-01-07 08:47:54,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.342609405517578
2023-01-07 08:47:54,869 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -2.276177406311035
2023-01-07 08:47:54,869 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,869 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,869 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:54,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,869 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1373.99365234375
2023-01-07 08:47:54,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.204286575317383
2023-01-07 08:47:54,871 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 173.4424591064453
2023-01-07 08:47:54,871 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,871 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,871 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:54,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,871 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 162.9622344970703
2023-01-07 08:47:54,871 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.28518295288086
2023-01-07 08:47:54,872 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -43.0030517578125
2023-01-07 08:47:54,872 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,873 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,873 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:54,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,873 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 181.6950225830078
2023-01-07 08:47:54,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,873 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1316.02978515625
2023-01-07 08:47:54,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.726297378540039
2023-01-07 08:47:54,874 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 177.07254028320312
2023-01-07 08:47:54,875 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,875 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,875 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:54,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,875 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1342.18603515625
2023-01-07 08:47:54,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.747314453125
2023-01-07 08:47:54,876 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -0.8092410564422607
2023-01-07 08:47:54,876 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,876 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,876 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:54,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,877 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 1332.53271484375
2023-01-07 08:47:54,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.05696105957031
2023-01-07 08:47:54,878 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 1330.1376953125
2023-01-07 08:47:54,878 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,878 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,878 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:54,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,878 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -2.32682466506958
2023-01-07 08:47:54,878 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,879 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -33.934532165527344
2023-01-07 08:47:54,879 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,879 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,879 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -417.462646484375
2023-01-07 08:47:54,879 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.30414962768555
2023-01-07 08:47:54,881 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -2.5899972915649414
2023-01-07 08:47:54,881 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,881 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,881 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:54,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,881 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1963.850341796875
2023-01-07 08:47:54,881 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.25382995605469
2023-01-07 08:47:54,882 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -9.228665351867676
2023-01-07 08:47:54,882 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,883 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,883 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -32.44528579711914
2023-01-07 08:47:54,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,883 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -424.705322265625
2023-01-07 08:47:54,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.33367919921875
2023-01-07 08:47:54,884 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -26.979259490966797
2023-01-07 08:47:54,884 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,884 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,884 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:54,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,885 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1979.7000732421875
2023-01-07 08:47:54,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.578657150268555
2023-01-07 08:47:54,886 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -419.18206787109375
2023-01-07 08:47:54,886 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,886 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,886 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:54,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,886 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1976.30908203125
2023-01-07 08:47:54,887 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.82009220123291
2023-01-07 08:47:54,887 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -7.748607635498047
2023-01-07 08:47:54,888 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,888 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,888 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:54,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,888 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1979.5438232421875
2023-01-07 08:47:54,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.6883134841918945
2023-01-07 08:47:54,889 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -2013.2152099609375
2023-01-07 08:47:54,889 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,889 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,889 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,890 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.11595511436462402
2023-01-07 08:47:54,890 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,890 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,890 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -456.24688720703125
2023-01-07 08:47:54,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.055654525756836
2023-01-07 08:47:54,891 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.6062942743301392
2023-01-07 08:47:54,891 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,891 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,891 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -13.1309175491333
2023-01-07 08:47:54,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,892 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -443.08648681640625
2023-01-07 08:47:54,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.198883056640625
2023-01-07 08:47:54,893 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -445.34710693359375
2023-01-07 08:47:54,893 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,893 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,893 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:54,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,893 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.09030759334564209
2023-01-07 08:47:54,893 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,894 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1049.973388671875
2023-01-07 08:47:54,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8525283336639404
2023-01-07 08:47:54,895 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.5344178676605225
2023-01-07 08:47:54,895 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,895 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,895 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.17344856262207
2023-01-07 08:47:54,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,895 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1047.21044921875
2023-01-07 08:47:54,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.009199142456055
2023-01-07 08:47:54,896 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1043.1514892578125
2023-01-07 08:47:54,897 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,897 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,897 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:54,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,897 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -77.31773376464844
2023-01-07 08:47:54,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.29225492477417
2023-01-07 08:47:54,898 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.034331321716309
2023-01-07 08:47:54,898 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,898 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,898 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:54,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,899 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -81.03665161132812
2023-01-07 08:47:54,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.1223783493042
2023-01-07 08:47:54,900 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -88.05276489257812
2023-01-07 08:47:54,900 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,900 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,900 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,900 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.48990750312805176
2023-01-07 08:47:54,900 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,901 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -141.86732482910156
2023-01-07 08:47:54,901 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.72380828857422
2023-01-07 08:47:54,902 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.8427042961120605
2023-01-07 08:47:54,902 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,902 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,902 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.533244132995605
2023-01-07 08:47:54,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,902 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -147.9616241455078
2023-01-07 08:47:54,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.052828788757324
2023-01-07 08:47:54,904 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -130.73696899414062
2023-01-07 08:47:54,904 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,904 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,904 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,904 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.33749866485595703
2023-01-07 08:47:54,904 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,904 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 105.9356918334961
2023-01-07 08:47:54,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1626925468444824
2023-01-07 08:47:54,906 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.6000254154205322
2023-01-07 08:47:54,906 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,906 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,906 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 31.334651947021484
2023-01-07 08:47:54,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,906 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 110.17242431640625
2023-01-07 08:47:54,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2911640405654907
2023-01-07 08:47:54,907 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 105.33500671386719
2023-01-07 08:47:54,907 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,907 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,908 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:54,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,908 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -31.097576141357422
2023-01-07 08:47:54,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7229055166244507
2023-01-07 08:47:54,909 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -13.026065826416016
2023-01-07 08:47:54,909 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,909 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,909 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:54,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,909 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -29.45629119873047
2023-01-07 08:47:54,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6291377544403076
2023-01-07 08:47:54,911 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -29.509353637695312
2023-01-07 08:47:54,911 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,911 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,911 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,911 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.40200909972190857
2023-01-07 08:47:54,911 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,912 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -84.42646789550781
2023-01-07 08:47:54,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.6863789558410645
2023-01-07 08:47:54,913 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.502395749092102
2023-01-07 08:47:54,913 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,913 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,913 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.319789171218872
2023-01-07 08:47:54,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,913 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -90.34150695800781
2023-01-07 08:47:54,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.275161266326904
2023-01-07 08:47:54,914 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -79.17536926269531
2023-01-07 08:47:54,915 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,915 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,915 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,915 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.1329137086868286
2023-01-07 08:47:54,915 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,915 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 27.246429443359375
2023-01-07 08:47:54,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2073042392730713
2023-01-07 08:47:54,916 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.29888904094696045
2023-01-07 08:47:54,917 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,917 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,917 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 6.020914077758789
2023-01-07 08:47:54,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,917 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 25.029850006103516
2023-01-07 08:47:54,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.783796310424805
2023-01-07 08:47:54,918 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 20.917144775390625
2023-01-07 08:47:54,918 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,918 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,918 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:54,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,919 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.3841164708137512
2023-01-07 08:47:54,919 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,919 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,919 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,919 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.77766227722168
2023-01-07 08:47:54,919 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.257412910461426
2023-01-07 08:47:54,920 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -1.2904084920883179
2023-01-07 08:47:54,920 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,920 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,920 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 29.095399856567383
2023-01-07 08:47:54,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,921 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -36.0971794128418
2023-01-07 08:47:54,921 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0063800811767578125
2023-01-07 08:47:54,922 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -39.897857666015625
2023-01-07 08:47:54,922 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,922 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,922 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,922 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,922 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,922 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.10825739800930023
2023-01-07 08:47:54,923 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,923 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 6.3167266845703125
2023-01-07 08:47:54,923 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.589014053344727
2023-01-07 08:47:54,924 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.1441088616847992
2023-01-07 08:47:54,924 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,924 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,924 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -7.9246392250061035
2023-01-07 08:47:54,924 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,924 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,924 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 33.95044708251953
2023-01-07 08:47:54,925 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.298115253448486
2023-01-07 08:47:54,926 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 42.673675537109375
2023-01-07 08:47:54,926 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,926 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,926 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:54,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,927 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.38600897789001465
2023-01-07 08:47:54,927 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,927 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,927 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,927 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -37.626182556152344
2023-01-07 08:47:54,927 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6495842933654785
2023-01-07 08:47:54,928 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.2954140305519104
2023-01-07 08:47:54,928 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,928 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,928 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.432145118713379
2023-01-07 08:47:54,929 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,929 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -33.7274169921875
2023-01-07 08:47:54,929 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3750290870666504
2023-01-07 08:47:54,930 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -36.60150146484375
2023-01-07 08:47:54,930 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,930 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,930 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:54,930 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,930 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,930 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.12601006031036377
2023-01-07 08:47:54,930 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,931 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,931 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,931 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.232741355895996
2023-01-07 08:47:54,931 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.632767200469971
2023-01-07 08:47:54,932 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.3872734308242798
2023-01-07 08:47:54,932 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,932 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,932 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -22.02167510986328
2023-01-07 08:47:54,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,932 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -9.249368667602539
2023-01-07 08:47:54,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41088828444480896
2023-01-07 08:47:54,934 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -5.040931701660156
2023-01-07 08:47:54,934 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,934 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,934 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:54,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,934 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.4809773862361908
2023-01-07 08:47:54,934 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,934 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -528.6262817382812
2023-01-07 08:47:54,935 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.098094940185547
2023-01-07 08:47:54,936 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.15705683827400208
2023-01-07 08:47:54,936 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,936 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,936 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.97386360168457
2023-01-07 08:47:54,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,936 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -536.3887329101562
2023-01-07 08:47:54,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.247522354125977
2023-01-07 08:47:54,937 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -542.8806762695312
2023-01-07 08:47:54,938 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,938 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,938 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:54,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,938 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 158.2333984375
2023-01-07 08:47:54,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.165005087852478
2023-01-07 08:47:54,939 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.4902404546737671
2023-01-07 08:47:54,939 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,939 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,939 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:54,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,940 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 165.00997924804688
2023-01-07 08:47:54,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.437332630157471
2023-01-07 08:47:54,941 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 169.92991638183594
2023-01-07 08:47:54,941 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,941 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,941 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:54,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,941 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 501.4483947753906
2023-01-07 08:47:54,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.519883155822754
2023-01-07 08:47:54,943 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 14.829877853393555
2023-01-07 08:47:54,943 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,943 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,943 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:54,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,943 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 501.4364013671875
2023-01-07 08:47:54,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.379855155944824
2023-01-07 08:47:54,944 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 508.2941589355469
2023-01-07 08:47:54,944 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,944 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,945 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:54,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,945 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 261.214599609375
2023-01-07 08:47:54,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.017765998840332
2023-01-07 08:47:54,946 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -4.578787803649902
2023-01-07 08:47:54,946 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,946 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,946 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:54,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,946 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 259.0451354980469
2023-01-07 08:47:54,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.5121307373046875
2023-01-07 08:47:54,948 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 255.9478759765625
2023-01-07 08:47:54,948 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,948 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,948 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:54,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.05635377764701843
2023-01-07 08:47:54,948 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:47:54,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,948 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 602.9768676757812
2023-01-07 08:47:54,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4204484224319458
2023-01-07 08:47:54,950 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.37355923652648926
2023-01-07 08:47:54,950 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,950 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,950 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.42170524597168
2023-01-07 08:47:54,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,950 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 607.7222290039062
2023-01-07 08:47:54,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.589256763458252
2023-01-07 08:47:54,952 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 610.4116821289062
2023-01-07 08:47:54,952 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,952 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,952 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:54,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,952 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 142.9886474609375
2023-01-07 08:47:54,952 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0868468284606934
2023-01-07 08:47:54,953 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 1.6635781526565552
2023-01-07 08:47:54,953 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,953 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,953 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:54,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,954 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 144.16363525390625
2023-01-07 08:47:54,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.540499448776245
2023-01-07 08:47:54,955 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 142.96044921875
2023-01-07 08:47:54,955 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,955 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,955 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:54,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,955 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.610001564025879
2023-01-07 08:47:54,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3684005737304688
2023-01-07 08:47:54,956 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 8.683860778808594
2023-01-07 08:47:54,957 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,957 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,957 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:54,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,957 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.54608154296875
2023-01-07 08:47:54,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.182236671447754
2023-01-07 08:47:54,958 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 7.4102888107299805
2023-01-07 08:47:54,958 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,958 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,959 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:54,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,959 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 2.3642871379852295
2023-01-07 08:47:54,959 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9399876594543457
2023-01-07 08:47:54,960 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 2.3915534019470215
2023-01-07 08:47:54,960 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,960 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,960 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:54,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,960 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 0.5779005885124207
2023-01-07 08:47:54,961 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9366557598114014
2023-01-07 08:47:54,962 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 1.5507118701934814
2023-01-07 08:47:54,962 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,962 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,962 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:54,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,962 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 4.02442741394043
2023-01-07 08:47:54,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.41188567876815796
2023-01-07 08:47:54,963 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 1.0524370670318604
2023-01-07 08:47:54,963 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,963 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,964 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:54,964 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,964 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,964 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 4.493172645568848
2023-01-07 08:47:54,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.36152464151382446
2023-01-07 08:47:54,965 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 3.6977601051330566
2023-01-07 08:47:54,965 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,965 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,965 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:54,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,965 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 5.387682914733887
2023-01-07 08:47:54,966 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0108463764190674
2023-01-07 08:47:54,967 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -1.6248571872711182
2023-01-07 08:47:54,967 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,967 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,967 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:54,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,967 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 6.033219337463379
2023-01-07 08:47:54,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9877267479896545
2023-01-07 08:47:54,968 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 8.242161750793457
2023-01-07 08:47:54,969 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,969 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,969 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:54,969 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,969 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 657.732421875
2023-01-07 08:47:54,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21055418252944946
2023-01-07 08:47:54,970 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.6764718890190125
2023-01-07 08:47:54,970 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,970 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,970 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:54,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,971 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 656.6593017578125
2023-01-07 08:47:54,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9567089080810547
2023-01-07 08:47:54,972 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 655.8907470703125
2023-01-07 08:47:54,972 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,972 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,972 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:54,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -9.615823745727539
2023-01-07 08:47:54,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1802613735198975
2023-01-07 08:47:54,973 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.6111910343170166
2023-01-07 08:47:54,974 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,974 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,974 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:54,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,974 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -10.539306640625
2023-01-07 08:47:54,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7959991693496704
2023-01-07 08:47:54,975 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -10.517909049987793
2023-01-07 08:47:54,975 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,975 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,975 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:54,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 8.801294326782227
2023-01-07 08:47:54,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6597375869750977
2023-01-07 08:47:54,977 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 1.8901894092559814
2023-01-07 08:47:54,977 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,977 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,977 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:54,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,977 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 9.65945816040039
2023-01-07 08:47:54,977 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9274857044219971
2023-01-07 08:47:54,979 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 10.462371826171875
2023-01-07 08:47:54,979 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,979 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,979 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:54,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,979 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 373.66339111328125
2023-01-07 08:47:54,979 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5726072192192078
2023-01-07 08:47:54,980 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.9678112268447876
2023-01-07 08:47:54,980 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,980 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,981 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:54,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,981 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 370.6883544921875
2023-01-07 08:47:54,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3775215744972229
2023-01-07 08:47:54,982 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 370.9625244140625
2023-01-07 08:47:54,982 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,982 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,982 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:54,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,982 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1618.9171142578125
2023-01-07 08:47:54,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22457991540431976
2023-01-07 08:47:54,984 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 1.2162048816680908
2023-01-07 08:47:54,984 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,984 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,984 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:54,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,984 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1620.2061767578125
2023-01-07 08:47:54,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2777445316314697
2023-01-07 08:47:54,985 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1620.34912109375
2023-01-07 08:47:54,985 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,985 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,986 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:54,986 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 418.3106689453125
2023-01-07 08:47:54,986 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8098456859588623
2023-01-07 08:47:54,987 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 3.8531455993652344
2023-01-07 08:47:54,987 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,987 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,987 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:54,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 417.62109375
2023-01-07 08:47:54,988 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6255069375038147
2023-01-07 08:47:54,989 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 417.9132385253906
2023-01-07 08:47:54,989 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,989 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,989 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:54,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 858.73193359375
2023-01-07 08:47:54,989 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3203837871551514
2023-01-07 08:47:54,991 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.47314774990081787
2023-01-07 08:47:54,991 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,991 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,991 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:54,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 858.9715576171875
2023-01-07 08:47:54,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3719331622123718
2023-01-07 08:47:54,993 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 859.468994140625
2023-01-07 08:47:54,993 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,993 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,993 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:54,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,994 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 481.61871337890625
2023-01-07 08:47:54,994 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0597977340221405
2023-01-07 08:47:54,995 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.15717440843582153
2023-01-07 08:47:54,995 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,995 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,995 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:54,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 480.77874755859375
2023-01-07 08:47:54,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.697583019733429
2023-01-07 08:47:54,996 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 480.71075439453125
2023-01-07 08:47:54,997 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,997 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,997 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:54,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,997 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1132.0877685546875
2023-01-07 08:47:54,997 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.156518578529358
2023-01-07 08:47:54,998 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.09960056841373444
2023-01-07 08:47:54,998 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:54,998 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:54,998 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:54,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:54,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:54,999 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1133.2490234375
2023-01-07 08:47:54,999 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11933128535747528
2023-01-07 08:47:55,000 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1131.93896484375
2023-01-07 08:47:55,000 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,000 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,000 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:55,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,000 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2184.130859375
2023-01-07 08:47:55,001 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07222472876310349
2023-01-07 08:47:55,002 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0641760528087616
2023-01-07 08:47:55,002 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,002 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,002 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:55,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,002 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 2184.600830078125
2023-01-07 08:47:55,002 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3902495801448822
2023-01-07 08:47:55,003 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 2184.34521484375
2023-01-07 08:47:55,003 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,003 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,004 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:55,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,004 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 648.0328369140625
2023-01-07 08:47:55,004 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7512763738632202
2023-01-07 08:47:55,005 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.16166290640830994
2023-01-07 08:47:55,005 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,005 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,005 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:55,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,006 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 648.8231201171875
2023-01-07 08:47:55,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14499256014823914
2023-01-07 08:47:55,007 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 649.1152954101562
2023-01-07 08:47:55,007 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,007 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,007 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:55,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,007 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3092.97607421875
2023-01-07 08:47:55,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7233550548553467
2023-01-07 08:47:55,008 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -59.635047912597656
2023-01-07 08:47:55,009 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,009 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,009 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:55,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,009 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3094.66162109375
2023-01-07 08:47:55,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17656145989894867
2023-01-07 08:47:55,010 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 3094.75048828125
2023-01-07 08:47:55,010 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,010 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,010 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:55,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,011 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1074.70947265625
2023-01-07 08:47:55,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1634466648101807
2023-01-07 08:47:55,012 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.8386672735214233
2023-01-07 08:47:55,012 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,012 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,012 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:55,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,012 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1074.356689453125
2023-01-07 08:47:55,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2820896804332733
2023-01-07 08:47:55,014 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1074.794189453125
2023-01-07 08:47:55,014 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,014 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,014 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:55,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,014 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3989.922119140625
2023-01-07 08:47:55,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8796740770339966
2023-01-07 08:47:55,015 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -59.089229583740234
2023-01-07 08:47:55,015 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,016 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:55,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,016 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3990.0634765625
2023-01-07 08:47:55,016 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08000503480434418
2023-01-07 08:47:55,017 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 3990.4111328125
2023-01-07 08:47:55,017 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,017 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,017 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:55,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,017 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1421.163818359375
2023-01-07 08:47:55,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.41986584663391113
2023-01-07 08:47:55,019 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.08862607181072235
2023-01-07 08:47:55,019 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:55,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,019 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1421.32275390625
2023-01-07 08:47:55,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08756392449140549
2023-01-07 08:47:55,020 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -1421.20458984375
2023-01-07 08:47:55,020 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:55,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,021 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8762.2724609375
2023-01-07 08:47:55,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2961638271808624
2023-01-07 08:47:55,022 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -58.83890151977539
2023-01-07 08:47:55,022 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,022 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,022 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:55,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,023 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8762.55078125
2023-01-07 08:47:55,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32637566328048706
2023-01-07 08:47:55,024 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8762.498046875
2023-01-07 08:47:55,024 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,024 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:55,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,024 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6149.7900390625
2023-01-07 08:47:55,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25498056411743164
2023-01-07 08:47:55,025 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -58.48936080932617
2023-01-07 08:47:55,026 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:55,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,026 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6150.07080078125
2023-01-07 08:47:55,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10919111222028732
2023-01-07 08:47:55,027 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 6150.15478515625
2023-01-07 08:47:55,027 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,027 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,027 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:55,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,028 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2938.138916015625
2023-01-07 08:47:55,028 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13468247652053833
2023-01-07 08:47:55,029 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.1316206157207489
2023-01-07 08:47:55,029 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,029 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:55,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,029 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2938.17431640625
2023-01-07 08:47:55,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1154385358095169
2023-01-07 08:47:55,031 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 2938.18212890625
2023-01-07 08:47:55,031 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,031 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,031 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:55,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,031 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 15901.0634765625
2023-01-07 08:47:55,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0732507705688477
2023-01-07 08:47:55,032 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -423.2245178222656
2023-01-07 08:47:55,032 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,033 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:55,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,033 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 15902.4521484375
2023-01-07 08:47:55,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0910686254501343
2023-01-07 08:47:55,034 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 15903.2802734375
2023-01-07 08:47:55,034 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:55,034 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:55,035 > [DEBUG] 0 :: 7.186309337615967
2023-01-07 08:47:55,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,039 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0067138671875
2023-01-07 08:47:55,039 > [DEBUG] 0 :: before allreduce fusion buffer :: -303.62939453125
2023-01-07 08:47:55,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,042 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1617850363254547
2023-01-07 08:47:55,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,042 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -243.0721435546875
2023-01-07 08:47:55,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -383.40191650390625
2023-01-07 08:47:55,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,046 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.948441505432129
2023-01-07 08:47:55,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15586209297180176
2023-01-07 08:47:55,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,048 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.014765674248337746
2023-01-07 08:47:55,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,049 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4.909876823425293
2023-01-07 08:47:55,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10649394989013672
2023-01-07 08:47:55,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,052 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 5.264651775360107
2023-01-07 08:47:55,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14206238090991974
2023-01-07 08:47:55,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,053 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.00880400836467743
2023-01-07 08:47:55,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,053 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.915533542633057
2023-01-07 08:47:55,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.054168701171875
2023-01-07 08:47:55,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,055 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.077630996704102
2023-01-07 08:47:55,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14779134094715118
2023-01-07 08:47:55,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,056 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.10142118483781815
2023-01-07 08:47:55,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,056 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.649085998535156
2023-01-07 08:47:55,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.656705379486084
2023-01-07 08:47:55,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,058 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 0.8969976902008057
2023-01-07 08:47:55,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04119546711444855
2023-01-07 08:47:55,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,059 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.0495520681142807
2023-01-07 08:47:55,059 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,059 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,059 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 1.1389243602752686
2023-01-07 08:47:55,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2372722029685974
2023-01-07 08:47:55,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,061 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.83211088180542
2023-01-07 08:47:55,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7628702521324158
2023-01-07 08:47:55,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,062 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.01685348153114319
2023-01-07 08:47:55,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,062 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.6841959953308105
2023-01-07 08:47:55,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4777100086212158
2023-01-07 08:47:55,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,064 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.266450881958008
2023-01-07 08:47:55,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5569362640380859
2023-01-07 08:47:55,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,065 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.2115592062473297
2023-01-07 08:47:55,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,066 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 11.457273483276367
2023-01-07 08:47:55,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7509541511535645
2023-01-07 08:47:55,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,067 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,067 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 20.446125030517578
2023-01-07 08:47:55,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.81444251537323
2023-01-07 08:47:55,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,068 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.22989919781684875
2023-01-07 08:47:55,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,069 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 19.898590087890625
2023-01-07 08:47:55,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.630971670150757
2023-01-07 08:47:55,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,070 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.109726905822754
2023-01-07 08:47:55,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5199911594390869
2023-01-07 08:47:55,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,071 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.023577265441417694
2023-01-07 08:47:55,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,072 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 12.291518211364746
2023-01-07 08:47:55,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5465326309204102
2023-01-07 08:47:55,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,073 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -20.127716064453125
2023-01-07 08:47:55,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2847028970718384
2023-01-07 08:47:55,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,075 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.09969877451658249
2023-01-07 08:47:55,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,075 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -20.534381866455078
2023-01-07 08:47:55,075 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.044301487505435944
2023-01-07 08:47:55,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,076 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 5.552958011627197
2023-01-07 08:47:55,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.887266993522644
2023-01-07 08:47:55,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,078 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.09028744697570801
2023-01-07 08:47:55,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,078 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 3.444955348968506
2023-01-07 08:47:55,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8214495778083801
2023-01-07 08:47:55,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,080 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.9018299579620361
2023-01-07 08:47:55,080 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23494549095630646
2023-01-07 08:47:55,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,081 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.07945521920919418
2023-01-07 08:47:55,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,082 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.346611499786377
2023-01-07 08:47:55,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0896868705749512
2023-01-07 08:47:55,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,083 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -4.386726379394531
2023-01-07 08:47:55,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2082295417785645
2023-01-07 08:47:55,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,084 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.037775516510009766
2023-01-07 08:47:55,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,085 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -4.228089809417725
2023-01-07 08:47:55,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33727890253067017
2023-01-07 08:47:55,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.793386459350586
2023-01-07 08:47:55,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5800199508666992
2023-01-07 08:47:55,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,087 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.16042375564575195
2023-01-07 08:47:55,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.872766017913818
2023-01-07 08:47:55,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1830954551696777
2023-01-07 08:47:55,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,089 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1.73109769821167
2023-01-07 08:47:55,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.213545560836792
2023-01-07 08:47:55,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,090 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.19733430445194244
2023-01-07 08:47:55,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,091 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,091 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.681405544281006
2023-01-07 08:47:55,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3119614124298096
2023-01-07 08:47:55,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -10.701471328735352
2023-01-07 08:47:55,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25852739810943604
2023-01-07 08:47:55,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,094 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.12409397214651108
2023-01-07 08:47:55,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,094 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -9.223042488098145
2023-01-07 08:47:55,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6459716558456421
2023-01-07 08:47:55,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,095 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -10.339380264282227
2023-01-07 08:47:55,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4507969915866852
2023-01-07 08:47:55,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.18976618349552155
2023-01-07 08:47:55,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,097 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -9.248554229736328
2023-01-07 08:47:55,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8197554349899292
2023-01-07 08:47:55,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.606302261352539
2023-01-07 08:47:55,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.385042667388916
2023-01-07 08:47:55,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.1531694531440735
2023-01-07 08:47:55,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,100 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,100 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -6.463106632232666
2023-01-07 08:47:55,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4779150187969208
2023-01-07 08:47:55,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,102 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -28.133216857910156
2023-01-07 08:47:55,102 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3746538758277893
2023-01-07 08:47:55,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.03600592166185379
2023-01-07 08:47:55,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -28.301692962646484
2023-01-07 08:47:55,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.854043960571289
2023-01-07 08:47:55,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.6762232780456543
2023-01-07 08:47:55,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1781831681728363
2023-01-07 08:47:55,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.22152340412139893
2023-01-07 08:47:55,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.0409622192382812
2023-01-07 08:47:55,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1273319721221924
2023-01-07 08:47:55,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -17.56943130493164
2023-01-07 08:47:55,108 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1372791528701782
2023-01-07 08:47:55,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.023649200797080994
2023-01-07 08:47:55,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -17.450294494628906
2023-01-07 08:47:55,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12760907411575317
2023-01-07 08:47:55,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -78.9783706665039
2023-01-07 08:47:55,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7283551692962646
2023-01-07 08:47:55,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.22720389068126678
2023-01-07 08:47:55,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -79.91818237304688
2023-01-07 08:47:55,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.54694938659668
2023-01-07 08:47:55,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.306680679321289
2023-01-07 08:47:55,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7846829891204834
2023-01-07 08:47:55,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.15518054366111755
2023-01-07 08:47:55,115 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,115 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,115 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 0.4390096664428711
2023-01-07 08:47:55,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8242666721343994
2023-01-07 08:47:55,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 4.828413963317871
2023-01-07 08:47:55,117 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8489885330200195
2023-01-07 08:47:55,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.4332663118839264
2023-01-07 08:47:55,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,118 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 9.151135444641113
2023-01-07 08:47:55,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2198643684387207
2023-01-07 08:47:55,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,120 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -23.70358657836914
2023-01-07 08:47:55,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3115788698196411
2023-01-07 08:47:55,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,122 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -18.758228302001953
2023-01-07 08:47:55,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.755505323410034
2023-01-07 08:47:55,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,123 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -7.4782915115356445
2023-01-07 08:47:55,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7910797595977783
2023-01-07 08:47:55,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,124 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.30399641394615173
2023-01-07 08:47:55,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -5.8291826248168945
2023-01-07 08:47:55,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.95900297164917
2023-01-07 08:47:55,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,126 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -29.44049072265625
2023-01-07 08:47:55,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.911233901977539
2023-01-07 08:47:55,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.01721101999282837
2023-01-07 08:47:55,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,128 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -25.343442916870117
2023-01-07 08:47:55,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.595539093017578
2023-01-07 08:47:55,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -11.933643341064453
2023-01-07 08:47:55,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4335026144981384
2023-01-07 08:47:55,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,130 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.3517451286315918
2023-01-07 08:47:55,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,131 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -5.00115966796875
2023-01-07 08:47:55,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5518178939819336
2023-01-07 08:47:55,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -158.1785125732422
2023-01-07 08:47:55,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.197249412536621
2023-01-07 08:47:55,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -154.03871154785156
2023-01-07 08:47:55,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.27362585067749
2023-01-07 08:47:55,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -13.376667976379395
2023-01-07 08:47:55,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.434410572052002
2023-01-07 08:47:55,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,136 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -12.554018020629883
2023-01-07 08:47:55,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.385320663452148
2023-01-07 08:47:55,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,138 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 12.324743270874023
2023-01-07 08:47:55,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6785423755645752
2023-01-07 08:47:55,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,139 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 17.373598098754883
2023-01-07 08:47:55,139 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08374691009521484
2023-01-07 08:47:55,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,140 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -125.35526275634766
2023-01-07 08:47:55,141 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4000774621963501
2023-01-07 08:47:55,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,142 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.52155303955078
2023-01-07 08:47:55,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.321334838867188
2023-01-07 08:47:55,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,143 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -43.81306457519531
2023-01-07 08:47:55,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.439312934875488
2023-01-07 08:47:55,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,144 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.123903274536133
2023-01-07 08:47:55,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.862961769104004
2023-01-07 08:47:55,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 14.414212226867676
2023-01-07 08:47:55,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.235034704208374
2023-01-07 08:47:55,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,147 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 14.73156452178955
2023-01-07 08:47:55,147 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.069051742553711
2023-01-07 08:47:55,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,148 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -80.10757446289062
2023-01-07 08:47:55,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.369325637817383
2023-01-07 08:47:55,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,150 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -67.11096954345703
2023-01-07 08:47:55,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.60321044921875
2023-01-07 08:47:55,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,151 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -52.15263366699219
2023-01-07 08:47:55,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.714395999908447
2023-01-07 08:47:55,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,152 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.6998469829559326
2023-01-07 08:47:55,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,153 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -62.03534698486328
2023-01-07 08:47:55,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.131797790527344
2023-01-07 08:47:55,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,154 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 22.468772888183594
2023-01-07 08:47:55,155 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6279157400131226
2023-01-07 08:47:55,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,156 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 23.98647689819336
2023-01-07 08:47:55,156 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8094484806060791
2023-01-07 08:47:55,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,157 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 48.97964096069336
2023-01-07 08:47:55,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.790149688720703
2023-01-07 08:47:55,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,158 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 29.382801055908203
2023-01-07 08:47:55,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.11444854736328
2023-01-07 08:47:55,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,160 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -9.506937026977539
2023-01-07 08:47:55,160 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.544416904449463
2023-01-07 08:47:55,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,161 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.5644387006759644
2023-01-07 08:47:55,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,161 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 35.675132751464844
2023-01-07 08:47:55,162 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.638877868652344
2023-01-07 08:47:55,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,163 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -21.865440368652344
2023-01-07 08:47:55,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.300576210021973
2023-01-07 08:47:55,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,164 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -50.29511260986328
2023-01-07 08:47:55,164 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.887603759765625
2023-01-07 08:47:55,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,165 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 30.245311737060547
2023-01-07 08:47:55,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.057755470275879
2023-01-07 08:47:55,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,167 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -674.0158081054688
2023-01-07 08:47:55,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.086977005004883
2023-01-07 08:47:55,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,168 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -143.4024658203125
2023-01-07 08:47:55,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.446878433227539
2023-01-07 08:47:55,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,169 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 0.6345040798187256
2023-01-07 08:47:55,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,170 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -798.4464721679688
2023-01-07 08:47:55,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.37929916381836
2023-01-07 08:47:55,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,171 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1326.3233642578125
2023-01-07 08:47:55,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.687164306640625
2023-01-07 08:47:55,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,173 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.5768964290618896
2023-01-07 08:47:55,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 33.49709701538086
2023-01-07 08:47:55,173 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,173 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,173 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1133.736328125
2023-01-07 08:47:55,173 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.47639083862305
2023-01-07 08:47:55,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,175 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -418.6421203613281
2023-01-07 08:47:55,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.781578063964844
2023-01-07 08:47:55,176 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,176 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -735.3148803710938
2023-01-07 08:47:55,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.91058349609375
2023-01-07 08:47:55,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,177 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 33.21256637573242
2023-01-07 08:47:55,178 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.535609245300293
2023-01-07 08:47:55,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,179 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.5679428577423096
2023-01-07 08:47:55,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,179 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -538.5877685546875
2023-01-07 08:47:55,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.92019271850586
2023-01-07 08:47:55,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,181 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -295.56939697265625
2023-01-07 08:47:55,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.045053482055664
2023-01-07 08:47:55,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,182 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 0.8253376483917236
2023-01-07 08:47:55,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,182 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -5.31412410736084
2023-01-07 08:47:55,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5030765533447266
2023-01-07 08:47:55,183 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,183 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,184 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1220.0399169921875
2023-01-07 08:47:55,184 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.641651153564453
2023-01-07 08:47:55,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,185 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -7.085329055786133
2023-01-07 08:47:55,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,185 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1031.44384765625
2023-01-07 08:47:55,185 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.840133666992188
2023-01-07 08:47:55,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,187 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2125.620849609375
2023-01-07 08:47:55,187 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006292819976806641
2023-01-07 08:47:55,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,188 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 138.4722900390625
2023-01-07 08:47:55,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.56078338623047
2023-01-07 08:47:55,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,189 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -3052.9970703125
2023-01-07 08:47:55,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.58591651916504
2023-01-07 08:47:55,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,191 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.5975041389465332
2023-01-07 08:47:55,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,191 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 21.27361488342285
2023-01-07 08:47:55,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,192 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -3961.574951171875
2023-01-07 08:47:55,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,192 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1035.34033203125
2023-01-07 08:47:55,192 > [DEBUG] 0 :: before allreduce fusion buffer :: -83.57341766357422
2023-01-07 08:47:55,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,194 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -3973.762451171875
2023-01-07 08:47:55,194 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.85436248779297
2023-01-07 08:47:55,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,195 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -125.72610473632812
2023-01-07 08:47:55,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -97.71784973144531
2023-01-07 08:47:55,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,196 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -4056.524169921875
2023-01-07 08:47:55,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8390002250671387
2023-01-07 08:47:55,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,197 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1118.5299072265625
2023-01-07 08:47:55,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,198 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1039.553955078125
2023-01-07 08:47:55,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.800304412841797
2023-01-07 08:47:55,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,200 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 208.90940856933594
2023-01-07 08:47:55,201 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.227264404296875
2023-01-07 08:47:55,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.248093843460083
2023-01-07 08:47:55,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -782.443115234375
2023-01-07 08:47:55,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 121.12533569335938
2023-01-07 08:47:55,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,204 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1847.8173828125
2023-01-07 08:47:55,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 169.4449920654297
2023-01-07 08:47:55,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2707.10302734375
2023-01-07 08:47:55,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 188.0822296142578
2023-01-07 08:47:55,207 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:47:55,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,207 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 1926.47705078125
2023-01-07 08:47:55,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,207 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -3525.85498046875
2023-01-07 08:47:55,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,208 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1043.59814453125
2023-01-07 08:47:55,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,208 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1447.917724609375
2023-01-07 08:47:55,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,208 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -19.589500427246094
2023-01-07 08:47:55,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 109.29186248779297
2023-01-07 08:47:55,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 4.15089225769043
2023-01-07 08:47:55,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -8.548492431640625
2023-01-07 08:47:55,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -65.02931213378906
2023-01-07 08:47:55,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,210 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -110.36502075195312
2023-01-07 08:47:55,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,210 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 43.39140319824219
2023-01-07 08:47:55,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,210 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -7.136950492858887
2023-01-07 08:47:55,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,211 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -96.07349395751953
2023-01-07 08:47:55,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,211 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 34.470558166503906
2023-01-07 08:47:55,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,211 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 21.6414794921875
2023-01-07 08:47:55,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -142.51699829101562
2023-01-07 08:47:55,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 29.373451232910156
2023-01-07 08:47:55,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -33.57832717895508
2023-01-07 08:47:55,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -11.841240882873535
2023-01-07 08:47:55,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,213 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -34.4254264831543
2023-01-07 08:47:55,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,213 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 7.78858757019043
2023-01-07 08:47:55,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,213 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 10.806835174560547
2023-01-07 08:47:55,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,214 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -73.61471557617188
2023-01-07 08:47:55,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,214 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -11.670677185058594
2023-01-07 08:47:55,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,214 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.214694023132324
2023-01-07 08:47:55,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,215 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -28.524900436401367
2023-01-07 08:47:55,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 1215.8985595703125
2023-01-07 08:47:55,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.897932052612305
2023-01-07 08:47:55,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -11.072309494018555
2023-01-07 08:47:55,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -3.953958511352539
2023-01-07 08:47:55,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4.445549488067627
2023-01-07 08:47:55,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,218 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 5.2358479499816895
2023-01-07 08:47:55,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,218 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -7.593581676483154
2023-01-07 08:47:55,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,218 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -2.587833881378174
2023-01-07 08:47:55,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,219 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 2.907236099243164
2023-01-07 08:47:55,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,219 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -22.303966522216797
2023-01-07 08:47:55,219 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.3314094543457
2023-01-07 08:47:55,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,220 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 14.228629112243652
2023-01-07 08:47:55,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 21.425071716308594
2023-01-07 08:47:55,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 13.779960632324219
2023-01-07 08:47:55,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,221 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.9122719764709473
2023-01-07 08:47:55,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 65.4275894165039
2023-01-07 08:47:55,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,222 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 0.6400692462921143
2023-01-07 08:47:55,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,223 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.218770980834961
2023-01-07 08:47:55,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,223 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 5.689291954040527
2023-01-07 08:47:55,223 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.524446487426758
2023-01-07 08:47:55,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,224 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 5.850029468536377
2023-01-07 08:47:55,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:55,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:55,224 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -709.21826171875
2023-01-07 08:47:55,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 684.1383666992188
2023-01-07 08:47:56,065 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -665.8294677734375
2023-01-07 08:47:56,065 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,065 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:56,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,066 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 0.4581725597381592
2023-01-07 08:47:56,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,066 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -3188.010009765625
2023-01-07 08:47:56,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.77239227294922
2023-01-07 08:47:56,068 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -2.4243154525756836
2023-01-07 08:47:56,068 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,069 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.6681442260742188
2023-01-07 08:47:56,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,069 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -36.03797149658203
2023-01-07 08:47:56,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,069 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -3287.5400390625
2023-01-07 08:47:56,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.14453887939453
2023-01-07 08:47:56,071 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -79.68057250976562
2023-01-07 08:47:56,071 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,071 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,071 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:56,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,071 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -3292.64208984375
2023-01-07 08:47:56,071 > [DEBUG] 0 :: before allreduce fusion buffer :: -94.27154541015625
2023-01-07 08:47:56,072 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -62.11273956298828
2023-01-07 08:47:56,072 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,073 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:56,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,073 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -3319.03369140625
2023-01-07 08:47:56,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 173.07254028320312
2023-01-07 08:47:56,074 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -3219.510498046875
2023-01-07 08:47:56,074 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:56,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,074 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.6831238269805908
2023-01-07 08:47:56,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,075 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 17.824058532714844
2023-01-07 08:47:56,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,075 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -4997.1953125
2023-01-07 08:47:56,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.3770751953125
2023-01-07 08:47:56,077 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -2.2988781929016113
2023-01-07 08:47:56,077 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,077 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:56,077 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,077 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,077 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -4999.2333984375
2023-01-07 08:47:56,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.649986267089844
2023-01-07 08:47:56,078 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 64.01203918457031
2023-01-07 08:47:56,078 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,079 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:56,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,079 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 3.623290538787842
2023-01-07 08:47:56,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,079 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -777.3204345703125
2023-01-07 08:47:56,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.88304138183594
2023-01-07 08:47:56,080 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 10.378358840942383
2023-01-07 08:47:56,080 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,081 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:56,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,081 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -813.9692993164062
2023-01-07 08:47:56,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.87078857421875
2023-01-07 08:47:56,082 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -2203.55419921875
2023-01-07 08:47:56,082 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:56,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,082 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -877.9176635742188
2023-01-07 08:47:56,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.43653869628906
2023-01-07 08:47:56,084 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -76.99534606933594
2023-01-07 08:47:56,084 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,084 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,084 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:56,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,084 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,084 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -812.556884765625
2023-01-07 08:47:56,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.71794509887695
2023-01-07 08:47:56,085 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -788.516845703125
2023-01-07 08:47:56,085 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,086 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:56,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,086 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -0.9214644432067871
2023-01-07 08:47:56,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,086 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -890.6279296875
2023-01-07 08:47:56,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.46907424926758
2023-01-07 08:47:56,088 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 4.196465492248535
2023-01-07 08:47:56,088 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,088 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:56,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,088 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -938.66455078125
2023-01-07 08:47:56,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,088 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -959.7457275390625
2023-01-07 08:47:56,089 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.36378002166748
2023-01-07 08:47:56,090 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -957.619140625
2023-01-07 08:47:56,090 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:56,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,090 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 0.23254060745239258
2023-01-07 08:47:56,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,091 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -940.0639038085938
2023-01-07 08:47:56,091 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4118175506591797
2023-01-07 08:47:56,092 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.09453678131103516
2023-01-07 08:47:56,092 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:56,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,092 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -4969.41748046875
2023-01-07 08:47:56,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.77458953857422
2023-01-07 08:47:56,093 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -907.4879760742188
2023-01-07 08:47:56,094 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,094 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:56,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,094 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -863.0244140625
2023-01-07 08:47:56,094 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.373502254486084
2023-01-07 08:47:56,095 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -56.21826171875
2023-01-07 08:47:56,095 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,095 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,095 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:56,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,096 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -883.3492431640625
2023-01-07 08:47:56,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,096 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -5001.7802734375
2023-01-07 08:47:56,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.572239875793457
2023-01-07 08:47:56,097 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -867.0646362304688
2023-01-07 08:47:56,097 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,097 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,097 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:56,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,098 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -5032.69775390625
2023-01-07 08:47:56,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.683555603027344
2023-01-07 08:47:56,099 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -26.890766143798828
2023-01-07 08:47:56,099 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,099 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:56,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,099 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -5051.65234375
2023-01-07 08:47:56,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.65909957885742
2023-01-07 08:47:56,100 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -5000.7822265625
2023-01-07 08:47:56,101 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:56,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,101 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.7377058267593384
2023-01-07 08:47:56,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,101 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 22.255939483642578
2023-01-07 08:47:56,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,102 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1390.45166015625
2023-01-07 08:47:56,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.78257751464844
2023-01-07 08:47:56,103 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -1.1974451541900635
2023-01-07 08:47:56,103 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:56,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,104 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -933.09228515625
2023-01-07 08:47:56,104 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.359379768371582
2023-01-07 08:47:56,105 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 67.19096374511719
2023-01-07 08:47:56,105 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,105 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,105 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -32.44528579711914
2023-01-07 08:47:56,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,105 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,105 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1372.745849609375
2023-01-07 08:47:56,106 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.503093957901001
2023-01-07 08:47:56,107 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -56.143951416015625
2023-01-07 08:47:56,107 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,107 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:56,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,107 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -957.6959228515625
2023-01-07 08:47:56,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.91191291809082
2023-01-07 08:47:56,108 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1361.16943359375
2023-01-07 08:47:56,109 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:56,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,109 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -991.0592041015625
2023-01-07 08:47:56,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.278538703918457
2023-01-07 08:47:56,110 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -27.676973342895508
2023-01-07 08:47:56,110 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,110 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,110 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:56,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,111 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -945.63671875
2023-01-07 08:47:56,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1657497882843018
2023-01-07 08:47:56,112 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -899.5853881835938
2023-01-07 08:47:56,112 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,112 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,112 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,112 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.8921865224838257
2023-01-07 08:47:56,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,113 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1495.685302734375
2023-01-07 08:47:56,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.626235961914062
2023-01-07 08:47:56,114 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.934943675994873
2023-01-07 08:47:56,114 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -13.1309175491333
2023-01-07 08:47:56,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,114 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1470.221923828125
2023-01-07 08:47:56,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1400654315948486
2023-01-07 08:47:56,115 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -1469.277587890625
2023-01-07 08:47:56,116 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,116 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,116 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:56,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,116 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.4579269289970398
2023-01-07 08:47:56,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,116 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -22.727447509765625
2023-01-07 08:47:56,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.374642848968506
2023-01-07 08:47:56,117 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 1.3681771755218506
2023-01-07 08:47:56,118 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,118 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,118 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.17344856262207
2023-01-07 08:47:56,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,118 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.5325889587402344
2023-01-07 08:47:56,118 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.837721347808838
2023-01-07 08:47:56,119 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -0.9476299285888672
2023-01-07 08:47:56,119 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:56,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,119 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 103.60553741455078
2023-01-07 08:47:56,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40043020248413086
2023-01-07 08:47:56,121 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 14.191946029663086
2023-01-07 08:47:56,121 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,121 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,121 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:56,121 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,121 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 108.33592224121094
2023-01-07 08:47:56,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.891361236572266
2023-01-07 08:47:56,122 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 111.63966369628906
2023-01-07 08:47:56,122 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,123 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,123 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.6935263872146606
2023-01-07 08:47:56,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,123 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,123 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -189.23199462890625
2023-01-07 08:47:56,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -124.23432922363281
2023-01-07 08:47:56,124 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.570419430732727
2023-01-07 08:47:56,124 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.533244132995605
2023-01-07 08:47:56,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,125 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -136.29039001464844
2023-01-07 08:47:56,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.280532836914062
2023-01-07 08:47:56,126 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -116.0750732421875
2023-01-07 08:47:56,126 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,126 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,126 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,126 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,127 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.22681891918182373
2023-01-07 08:47:56,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,127 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -44.28764343261719
2023-01-07 08:47:56,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.847539901733398
2023-01-07 08:47:56,128 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.3777611255645752
2023-01-07 08:47:56,128 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 31.334651947021484
2023-01-07 08:47:56,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,128 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,129 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -24.374082565307617
2023-01-07 08:47:56,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.780805826187134
2023-01-07 08:47:56,130 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -23.489351272583008
2023-01-07 08:47:56,130 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:56,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,130 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -65.39949798583984
2023-01-07 08:47:56,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0975682735443115
2023-01-07 08:47:56,131 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -14.4586181640625
2023-01-07 08:47:56,132 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:56,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,132 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -50.48140335083008
2023-01-07 08:47:56,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.169872283935547
2023-01-07 08:47:56,133 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -42.873931884765625
2023-01-07 08:47:56,133 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,134 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5429706573486328
2023-01-07 08:47:56,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,134 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -109.02301788330078
2023-01-07 08:47:56,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.62640380859375
2023-01-07 08:47:56,135 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.7846997976303101
2023-01-07 08:47:56,135 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.319789171218872
2023-01-07 08:47:56,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,136 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -148.41873168945312
2023-01-07 08:47:56,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.2717390060424805
2023-01-07 08:47:56,137 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -148.3241424560547
2023-01-07 08:47:56,137 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,137 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,137 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.6262131333351135
2023-01-07 08:47:56,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,138 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 42.339271545410156
2023-01-07 08:47:56,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.570490837097168
2023-01-07 08:47:56,139 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8192596435546875
2023-01-07 08:47:56,139 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,139 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,139 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 6.020914077758789
2023-01-07 08:47:56,139 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,139 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,139 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 31.508007049560547
2023-01-07 08:47:56,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4165401458740234
2023-01-07 08:47:56,141 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 33.14081573486328
2023-01-07 08:47:56,141 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:56,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,141 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.7476553320884705
2023-01-07 08:47:56,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,141 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -9.748834609985352
2023-01-07 08:47:56,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.366473197937012
2023-01-07 08:47:56,142 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.3074793219566345
2023-01-07 08:47:56,143 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 29.095399856567383
2023-01-07 08:47:56,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,143 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -15.046330451965332
2023-01-07 08:47:56,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2440969944000244
2023-01-07 08:47:56,144 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -19.896800994873047
2023-01-07 08:47:56,144 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,145 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.29819589853286743
2023-01-07 08:47:56,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,145 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -91.17512512207031
2023-01-07 08:47:56,145 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0665955543518066
2023-01-07 08:47:56,146 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.31762033700942993
2023-01-07 08:47:56,146 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,147 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -7.9246392250061035
2023-01-07 08:47:56,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,147 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -50.7027587890625
2023-01-07 08:47:56,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.229206085205078
2023-01-07 08:47:56,148 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -40.803565979003906
2023-01-07 08:47:56,148 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,148 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,148 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:56,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,148 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.30793964862823486
2023-01-07 08:47:56,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,149 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 34.242034912109375
2023-01-07 08:47:56,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.061370849609375
2023-01-07 08:47:56,150 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -1.0457724332809448
2023-01-07 08:47:56,150 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,150 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,150 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.432145118713379
2023-01-07 08:47:56,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,151 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 36.01629638671875
2023-01-07 08:47:56,151 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.865656852722168
2023-01-07 08:47:56,152 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 42.959564208984375
2023-01-07 08:47:56,152 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,152 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,152 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:56,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,152 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.6664274334907532
2023-01-07 08:47:56,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,153 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 21.45185661315918
2023-01-07 08:47:56,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.305477142333984
2023-01-07 08:47:56,154 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.7435752153396606
2023-01-07 08:47:56,154 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -22.02167510986328
2023-01-07 08:47:56,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 31.14906120300293
2023-01-07 08:47:56,155 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.81552267074585
2023-01-07 08:47:56,156 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 37.38048553466797
2023-01-07 08:47:56,156 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:56,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,156 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.36518675088882446
2023-01-07 08:47:56,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,156 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -581.1295776367188
2023-01-07 08:47:56,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05473613739013672
2023-01-07 08:47:56,158 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.5526478290557861
2023-01-07 08:47:56,158 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,158 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,158 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.97386360168457
2023-01-07 08:47:56,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,158 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -585.05322265625
2023-01-07 08:47:56,158 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.80377197265625
2023-01-07 08:47:56,159 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -577.276123046875
2023-01-07 08:47:56,159 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,160 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:56,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,160 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 139.18124389648438
2023-01-07 08:47:56,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2765047550201416
2023-01-07 08:47:56,161 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 3.241605043411255
2023-01-07 08:47:56,161 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:56,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,162 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 137.0626220703125
2023-01-07 08:47:56,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09411215782165527
2023-01-07 08:47:56,163 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 133.87652587890625
2023-01-07 08:47:56,163 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,163 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,163 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:56,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,163 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 375.19598388671875
2023-01-07 08:47:56,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.682553768157959
2023-01-07 08:47:56,164 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 7.471732139587402
2023-01-07 08:47:56,165 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,165 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,165 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:56,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,165 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 366.49652099609375
2023-01-07 08:47:56,165 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4892507791519165
2023-01-07 08:47:56,166 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 368.6673889160156
2023-01-07 08:47:56,166 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:56,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,167 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 255.71365356445312
2023-01-07 08:47:56,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0598227977752686
2023-01-07 08:47:56,168 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 3.9544506072998047
2023-01-07 08:47:56,168 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,168 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,168 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:56,168 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,168 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,168 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 252.9250946044922
2023-01-07 08:47:56,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34053459763526917
2023-01-07 08:47:56,170 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 249.21514892578125
2023-01-07 08:47:56,170 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:56,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,170 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.2210768610239029
2023-01-07 08:47:56,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,170 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,170 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 600.445068359375
2023-01-07 08:47:56,171 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4061439037322998
2023-01-07 08:47:56,172 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.13519571721553802
2023-01-07 08:47:56,172 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.42170524597168
2023-01-07 08:47:56,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,172 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 595.6510009765625
2023-01-07 08:47:56,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.332784652709961
2023-01-07 08:47:56,173 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 597.4264526367188
2023-01-07 08:47:56,173 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:56,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,174 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 130.44064331054688
2023-01-07 08:47:56,174 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.472855567932129
2023-01-07 08:47:56,175 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 1.745452642440796
2023-01-07 08:47:56,175 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,175 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,175 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:56,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,176 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 128.439208984375
2023-01-07 08:47:56,176 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0898618698120117
2023-01-07 08:47:56,177 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 131.92636108398438
2023-01-07 08:47:56,177 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:56,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,177 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 14.938172340393066
2023-01-07 08:47:56,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.933121681213379
2023-01-07 08:47:56,178 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 2.0079874992370605
2023-01-07 08:47:56,178 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:56,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,179 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.7176513671875
2023-01-07 08:47:56,179 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8607046604156494
2023-01-07 08:47:56,180 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 7.704272270202637
2023-01-07 08:47:56,180 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:56,181 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,181 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,181 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -5.490021705627441
2023-01-07 08:47:56,181 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.937408924102783
2023-01-07 08:47:56,182 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.7072514891624451
2023-01-07 08:47:56,182 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,182 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:56,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,182 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -7.627531051635742
2023-01-07 08:47:56,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0002975463867188
2023-01-07 08:47:56,184 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -5.604968070983887
2023-01-07 08:47:56,184 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,184 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,184 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:56,184 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,184 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,184 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 11.223040580749512
2023-01-07 08:47:56,184 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.102830171585083
2023-01-07 08:47:56,185 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.45187461376190186
2023-01-07 08:47:56,185 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,186 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:56,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,186 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 10.699583053588867
2023-01-07 08:47:56,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07690054178237915
2023-01-07 08:47:56,187 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 12.502300262451172
2023-01-07 08:47:56,187 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,187 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,187 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:56,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,188 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 0.6999573111534119
2023-01-07 08:47:56,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8294232487678528
2023-01-07 08:47:56,189 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.02451115846633911
2023-01-07 08:47:56,189 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,189 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,189 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:56,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,189 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -0.17454314231872559
2023-01-07 08:47:56,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05261556804180145
2023-01-07 08:47:56,190 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -3.01237154006958
2023-01-07 08:47:56,191 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:56,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,191 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1329.7215576171875
2023-01-07 08:47:56,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.064937114715576
2023-01-07 08:47:56,192 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.6742910146713257
2023-01-07 08:47:56,192 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:56,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,193 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1330.70556640625
2023-01-07 08:47:56,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.683691024780273
2023-01-07 08:47:56,194 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -1329.763671875
2023-01-07 08:47:56,194 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,194 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,194 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:56,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,194 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -11.517022132873535
2023-01-07 08:47:56,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4262588024139404
2023-01-07 08:47:56,196 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.632239580154419
2023-01-07 08:47:56,196 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:56,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,196 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -13.253899574279785
2023-01-07 08:47:56,196 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9214917421340942
2023-01-07 08:47:56,197 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -12.8171968460083
2023-01-07 08:47:56,197 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:56,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,198 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -85.19581604003906
2023-01-07 08:47:56,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18379738926887512
2023-01-07 08:47:56,199 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 1.0804284811019897
2023-01-07 08:47:56,199 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:56,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,200 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -86.32259368896484
2023-01-07 08:47:56,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9156655073165894
2023-01-07 08:47:56,201 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -87.4559326171875
2023-01-07 08:47:56,201 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:56,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,201 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -349.10443115234375
2023-01-07 08:47:56,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1731257438659668
2023-01-07 08:47:56,203 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.46682387590408325
2023-01-07 08:47:56,203 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,203 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,203 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:56,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,203 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -350.28131103515625
2023-01-07 08:47:56,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11099201440811157
2023-01-07 08:47:56,204 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -350.39007568359375
2023-01-07 08:47:56,204 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,205 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:56,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,205 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1466.60986328125
2023-01-07 08:47:56,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3686908483505249
2023-01-07 08:47:56,206 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.20408841967582703
2023-01-07 08:47:56,206 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:56,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,207 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1466.1563720703125
2023-01-07 08:47:56,207 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4810720384120941
2023-01-07 08:47:56,208 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1465.5343017578125
2023-01-07 08:47:56,208 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:56,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,208 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 307.4014587402344
2023-01-07 08:47:56,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5879755020141602
2023-01-07 08:47:56,209 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.951112687587738
2023-01-07 08:47:56,210 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,210 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:56,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,210 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 305.8148193359375
2023-01-07 08:47:56,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7348642349243164
2023-01-07 08:47:56,211 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 303.8870544433594
2023-01-07 08:47:56,211 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,211 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,211 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:56,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,212 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 446.9068908691406
2023-01-07 08:47:56,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0031927824020386
2023-01-07 08:47:56,213 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.151089608669281
2023-01-07 08:47:56,213 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,213 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:56,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,213 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 447.0748291015625
2023-01-07 08:47:56,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1527725458145142
2023-01-07 08:47:56,215 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 447.6784973144531
2023-01-07 08:47:56,215 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:56,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,215 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 466.433349609375
2023-01-07 08:47:56,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7518539428710938
2023-01-07 08:47:56,216 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.6309378147125244
2023-01-07 08:47:56,216 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:56,217 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,217 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,217 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 466.3818359375
2023-01-07 08:47:56,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3591352701187134
2023-01-07 08:47:56,218 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 466.14654541015625
2023-01-07 08:47:56,218 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,218 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,218 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:56,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,219 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1120.9755859375
2023-01-07 08:47:56,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8454751968383789
2023-01-07 08:47:56,220 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -1.380207896232605
2023-01-07 08:47:56,220 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,220 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:56,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,220 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1122.14404296875
2023-01-07 08:47:56,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8622126579284668
2023-01-07 08:47:56,222 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1121.771240234375
2023-01-07 08:47:56,222 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:56,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,222 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1764.3260498046875
2023-01-07 08:47:56,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5870741605758667
2023-01-07 08:47:56,223 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.48648208379745483
2023-01-07 08:47:56,223 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,224 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:56,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,224 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1764.38232421875
2023-01-07 08:47:56,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7580115795135498
2023-01-07 08:47:56,225 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -1764.21923828125
2023-01-07 08:47:56,225 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:56,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,225 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -895.9230346679688
2023-01-07 08:47:56,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1906176656484604
2023-01-07 08:47:56,227 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.5118721723556519
2023-01-07 08:47:56,227 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,227 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:56,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,227 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -895.7674560546875
2023-01-07 08:47:56,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23544445633888245
2023-01-07 08:47:56,228 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -897.0546264648438
2023-01-07 08:47:56,229 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,229 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,229 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:56,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,229 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2389.553466796875
2023-01-07 08:47:56,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1211485862731934
2023-01-07 08:47:56,230 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.5153865814208984
2023-01-07 08:47:56,230 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:56,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,231 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2387.728271484375
2023-01-07 08:47:56,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7471040487289429
2023-01-07 08:47:56,232 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2387.609619140625
2023-01-07 08:47:56,232 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,232 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,232 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:56,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,232 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1113.0556640625
2023-01-07 08:47:56,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30408817529678345
2023-01-07 08:47:56,233 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.642459511756897
2023-01-07 08:47:56,233 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:56,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,234 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1113.07080078125
2023-01-07 08:47:56,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04058125987648964
2023-01-07 08:47:56,235 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1113.4278564453125
2023-01-07 08:47:56,235 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,235 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,235 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:56,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7073.744140625
2023-01-07 08:47:56,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2544836103916168
2023-01-07 08:47:56,237 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -0.38868528604507446
2023-01-07 08:47:56,237 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:56,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7074.71484375
2023-01-07 08:47:56,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5523638725280762
2023-01-07 08:47:56,239 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -7075.00634765625
2023-01-07 08:47:56,239 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:56,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,239 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2750.46240234375
2023-01-07 08:47:56,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22152765095233917
2023-01-07 08:47:56,240 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.22633647918701172
2023-01-07 08:47:56,240 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:56,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,241 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2750.1962890625
2023-01-07 08:47:56,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13968807458877563
2023-01-07 08:47:56,242 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2750.990234375
2023-01-07 08:47:56,242 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:56,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,242 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8738.7822265625
2023-01-07 08:47:56,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11587788164615631
2023-01-07 08:47:56,244 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.2970914840698242
2023-01-07 08:47:56,244 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:56,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,244 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8738.955078125
2023-01-07 08:47:56,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30108293890953064
2023-01-07 08:47:56,245 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8739.046875
2023-01-07 08:47:56,245 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:56,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,246 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -9593.4541015625
2023-01-07 08:47:56,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0877939909696579
2023-01-07 08:47:56,247 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.17313522100448608
2023-01-07 08:47:56,247 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:56,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,247 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -9593.5078125
2023-01-07 08:47:56,248 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3406984508037567
2023-01-07 08:47:56,249 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -9593.9404296875
2023-01-07 08:47:56,249 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:56,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,250 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3531.489990234375
2023-01-07 08:47:56,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.28522399067878723
2023-01-07 08:47:56,251 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.19863449037075043
2023-01-07 08:47:56,251 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:56,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,252 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3531.578125
2023-01-07 08:47:56,252 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12576492130756378
2023-01-07 08:47:56,253 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -3532.189208984375
2023-01-07 08:47:56,253 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:56,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,254 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 12727.6787109375
2023-01-07 08:47:56,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.235797643661499
2023-01-07 08:47:56,255 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -412.8411865234375
2023-01-07 08:47:56,255 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:56,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,255 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 12729.0146484375
2023-01-07 08:47:56,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2064197063446045
2023-01-07 08:47:56,257 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 12729.7490234375
2023-01-07 08:47:56,257 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:56,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:56,258 > [DEBUG] 0 :: 7.201435565948486
2023-01-07 08:47:56,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,261 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0001220703125
2023-01-07 08:47:56,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -314.03912353515625
2023-01-07 08:47:56,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,264 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2330578863620758
2023-01-07 08:47:56,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,265 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -348.2110595703125
2023-01-07 08:47:56,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -203.11700439453125
2023-01-07 08:47:56,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,269 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 0.14583492279052734
2023-01-07 08:47:56,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06159811094403267
2023-01-07 08:47:56,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,271 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.014968659728765488
2023-01-07 08:47:56,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,272 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -66.7491226196289
2023-01-07 08:47:56,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3265045881271362
2023-01-07 08:47:56,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,273 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -12.353631973266602
2023-01-07 08:47:56,274 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18084192276000977
2023-01-07 08:47:56,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,275 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.03806382417678833
2023-01-07 08:47:56,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,275 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -79.11414337158203
2023-01-07 08:47:56,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4936729073524475
2023-01-07 08:47:56,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,277 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -3.392148733139038
2023-01-07 08:47:56,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.48540252447128296
2023-01-07 08:47:56,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,278 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2553909420967102
2023-01-07 08:47:56,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,278 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -70.67198181152344
2023-01-07 08:47:56,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2457014322280884
2023-01-07 08:47:56,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,280 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1.8063971996307373
2023-01-07 08:47:56,280 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23859791457653046
2023-01-07 08:47:56,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,281 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.021504517644643784
2023-01-07 08:47:56,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,281 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -68.66559600830078
2023-01-07 08:47:56,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12984003126621246
2023-01-07 08:47:56,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,283 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -0.9662013053894043
2023-01-07 08:47:56,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6631128787994385
2023-01-07 08:47:56,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,284 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.03623802214860916
2023-01-07 08:47:56,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,284 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.3709535598754883
2023-01-07 08:47:56,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09893132746219635
2023-01-07 08:47:56,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,286 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.321676254272461
2023-01-07 08:47:56,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18321388959884644
2023-01-07 08:47:56,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,287 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.241437628865242
2023-01-07 08:47:56,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,288 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 11.375511169433594
2023-01-07 08:47:56,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.293133020401001
2023-01-07 08:47:56,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,289 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 17.40982437133789
2023-01-07 08:47:56,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2673468589782715
2023-01-07 08:47:56,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,291 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.06802264600992203
2023-01-07 08:47:56,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,291 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 15.786614418029785
2023-01-07 08:47:56,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0874135494232178
2023-01-07 08:47:56,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,292 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 18.71131134033203
2023-01-07 08:47:56,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9730424284934998
2023-01-07 08:47:56,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,294 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.00934603065252304
2023-01-07 08:47:56,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,294 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 18.118457794189453
2023-01-07 08:47:56,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.321543574333191
2023-01-07 08:47:56,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,296 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -54.917442321777344
2023-01-07 08:47:56,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21836751699447632
2023-01-07 08:47:56,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,297 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.09827981144189835
2023-01-07 08:47:56,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,297 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -57.30077362060547
2023-01-07 08:47:56,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.014012336730957
2023-01-07 08:47:56,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,299 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -7.063258171081543
2023-01-07 08:47:56,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2048816680908203
2023-01-07 08:47:56,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,300 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.07754598557949066
2023-01-07 08:47:56,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,300 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -7.57570219039917
2023-01-07 08:47:56,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3481699824333191
2023-01-07 08:47:56,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,302 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.797580718994141
2023-01-07 08:47:56,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22862690687179565
2023-01-07 08:47:56,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,304 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.007655143737792969
2023-01-07 08:47:56,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,304 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 6.869513511657715
2023-01-07 08:47:56,304 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1864197701215744
2023-01-07 08:47:56,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 19.156587600708008
2023-01-07 08:47:56,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5329546928405762
2023-01-07 08:47:56,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,307 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.0608220249414444
2023-01-07 08:47:56,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,307 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 18.402603149414062
2023-01-07 08:47:56,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.241093635559082
2023-01-07 08:47:56,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,309 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 21.381996154785156
2023-01-07 08:47:56,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11493293941020966
2023-01-07 08:47:56,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,310 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.05579321086406708
2023-01-07 08:47:56,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,310 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 19.8769588470459
2023-01-07 08:47:56,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4999897480010986
2023-01-07 08:47:56,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,312 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 8.656192779541016
2023-01-07 08:47:56,312 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20401114225387573
2023-01-07 08:47:56,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,313 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.10011659562587738
2023-01-07 08:47:56,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,314 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 7.880292892456055
2023-01-07 08:47:56,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0593502521514893
2023-01-07 08:47:56,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,315 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 10.286138534545898
2023-01-07 08:47:56,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02224630117416382
2023-01-07 08:47:56,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,317 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.09605839848518372
2023-01-07 08:47:56,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,317 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 9.091621398925781
2023-01-07 08:47:56,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.478747844696045
2023-01-07 08:47:56,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,318 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 4.985530853271484
2023-01-07 08:47:56,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5917704105377197
2023-01-07 08:47:56,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,320 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.06782584637403488
2023-01-07 08:47:56,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,320 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 3.2231884002685547
2023-01-07 08:47:56,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.237246513366699
2023-01-07 08:47:56,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,322 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 0.1262679100036621
2023-01-07 08:47:56,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.669426679611206
2023-01-07 08:47:56,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,323 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.055025577545166016
2023-01-07 08:47:56,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,323 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -0.817237377166748
2023-01-07 08:47:56,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5845170021057129
2023-01-07 08:47:56,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,325 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 21.217370986938477
2023-01-07 08:47:56,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7679903507232666
2023-01-07 08:47:56,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,326 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.19722366333007812
2023-01-07 08:47:56,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,327 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 17.5374813079834
2023-01-07 08:47:56,327 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.264406681060791
2023-01-07 08:47:56,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 2.1224851608276367
2023-01-07 08:47:56,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2058706283569336
2023-01-07 08:47:56,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.09692877531051636
2023-01-07 08:47:56,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 4.968173980712891
2023-01-07 08:47:56,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.292015552520752
2023-01-07 08:47:56,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 4.768662452697754
2023-01-07 08:47:56,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.344083547592163
2023-01-07 08:47:56,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.10941219329833984
2023-01-07 08:47:56,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 2.352614402770996
2023-01-07 08:47:56,333 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6312627792358398
2023-01-07 08:47:56,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,335 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -10.796963691711426
2023-01-07 08:47:56,335 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1414670944213867
2023-01-07 08:47:56,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.052303120493888855
2023-01-07 08:47:56,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -17.916040420532227
2023-01-07 08:47:56,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.421413421630859
2023-01-07 08:47:56,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,338 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -8.191200256347656
2023-01-07 08:47:56,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.086756944656372
2023-01-07 08:47:56,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,339 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.31155967712402344
2023-01-07 08:47:56,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,340 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -11.283092498779297
2023-01-07 08:47:56,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3351041376590729
2023-01-07 08:47:56,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,341 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2.597877025604248
2023-01-07 08:47:56,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5525453686714172
2023-01-07 08:47:56,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,343 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.05526556074619293
2023-01-07 08:47:56,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,343 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1.7082390785217285
2023-01-07 08:47:56,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8768417835235596
2023-01-07 08:47:56,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 11.059043884277344
2023-01-07 08:47:56,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.592625617980957
2023-01-07 08:47:56,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,346 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 10.784811019897461
2023-01-07 08:47:56,347 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.253759384155273
2023-01-07 08:47:56,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,348 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -21.920848846435547
2023-01-07 08:47:56,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3501009941101074
2023-01-07 08:47:56,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.1943952441215515
2023-01-07 08:47:56,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -27.282577514648438
2023-01-07 08:47:56,350 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3454477787017822
2023-01-07 08:47:56,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,351 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -55.058685302734375
2023-01-07 08:47:56,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7627845406532288
2023-01-07 08:47:56,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.2587857246398926
2023-01-07 08:47:56,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,353 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -65.65093231201172
2023-01-07 08:47:56,353 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.138320207595825
2023-01-07 08:47:56,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -16.944175720214844
2023-01-07 08:47:56,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.436305999755859
2023-01-07 08:47:56,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,356 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.12543442845344543
2023-01-07 08:47:56,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,356 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -24.624771118164062
2023-01-07 08:47:56,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.244726181030273
2023-01-07 08:47:56,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,358 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -245.04006958007812
2023-01-07 08:47:56,358 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.353755474090576
2023-01-07 08:47:56,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -252.4337158203125
2023-01-07 08:47:56,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.998506546020508
2023-01-07 08:47:56,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,361 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -31.26028060913086
2023-01-07 08:47:56,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.955144882202148
2023-01-07 08:47:56,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -48.27054214477539
2023-01-07 08:47:56,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.248542070388794
2023-01-07 08:47:56,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,364 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 10.422501564025879
2023-01-07 08:47:56,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6894986629486084
2023-01-07 08:47:56,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,366 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 5.625026702880859
2023-01-07 08:47:56,366 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3193802833557129
2023-01-07 08:47:56,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,367 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -0.9851646423339844
2023-01-07 08:47:56,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09465004503726959
2023-01-07 08:47:56,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,368 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -4.974315643310547
2023-01-07 08:47:56,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.362982749938965
2023-01-07 08:47:56,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,370 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -2.197068214416504
2023-01-07 08:47:56,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.782047271728516
2023-01-07 08:47:56,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,371 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -20.73711395263672
2023-01-07 08:47:56,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.874668121337891
2023-01-07 08:47:56,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,373 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -6.217686653137207
2023-01-07 08:47:56,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9725741147994995
2023-01-07 08:47:56,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -70.34268951416016
2023-01-07 08:47:56,375 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2486941814422607
2023-01-07 08:47:56,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -180.862548828125
2023-01-07 08:47:56,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.695115089416504
2023-01-07 08:47:56,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,378 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -186.64013671875
2023-01-07 08:47:56,378 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.211872100830078
2023-01-07 08:47:56,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,379 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -29.72003936767578
2023-01-07 08:47:56,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.589831352233887
2023-01-07 08:47:56,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,381 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.06369498372077942
2023-01-07 08:47:56,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,381 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -34.40497589111328
2023-01-07 08:47:56,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6764644384384155
2023-01-07 08:47:56,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,383 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 12.864669799804688
2023-01-07 08:47:56,383 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.688230276107788
2023-01-07 08:47:56,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,384 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 27.809070587158203
2023-01-07 08:47:56,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6979210376739502
2023-01-07 08:47:56,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,386 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -184.1464385986328
2023-01-07 08:47:56,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.66975212097168
2023-01-07 08:47:56,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,387 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -170.82057189941406
2023-01-07 08:47:56,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.12258529663086
2023-01-07 08:47:56,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,389 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -49.79618835449219
2023-01-07 08:47:56,389 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7858679294586182
2023-01-07 08:47:56,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,390 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.40010344982147217
2023-01-07 08:47:56,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,390 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 0.000766754150390625
2023-01-07 08:47:56,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.032450556755066
2023-01-07 08:47:56,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,392 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -10.486457824707031
2023-01-07 08:47:56,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7603611350059509
2023-01-07 08:47:56,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,393 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -13.434843063354492
2023-01-07 08:47:56,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.76674747467041
2023-01-07 08:47:56,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,395 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 14.360976219177246
2023-01-07 08:47:56,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.564162254333496
2023-01-07 08:47:56,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,396 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 35.13771057128906
2023-01-07 08:47:56,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.701050281524658
2023-01-07 08:47:56,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,398 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 274.8064270019531
2023-01-07 08:47:56,398 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.975663185119629
2023-01-07 08:47:56,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,399 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 2.4912285804748535
2023-01-07 08:47:56,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,399 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 276.4137268066406
2023-01-07 08:47:56,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.888065338134766
2023-01-07 08:47:56,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,401 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 273.732177734375
2023-01-07 08:47:56,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.116268157958984
2023-01-07 08:47:56,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,402 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.33867430686950684
2023-01-07 08:47:56,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,403 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -90.7149887084961
2023-01-07 08:47:56,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,403 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 268.07415771484375
2023-01-07 08:47:56,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.982181549072266
2023-01-07 08:47:56,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,405 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 268.1853942871094
2023-01-07 08:47:56,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.62582015991211
2023-01-07 08:47:56,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,406 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -84.89429473876953
2023-01-07 08:47:56,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.69232177734375
2023-01-07 08:47:56,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,407 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -91.62408447265625
2023-01-07 08:47:56,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.301338195800781
2023-01-07 08:47:56,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,409 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -2.2250759601593018
2023-01-07 08:47:56,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,409 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -88.95562744140625
2023-01-07 08:47:56,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.474499702453613
2023-01-07 08:47:56,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,411 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -121.1796875
2023-01-07 08:47:56,411 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.802268981933594
2023-01-07 08:47:56,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,412 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 2.4729442596435547
2023-01-07 08:47:56,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,413 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 40.23948669433594
2023-01-07 08:47:56,413 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.729063034057617
2023-01-07 08:47:56,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,414 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -93.98890686035156
2023-01-07 08:47:56,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.27452278137207
2023-01-07 08:47:56,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,416 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -7.260440826416016
2023-01-07 08:47:56,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,416 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 242.48805236816406
2023-01-07 08:47:56,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.212649822235107
2023-01-07 08:47:56,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,418 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -129.6021270751953
2023-01-07 08:47:56,418 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.758875370025635
2023-01-07 08:47:56,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,419 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -22.622663497924805
2023-01-07 08:47:56,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.609291076660156
2023-01-07 08:47:56,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,420 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -191.49990844726562
2023-01-07 08:47:56,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2668333053588867
2023-01-07 08:47:56,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,422 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.782530665397644
2023-01-07 08:47:56,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,422 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 13.42927360534668
2023-01-07 08:47:56,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,423 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -260.86529541015625
2023-01-07 08:47:56,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,423 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 270.8522644042969
2023-01-07 08:47:56,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.761600494384766
2023-01-07 08:47:56,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,425 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -254.18081665039062
2023-01-07 08:47:56,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.22774600982666
2023-01-07 08:47:56,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,426 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 13.353538513183594
2023-01-07 08:47:56,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.34454917907715
2023-01-07 08:47:56,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,427 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -293.0856018066406
2023-01-07 08:47:56,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.37151336669922
2023-01-07 08:47:56,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,429 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -52.37542724609375
2023-01-07 08:47:56,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,429 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 283.1142578125
2023-01-07 08:47:56,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.425107955932617
2023-01-07 08:47:56,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,430 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 313.9827880859375
2023-01-07 08:47:56,431 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.742279052734375
2023-01-07 08:47:56,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,432 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.446154832839966
2023-01-07 08:47:56,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,432 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 130.7828826904297
2023-01-07 08:47:56,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -95.24273681640625
2023-01-07 08:47:56,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,434 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -45.842803955078125
2023-01-07 08:47:56,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 114.22388458251953
2023-01-07 08:47:56,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,435 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -461.7094421386719
2023-01-07 08:47:56,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.758277893066406
2023-01-07 08:47:56,440 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:47:56,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,440 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1111.525390625
2023-01-07 08:47:56,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,441 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -581.1495361328125
2023-01-07 08:47:56,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 282.96661376953125
2023-01-07 08:47:56,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,443 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 96.83320617675781
2023-01-07 08:47:56,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,443 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 20.600914001464844
2023-01-07 08:47:56,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 421.9240417480469
2023-01-07 08:47:56,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -143.1537322998047
2023-01-07 08:47:56,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -61.38715362548828
2023-01-07 08:47:56,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,446 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 31.000652313232422
2023-01-07 08:47:56,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,447 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -105.05462646484375
2023-01-07 08:47:56,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,447 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -145.18478393554688
2023-01-07 08:47:56,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -21.288829803466797
2023-01-07 08:47:56,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -13.60205078125
2023-01-07 08:47:56,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 11.424012184143066
2023-01-07 08:47:56,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,448 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -66.72509002685547
2023-01-07 08:47:56,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -252.2282257080078
2023-01-07 08:47:56,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -63.3237190246582
2023-01-07 08:47:56,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -66.6925048828125
2023-01-07 08:47:56,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,450 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -27.809593200683594
2023-01-07 08:47:56,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,450 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -5.095672607421875
2023-01-07 08:47:56,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,450 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 0.846163272857666
2023-01-07 08:47:56,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,451 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -2.6044273376464844
2023-01-07 08:47:56,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,451 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -22.863073348999023
2023-01-07 08:47:56,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,451 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -10.039849281311035
2023-01-07 08:47:56,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 4.36843204498291
2023-01-07 08:47:56,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 18.09275245666504
2023-01-07 08:47:56,452 > [DEBUG] 0 :: before allreduce fusion buffer :: -2047.049560546875
2023-01-07 08:47:56,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 1.8592486381530762
2023-01-07 08:47:56,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 3.1335325241088867
2023-01-07 08:47:56,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.863069534301758
2023-01-07 08:47:56,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 10.3253812789917
2023-01-07 08:47:56,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 18.6744384765625
2023-01-07 08:47:56,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,456 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 15.639480590820312
2023-01-07 08:47:56,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,456 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.929131507873535
2023-01-07 08:47:56,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,456 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -8.386852264404297
2023-01-07 08:47:56,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,457 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -58.93678665161133
2023-01-07 08:47:56,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.32038402557373
2023-01-07 08:47:56,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 15.065412521362305
2023-01-07 08:47:56,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 13.329275131225586
2023-01-07 08:47:56,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 11.152098655700684
2023-01-07 08:47:56,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1.4100351333618164
2023-01-07 08:47:56,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 45.88002014160156
2023-01-07 08:47:56,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,460 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -69.1287841796875
2023-01-07 08:47:56,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,460 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -71.4612045288086
2023-01-07 08:47:56,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,460 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -79.47795104980469
2023-01-07 08:47:56,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.278902053833008
2023-01-07 08:47:56,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -66.8171615600586
2023-01-07 08:47:56,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:56,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:56,462 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -897.57958984375
2023-01-07 08:47:56,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 511.777587890625
2023-01-07 08:47:57,304 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 0.5030450820922852
2023-01-07 08:47:57,305 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:57,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,305 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -1.7538700103759766
2023-01-07 08:47:57,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,306 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -356.93389892578125
2023-01-07 08:47:57,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 222.9432373046875
2023-01-07 08:47:57,307 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -0.6861143112182617
2023-01-07 08:47:57,307 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.6681442260742188
2023-01-07 08:47:57,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,307 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 175.83538818359375
2023-01-07 08:47:57,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,308 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -309.9444885253906
2023-01-07 08:47:57,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 179.10890197753906
2023-01-07 08:47:57,309 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -66.02064514160156
2023-01-07 08:47:57,309 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:57,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,310 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -298.79718017578125
2023-01-07 08:47:57,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 267.93865966796875
2023-01-07 08:47:57,311 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -66.54612731933594
2023-01-07 08:47:57,311 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.03573989868164
2023-01-07 08:47:57,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,311 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -417.5958251953125
2023-01-07 08:47:57,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.622282028198242
2023-01-07 08:47:57,312 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -329.4714050292969
2023-01-07 08:47:57,313 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:57,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,313 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -1.7530536651611328
2023-01-07 08:47:57,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,313 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -35.55458068847656
2023-01-07 08:47:57,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,313 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -413.8365478515625
2023-01-07 08:47:57,314 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.03863525390625
2023-01-07 08:47:57,315 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -3.9024343490600586
2023-01-07 08:47:57,315 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,315 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:57,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,315 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -396.4895324707031
2023-01-07 08:47:57,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.578514099121094
2023-01-07 08:47:57,316 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -82.21401977539062
2023-01-07 08:47:57,317 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,317 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:57,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -1.714914321899414
2023-01-07 08:47:57,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -68.76721954345703
2023-01-07 08:47:57,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.527851104736328
2023-01-07 08:47:57,318 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -4.944092750549316
2023-01-07 08:47:57,319 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,320 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:57,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,320 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -149.59808349609375
2023-01-07 08:47:57,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -104.69892120361328
2023-01-07 08:47:57,321 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -160.88818359375
2023-01-07 08:47:57,321 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:57,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,321 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -192.9892578125
2023-01-07 08:47:57,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.757423400878906
2023-01-07 08:47:57,323 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -24.524471282958984
2023-01-07 08:47:57,323 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:57,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,323 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -326.9888916015625
2023-01-07 08:47:57,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.243967056274414
2023-01-07 08:47:57,324 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -65.52965545654297
2023-01-07 08:47:57,324 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,325 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:57,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,325 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.4078714847564697
2023-01-07 08:47:57,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,325 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -314.72998046875
2023-01-07 08:47:57,325 > [DEBUG] 0 :: before allreduce fusion buffer :: 50.88020706176758
2023-01-07 08:47:57,327 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.8281915187835693
2023-01-07 08:47:57,327 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -15.939157485961914
2023-01-07 08:47:57,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,327 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -184.8974609375
2023-01-07 08:47:57,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 126.54486846923828
2023-01-07 08:47:57,328 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.614200592041016
2023-01-07 08:47:57,329 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -177.21754455566406
2023-01-07 08:47:57,329 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:57,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,329 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 3.131901979446411
2023-01-07 08:47:57,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,329 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -116.142333984375
2023-01-07 08:47:57,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.598664283752441
2023-01-07 08:47:57,331 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 2.0329017639160156
2023-01-07 08:47:57,331 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:57,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,331 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -440.61407470703125
2023-01-07 08:47:57,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.75046157836914
2023-01-07 08:47:57,332 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -109.56964874267578
2023-01-07 08:47:57,332 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:57,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,333 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 8.492202758789062
2023-01-07 08:47:57,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 42.94734573364258
2023-01-07 08:47:57,334 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -11.257925033569336
2023-01-07 08:47:57,334 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,334 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,334 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 5.868078231811523
2023-01-07 08:47:57,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,334 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 66.77108764648438
2023-01-07 08:47:57,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,335 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -418.6253662109375
2023-01-07 08:47:57,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.746478080749512
2023-01-07 08:47:57,336 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 58.286293029785156
2023-01-07 08:47:57,336 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:57,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,337 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -402.9981384277344
2023-01-07 08:47:57,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.501914978027344
2023-01-07 08:47:57,338 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -14.949073791503906
2023-01-07 08:47:57,338 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -4.729685306549072
2023-01-07 08:47:57,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,338 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -348.06072998046875
2023-01-07 08:47:57,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.592105865478516
2023-01-07 08:47:57,339 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -356.0721130371094
2023-01-07 08:47:57,339 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:47:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.008229494094848633
2023-01-07 08:47:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 6.649609088897705
2023-01-07 08:47:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -61.62629699707031
2023-01-07 08:47:57,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.141024589538574
2023-01-07 08:47:57,342 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.361630916595459
2023-01-07 08:47:57,342 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:57,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,342 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 126.09557342529297
2023-01-07 08:47:57,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.261106491088867
2023-01-07 08:47:57,344 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 46.336273193359375
2023-01-07 08:47:57,344 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -32.44528579711914
2023-01-07 08:47:57,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,344 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -75.846435546875
2023-01-07 08:47:57,344 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.42344665527344
2023-01-07 08:47:57,345 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -7.343093395233154
2023-01-07 08:47:57,345 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:57,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,346 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 133.4484405517578
2023-01-07 08:47:57,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.352836608886719
2023-01-07 08:47:57,347 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -103.57160949707031
2023-01-07 08:47:57,347 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:57,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,348 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 144.36721801757812
2023-01-07 08:47:57,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.601633071899414
2023-01-07 08:47:57,349 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -7.4335832595825195
2023-01-07 08:47:57,349 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.15638256072998
2023-01-07 08:47:57,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,349 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 116.52493286132812
2023-01-07 08:47:57,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.68861389160156
2023-01-07 08:47:57,350 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 138.765380859375
2023-01-07 08:47:57,351 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,351 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,351 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.6547563076019287
2023-01-07 08:47:57,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,351 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 88.66556549072266
2023-01-07 08:47:57,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.14409637451172
2023-01-07 08:47:57,352 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 1.0046119689941406
2023-01-07 08:47:57,353 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,353 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -13.1309175491333
2023-01-07 08:47:57,353 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,353 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,353 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 91.22488403320312
2023-01-07 08:47:57,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.143549919128418
2023-01-07 08:47:57,354 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 90.46563720703125
2023-01-07 08:47:57,354 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:57,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,355 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 1.1025657653808594
2023-01-07 08:47:57,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,355 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 21.846309661865234
2023-01-07 08:47:57,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.0185661315918
2023-01-07 08:47:57,356 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.03526139259338379
2023-01-07 08:47:57,356 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.17344856262207
2023-01-07 08:47:57,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,357 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 42.03599166870117
2023-01-07 08:47:57,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.594233512878418
2023-01-07 08:47:57,358 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 51.79768753051758
2023-01-07 08:47:57,358 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:57,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,358 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 457.9371032714844
2023-01-07 08:47:57,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.316678285598755
2023-01-07 08:47:57,359 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -8.749027252197266
2023-01-07 08:47:57,360 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 38.826271057128906
2023-01-07 08:47:57,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,360 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 471.86456298828125
2023-01-07 08:47:57,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.934974670410156
2023-01-07 08:47:57,361 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 478.00439453125
2023-01-07 08:47:57,361 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,362 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 1.154305100440979
2023-01-07 08:47:57,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,362 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 87.64574432373047
2023-01-07 08:47:57,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.398151397705078
2023-01-07 08:47:57,363 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.6003516912460327
2023-01-07 08:47:57,363 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.533244132995605
2023-01-07 08:47:57,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,364 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 60.16582489013672
2023-01-07 08:47:57,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.272130966186523
2023-01-07 08:47:57,365 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 16.5137939453125
2023-01-07 08:47:57,365 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,365 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.04068005084991455
2023-01-07 08:47:57,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,366 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -57.792198181152344
2023-01-07 08:47:57,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.283552169799805
2023-01-07 08:47:57,367 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -0.7960507273674011
2023-01-07 08:47:57,367 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 31.334651947021484
2023-01-07 08:47:57,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,368 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -40.09461975097656
2023-01-07 08:47:57,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2696104049682617
2023-01-07 08:47:57,369 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -39.001190185546875
2023-01-07 08:47:57,369 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,369 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:57,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,369 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 30.408235549926758
2023-01-07 08:47:57,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.621462106704712
2023-01-07 08:47:57,370 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -14.750941276550293
2023-01-07 08:47:57,370 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 69.69357299804688
2023-01-07 08:47:57,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,371 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 30.398805618286133
2023-01-07 08:47:57,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.841709613800049
2023-01-07 08:47:57,372 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 29.742568969726562
2023-01-07 08:47:57,372 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,373 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.17025607824325562
2023-01-07 08:47:57,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,373 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -104.28439331054688
2023-01-07 08:47:57,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.203506469726562
2023-01-07 08:47:57,374 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.5949209332466125
2023-01-07 08:47:57,374 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -2.319789171218872
2023-01-07 08:47:57,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -81.11915588378906
2023-01-07 08:47:57,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.97974395751953
2023-01-07 08:47:57,376 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -86.03557586669922
2023-01-07 08:47:57,376 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,376 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.5205366611480713
2023-01-07 08:47:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -145.27322387695312
2023-01-07 08:47:57,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.995061874389648
2023-01-07 08:47:57,378 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.13086700439453125
2023-01-07 08:47:57,378 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 6.020914077758789
2023-01-07 08:47:57,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,378 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -137.52218627929688
2023-01-07 08:47:57,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.710709571838379
2023-01-07 08:47:57,380 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -139.78195190429688
2023-01-07 08:47:57,380 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:57,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,380 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.33993765711784363
2023-01-07 08:47:57,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,380 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -21.171106338500977
2023-01-07 08:47:57,381 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9407176971435547
2023-01-07 08:47:57,382 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -0.2812443971633911
2023-01-07 08:47:57,382 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,382 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,382 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 29.095399856567383
2023-01-07 08:47:57,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -18.045730590820312
2023-01-07 08:47:57,382 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.150166034698486
2023-01-07 08:47:57,383 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -20.982961654663086
2023-01-07 08:47:57,383 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,384 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.13365545868873596
2023-01-07 08:47:57,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,384 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -11.822397232055664
2023-01-07 08:47:57,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.745624542236328
2023-01-07 08:47:57,385 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -0.3900541365146637
2023-01-07 08:47:57,385 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -7.9246392250061035
2023-01-07 08:47:57,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,386 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 24.90044403076172
2023-01-07 08:47:57,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.840564727783203
2023-01-07 08:47:57,387 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 26.858186721801758
2023-01-07 08:47:57,388 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:47:57,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,388 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.5417532324790955
2023-01-07 08:47:57,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,388 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 11.383262634277344
2023-01-07 08:47:57,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.267448425292969
2023-01-07 08:47:57,389 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.22466258704662323
2023-01-07 08:47:57,389 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,390 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 8.432145118713379
2023-01-07 08:47:57,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,390 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 18.180564880371094
2023-01-07 08:47:57,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.56791353225708
2023-01-07 08:47:57,391 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 19.951807022094727
2023-01-07 08:47:57,391 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:47:57,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,392 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.5170749425888062
2023-01-07 08:47:57,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,392 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -72.86405944824219
2023-01-07 08:47:57,392 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9338293075561523
2023-01-07 08:47:57,393 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -0.40080273151397705
2023-01-07 08:47:57,393 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -22.02167510986328
2023-01-07 08:47:57,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,394 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -70.12673950195312
2023-01-07 08:47:57,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.693124294281006
2023-01-07 08:47:57,395 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -73.2943344116211
2023-01-07 08:47:57,395 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,395 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:57,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.7749730944633484
2023-01-07 08:47:57,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,396 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -545.3970336914062
2023-01-07 08:47:57,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.234210968017578
2023-01-07 08:47:57,397 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5267699360847473
2023-01-07 08:47:57,397 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 8.97386360168457
2023-01-07 08:47:57,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -530.9885864257812
2023-01-07 08:47:57,398 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.954017639160156
2023-01-07 08:47:57,399 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -526.2999267578125
2023-01-07 08:47:57,399 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:57,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,399 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 46.82197570800781
2023-01-07 08:47:57,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7604848146438599
2023-01-07 08:47:57,400 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -2.973567247390747
2023-01-07 08:47:57,400 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -29.509647369384766
2023-01-07 08:47:57,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,401 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.16623306274414
2023-01-07 08:47:57,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7949066162109375
2023-01-07 08:47:57,402 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 45.617897033691406
2023-01-07 08:47:57,402 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:57,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 361.4782409667969
2023-01-07 08:47:57,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.115604400634766
2023-01-07 08:47:57,404 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -26.03289031982422
2023-01-07 08:47:57,404 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -38.889408111572266
2023-01-07 08:47:57,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,404 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 359.17254638671875
2023-01-07 08:47:57,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.045694351196289
2023-01-07 08:47:57,405 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 358.9486389160156
2023-01-07 08:47:57,406 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,406 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,406 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:57,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,406 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 254.033447265625
2023-01-07 08:47:57,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8327673077583313
2023-01-07 08:47:57,407 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -8.403985023498535
2023-01-07 08:47:57,407 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 15.997356414794922
2023-01-07 08:47:57,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,408 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 255.5137481689453
2023-01-07 08:47:57,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6209540367126465
2023-01-07 08:47:57,409 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 254.67709350585938
2023-01-07 08:47:57,409 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,409 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:47:57,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,409 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.27930161356925964
2023-01-07 08:47:57,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 564.8917236328125
2023-01-07 08:47:57,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.592473030090332
2023-01-07 08:47:57,411 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.8414417505264282
2023-01-07 08:47:57,411 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,411 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,411 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -10.42170524597168
2023-01-07 08:47:57,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,412 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 572.595703125
2023-01-07 08:47:57,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.161548614501953
2023-01-07 08:47:57,413 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 570.8067626953125
2023-01-07 08:47:57,413 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:57,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,413 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 135.05657958984375
2023-01-07 08:47:57,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.759940147399902
2023-01-07 08:47:57,414 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 1.5710026025772095
2023-01-07 08:47:57,415 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 17.1250057220459
2023-01-07 08:47:57,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,415 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 134.19052124023438
2023-01-07 08:47:57,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.376048743724823
2023-01-07 08:47:57,416 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 128.20913696289062
2023-01-07 08:47:57,416 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,416 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:57,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,417 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 6.116336345672607
2023-01-07 08:47:57,417 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5061712265014648
2023-01-07 08:47:57,418 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -7.90610933303833
2023-01-07 08:47:57,418 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 22.394729614257812
2023-01-07 08:47:57,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 7.394380569458008
2023-01-07 08:47:57,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.996770143508911
2023-01-07 08:47:57,420 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 11.989557266235352
2023-01-07 08:47:57,420 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:57,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.185052871704102
2023-01-07 08:47:57,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.608930587768555
2023-01-07 08:47:57,421 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -2.380173921585083
2023-01-07 08:47:57,421 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,421 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 26.777698516845703
2023-01-07 08:47:57,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,422 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -7.454805374145508
2023-01-07 08:47:57,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.292882919311523
2023-01-07 08:47:57,423 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -10.593119621276855
2023-01-07 08:47:57,423 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,423 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,423 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:57,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,424 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -21.786174774169922
2023-01-07 08:47:57,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.450806140899658
2023-01-07 08:47:57,425 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -1.9765996932983398
2023-01-07 08:47:57,425 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 20.944562911987305
2023-01-07 08:47:57,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,425 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -22.84027099609375
2023-01-07 08:47:57,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6612930297851562
2023-01-07 08:47:57,426 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -27.149612426757812
2023-01-07 08:47:57,427 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,427 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:57,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,427 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -9.682540893554688
2023-01-07 08:47:57,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4196746349334717
2023-01-07 08:47:57,428 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -2.316148281097412
2023-01-07 08:47:57,428 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -39.973758697509766
2023-01-07 08:47:57,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,429 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -12.426046371459961
2023-01-07 08:47:57,429 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.099364280700684
2023-01-07 08:47:57,430 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -10.425920486450195
2023-01-07 08:47:57,430 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,430 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,430 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:57,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -63.08830261230469
2023-01-07 08:47:57,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8058196306228638
2023-01-07 08:47:57,431 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -1.2232022285461426
2023-01-07 08:47:57,432 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,432 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,432 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 11.881229400634766
2023-01-07 08:47:57,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,432 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -59.04884338378906
2023-01-07 08:47:57,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.676053047180176
2023-01-07 08:47:57,433 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -60.505252838134766
2023-01-07 08:47:57,433 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,434 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:57,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,434 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -7.835846424102783
2023-01-07 08:47:57,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3391385078430176
2023-01-07 08:47:57,435 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -1.2601549625396729
2023-01-07 08:47:57,435 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: -13.458125114440918
2023-01-07 08:47:57,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,435 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -8.429296493530273
2023-01-07 08:47:57,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.332033634185791
2023-01-07 08:47:57,437 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -10.787359237670898
2023-01-07 08:47:57,437 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,437 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,437 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:57,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,437 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 13.430371284484863
2023-01-07 08:47:57,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.561528205871582
2023-01-07 08:47:57,438 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -1.3630831241607666
2023-01-07 08:47:57,438 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -3.974179267883301
2023-01-07 08:47:57,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,439 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 12.59850788116455
2023-01-07 08:47:57,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8783609867095947
2023-01-07 08:47:57,440 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 12.139960289001465
2023-01-07 08:47:57,440 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,440 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,440 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:57,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,441 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -545.441162109375
2023-01-07 08:47:57,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.2989821434021
2023-01-07 08:47:57,442 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.7339833974838257
2023-01-07 08:47:57,442 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,442 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 34.85069274902344
2023-01-07 08:47:57,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,442 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -546.30029296875
2023-01-07 08:47:57,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.49302542209625244
2023-01-07 08:47:57,443 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -547.62060546875
2023-01-07 08:47:57,444 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,444 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:57,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,444 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1669.334228515625
2023-01-07 08:47:57,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5220791101455688
2023-01-07 08:47:57,445 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.7511260509490967
2023-01-07 08:47:57,445 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,445 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 1.55181884765625
2023-01-07 08:47:57,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,446 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1669.99169921875
2023-01-07 08:47:57,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5250440835952759
2023-01-07 08:47:57,447 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -1670.73876953125
2023-01-07 08:47:57,447 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:57,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,447 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 406.144287109375
2023-01-07 08:47:57,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5600569248199463
2023-01-07 08:47:57,448 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.6531928777694702
2023-01-07 08:47:57,449 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: 42.79032897949219
2023-01-07 08:47:57,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 405.2669677734375
2023-01-07 08:47:57,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6678442358970642
2023-01-07 08:47:57,450 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 406.86358642578125
2023-01-07 08:47:57,450 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:57,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,451 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 573.4064331054688
2023-01-07 08:47:57,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0850989818573
2023-01-07 08:47:57,452 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.7492457032203674
2023-01-07 08:47:57,452 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 40.70623016357422
2023-01-07 08:47:57,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 573.3467407226562
2023-01-07 08:47:57,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23246456682682037
2023-01-07 08:47:57,454 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 575.4923706054688
2023-01-07 08:47:57,454 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,454 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:57,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 475.80230712890625
2023-01-07 08:47:57,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2818231582641602
2023-01-07 08:47:57,455 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -0.5508977770805359
2023-01-07 08:47:57,456 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 4.519495964050293
2023-01-07 08:47:57,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,456 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 476.123046875
2023-01-07 08:47:57,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3033037483692169
2023-01-07 08:47:57,457 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 475.8698425292969
2023-01-07 08:47:57,457 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:57,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,458 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1109.779296875
2023-01-07 08:47:57,458 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7710506916046143
2023-01-07 08:47:57,459 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -1.8320820331573486
2023-01-07 08:47:57,459 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,459 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,459 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.904972076416016
2023-01-07 08:47:57,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,459 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1109.510986328125
2023-01-07 08:47:57,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09425220638513565
2023-01-07 08:47:57,461 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 1108.814208984375
2023-01-07 08:47:57,461 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:57,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,461 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 728.4189453125
2023-01-07 08:47:57,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.314068466424942
2023-01-07 08:47:57,462 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.7176740765571594
2023-01-07 08:47:57,462 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -9.206289291381836
2023-01-07 08:47:57,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,463 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 728.7064208984375
2023-01-07 08:47:57,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15722976624965668
2023-01-07 08:47:57,464 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 729.0619506835938
2023-01-07 08:47:57,464 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:57,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,465 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -942.1343994140625
2023-01-07 08:47:57,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27682146430015564
2023-01-07 08:47:57,466 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.24721166491508484
2023-01-07 08:47:57,466 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,466 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,466 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.841129302978516
2023-01-07 08:47:57,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,466 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -941.0338134765625
2023-01-07 08:47:57,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.825173556804657
2023-01-07 08:47:57,468 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -941.53271484375
2023-01-07 08:47:57,468 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,468 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:57,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,468 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2290.4736328125
2023-01-07 08:47:57,468 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8800573348999023
2023-01-07 08:47:57,469 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -0.48168736696243286
2023-01-07 08:47:57,469 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 37.729881286621094
2023-01-07 08:47:57,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,470 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2289.328125
2023-01-07 08:47:57,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8554176092147827
2023-01-07 08:47:57,471 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2289.372314453125
2023-01-07 08:47:57,471 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,471 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:57,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,471 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1108.846435546875
2023-01-07 08:47:57,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011823214590549469
2023-01-07 08:47:57,472 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.5450150966644287
2023-01-07 08:47:57,473 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,473 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,473 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 106.68621063232422
2023-01-07 08:47:57,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,473 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 1109.5440673828125
2023-01-07 08:47:57,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16751611232757568
2023-01-07 08:47:57,474 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 1107.9920654296875
2023-01-07 08:47:57,474 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:57,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,475 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -2417.20947265625
2023-01-07 08:47:57,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6498664617538452
2023-01-07 08:47:57,476 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -1.0171632766723633
2023-01-07 08:47:57,476 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 26.0207576751709
2023-01-07 08:47:57,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,476 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -2416.61865234375
2023-01-07 08:47:57,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08657731860876083
2023-01-07 08:47:57,478 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -2416.536865234375
2023-01-07 08:47:57,478 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:57,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,478 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2771.62451171875
2023-01-07 08:47:57,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8269679546356201
2023-01-07 08:47:57,479 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.2214946150779724
2023-01-07 08:47:57,479 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,479 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.245820045471191
2023-01-07 08:47:57,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,480 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -2771.67431640625
2023-01-07 08:47:57,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18479537963867188
2023-01-07 08:47:57,481 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2771.9072265625
2023-01-07 08:47:57,481 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:57,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,482 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8651.388671875
2023-01-07 08:47:57,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2059352993965149
2023-01-07 08:47:57,483 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -66.5770034790039
2023-01-07 08:47:57,483 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 76.26571655273438
2023-01-07 08:47:57,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,483 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 8651.09765625
2023-01-07 08:47:57,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.323192834854126
2023-01-07 08:47:57,484 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 8650.873046875
2023-01-07 08:47:57,485 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,485 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,485 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:57,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,485 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 373.9753723144531
2023-01-07 08:47:57,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19135180115699768
2023-01-07 08:47:57,486 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -67.01539611816406
2023-01-07 08:47:57,486 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -52.239898681640625
2023-01-07 08:47:57,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,487 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 374.0981750488281
2023-01-07 08:47:57,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33124905824661255
2023-01-07 08:47:57,488 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 374.1523132324219
2023-01-07 08:47:57,488 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,488 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,488 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:57,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,488 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4301.2001953125
2023-01-07 08:47:57,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3869589865207672
2023-01-07 08:47:57,489 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.04744582250714302
2023-01-07 08:47:57,490 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,490 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,490 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 3.7674198150634766
2023-01-07 08:47:57,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,490 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -4301.4384765625
2023-01-07 08:47:57,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3702874183654785
2023-01-07 08:47:57,491 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -4301.44970703125
2023-01-07 08:47:57,491 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:57,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,492 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 12817.357421875
2023-01-07 08:47:57,492 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0642590522766113
2023-01-07 08:47:57,493 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -410.3653564453125
2023-01-07 08:47:57,493 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,493 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,493 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.19533872604370117
2023-01-07 08:47:57,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,493 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 12818.8232421875
2023-01-07 08:47:57,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.179945707321167
2023-01-07 08:47:57,495 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 12819.5947265625
2023-01-07 08:47:57,495 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:47:57,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:47:57,496 > [DEBUG] 0 :: 7.041346073150635
2023-01-07 08:47:57,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,500 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0364990234375
2023-01-07 08:47:57,500 > [DEBUG] 0 :: before allreduce fusion buffer :: -331.470458984375
2023-01-07 08:47:57,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,503 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.18943913280963898
2023-01-07 08:47:57,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,504 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -260.11871337890625
2023-01-07 08:47:57,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -324.2761535644531
2023-01-07 08:47:57,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.11015880107879639
2023-01-07 08:47:57,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03708932548761368
2023-01-07 08:47:57,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.01154683530330658
2023-01-07 08:47:57,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -108.21195983886719
2023-01-07 08:47:57,511 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.246870994567871
2023-01-07 08:47:57,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,513 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 6.597483158111572
2023-01-07 08:47:57,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4198632836341858
2023-01-07 08:47:57,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,514 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.01589871011674404
2023-01-07 08:47:57,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,514 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -101.28799438476562
2023-01-07 08:47:57,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3710295557975769
2023-01-07 08:47:57,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,516 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.20566558837890625
2023-01-07 08:47:57,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09780673682689667
2023-01-07 08:47:57,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,517 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.10612478107213974
2023-01-07 08:47:57,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,517 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -108.23138427734375
2023-01-07 08:47:57,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5251861810684204
2023-01-07 08:47:57,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,519 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2.411074161529541
2023-01-07 08:47:57,519 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3680492639541626
2023-01-07 08:47:57,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,520 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.047173354774713516
2023-01-07 08:47:57,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,520 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -105.48774719238281
2023-01-07 08:47:57,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8095955848693848
2023-01-07 08:47:57,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,522 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 20.746688842773438
2023-01-07 08:47:57,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34116417169570923
2023-01-07 08:47:57,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,523 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.036444976925849915
2023-01-07 08:47:57,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,524 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 21.214569091796875
2023-01-07 08:47:57,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2526320219039917
2023-01-07 08:47:57,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,525 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 12.477667808532715
2023-01-07 08:47:57,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2513785660266876
2023-01-07 08:47:57,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,526 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.1481398493051529
2023-01-07 08:47:57,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,527 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 13.796698570251465
2023-01-07 08:47:57,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2448248863220215
2023-01-07 08:47:57,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,528 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7.017782211303711
2023-01-07 08:47:57,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5953364372253418
2023-01-07 08:47:57,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,530 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1288403868675232
2023-01-07 08:47:57,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,530 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -5.267569541931152
2023-01-07 08:47:57,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0962097644805908
2023-01-07 08:47:57,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,531 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -0.28687381744384766
2023-01-07 08:47:57,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2522440552711487
2023-01-07 08:47:57,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,533 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.012984231114387512
2023-01-07 08:47:57,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,533 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.0454049110412598
2023-01-07 08:47:57,533 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.800297498703003
2023-01-07 08:47:57,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,536 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -34.07612991333008
2023-01-07 08:47:57,536 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7545431852340698
2023-01-07 08:47:57,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,537 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0019050240516662598
2023-01-07 08:47:57,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,537 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -37.02024841308594
2023-01-07 08:47:57,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2575717568397522
2023-01-07 08:47:57,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,539 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -11.645602226257324
2023-01-07 08:47:57,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3012385368347168
2023-01-07 08:47:57,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,540 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.15993468463420868
2023-01-07 08:47:57,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,540 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -14.609551429748535
2023-01-07 08:47:57,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.486202597618103
2023-01-07 08:47:57,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,542 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 5.218864917755127
2023-01-07 08:47:57,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24565887451171875
2023-01-07 08:47:57,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.055880751460790634
2023-01-07 08:47:57,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 2.673612356185913
2023-01-07 08:47:57,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3086981177330017
2023-01-07 08:47:57,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,545 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -22.52374267578125
2023-01-07 08:47:57,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32492005825042725
2023-01-07 08:47:57,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.028229989111423492
2023-01-07 08:47:57,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -24.09347152709961
2023-01-07 08:47:57,547 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6994438171386719
2023-01-07 08:47:57,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -11.289437294006348
2023-01-07 08:47:57,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2068377137184143
2023-01-07 08:47:57,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.020269010215997696
2023-01-07 08:47:57,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -14.823010444641113
2023-01-07 08:47:57,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.853379011154175
2023-01-07 08:47:57,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 3.0636532306671143
2023-01-07 08:47:57,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6271637082099915
2023-01-07 08:47:57,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,553 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.032783541828393936
2023-01-07 08:47:57,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,553 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 0.8798007965087891
2023-01-07 08:47:57,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.772874355316162
2023-01-07 08:47:57,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.5947494506835938
2023-01-07 08:47:57,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24386313557624817
2023-01-07 08:47:57,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,556 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.12245163321495056
2023-01-07 08:47:57,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,556 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 3.942068099975586
2023-01-07 08:47:57,556 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8235647678375244
2023-01-07 08:47:57,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 9.907817840576172
2023-01-07 08:47:57,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0378432273864746
2023-01-07 08:47:57,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,559 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.08322550356388092
2023-01-07 08:47:57,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,559 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 13.199556350708008
2023-01-07 08:47:57,559 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6154255270957947
2023-01-07 08:47:57,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,561 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 9.983582496643066
2023-01-07 08:47:57,561 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.125176191329956
2023-01-07 08:47:57,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.015345897525548935
2023-01-07 08:47:57,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 11.843074798583984
2023-01-07 08:47:57,563 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16654357314109802
2023-01-07 08:47:57,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,564 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -47.284175872802734
2023-01-07 08:47:57,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2757634222507477
2023-01-07 08:47:57,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.17238987982273102
2023-01-07 08:47:57,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -42.946205139160156
2023-01-07 08:47:57,566 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2761387825012207
2023-01-07 08:47:57,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1.295459270477295
2023-01-07 08:47:57,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5199674367904663
2023-01-07 08:47:57,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.34858959913253784
2023-01-07 08:47:57,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 4.324718475341797
2023-01-07 08:47:57,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1878819465637207
2023-01-07 08:47:57,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 7.6937150955200195
2023-01-07 08:47:57,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0250270366668701
2023-01-07 08:47:57,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.19038686156272888
2023-01-07 08:47:57,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 12.130887031555176
2023-01-07 08:47:57,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3870649337768555
2023-01-07 08:47:57,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 31.939390182495117
2023-01-07 08:47:57,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.029736518859863
2023-01-07 08:47:57,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.1039770245552063
2023-01-07 08:47:57,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 35.73484802246094
2023-01-07 08:47:57,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.370092391967773
2023-01-07 08:47:57,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 6.352456092834473
2023-01-07 08:47:57,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08517313003540039
2023-01-07 08:47:57,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,577 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.530866265296936
2023-01-07 08:47:57,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 18.14935874938965
2023-01-07 08:47:57,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.251950979232788
2023-01-07 08:47:57,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,579 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 27.899845123291016
2023-01-07 08:47:57,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02642083168029785
2023-01-07 08:47:57,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.013148263096809387
2023-01-07 08:47:57,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 18.174972534179688
2023-01-07 08:47:57,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.92321252822876
2023-01-07 08:47:57,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,583 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -79.05342102050781
2023-01-07 08:47:57,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9857050180435181
2023-01-07 08:47:57,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,584 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -78.08647155761719
2023-01-07 08:47:57,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.154176712036133
2023-01-07 08:47:57,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,585 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -6.52048921585083
2023-01-07 08:47:57,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8484326601028442
2023-01-07 08:47:57,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.13784870505332947
2023-01-07 08:47:57,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1.8662500381469727
2023-01-07 08:47:57,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.727302551269531
2023-01-07 08:47:57,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,588 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 68.77755737304688
2023-01-07 08:47:57,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.577636241912842
2023-01-07 08:47:57,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.5031305551528931
2023-01-07 08:47:57,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 70.8687973022461
2023-01-07 08:47:57,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7617897987365723
2023-01-07 08:47:57,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,591 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 37.511863708496094
2023-01-07 08:47:57,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.455033779144287
2023-01-07 08:47:57,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.0655982494354248
2023-01-07 08:47:57,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 41.20330047607422
2023-01-07 08:47:57,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.550265312194824
2023-01-07 08:47:57,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 148.07177734375
2023-01-07 08:47:57,595 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.181294441223145
2023-01-07 08:47:57,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 152.99122619628906
2023-01-07 08:47:57,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.158844947814941
2023-01-07 08:47:57,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,597 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 11.953763961791992
2023-01-07 08:47:57,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3110949993133545
2023-01-07 08:47:57,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -10.364530563354492
2023-01-07 08:47:57,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.734170913696289
2023-01-07 08:47:57,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,600 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 5.168032646179199
2023-01-07 08:47:57,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4439620971679688
2023-01-07 08:47:57,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,601 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 0.6884708404541016
2023-01-07 08:47:57,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.928467750549316
2023-01-07 08:47:57,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,603 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -162.64596557617188
2023-01-07 08:47:57,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.968149662017822
2023-01-07 08:47:57,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,604 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -170.268798828125
2023-01-07 08:47:57,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.330821990966797
2023-01-07 08:47:57,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,605 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -19.264209747314453
2023-01-07 08:47:57,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3619718551635742
2023-01-07 08:47:57,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -24.080766677856445
2023-01-07 08:47:57,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9113175868988037
2023-01-07 08:47:57,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 3.3793163299560547
2023-01-07 08:47:57,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0349721908569336
2023-01-07 08:47:57,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,609 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 15.33030891418457
2023-01-07 08:47:57,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0651627779006958
2023-01-07 08:47:57,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 67.33849334716797
2023-01-07 08:47:57,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.823331832885742
2023-01-07 08:47:57,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 66.22530364990234
2023-01-07 08:47:57,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.43843936920166
2023-01-07 08:47:57,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,614 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -23.272775650024414
2023-01-07 08:47:57,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6918860673904419
2023-01-07 08:47:57,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.37279850244522095
2023-01-07 08:47:57,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -24.442724227905273
2023-01-07 08:47:57,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1634621620178223
2023-01-07 08:47:57,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,617 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -13.83966064453125
2023-01-07 08:47:57,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0467851161956787
2023-01-07 08:47:57,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,618 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -20.63488006591797
2023-01-07 08:47:57,618 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4774317741394043
2023-01-07 08:47:57,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,620 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 48.97734069824219
2023-01-07 08:47:57,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.44945228099823
2023-01-07 08:47:57,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 6.802513122558594
2023-01-07 08:47:57,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 79.9981918334961
2023-01-07 08:47:57,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,622 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 19.386943817138672
2023-01-07 08:47:57,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.650254964828491
2023-01-07 08:47:57,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,623 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.0048103332519531
2023-01-07 08:47:57,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -24.784421920776367
2023-01-07 08:47:57,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7201077938079834
2023-01-07 08:47:57,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,625 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -53.089176177978516
2023-01-07 08:47:57,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8742138147354126
2023-01-07 08:47:57,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,627 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -65.27668762207031
2023-01-07 08:47:57,627 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.19562816619873
2023-01-07 08:47:57,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,628 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -37.00703430175781
2023-01-07 08:47:57,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.746144771575928
2023-01-07 08:47:57,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,629 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -105.02810668945312
2023-01-07 08:47:57,629 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.568610191345215
2023-01-07 08:47:57,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,631 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -359.46148681640625
2023-01-07 08:47:57,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.25234031677246
2023-01-07 08:47:57,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,632 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.3736083507537842
2023-01-07 08:47:57,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,632 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -431.78448486328125
2023-01-07 08:47:57,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.392892837524414
2023-01-07 08:47:57,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,634 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -550.1495361328125
2023-01-07 08:47:57,634 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.522910118103027
2023-01-07 08:47:57,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,635 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8551222085952759
2023-01-07 08:47:57,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,635 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -95.4090347290039
2023-01-07 08:47:57,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -630.2388916015625
2023-01-07 08:47:57,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6296656131744385
2023-01-07 08:47:57,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -665.7450561523438
2023-01-07 08:47:57,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1720261573791504
2023-01-07 08:47:57,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,638 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -274.3583984375
2023-01-07 08:47:57,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -58.321598052978516
2023-01-07 08:47:57,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,640 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -263.783203125
2023-01-07 08:47:57,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.57096481323242
2023-01-07 08:47:57,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,641 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.2053864002227783
2023-01-07 08:47:57,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,641 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -332.2088623046875
2023-01-07 08:47:57,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -78.47229766845703
2023-01-07 08:47:57,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,643 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -568.897705078125
2023-01-07 08:47:57,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.396244049072266
2023-01-07 08:47:57,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 4.475648403167725
2023-01-07 08:47:57,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -33.16648864746094
2023-01-07 08:47:57,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.115434169769287
2023-01-07 08:47:57,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -767.607421875
2023-01-07 08:47:57,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.922648906707764
2023-01-07 08:47:57,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -43.23617935180664
2023-01-07 08:47:57,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,647 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -713.0237426757812
2023-01-07 08:47:57,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.72527313232422
2023-01-07 08:47:57,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -891.5291748046875
2023-01-07 08:47:57,649 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.846435546875
2023-01-07 08:47:57,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,650 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 151.99551391601562
2023-01-07 08:47:57,650 > [DEBUG] 0 :: before allreduce fusion buffer :: -85.29600524902344
2023-01-07 08:47:57,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,652 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1019.056396484375
2023-01-07 08:47:57,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7491321563720703
2023-01-07 08:47:57,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,653 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 7.497010231018066
2023-01-07 08:47:57,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,654 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 16.803890228271484
2023-01-07 08:47:57,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,654 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1230.6295166015625
2023-01-07 08:47:57,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -764.16162109375
2023-01-07 08:47:57,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.920133590698242
2023-01-07 08:47:57,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,656 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1278.26513671875
2023-01-07 08:47:57,656 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23547649383544922
2023-01-07 08:47:57,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,657 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -56.16027069091797
2023-01-07 08:47:57,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.65357208251953
2023-01-07 08:47:57,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,659 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1356.045166015625
2023-01-07 08:47:57,659 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.871654510498047
2023-01-07 08:47:57,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -465.083251953125
2023-01-07 08:47:57,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,660 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -743.2293701171875
2023-01-07 08:47:57,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.596675872802734
2023-01-07 08:47:57,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,662 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 214.86122131347656
2023-01-07 08:47:57,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.7056884765625
2023-01-07 08:47:57,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -0.2660691738128662
2023-01-07 08:47:57,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,663 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -22.226318359375
2023-01-07 08:47:57,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.05131530761719
2023-01-07 08:47:57,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,665 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -431.21099853515625
2023-01-07 08:47:57,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 170.5592803955078
2023-01-07 08:47:57,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:47:57,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:47:57,666 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -807.873046875
2023-01-07 08:47:57,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.01905059814453
