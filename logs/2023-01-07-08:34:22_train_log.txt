2023-01-07 08:34:29,476 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:34:29,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:29,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:29,514 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 9408
2023-01-07 08:34:29,514 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:29,514 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:29,514 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0]]]
2023-01-07 08:34:29,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,360 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,360 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,360 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,360 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0]]]
2023-01-07 08:34:30,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,362 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,363 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:34:30,363 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,363 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,363 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 0 0]]]
2023-01-07 08:34:30,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,364 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,364 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,364 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,364 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,364 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,365 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:34:30,366 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,366 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,366 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0]]]
2023-01-07 08:34:30,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,404 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,404 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,405 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,405 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,406 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,406 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,406 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,406 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [17, torch.Size([128]) 0 0], [18, torch.Size([36864]) 0 0]]]
2023-01-07 08:34:30,406 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,408 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,408 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,408 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,408 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,409 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,409 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,409 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,409 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,410 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,410 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,410 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,410 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,411 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,411 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,411 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,411 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 0 0]]]
2023-01-07 08:34:30,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,413 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,413 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,413 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,413 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,414 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:34:30,414 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,414 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,414 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 0 0]]]
2023-01-07 08:34:30,414 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,415 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,415 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,415 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,416 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,417 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,417 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,417 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,417 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[26, torch.Size([65536]) 0 0]]]
2023-01-07 08:34:30,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,418 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,418 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,418 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,418 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,419 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,420 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,420 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,420 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,420 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,420 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,421 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,421 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,421 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,421 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,422 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 36864
2023-01-07 08:34:30,422 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,422 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,422 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [24, torch.Size([147456]) 0 0]]]
2023-01-07 08:34:30,422 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,423 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 128
2023-01-07 08:34:30,423 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,423 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,423 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,425 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 16384
2023-01-07 08:34:30,425 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,425 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,425 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 0 0]]]
2023-01-07 08:34:30,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,426 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,426 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,426 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,426 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,427 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 32768
2023-01-07 08:34:30,427 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,427 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,427 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,429 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,429 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,429 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,429 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,430 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:34:30,430 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,430 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,430 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,432 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,432 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,432 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,432 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,433 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,433 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,433 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,433 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0]]]
2023-01-07 08:34:30,433 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,434 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,434 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,434 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,434 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,436 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:34:30,436 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,436 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,436 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,437 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,437 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,437 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,437 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,438 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,438 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,438 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,438 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,440 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,440 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,440 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,440 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,441 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:34:30,441 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,441 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,441 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,443 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,443 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,443 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,443 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,444 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,444 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,444 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,444 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,445 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,445 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,445 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,445 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,446 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,446 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,446 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,446 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,448 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,448 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,448 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,448 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,449 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:34:30,449 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,449 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,449 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,450 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,450 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,450 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,451 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,451 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,452 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,452 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,452 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,452 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[42, torch.Size([65536]) 0 0]]]
2023-01-07 08:34:30,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,453 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,453 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,453 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,453 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,453 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,454 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,454 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,454 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,455 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,455 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0]]]
2023-01-07 08:34:30,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,456 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,456 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,456 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,456 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,456 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,457 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 147456
2023-01-07 08:34:30,457 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,457 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,457 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 0 0]]]
2023-01-07 08:34:30,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,459 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 256
2023-01-07 08:34:30,459 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,459 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,459 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,460 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 65536
2023-01-07 08:34:30,460 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,460 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,460 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0]]]
2023-01-07 08:34:30,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,461 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,461 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,461 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,461 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,463 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 131072
2023-01-07 08:34:30,463 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,463 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,463 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0]]]
2023-01-07 08:34:30,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,464 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,464 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,464 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,464 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,466 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,466 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,466 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,466 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[53, torch.Size([2048]) 0 0]]]
2023-01-07 08:34:30,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,467 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,467 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,467 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,467 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,468 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,469 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,469 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,469 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,470 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,470 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,470 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,470 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,470 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,471 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:34:30,471 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,471 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,471 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[88, torch.Size([2359296]) 0 0]]]
2023-01-07 08:34:30,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,472 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,472 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,472 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,472 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,474 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,474 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,474 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,474 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 0 0]]]
2023-01-07 08:34:30,474 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,475 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,475 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,475 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,475 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,476 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,476 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,476 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,477 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,478 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,478 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,478 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,478 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,479 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,479 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,479 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,479 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[74, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,481 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,481 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,481 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,481 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,482 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,482 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,482 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,482 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 0 0]]]
2023-01-07 08:34:30,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,483 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,483 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,484 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,484 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,485 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,485 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,485 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,485 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,486 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,486 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,486 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,486 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,487 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,487 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,487 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,488 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,489 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,489 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,489 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,489 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,490 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,490 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,490 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,490 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 0 0]]]
2023-01-07 08:34:30,490 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,491 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,492 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,492 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,492 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,492 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,493 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,493 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,493 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,493 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,494 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,494 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,494 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,494 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,495 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,495 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,495 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,496 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,496 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[82, torch.Size([589824]) 0 0]]]
2023-01-07 08:34:30,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,497 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,497 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,497 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,497 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,497 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,498 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,498 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,498 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,498 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 0 0]]]
2023-01-07 08:34:30,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,500 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,500 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,500 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,500 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,501 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,501 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,501 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,501 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,502 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,502 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,502 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,502 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,503 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,504 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,504 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,504 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,505 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,505 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,505 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,505 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,506 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,506 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,506 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,506 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[102, torch.Size([2359296]) 0 0]]]
2023-01-07 08:34:30,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,507 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,507 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,507 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,507 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,509 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 589824
2023-01-07 08:34:30,509 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,509 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,509 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 0 0]]]
2023-01-07 08:34:30,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,510 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 512
2023-01-07 08:34:30,510 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,510 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,510 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,511 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 262144
2023-01-07 08:34:30,511 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,511 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,511 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,512 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2048
2023-01-07 08:34:30,512 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,512 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,512 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,514 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 524288
2023-01-07 08:34:30,514 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,514 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,514 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,515 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,515 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,515 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,515 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,516 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:34:30,516 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,516 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,516 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,517 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,517 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,517 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,517 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,518 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:34:30,519 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,519 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,519 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 0 0]]]
2023-01-07 08:34:30,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,520 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:34:30,520 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,520 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,520 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,521 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2097152
2023-01-07 08:34:30,521 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,521 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,521 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,522 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:34:30,522 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,522 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,522 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,523 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:34:30,523 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,523 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,524 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,524 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,524 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,525 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,525 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,525 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,526 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:34:30,526 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,526 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,526 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[100, torch.Size([1048576]) 0 0]]]
2023-01-07 08:34:30,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,527 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,528 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,528 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,528 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,528 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,529 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:34:30,529 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,529 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,529 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,530 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:34:30,530 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,530 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,530 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,531 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:34:30,531 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,531 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,531 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,532 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,532 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,532 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,532 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,533 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2359296
2023-01-07 08:34:30,534 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,534 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,534 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,535 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1024
2023-01-07 08:34:30,535 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,535 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,535 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,536 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 1048576
2023-01-07 08:34:30,536 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,536 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,536 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 0 0]]]
2023-01-07 08:34:30,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,537 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 4096
2023-01-07 08:34:30,537 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,537 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,537 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,539 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 0 orig size : 2049000
2023-01-07 08:34:30,539 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,539 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 08:34:30,539 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 08:34:30,540 > [DEBUG] 0 :: 7.281585693359375
2023-01-07 08:34:30,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,547 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,547 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.2522088587284088
2023-01-07 08:34:30,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2522088587284088
2023-01-07 08:34:30,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,559 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,559 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.017444461584091187
2023-01-07 08:34:30,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,559 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,559 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.4912500381469727
2023-01-07 08:34:30,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5422520637512207
2023-01-07 08:34:30,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,562 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,562 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.020699188113212585
2023-01-07 08:34:30,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.020699188113212585
2023-01-07 08:34:30,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,564 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,564 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.18277275562286377
2023-01-07 08:34:30,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18277275562286377
2023-01-07 08:34:30,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,566 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,566 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.02335541322827339
2023-01-07 08:34:30,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,567 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,567 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 0.10684061050415039
2023-01-07 08:34:30,567 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6897563934326172
2023-01-07 08:34:30,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,569 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,569 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.04498010128736496
2023-01-07 08:34:30,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04498010128736496
2023-01-07 08:34:30,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,571 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,571 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.2732294797897339
2023-01-07 08:34:30,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,571 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,571 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -3.4536895751953125
2023-01-07 08:34:30,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.034550465643405914
2023-01-07 08:34:30,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,573 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.3047933876514435
2023-01-07 08:34:30,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3047933876514435
2023-01-07 08:34:30,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,575 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.03766286373138428
2023-01-07 08:34:30,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,576 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,576 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -6.916170120239258
2023-01-07 08:34:30,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24389296770095825
2023-01-07 08:34:30,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,578 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,578 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0004308149218559265
2023-01-07 08:34:30,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0004308149218559265
2023-01-07 08:34:30,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,581 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.09122281521558762
2023-01-07 08:34:30,581 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09122281521558762
2023-01-07 08:34:30,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,583 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,583 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.01063835620880127
2023-01-07 08:34:30,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,583 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,583 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -19.951160430908203
2023-01-07 08:34:30,583 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08945247530937195
2023-01-07 08:34:30,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,586 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,586 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.029558081179857254
2023-01-07 08:34:30,586 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.029558081179857254
2023-01-07 08:34:30,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,588 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,588 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.006792820990085602
2023-01-07 08:34:30,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.006792820990085602
2023-01-07 08:34:30,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,590 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.05099412798881531
2023-01-07 08:34:30,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,590 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -2.2591757774353027
2023-01-07 08:34:30,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2138395309448242
2023-01-07 08:34:30,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,592 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.10852521657943726
2023-01-07 08:34:30,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10852521657943726
2023-01-07 08:34:30,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,594 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,594 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.05767802149057388
2023-01-07 08:34:30,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05767802149057388
2023-01-07 08:34:30,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,596 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.12940916419029236
2023-01-07 08:34:30,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,596 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,596 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -0.7916562557220459
2023-01-07 08:34:30,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4693346619606018
2023-01-07 08:34:30,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,598 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,598 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.14879725873470306
2023-01-07 08:34:30,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14879725873470306
2023-01-07 08:34:30,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,600 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,600 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.11601893603801727
2023-01-07 08:34:30,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11601893603801727
2023-01-07 08:34:30,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,602 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.000621989369392395
2023-01-07 08:34:30,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.000621989369392395
2023-01-07 08:34:30,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,604 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.24424073100090027
2023-01-07 08:34:30,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24424073100090027
2023-01-07 08:34:30,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,606 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,606 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.06329140067100525
2023-01-07 08:34:30,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06329140067100525
2023-01-07 08:34:30,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,608 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.09517168998718262
2023-01-07 08:34:30,609 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09517168998718262
2023-01-07 08:34:30,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,610 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,610 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 23.679262161254883
2023-01-07 08:34:30,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8841586112976074
2023-01-07 08:34:30,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,612 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.18812595307826996
2023-01-07 08:34:30,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,613 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,613 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -14.972888946533203
2023-01-07 08:34:30,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3251409530639648
2023-01-07 08:34:30,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,615 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -51.009586334228516
2023-01-07 08:34:30,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1344645023345947
2023-01-07 08:34:30,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,617 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.30400341749191284
2023-01-07 08:34:30,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,617 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -15.567187309265137
2023-01-07 08:34:30,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.122160911560059
2023-01-07 08:34:30,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,620 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -137.17886352539062
2023-01-07 08:34:30,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4757106602191925
2023-01-07 08:34:30,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,624 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,624 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -23.127958297729492
2023-01-07 08:34:30,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9546403884887695
2023-01-07 08:34:30,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,626 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,627 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 77.22732543945312
2023-01-07 08:34:30,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.014373779296875
2023-01-07 08:34:30,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,630 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,630 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.24235334992408752
2023-01-07 08:34:30,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24235334992408752
2023-01-07 08:34:30,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,632 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,632 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -51.58417510986328
2023-01-07 08:34:30,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.775243759155273
2023-01-07 08:34:30,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,635 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,635 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -19.43796730041504
2023-01-07 08:34:30,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.214385509490967
2023-01-07 08:34:30,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,637 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -212.46078491210938
2023-01-07 08:34:30,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.52910614013672
2023-01-07 08:34:30,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,639 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,639 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.4955648183822632
2023-01-07 08:34:30,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4955648183822632
2023-01-07 08:34:30,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,643 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,643 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 59.70417022705078
2023-01-07 08:34:30,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.890789985656738
2023-01-07 08:34:30,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,645 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,645 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 10.781553268432617
2023-01-07 08:34:30,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.498207092285156
2023-01-07 08:34:30,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,647 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.912416934967041
2023-01-07 08:34:30,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,648 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,648 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.48909294605255127
2023-01-07 08:34:30,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4233242273330688
2023-01-07 08:34:30,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,650 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,650 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.5049711465835571
2023-01-07 08:34:30,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5049711465835571
2023-01-07 08:34:30,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,654 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,654 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.0059843063354492
2023-01-07 08:34:30,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,654 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -3.4546914100646973
2023-01-07 08:34:30,655 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.418928146362305
2023-01-07 08:34:30,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,656 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,656 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -40.52263641357422
2023-01-07 08:34:30,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.814586639404297
2023-01-07 08:34:30,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,660 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,660 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.4097938537597656
2023-01-07 08:34:30,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,660 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,660 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 56.06034851074219
2023-01-07 08:34:30,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.935004711151123
2023-01-07 08:34:30,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,662 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,662 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 173.38992309570312
2023-01-07 08:34:30,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 75.64752197265625
2023-01-07 08:34:30,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,664 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,664 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 108.33724975585938
2023-01-07 08:34:30,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 96.65963745117188
2023-01-07 08:34:30,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,666 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,666 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 3.933739185333252
2023-01-07 08:34:30,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.933739185333252
2023-01-07 08:34:30,671 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:34:30,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,671 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,671 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -78.94952392578125
2023-01-07 08:34:30,672 > [DEBUG] 0 :: before allreduce fusion buffer :: -78.94952392578125
2023-01-07 08:34:30,673 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,674 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 398.17041015625
2023-01-07 08:34:30,674 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,674 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,675 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -1.6559860706329346
2023-01-07 08:34:30,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6559860706329346
2023-01-07 08:34:30,677 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,677 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 603.0130004882812
2023-01-07 08:34:30,678 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,678 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,678 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.1111645698547363
2023-01-07 08:34:30,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,678 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,678 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -7.359689712524414
2023-01-07 08:34:30,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.359689712524414
2023-01-07 08:34:30,680 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,680 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -1734.1005859375
2023-01-07 08:34:30,681 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,681 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 22.93356704711914
2023-01-07 08:34:30,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,681 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,681 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 595.4427490234375
2023-01-07 08:34:30,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.50959014892578
2023-01-07 08:34:30,682 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,682 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -2.000422716140747
2023-01-07 08:34:30,682 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,683 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,683 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 4240 orig size : 36864
2023-01-07 08:34:30,683 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -929.42578125
2023-01-07 08:34:30,684 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,684 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,684 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,684 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 0.1995002031326294
2023-01-07 08:34:30,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,684 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,684 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -11.607571601867676
2023-01-07 08:34:30,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.408071517944336
2023-01-07 08:34:30,685 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,685 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.1995002031326294
2023-01-07 08:34:30,686 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,686 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,686 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,686 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.6745815277099609
2023-01-07 08:34:30,687 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,687 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:30,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,687 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,687 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 0.5470356941223145
2023-01-07 08:34:30,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,687 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,687 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -1.1262476444244385
2023-01-07 08:34:30,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,687 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,688 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -20.49530029296875
2023-01-07 08:34:30,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -137.6763458251953
2023-01-07 08:34:30,689 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,689 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -5.636486530303955
2023-01-07 08:34:30,689 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,690 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8824 orig size : 16384
2023-01-07 08:34:30,690 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1788.455322265625
2023-01-07 08:34:30,690 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,690 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,691 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,691 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -3.926438808441162
2023-01-07 08:34:30,691 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,692 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7699 orig size : 16384
2023-01-07 08:34:30,692 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 1283.646484375
2023-01-07 08:34:30,692 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,693 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,693 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.3589212894439697
2023-01-07 08:34:30,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,693 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,693 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -180.64263916015625
2023-01-07 08:34:30,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -71.76260375976562
2023-01-07 08:34:30,694 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,694 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.3589212894439697
2023-01-07 08:34:30,694 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,695 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 19439 orig size : 36864
2023-01-07 08:34:30,696 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -100.70912170410156
2023-01-07 08:34:30,696 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,696 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,696 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,696 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.179553508758545
2023-01-07 08:34:30,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,696 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,696 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 1.7216949462890625
2023-01-07 08:34:30,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.091888427734375
2023-01-07 08:34:30,698 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,698 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -1.4100427627563477
2023-01-07 08:34:30,698 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,698 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,699 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10946 orig size : 16384
2023-01-07 08:34:30,699 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -13.210408210754395
2023-01-07 08:34:30,699 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.240619659423828
2023-01-07 08:34:30,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,699 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 14.309721946716309
2023-01-07 08:34:30,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.387667417526245
2023-01-07 08:34:30,700 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,701 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -3.330552816390991
2023-01-07 08:34:30,701 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,701 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,702 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7683 orig size : 16384
2023-01-07 08:34:30,702 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 1048.78515625
2023-01-07 08:34:30,702 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,703 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,703 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 9.774616241455078
2023-01-07 08:34:30,703 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,703 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,704 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10257 orig size : 36864
2023-01-07 08:34:30,704 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -26.400192260742188
2023-01-07 08:34:30,704 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,704 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,704 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,704 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,705 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -0.02010434865951538
2023-01-07 08:34:30,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,705 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,705 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -5.651400566101074
2023-01-07 08:34:30,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,705 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,705 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -621.4122314453125
2023-01-07 08:34:30,705 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.27947998046875
2023-01-07 08:34:30,706 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,706 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.02010434865951538
2023-01-07 08:34:30,706 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,707 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,707 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,707 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 7.240484237670898
2023-01-07 08:34:30,707 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.267199516296387
2023-01-07 08:34:30,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,708 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,708 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -13.357438087463379
2023-01-07 08:34:30,708 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.064411163330078
2023-01-07 08:34:30,709 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,709 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -5.841507911682129
2023-01-07 08:34:30,709 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,710 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8069 orig size : 32768
2023-01-07 08:34:30,710 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 20.746715545654297
2023-01-07 08:34:30,710 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,711 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,711 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -4.411238193511963
2023-01-07 08:34:30,711 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,712 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10234 orig size : 147456
2023-01-07 08:34:30,712 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -622.6713256835938
2023-01-07 08:34:30,713 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,713 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,713 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,713 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.7158271670341492
2023-01-07 08:34:30,713 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7158271670341492
2023-01-07 08:34:30,714 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,714 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.5337748527526855
2023-01-07 08:34:30,714 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,715 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7003 orig size : 65536
2023-01-07 08:34:30,715 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 7.168420791625977
2023-01-07 08:34:30,715 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,716 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,716 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:30,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,716 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,716 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1.1589821577072144
2023-01-07 08:34:30,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1589821577072144
2023-01-07 08:34:30,717 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,717 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -2.925529718399048
2023-01-07 08:34:30,717 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,718 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6105 orig size : 131072
2023-01-07 08:34:30,718 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 641.7537231445312
2023-01-07 08:34:30,718 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,719 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,719 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.10993790626525879
2023-01-07 08:34:30,719 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,719 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,720 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6034 orig size : 65536
2023-01-07 08:34:30,720 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 304.57586669921875
2023-01-07 08:34:30,720 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,721 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,721 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.22454750537872314
2023-01-07 08:34:30,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22454750537872314
2023-01-07 08:34:30,722 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,722 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.005612671375274658
2023-01-07 08:34:30,722 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,722 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,723 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6601 orig size : 147456
2023-01-07 08:34:30,723 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 1232.4285888671875
2023-01-07 08:34:30,723 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,723 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,723 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,724 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -1.0302436351776123
2023-01-07 08:34:30,724 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0302436351776123
2023-01-07 08:34:30,725 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,725 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -1.7093133926391602
2023-01-07 08:34:30,725 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,725 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,726 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8376 orig size : 65536
2023-01-07 08:34:30,726 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 1681.7484130859375
2023-01-07 08:34:30,726 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,727 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,727 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 15.633548736572266
2023-01-07 08:34:30,727 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,728 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,728 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8290 orig size : 65536
2023-01-07 08:34:30,728 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 1672.067138671875
2023-01-07 08:34:30,728 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,728 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,728 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,728 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.43846067786216736
2023-01-07 08:34:30,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43846067786216736
2023-01-07 08:34:30,730 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,730 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.684233546257019
2023-01-07 08:34:30,730 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,730 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,731 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6624 orig size : 147456
2023-01-07 08:34:30,731 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 1442.46484375
2023-01-07 08:34:30,731 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,731 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,732 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,732 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.3266943395137787
2023-01-07 08:34:30,732 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3266943395137787
2023-01-07 08:34:30,733 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,733 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 1.3787455558776855
2023-01-07 08:34:30,733 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,733 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,734 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,734 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5265 orig size : 65536
2023-01-07 08:34:30,734 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 506.5122985839844
2023-01-07 08:34:30,734 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,734 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 18.716629028320312
2023-01-07 08:34:30,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,734 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,735 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -30.123777389526367
2023-01-07 08:34:30,735 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.169466018676758
2023-01-07 08:34:30,735 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,736 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 6.987919807434082
2023-01-07 08:34:30,736 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,736 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,737 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5001 orig size : 65536
2023-01-07 08:34:30,737 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -32.96989440917969
2023-01-07 08:34:30,737 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,737 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,737 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,737 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.21844467520713806
2023-01-07 08:34:30,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21844467520713806
2023-01-07 08:34:30,738 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,739 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.07026290893554688
2023-01-07 08:34:30,739 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,739 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,740 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6593 orig size : 147456
2023-01-07 08:34:30,740 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 1531.4940185546875
2023-01-07 08:34:30,740 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,740 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.2726895809173584
2023-01-07 08:34:30,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,741 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,741 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -1.2929519414901733
2023-01-07 08:34:30,741 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.224825859069824
2023-01-07 08:34:30,742 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,742 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.2726895809173584
2023-01-07 08:34:30,742 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,742 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,743 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8226 orig size : 65536
2023-01-07 08:34:30,743 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -8.420377731323242
2023-01-07 08:34:30,743 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,743 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:30,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,744 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,744 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.6026425957679749
2023-01-07 08:34:30,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6026425957679749
2023-01-07 08:34:30,745 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,745 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.5359042286872864
2023-01-07 08:34:30,745 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,745 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,746 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5254 orig size : 131072
2023-01-07 08:34:30,746 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 473.56256103515625
2023-01-07 08:34:30,746 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,746 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,746 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:30,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,746 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.6688069105148315
2023-01-07 08:34:30,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6688069105148315
2023-01-07 08:34:30,748 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,748 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.5391427278518677
2023-01-07 08:34:30,748 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,748 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,749 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6696 orig size : 589824
2023-01-07 08:34:30,749 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 1362.3624267578125
2023-01-07 08:34:30,749 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn3._dp_wrapped_module.flat_param_0 value:: 1024.0
2023-01-07 08:34:30,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,750 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.005888819694519043
2023-01-07 08:34:30,750 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005888819694519043
2023-01-07 08:34:30,751 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,751 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -1.6389073133468628
2023-01-07 08:34:30,751 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,751 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,752 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 12862 orig size : 262144
2023-01-07 08:34:30,752 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 276.6896057128906
2023-01-07 08:34:30,752 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,752 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,753 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,753 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.06790925562381744
2023-01-07 08:34:30,753 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,754 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7654 orig size : 524288
2023-01-07 08:34:30,754 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 925.5169067382812
2023-01-07 08:34:30,754 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,754 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,754 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -36.73724365234375
2023-01-07 08:34:30,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,754 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,754 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -38.624267578125
2023-01-07 08:34:30,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1994071006774902
2023-01-07 08:34:30,756 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,756 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.9179142713546753
2023-01-07 08:34:30,756 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,757 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,757 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 11247 orig size : 262144
2023-01-07 08:34:30,757 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 274.60223388671875
2023-01-07 08:34:30,757 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,757 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,757 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:30,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,757 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.32213449478149414
2023-01-07 08:34:30,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,758 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 27.810895919799805
2023-01-07 08:34:30,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4123848676681519
2023-01-07 08:34:30,759 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,759 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.4245353043079376
2023-01-07 08:34:30,759 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,759 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,760 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9776 orig size : 589824
2023-01-07 08:34:30,760 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 20.426027297973633
2023-01-07 08:34:30,761 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,761 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,761 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 16.759733200073242
2023-01-07 08:34:30,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,761 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,761 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -4.344417572021484
2023-01-07 08:34:30,761 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39564087986946106
2023-01-07 08:34:30,762 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,762 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -0.11767303943634033
2023-01-07 08:34:30,762 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,763 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7698 orig size : 262144
2023-01-07 08:34:30,763 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -5.255061149597168
2023-01-07 08:34:30,764 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,764 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,764 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 56.13298034667969
2023-01-07 08:34:30,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,764 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -4.722558975219727
2023-01-07 08:34:30,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6630511283874512
2023-01-07 08:34:30,765 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,765 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -0.29204076528549194
2023-01-07 08:34:30,765 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,765 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,766 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6637 orig size : 262144
2023-01-07 08:34:30,766 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 1504.3690185546875
2023-01-07 08:34:30,766 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,767 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,767 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.015634536743164062
2023-01-07 08:34:30,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,767 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,767 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 68.57546997070312
2023-01-07 08:34:30,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1838948726654053
2023-01-07 08:34:30,768 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,768 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.3305627405643463
2023-01-07 08:34:30,768 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,769 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,769 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6836 orig size : 589824
2023-01-07 08:34:30,770 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 65.81104278564453
2023-01-07 08:34:30,770 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 38.4240608215332
2023-01-07 08:34:30,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,770 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,770 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -2.45273494720459
2023-01-07 08:34:30,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0000522136688232
2023-01-07 08:34:30,771 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,771 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.08632200956344604
2023-01-07 08:34:30,771 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,772 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7727 orig size : 262144
2023-01-07 08:34:30,773 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -4.842746734619141
2023-01-07 08:34:30,773 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,773 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,773 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -34.67656326293945
2023-01-07 08:34:30,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,773 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,773 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 1.8882942199707031
2023-01-07 08:34:30,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5572388172149658
2023-01-07 08:34:30,774 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,774 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.11171364784240723
2023-01-07 08:34:30,774 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,775 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6486 orig size : 262144
2023-01-07 08:34:30,776 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -2.1419677734375
2023-01-07 08:34:30,776 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 16.781757354736328
2023-01-07 08:34:30,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,776 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,776 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -5.548290252685547
2023-01-07 08:34:30,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10045069456100464
2023-01-07 08:34:30,777 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,777 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.06576907634735107
2023-01-07 08:34:30,777 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,777 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,778 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:30,779 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -0.8774623870849609
2023-01-07 08:34:30,779 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 30.224605560302734
2023-01-07 08:34:30,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,779 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,779 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -0.7941687107086182
2023-01-07 08:34:30,779 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3385341763496399
2023-01-07 08:34:30,780 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,780 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.21456634998321533
2023-01-07 08:34:30,780 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,780 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,781 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7762 orig size : 262144
2023-01-07 08:34:30,782 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -3.7678442001342773
2023-01-07 08:34:30,782 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,782 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 1.4394025802612305
2023-01-07 08:34:30,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,782 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,782 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -12.501359939575195
2023-01-07 08:34:30,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07624057680368423
2023-01-07 08:34:30,783 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,783 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.3218725025653839
2023-01-07 08:34:30,783 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,783 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,785 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6474 orig size : 262144
2023-01-07 08:34:30,785 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -4.67978572845459
2023-01-07 08:34:30,785 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.629247665405273
2023-01-07 08:34:30,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,785 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,785 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 15.66355037689209
2023-01-07 08:34:30,785 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7229534387588501
2023-01-07 08:34:30,786 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,786 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.12783950567245483
2023-01-07 08:34:30,786 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,786 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,787 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,788 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:30,788 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 16.870676040649414
2023-01-07 08:34:30,788 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,788 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,788 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.4688615798950195
2023-01-07 08:34:30,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,788 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,788 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -4.2923688888549805
2023-01-07 08:34:30,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9160116314888
2023-01-07 08:34:30,789 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,789 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.019314274191856384
2023-01-07 08:34:30,789 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,790 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,790 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7944 orig size : 262144
2023-01-07 08:34:30,791 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -5.91617488861084
2023-01-07 08:34:30,791 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,792 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,792 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.43052971363067627
2023-01-07 08:34:30,792 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,793 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,793 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9322 orig size : 262144
2023-01-07 08:34:30,793 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 283.90863037109375
2023-01-07 08:34:30,793 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,794 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 36.37263870239258
2023-01-07 08:34:30,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,794 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,794 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -7.158482074737549
2023-01-07 08:34:30,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2615714371204376
2023-01-07 08:34:30,795 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,795 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.07639873027801514
2023-01-07 08:34:30,795 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,795 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,796 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6449 orig size : 589824
2023-01-07 08:34:30,796 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -12.30148696899414
2023-01-07 08:34:30,796 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,797 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 1.565791130065918
2023-01-07 08:34:30,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,797 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,797 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -6.913413047790527
2023-01-07 08:34:30,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14349621534347534
2023-01-07 08:34:30,798 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,798 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.1059567928314209
2023-01-07 08:34:30,798 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,798 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,799 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8980 orig size : 262144
2023-01-07 08:34:30,799 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -7.742379188537598
2023-01-07 08:34:30,799 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,799 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,800 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,800 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.21782246232032776
2023-01-07 08:34:30,800 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,800 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,801 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,801 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9351 orig size : 524288
2023-01-07 08:34:30,801 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 266.5303649902344
2023-01-07 08:34:30,801 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,802 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,802 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.2545127272605896
2023-01-07 08:34:30,802 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,802 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,803 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6443 orig size : 2359296
2023-01-07 08:34:30,804 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -39.24751281738281
2023-01-07 08:34:30,804 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,804 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,804 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.2540819048881531
2023-01-07 08:34:30,804 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,805 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 43557 orig size : 1048576
2023-01-07 08:34:30,806 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 277.3154296875
2023-01-07 08:34:30,806 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 26.726709365844727
2023-01-07 08:34:30,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,806 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,806 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 8.994146347045898
2023-01-07 08:34:30,806 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1545557975769043
2023-01-07 08:34:30,807 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,807 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 126.05072021484375
2023-01-07 08:34:30,807 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,808 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6806 orig size : 2097152
2023-01-07 08:34:30,808 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 9.89852523803711
2023-01-07 08:34:30,808 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,808 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,809 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,809 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 125.74593353271484
2023-01-07 08:34:30,809 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,810 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 23419 orig size : 1048576
2023-01-07 08:34:30,810 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 156.02578735351562
2023-01-07 08:34:30,810 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,810 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,811 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,811 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.12771062552928925
2023-01-07 08:34:30,811 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,811 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,812 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,812 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9267 orig size : 2359296
2023-01-07 08:34:30,812 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 286.6582946777344
2023-01-07 08:34:30,812 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,813 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,813 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.19667911529541
2023-01-07 08:34:30,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,813 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,813 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.1635572910308838
2023-01-07 08:34:30,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1673361361026764
2023-01-07 08:34:30,814 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,814 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.17269070446491241
2023-01-07 08:34:30,814 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,814 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,815 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,815 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33875 orig size : 1048576
2023-01-07 08:34:30,815 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 282.1466369628906
2023-01-07 08:34:30,815 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,815 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,816 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,816 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 125.64594268798828
2023-01-07 08:34:30,816 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,816 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,817 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,817 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6434 orig size : 1048576
2023-01-07 08:34:30,817 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.3755805492401123
2023-01-07 08:34:30,817 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,817 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,818 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,818 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.16060000658035278
2023-01-07 08:34:30,818 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,818 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,819 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,819 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6363 orig size : 2359296
2023-01-07 08:34:30,819 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -6.855571746826172
2023-01-07 08:34:30,819 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,819 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,820 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,820 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.1399008184671402
2023-01-07 08:34:30,820 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,821 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33305 orig size : 1048576
2023-01-07 08:34:30,821 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 278.730224609375
2023-01-07 08:34:30,821 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,821 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,822 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.103188514709473
2023-01-07 08:34:30,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,822 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 08:34:30,822 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0133056640625
2023-01-07 08:34:30,822 > [DEBUG] 0 :: before allreduce fusion buffer :: -402.1966552734375
2023-01-07 08:34:30,823 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,823 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 125.69757080078125
2023-01-07 08:34:30,823 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,823 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,824 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,824 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6582 orig size : 2049000
2023-01-07 08:34:30,825 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -462.2982177734375
2023-01-07 08:34:30,825 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,825 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,826 > [DEBUG] 0 :: 7.00961971282959
2023-01-07 08:34:30,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,830 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.24865202605724335
2023-01-07 08:34:30,831 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24865202605724335
2023-01-07 08:34:30,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,834 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.04036092758178711
2023-01-07 08:34:30,834 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,834 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -5.784798622131348
2023-01-07 08:34:30,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33576157689094543
2023-01-07 08:34:30,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,837 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.05649549141526222
2023-01-07 08:34:30,837 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,837 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05649549141526222
2023-01-07 08:34:30,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,840 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.25479409098625183
2023-01-07 08:34:30,840 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25479409098625183
2023-01-07 08:34:30,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.01262892410159111
2023-01-07 08:34:30,843 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,843 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2.864208936691284
2023-01-07 08:34:30,843 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8848623037338257
2023-01-07 08:34:30,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,845 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.08843331038951874
2023-01-07 08:34:30,845 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08843331038951874
2023-01-07 08:34:30,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,847 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.07236076146364212
2023-01-07 08:34:30,847 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,848 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 5.573972702026367
2023-01-07 08:34:30,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.454291582107544
2023-01-07 08:34:30,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,850 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.030210845172405243
2023-01-07 08:34:30,850 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.030210845172405243
2023-01-07 08:34:30,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.02035048045217991
2023-01-07 08:34:30,852 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,852 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 3.2076172828674316
2023-01-07 08:34:30,852 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8193380236625671
2023-01-07 08:34:30,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,854 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.023161984980106354
2023-01-07 08:34:30,854 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.023161984980106354
2023-01-07 08:34:30,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,857 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.17197291553020477
2023-01-07 08:34:30,857 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,857 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17197291553020477
2023-01-07 08:34:30,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,859 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.04592963680624962
2023-01-07 08:34:30,859 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,860 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.433751106262207
2023-01-07 08:34:30,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2690377235412598
2023-01-07 08:34:30,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.045431576669216156
2023-01-07 08:34:30,862 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,862 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.045431576669216156
2023-01-07 08:34:30,864 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,864 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,864 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.14426971971988678
2023-01-07 08:34:30,864 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14426971971988678
2023-01-07 08:34:30,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,866 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.1396058052778244
2023-01-07 08:34:30,866 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,866 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -17.290536880493164
2023-01-07 08:34:30,866 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.023928269743919373
2023-01-07 08:34:30,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,868 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.17024357616901398
2023-01-07 08:34:30,868 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17024357616901398
2023-01-07 08:34:30,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,870 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.17939035594463348
2023-01-07 08:34:30,870 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,870 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17939035594463348
2023-01-07 08:34:30,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.15937837958335876
2023-01-07 08:34:30,872 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,872 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3.9739534854888916
2023-01-07 08:34:30,873 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9464567303657532
2023-01-07 08:34:30,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.17635637521743774
2023-01-07 08:34:30,875 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,875 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17635637521743774
2023-01-07 08:34:30,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,876 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.08201900124549866
2023-01-07 08:34:30,876 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08201900124549866
2023-01-07 08:34:30,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.25238853693008423
2023-01-07 08:34:30,878 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,879 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25238853693008423
2023-01-07 08:34:30,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,880 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.08748185634613037
2023-01-07 08:34:30,880 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,881 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08748185634613037
2023-01-07 08:34:30,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,882 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.2960529923439026
2023-01-07 08:34:30,882 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,883 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2960529923439026
2023-01-07 08:34:30,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.04136337339878082
2023-01-07 08:34:30,884 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04136337339878082
2023-01-07 08:34:30,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 2.8674192428588867
2023-01-07 08:34:30,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1364474296569824
2023-01-07 08:34:30,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,889 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.012113451957702637
2023-01-07 08:34:30,889 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,889 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -4.632813453674316
2023-01-07 08:34:30,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.999696254730225
2023-01-07 08:34:30,891 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,891 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2.133755683898926
2023-01-07 08:34:30,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.980857849121094
2023-01-07 08:34:30,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.3606715202331543
2023-01-07 08:34:30,893 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,893 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,893 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.35409164428711
2023-01-07 08:34:30,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.69495964050293
2023-01-07 08:34:30,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,895 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 12.831165313720703
2023-01-07 08:34:30,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.191436767578125
2023-01-07 08:34:30,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,899 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 20.71573257446289
2023-01-07 08:34:30,899 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.654706954956055
2023-01-07 08:34:30,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,900 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -74.23880767822266
2023-01-07 08:34:30,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.623620986938477
2023-01-07 08:34:30,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,904 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.42337730526924133
2023-01-07 08:34:30,904 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.42337730526924133
2023-01-07 08:34:30,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,906 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -73.43833923339844
2023-01-07 08:34:30,906 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.418373107910156
2023-01-07 08:34:30,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,909 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -3.993974447250366
2023-01-07 08:34:30,909 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.665188789367676
2023-01-07 08:34:30,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,911 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 117.8041000366211
2023-01-07 08:34:30,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.907299995422363
2023-01-07 08:34:30,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,913 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.056699514389038
2023-01-07 08:34:30,913 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.056699514389038
2023-01-07 08:34:30,916 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,916 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,916 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 128.88975524902344
2023-01-07 08:34:30,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.36973762512207
2023-01-07 08:34:30,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,918 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 15.51835823059082
2023-01-07 08:34:30,918 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.410087585449219
2023-01-07 08:34:30,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,920 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.7456177473068237
2023-01-07 08:34:30,920 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,921 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.8190202713012695
2023-01-07 08:34:30,921 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,921 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.564638376235962
2023-01-07 08:34:30,923 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,923 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,923 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.06248319149017334
2023-01-07 08:34:30,923 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,923 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06248319149017334
2023-01-07 08:34:30,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,926 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.3062773942947388
2023-01-07 08:34:30,926 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,926 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,926 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 41.842803955078125
2023-01-07 08:34:30,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.612831115722656
2023-01-07 08:34:30,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,929 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 49.79695129394531
2023-01-07 08:34:30,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.65900421142578
2023-01-07 08:34:30,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,932 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.7368096709251404
2023-01-07 08:34:30,932 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,932 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 47.03651428222656
2023-01-07 08:34:30,933 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.342973709106445
2023-01-07 08:34:30,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,934 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 93.21492767333984
2023-01-07 08:34:30,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 103.59255981445312
2023-01-07 08:34:30,936 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,936 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,936 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 112.905517578125
2023-01-07 08:34:30,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 60.22782897949219
2023-01-07 08:34:30,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,938 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,938 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -2.6059272289276123
2023-01-07 08:34:30,938 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,939 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6059272289276123
2023-01-07 08:34:30,945 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:34:30,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,945 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -1620.279541015625
2023-01-07 08:34:30,945 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,946 > [DEBUG] 0 :: before allreduce fusion buffer :: -1620.279541015625
2023-01-07 08:34:30,947 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,948 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -441.2095642089844
2023-01-07 08:34:30,948 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,948 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,948 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,949 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 1.8585991859436035
2023-01-07 08:34:30,949 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8585991859436035
2023-01-07 08:34:30,951 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,952 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 53.848907470703125
2023-01-07 08:34:30,952 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,952 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,952 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.1111645698547363
2023-01-07 08:34:30,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,953 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 88.104736328125
2023-01-07 08:34:30,953 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.104736328125
2023-01-07 08:34:30,956 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,956 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -2206.865234375
2023-01-07 08:34:30,956 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,956 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,956 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 22.93356704711914
2023-01-07 08:34:30,956 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,956 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,956 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 41.26580810546875
2023-01-07 08:34:30,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.139936447143555
2023-01-07 08:34:30,957 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,958 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 1.7170023918151855
2023-01-07 08:34:30,958 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,958 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,959 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 4240 orig size : 36864
2023-01-07 08:34:30,959 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -2354.55712890625
2023-01-07 08:34:30,959 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,959 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,959 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,959 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -1.2266030311584473
2023-01-07 08:34:30,960 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,960 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 8.83083724975586
2023-01-07 08:34:30,960 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.604230880737305
2023-01-07 08:34:30,961 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,961 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -1.2266030311584473
2023-01-07 08:34:30,961 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,961 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,962 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,962 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -71.75647735595703
2023-01-07 08:34:30,962 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,962 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,962 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:30,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,963 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -1.9996659755706787
2023-01-07 08:34:30,963 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,963 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.3996185064315796
2023-01-07 08:34:30,963 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,963 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 152.56576538085938
2023-01-07 08:34:30,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.882582664489746
2023-01-07 08:34:30,964 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,965 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -1.7011680603027344
2023-01-07 08:34:30,965 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,965 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,966 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8824 orig size : 16384
2023-01-07 08:34:30,966 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1375.37353515625
2023-01-07 08:34:30,966 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,966 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,966 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,966 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 8.559626579284668
2023-01-07 08:34:30,967 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,967 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,968 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7699 orig size : 16384
2023-01-07 08:34:30,968 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 1455.015625
2023-01-07 08:34:30,968 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,968 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,968 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,968 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -2.174469470977783
2023-01-07 08:34:30,968 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,969 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,969 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 289.3102722167969
2023-01-07 08:34:30,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 130.53091430664062
2023-01-07 08:34:30,970 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,970 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -2.174469470977783
2023-01-07 08:34:30,970 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,970 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,971 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 19439 orig size : 36864
2023-01-07 08:34:30,971 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 210.89645385742188
2023-01-07 08:34:30,971 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,971 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,971 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.8340847492218018
2023-01-07 08:34:30,972 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,972 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -7.794856071472168
2023-01-07 08:34:30,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.241619110107422
2023-01-07 08:34:30,973 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,973 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -0.561887264251709
2023-01-07 08:34:30,973 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,973 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,974 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10946 orig size : 16384
2023-01-07 08:34:30,975 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -15.09416389465332
2023-01-07 08:34:30,975 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,975 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,975 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.240619659423828
2023-01-07 08:34:30,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,975 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 60.563941955566406
2023-01-07 08:34:30,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.032159805297852
2023-01-07 08:34:30,976 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,976 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 10.677534103393555
2023-01-07 08:34:30,976 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,976 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,977 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7683 orig size : 16384
2023-01-07 08:34:30,977 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 1251.0780029296875
2023-01-07 08:34:30,978 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,978 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,978 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,978 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.842652440071106
2023-01-07 08:34:30,978 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,978 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,979 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,979 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10257 orig size : 36864
2023-01-07 08:34:30,980 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 67.42591857910156
2023-01-07 08:34:30,980 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,980 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,980 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:30,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,980 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1.8540856838226318
2023-01-07 08:34:30,980 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,980 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -45.04179382324219
2023-01-07 08:34:30,980 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,981 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 364.4513244628906
2023-01-07 08:34:30,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.14151382446289
2023-01-07 08:34:30,982 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,982 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.8540856838226318
2023-01-07 08:34:30,982 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,982 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,983 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,983 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -64.95166015625
2023-01-07 08:34:30,983 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,983 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,983 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.267199516296387
2023-01-07 08:34:30,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,983 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 11.435508728027344
2023-01-07 08:34:30,984 > [DEBUG] 0 :: before allreduce fusion buffer :: 49.25837707519531
2023-01-07 08:34:30,984 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,985 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 9.433740615844727
2023-01-07 08:34:30,985 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,985 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,986 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8069 orig size : 32768
2023-01-07 08:34:30,986 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 14.203914642333984
2023-01-07 08:34:30,986 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,986 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,987 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,987 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 2.8908607959747314
2023-01-07 08:34:30,987 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,987 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,988 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10234 orig size : 147456
2023-01-07 08:34:30,988 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 444.5871276855469
2023-01-07 08:34:30,988 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,988 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,988 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,988 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -1.5726318359375
2023-01-07 08:34:30,989 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,989 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5726318359375
2023-01-07 08:34:30,990 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,990 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -2.8029580116271973
2023-01-07 08:34:30,990 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,990 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,991 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7003 orig size : 65536
2023-01-07 08:34:30,991 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 62.41203689575195
2023-01-07 08:34:30,991 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,991 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,991 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:30,991 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,991 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,992 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.3774973154067993
2023-01-07 08:34:30,992 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3774973154067993
2023-01-07 08:34:30,993 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,993 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -2.5308990478515625
2023-01-07 08:34:30,993 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,993 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,994 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6105 orig size : 131072
2023-01-07 08:34:30,994 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -85.1583251953125
2023-01-07 08:34:30,994 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,994 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,994 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,995 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -827.3695068359375
2023-01-07 08:34:30,995 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,995 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,996 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6034 orig size : 65536
2023-01-07 08:34:30,996 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -536.6934814453125
2023-01-07 08:34:30,996 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,996 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,996 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,996 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.40370696783065796
2023-01-07 08:34:30,997 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:30,997 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40370696783065796
2023-01-07 08:34:30,997 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:30,998 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.8165393471717834
2023-01-07 08:34:30,998 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,998 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,998 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:30,999 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6601 orig size : 147456
2023-01-07 08:34:30,999 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 91.32349395751953
2023-01-07 08:34:30,999 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:30,999 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:30,999 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:30,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:30,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:30,999 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.34310898184776306
2023-01-07 08:34:31,000 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34310898184776306
2023-01-07 08:34:31,000 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,001 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -1.5061497688293457
2023-01-07 08:34:31,001 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,001 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,001 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,002 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8376 orig size : 65536
2023-01-07 08:34:31,002 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 1310.3212890625
2023-01-07 08:34:31,002 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,002 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,002 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,003 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -389.3504638671875
2023-01-07 08:34:31,003 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,003 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,004 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8290 orig size : 65536
2023-01-07 08:34:31,004 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 1332.8333740234375
2023-01-07 08:34:31,004 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,004 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,004 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,004 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.3156288266181946
2023-01-07 08:34:31,005 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3156288266181946
2023-01-07 08:34:31,005 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,006 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.6628129482269287
2023-01-07 08:34:31,006 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,006 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,007 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6624 orig size : 147456
2023-01-07 08:34:31,007 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -71.42327880859375
2023-01-07 08:34:31,007 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,007 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,007 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,007 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.9229778051376343
2023-01-07 08:34:31,007 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9229778051376343
2023-01-07 08:34:31,008 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,009 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.09535235166549683
2023-01-07 08:34:31,009 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,009 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,010 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5265 orig size : 65536
2023-01-07 08:34:31,010 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -59.60018539428711
2023-01-07 08:34:31,010 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,010 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,010 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 18.716629028320312
2023-01-07 08:34:31,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,010 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -18.29941177368164
2023-01-07 08:34:31,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.71962308883667
2023-01-07 08:34:31,011 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,011 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 4.690279960632324
2023-01-07 08:34:31,012 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,012 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,013 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5001 orig size : 65536
2023-01-07 08:34:31,013 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -16.343585968017578
2023-01-07 08:34:31,013 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,013 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,013 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,013 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.7530124187469482
2023-01-07 08:34:31,013 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7530124187469482
2023-01-07 08:34:31,014 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,014 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 1.3453054428100586
2023-01-07 08:34:31,015 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,016 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6593 orig size : 147456
2023-01-07 08:34:31,016 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -29.525854110717773
2023-01-07 08:34:31,016 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,016 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,016 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.394514799118042
2023-01-07 08:34:31,017 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,017 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.5533835887908936
2023-01-07 08:34:31,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.330804705619812
2023-01-07 08:34:31,018 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,018 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.394514799118042
2023-01-07 08:34:31,018 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,018 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,019 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,019 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8226 orig size : 65536
2023-01-07 08:34:31,019 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -5.999754905700684
2023-01-07 08:34:31,020 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,020 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,020 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:31,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,020 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.5697494745254517
2023-01-07 08:34:31,020 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,020 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5697494745254517
2023-01-07 08:34:31,021 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,021 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.8113328218460083
2023-01-07 08:34:31,021 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,022 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,022 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5254 orig size : 131072
2023-01-07 08:34:31,022 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 73.89774322509766
2023-01-07 08:34:31,022 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,023 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,023 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,023 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.11734658479690552
2023-01-07 08:34:31,023 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11734658479690552
2023-01-07 08:34:31,024 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,024 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.5124266147613525
2023-01-07 08:34:31,024 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,025 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6696 orig size : 589824
2023-01-07 08:34:31,025 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 34.18326187133789
2023-01-07 08:34:31,025 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,026 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,026 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn3._dp_wrapped_module.flat_param_0 value:: 1024.0
2023-01-07 08:34:31,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,026 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.32803261280059814
2023-01-07 08:34:31,026 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,026 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32803261280059814
2023-01-07 08:34:31,027 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,027 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 3.6582083702087402
2023-01-07 08:34:31,027 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,027 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,028 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 12862 orig size : 262144
2023-01-07 08:34:31,028 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 41.141357421875
2023-01-07 08:34:31,028 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,028 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,029 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,029 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.008453398942947388
2023-01-07 08:34:31,029 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,030 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7654 orig size : 524288
2023-01-07 08:34:31,030 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 35.7484245300293
2023-01-07 08:34:31,030 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,030 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,030 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -36.73724365234375
2023-01-07 08:34:31,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,031 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 16.976709365844727
2023-01-07 08:34:31,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7972215414047241
2023-01-07 08:34:31,032 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,032 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 4.288591384887695
2023-01-07 08:34:31,032 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,033 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 11247 orig size : 262144
2023-01-07 08:34:31,033 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 24.45773696899414
2023-01-07 08:34:31,033 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,033 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,033 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,034 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.08602464199066162
2023-01-07 08:34:31,034 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 08:34:31,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,034 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -37.05510711669922
2023-01-07 08:34:31,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5046730041503906
2023-01-07 08:34:31,035 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,035 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.15661919116973877
2023-01-07 08:34:31,035 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,035 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,036 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9776 orig size : 589824
2023-01-07 08:34:31,037 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -44.832969665527344
2023-01-07 08:34:31,037 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 16.759733200073242
2023-01-07 08:34:31,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,037 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 27.31389617919922
2023-01-07 08:34:31,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.476184844970703
2023-01-07 08:34:31,038 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,038 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.30754947662353516
2023-01-07 08:34:31,038 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,038 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,039 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7698 orig size : 262144
2023-01-07 08:34:31,040 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 33.84326934814453
2023-01-07 08:34:31,040 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,040 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,040 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 56.13298034667969
2023-01-07 08:34:31,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,040 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -12.373054504394531
2023-01-07 08:34:31,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.671710968017578
2023-01-07 08:34:31,041 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,041 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -16.908872604370117
2023-01-07 08:34:31,041 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,041 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,042 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,042 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6637 orig size : 262144
2023-01-07 08:34:31,043 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 37.70851516723633
2023-01-07 08:34:31,043 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,043 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,043 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.015634536743164062
2023-01-07 08:34:31,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,043 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -7.650888442993164
2023-01-07 08:34:31,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.969554901123047
2023-01-07 08:34:31,044 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,044 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -2.329943895339966
2023-01-07 08:34:31,044 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,044 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,045 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,045 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6836 orig size : 589824
2023-01-07 08:34:31,046 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -10.53033447265625
2023-01-07 08:34:31,046 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,046 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,046 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 38.4240608215332
2023-01-07 08:34:31,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,046 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 12.267486572265625
2023-01-07 08:34:31,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.310920000076294
2023-01-07 08:34:31,047 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,047 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.16052952408790588
2023-01-07 08:34:31,047 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,047 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,048 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,048 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7727 orig size : 262144
2023-01-07 08:34:31,049 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 13.27877426147461
2023-01-07 08:34:31,049 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,049 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,049 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -34.67656326293945
2023-01-07 08:34:31,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,049 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -4.03898811340332
2023-01-07 08:34:31,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.237793922424316
2023-01-07 08:34:31,050 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,050 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -3.624067783355713
2023-01-07 08:34:31,050 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,051 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,051 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6486 orig size : 262144
2023-01-07 08:34:31,051 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -2.776362419128418
2023-01-07 08:34:31,052 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,052 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,052 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 16.781757354736328
2023-01-07 08:34:31,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,052 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -18.81258773803711
2023-01-07 08:34:31,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.589504063129425
2023-01-07 08:34:31,053 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,053 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.08666443824768066
2023-01-07 08:34:31,053 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,053 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,054 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,054 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -17.60549545288086
2023-01-07 08:34:31,055 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,055 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,055 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 30.224605560302734
2023-01-07 08:34:31,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,055 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -5.189892768859863
2023-01-07 08:34:31,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2904224395751953
2023-01-07 08:34:31,056 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,056 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.48926272988319397
2023-01-07 08:34:31,056 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,057 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7762 orig size : 262144
2023-01-07 08:34:31,057 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -5.457447528839111
2023-01-07 08:34:31,057 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,058 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,058 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 1.4394025802612305
2023-01-07 08:34:31,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,058 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -16.456453323364258
2023-01-07 08:34:31,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6814119815826416
2023-01-07 08:34:31,059 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,059 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.7287360429763794
2023-01-07 08:34:31,059 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,059 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,060 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6474 orig size : 262144
2023-01-07 08:34:31,060 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -10.112517356872559
2023-01-07 08:34:31,061 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,061 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.629247665405273
2023-01-07 08:34:31,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,061 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 18.500926971435547
2023-01-07 08:34:31,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17765581607818604
2023-01-07 08:34:31,062 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,062 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.3557247519493103
2023-01-07 08:34:31,062 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,062 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,063 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,063 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 20.04220962524414
2023-01-07 08:34:31,063 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,064 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,064 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.4688615798950195
2023-01-07 08:34:31,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,064 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 2.3207926750183105
2023-01-07 08:34:31,064 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6344576478004456
2023-01-07 08:34:31,065 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,065 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.17246678471565247
2023-01-07 08:34:31,065 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,066 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,066 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7944 orig size : 262144
2023-01-07 08:34:31,066 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 3.6109108924865723
2023-01-07 08:34:31,066 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,067 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,067 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.4183694124221802
2023-01-07 08:34:31,067 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,067 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,068 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9322 orig size : 262144
2023-01-07 08:34:31,068 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -17.300708770751953
2023-01-07 08:34:31,068 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,069 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 36.37263870239258
2023-01-07 08:34:31,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,069 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,069 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -9.261800765991211
2023-01-07 08:34:31,069 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6040027141571045
2023-01-07 08:34:31,070 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,070 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.12869545817375183
2023-01-07 08:34:31,070 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,070 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,071 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6449 orig size : 589824
2023-01-07 08:34:31,071 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -16.623929977416992
2023-01-07 08:34:31,072 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,072 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,072 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 1.565791130065918
2023-01-07 08:34:31,072 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,072 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,072 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.49949443340301514
2023-01-07 08:34:31,072 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.160820484161377
2023-01-07 08:34:31,073 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,073 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.0849933847784996
2023-01-07 08:34:31,073 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,073 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,074 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8980 orig size : 262144
2023-01-07 08:34:31,074 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 0.35075604915618896
2023-01-07 08:34:31,074 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,075 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,075 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -0.14213430881500244
2023-01-07 08:34:31,075 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,075 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,076 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,076 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9351 orig size : 524288
2023-01-07 08:34:31,076 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -2.5324039459228516
2023-01-07 08:34:31,076 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,077 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,077 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.22402045130729675
2023-01-07 08:34:31,077 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,077 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,079 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6443 orig size : 2359296
2023-01-07 08:34:31,079 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 14.889665603637695
2023-01-07 08:34:31,079 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,079 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,080 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,080 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.023161984980106354
2023-01-07 08:34:31,080 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,081 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 43557 orig size : 1048576
2023-01-07 08:34:31,081 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 3.7537224292755127
2023-01-07 08:34:31,082 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,082 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,082 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 26.726709365844727
2023-01-07 08:34:31,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,082 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -13.622049331665039
2023-01-07 08:34:31,082 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07761524617671967
2023-01-07 08:34:31,083 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,083 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -0.4681265950202942
2023-01-07 08:34:31,083 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,084 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,084 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6806 orig size : 2097152
2023-01-07 08:34:31,084 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -14.881036758422852
2023-01-07 08:34:31,084 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,084 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,085 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,085 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.5543369054794312
2023-01-07 08:34:31,085 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,085 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,086 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 23419 orig size : 1048576
2023-01-07 08:34:31,086 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 2.7550454139709473
2023-01-07 08:34:31,086 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,086 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,087 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,087 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.14855629205703735
2023-01-07 08:34:31,087 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,087 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,088 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,088 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9267 orig size : 2359296
2023-01-07 08:34:31,088 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -2.5589232444763184
2023-01-07 08:34:31,088 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,089 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.19667911529541
2023-01-07 08:34:31,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,089 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 5.098650932312012
2023-01-07 08:34:31,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6382582187652588
2023-01-07 08:34:31,090 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,090 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.05688559636473656
2023-01-07 08:34:31,090 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,091 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33875 orig size : 1048576
2023-01-07 08:34:31,091 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 1.4133076667785645
2023-01-07 08:34:31,091 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,091 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,092 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,092 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.2812640368938446
2023-01-07 08:34:31,092 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,092 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,093 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6434 orig size : 1048576
2023-01-07 08:34:31,093 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 4.552725315093994
2023-01-07 08:34:31,093 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,093 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,094 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,094 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.08617784082889557
2023-01-07 08:34:31,094 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,094 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,095 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6363 orig size : 2359296
2023-01-07 08:34:31,095 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -8.93801212310791
2023-01-07 08:34:31,095 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,095 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,096 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,096 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.10910475254058838
2023-01-07 08:34:31,096 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,097 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,097 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33305 orig size : 1048576
2023-01-07 08:34:31,097 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -6.285186767578125
2023-01-07 08:34:31,097 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,097 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,098 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.103188514709473
2023-01-07 08:34:31,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,098 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.02288818359375
2023-01-07 08:34:31,098 > [DEBUG] 0 :: before allreduce fusion buffer :: -304.25665283203125
2023-01-07 08:34:31,099 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,099 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.42570194602012634
2023-01-07 08:34:31,099 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,100 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6582 orig size : 2049000
2023-01-07 08:34:31,100 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -429.71697998046875
2023-01-07 08:34:31,101 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,101 > [DEBUG] 0 :: 7.364412784576416
2023-01-07 08:34:31,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,106 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3065180480480194
2023-01-07 08:34:31,107 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3065180480480194
2023-01-07 08:34:31,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,111 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.03895091637969017
2023-01-07 08:34:31,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,112 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.2830326557159424
2023-01-07 08:34:31,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7677115797996521
2023-01-07 08:34:31,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,116 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.011999860405921936
2023-01-07 08:34:31,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011999860405921936
2023-01-07 08:34:31,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,118 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.2585849165916443
2023-01-07 08:34:31,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2585849165916443
2023-01-07 08:34:31,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,120 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.03346306085586548
2023-01-07 08:34:31,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,120 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,120 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -10.05655288696289
2023-01-07 08:34:31,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39953550696372986
2023-01-07 08:34:31,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,122 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.02080768346786499
2023-01-07 08:34:31,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02080768346786499
2023-01-07 08:34:31,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,124 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.22918759286403656
2023-01-07 08:34:31,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,125 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 13.335493087768555
2023-01-07 08:34:31,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1516199111938477
2023-01-07 08:34:31,126 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,127 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.19528906047344208
2023-01-07 08:34:31,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19528906047344208
2023-01-07 08:34:31,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,129 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.04339711368083954
2023-01-07 08:34:31,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,129 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 9.345775604248047
2023-01-07 08:34:31,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.436039924621582
2023-01-07 08:34:31,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,131 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,131 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.07801547646522522
2023-01-07 08:34:31,131 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07801547646522522
2023-01-07 08:34:31,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.09602174162864685
2023-01-07 08:34:31,133 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09602174162864685
2023-01-07 08:34:31,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.017416872084140778
2023-01-07 08:34:31,136 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,136 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,136 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 10.39950180053711
2023-01-07 08:34:31,136 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1610272228717804
2023-01-07 08:34:31,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.026250915601849556
2023-01-07 08:34:31,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.026250915601849556
2023-01-07 08:34:31,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.12637563049793243
2023-01-07 08:34:31,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12637563049793243
2023-01-07 08:34:31,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.0597902312874794
2023-01-07 08:34:31,142 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,142 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 7.793209552764893
2023-01-07 08:34:31,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22403591871261597
2023-01-07 08:34:31,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,144 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.11758942902088165
2023-01-07 08:34:31,144 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11758942902088165
2023-01-07 08:34:31,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,146 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.017563380300998688
2023-01-07 08:34:31,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.017563380300998688
2023-01-07 08:34:31,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.03325209021568298
2023-01-07 08:34:31,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,149 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -8.313838958740234
2023-01-07 08:34:31,149 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32756227254867554
2023-01-07 08:34:31,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.08580371737480164
2023-01-07 08:34:31,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08580371737480164
2023-01-07 08:34:31,152 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,152 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,153 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.18690535426139832
2023-01-07 08:34:31,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18690535426139832
2023-01-07 08:34:31,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.05998802185058594
2023-01-07 08:34:31,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05998802185058594
2023-01-07 08:34:31,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,157 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.20084834098815918
2023-01-07 08:34:31,157 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20084834098815918
2023-01-07 08:34:31,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,158 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.01116839051246643
2023-01-07 08:34:31,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01116839051246643
2023-01-07 08:34:31,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,160 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.16640830039978027
2023-01-07 08:34:31,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16640830039978027
2023-01-07 08:34:31,163 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,163 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,163 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -0.39774322509765625
2023-01-07 08:34:31,163 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.463728189468384
2023-01-07 08:34:31,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,165 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.1126292496919632
2023-01-07 08:34:31,165 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,165 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,165 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 33.17202377319336
2023-01-07 08:34:31,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.594193935394287
2023-01-07 08:34:31,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,167 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 66.61359405517578
2023-01-07 08:34:31,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0180693864822388
2023-01-07 08:34:31,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,169 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.47747132182121277
2023-01-07 08:34:31,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,169 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 29.62662124633789
2023-01-07 08:34:31,170 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.218396186828613
2023-01-07 08:34:31,171 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,172 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -14.84423828125
2023-01-07 08:34:31,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.526752471923828
2023-01-07 08:34:31,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,175 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,175 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 57.89993667602539
2023-01-07 08:34:31,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.271878242492676
2023-01-07 08:34:31,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,177 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -74.7243423461914
2023-01-07 08:34:31,177 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.638128280639648
2023-01-07 08:34:31,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,180 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.5206350684165955
2023-01-07 08:34:31,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5206350684165955
2023-01-07 08:34:31,182 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,182 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,182 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -105.25889587402344
2023-01-07 08:34:31,182 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.338298797607422
2023-01-07 08:34:31,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,186 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 15.093856811523438
2023-01-07 08:34:31,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3809806108474731
2023-01-07 08:34:31,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,187 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,188 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -258.601318359375
2023-01-07 08:34:31,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.632722854614258
2023-01-07 08:34:31,189 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,189 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,190 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.7108616828918457
2023-01-07 08:34:31,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7108616828918457
2023-01-07 08:34:31,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,193 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.890602111816406
2023-01-07 08:34:31,193 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.726974487304688
2023-01-07 08:34:31,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,195 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 5.521085739135742
2023-01-07 08:34:31,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.816938877105713
2023-01-07 08:34:31,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,197 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 1.871161937713623
2023-01-07 08:34:31,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,197 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.33363112807273865
2023-01-07 08:34:31,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2047929763793945
2023-01-07 08:34:31,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,199 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -0.9782335758209229
2023-01-07 08:34:31,199 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9782335758209229
2023-01-07 08:34:31,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,203 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -2.299652576446533
2023-01-07 08:34:31,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,203 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -49.070579528808594
2023-01-07 08:34:31,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.43014907836914
2023-01-07 08:34:31,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,205 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 21.851701736450195
2023-01-07 08:34:31,205 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.904195785522461
2023-01-07 08:34:31,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.3712105751037598
2023-01-07 08:34:31,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,209 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -1.6541532278060913
2023-01-07 08:34:31,209 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.813699722290039
2023-01-07 08:34:31,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,211 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -66.0473403930664
2023-01-07 08:34:31,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -73.99751281738281
2023-01-07 08:34:31,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,213 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -68.81703186035156
2023-01-07 08:34:31,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.690338134765625
2023-01-07 08:34:31,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,215 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -1.5371739864349365
2023-01-07 08:34:31,215 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5371739864349365
2023-01-07 08:34:31,218 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:34:31,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,219 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -5317.1552734375
2023-01-07 08:34:31,219 > [DEBUG] 0 :: before allreduce fusion buffer :: -5317.1552734375
2023-01-07 08:34:31,220 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,220 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -3079.61376953125
2023-01-07 08:34:31,220 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,221 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,221 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 1.676407814025879
2023-01-07 08:34:31,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.676407814025879
2023-01-07 08:34:31,224 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,224 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 72.2277603149414
2023-01-07 08:34:31,224 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,224 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.1111645698547363
2023-01-07 08:34:31,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,225 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -221.4630126953125
2023-01-07 08:34:31,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -221.4630126953125
2023-01-07 08:34:31,228 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,228 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -3826.37646484375
2023-01-07 08:34:31,228 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 22.93356704711914
2023-01-07 08:34:31,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,228 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -791.1023559570312
2023-01-07 08:34:31,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 90.10746765136719
2023-01-07 08:34:31,229 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,229 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 23.849977493286133
2023-01-07 08:34:31,230 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,231 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 4240 orig size : 36864
2023-01-07 08:34:31,231 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -4704.2197265625
2023-01-07 08:34:31,231 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,231 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.5073404312133789
2023-01-07 08:34:31,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,232 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -43.821468353271484
2023-01-07 08:34:31,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -44.32880401611328
2023-01-07 08:34:31,233 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,233 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -0.5073404312133789
2023-01-07 08:34:31,233 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,234 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,234 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.16524505615234375
2023-01-07 08:34:31,234 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,234 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.6381664872169495
2023-01-07 08:34:31,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5740114450454712
2023-01-07 08:34:31,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,235 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -11.77926254272461
2023-01-07 08:34:31,235 > [DEBUG] 0 :: before allreduce fusion buffer :: -77.54996490478516
2023-01-07 08:34:31,236 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,236 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -11.057465553283691
2023-01-07 08:34:31,237 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,237 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8824 orig size : 16384
2023-01-07 08:34:31,238 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2105.583984375
2023-01-07 08:34:31,238 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,238 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,238 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,238 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 2.5652291774749756
2023-01-07 08:34:31,238 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,239 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,240 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7699 orig size : 16384
2023-01-07 08:34:31,240 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 1390.273193359375
2023-01-07 08:34:31,240 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,240 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,240 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,240 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 0.21525859832763672
2023-01-07 08:34:31,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,241 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 35.365150451660156
2023-01-07 08:34:31,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.22571563720703
2023-01-07 08:34:31,242 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,242 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.21525859832763672
2023-01-07 08:34:31,242 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,243 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 19439 orig size : 36864
2023-01-07 08:34:31,243 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -134.63201904296875
2023-01-07 08:34:31,243 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,244 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -1.2922406196594238
2023-01-07 08:34:31,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,244 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -70.98538970947266
2023-01-07 08:34:31,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.39881896972656
2023-01-07 08:34:31,245 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,245 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 2.063631534576416
2023-01-07 08:34:31,245 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,247 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10946 orig size : 16384
2023-01-07 08:34:31,247 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -77.91704559326172
2023-01-07 08:34:31,247 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.240619659423828
2023-01-07 08:34:31,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,247 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 36.800514221191406
2023-01-07 08:34:31,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.058313369750977
2023-01-07 08:34:31,248 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,248 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 9.244349479675293
2023-01-07 08:34:31,248 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,248 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,250 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7683 orig size : 16384
2023-01-07 08:34:31,250 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 1557.126953125
2023-01-07 08:34:31,250 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,250 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,250 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -1.2103257179260254
2023-01-07 08:34:31,250 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,251 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,252 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10257 orig size : 36864
2023-01-07 08:34:31,252 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -52.43748474121094
2023-01-07 08:34:31,252 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,252 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.17452335357666016
2023-01-07 08:34:31,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,253 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -56.068260192871094
2023-01-07 08:34:31,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,253 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -119.27781677246094
2023-01-07 08:34:31,253 > [DEBUG] 0 :: before allreduce fusion buffer :: -104.07994842529297
2023-01-07 08:34:31,254 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,254 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.17452335357666016
2023-01-07 08:34:31,254 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,255 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,255 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -86.25053405761719
2023-01-07 08:34:31,255 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.267199516296387
2023-01-07 08:34:31,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,256 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -13.603044509887695
2023-01-07 08:34:31,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0316815376281738
2023-01-07 08:34:31,257 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,257 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 12.522235870361328
2023-01-07 08:34:31,257 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,258 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8069 orig size : 32768
2023-01-07 08:34:31,258 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -54.380889892578125
2023-01-07 08:34:31,258 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,259 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,259 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 3.715911865234375
2023-01-07 08:34:31,259 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,260 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10234 orig size : 147456
2023-01-07 08:34:31,260 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -205.7570037841797
2023-01-07 08:34:31,260 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,261 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.5718101263046265
2023-01-07 08:34:31,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5718101263046265
2023-01-07 08:34:31,262 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,262 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -1.0680971145629883
2023-01-07 08:34:31,262 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,263 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7003 orig size : 65536
2023-01-07 08:34:31,263 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 39.621917724609375
2023-01-07 08:34:31,263 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:31,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,264 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 1.4974286556243896
2023-01-07 08:34:31,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4974286556243896
2023-01-07 08:34:31,264 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,265 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 1.0199623107910156
2023-01-07 08:34:31,265 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,265 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,266 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6105 orig size : 131072
2023-01-07 08:34:31,266 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2835.998291015625
2023-01-07 08:34:31,266 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,266 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,267 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 1.1054911613464355
2023-01-07 08:34:31,267 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,267 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,268 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6034 orig size : 65536
2023-01-07 08:34:31,268 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 1345.8228759765625
2023-01-07 08:34:31,268 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,268 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.3358900547027588
2023-01-07 08:34:31,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3358900547027588
2023-01-07 08:34:31,269 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,269 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -0.7957175970077515
2023-01-07 08:34:31,270 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,271 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6601 orig size : 147456
2023-01-07 08:34:31,271 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -273.65863037109375
2023-01-07 08:34:31,271 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,271 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,271 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.6830067038536072
2023-01-07 08:34:31,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6830067038536072
2023-01-07 08:34:31,272 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,272 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.13551050424575806
2023-01-07 08:34:31,273 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,274 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8376 orig size : 65536
2023-01-07 08:34:31,274 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 2149.75537109375
2023-01-07 08:34:31,274 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,274 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,274 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,274 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 45.57825469970703
2023-01-07 08:34:31,274 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,276 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8290 orig size : 65536
2023-01-07 08:34:31,276 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 2224.1875
2023-01-07 08:34:31,276 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.06323349475860596
2023-01-07 08:34:31,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06323349475860596
2023-01-07 08:34:31,277 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,277 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 1.2700378894805908
2023-01-07 08:34:31,277 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,277 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,278 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6624 orig size : 147456
2023-01-07 08:34:31,279 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -155.9359893798828
2023-01-07 08:34:31,279 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.9421634078025818
2023-01-07 08:34:31,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9421634078025818
2023-01-07 08:34:31,280 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,280 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8157370686531067
2023-01-07 08:34:31,280 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,280 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,281 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5265 orig size : 65536
2023-01-07 08:34:31,282 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -73.43905639648438
2023-01-07 08:34:31,282 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,282 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,282 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 18.716629028320312
2023-01-07 08:34:31,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 5.0952982902526855
2023-01-07 08:34:31,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.302366256713867
2023-01-07 08:34:31,283 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,283 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 9.491155624389648
2023-01-07 08:34:31,283 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,284 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5001 orig size : 65536
2023-01-07 08:34:31,285 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 6.961866855621338
2023-01-07 08:34:31,285 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,285 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,285 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.6907657980918884
2023-01-07 08:34:31,285 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6907657980918884
2023-01-07 08:34:31,286 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,286 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.034322142601013184
2023-01-07 08:34:31,286 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,287 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6593 orig size : 147456
2023-01-07 08:34:31,287 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -46.58320617675781
2023-01-07 08:34:31,288 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,288 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,288 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.4058484733104706
2023-01-07 08:34:31,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -1.8188972473144531
2023-01-07 08:34:31,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0993766784667969
2023-01-07 08:34:31,290 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,290 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.9320096373558044
2023-01-07 08:34:31,290 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,291 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8226 orig size : 65536
2023-01-07 08:34:31,291 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -10.937217712402344
2023-01-07 08:34:31,291 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,291 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:31,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.09918880462646484
2023-01-07 08:34:31,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09918880462646484
2023-01-07 08:34:31,293 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,293 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 1.1663074493408203
2023-01-07 08:34:31,293 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,294 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5254 orig size : 131072
2023-01-07 08:34:31,294 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 78.18636322021484
2023-01-07 08:34:31,294 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,294 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.2227952480316162
2023-01-07 08:34:31,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2227952480316162
2023-01-07 08:34:31,295 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,296 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0030491501092910767
2023-01-07 08:34:31,296 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,297 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6696 orig size : 589824
2023-01-07 08:34:31,297 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 2.1847381591796875
2023-01-07 08:34:31,297 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn3._dp_wrapped_module.flat_param_0 value:: 1024.0
2023-01-07 08:34:31,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,297 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.13215312361717224
2023-01-07 08:34:31,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13215312361717224
2023-01-07 08:34:31,298 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,299 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.6824583411216736
2023-01-07 08:34:31,299 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,299 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,300 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 12862 orig size : 262144
2023-01-07 08:34:31,300 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 6.53730583190918
2023-01-07 08:34:31,300 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,300 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,300 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,301 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.3028380274772644
2023-01-07 08:34:31,301 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,301 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7654 orig size : 524288
2023-01-07 08:34:31,302 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 41.21030807495117
2023-01-07 08:34:31,302 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -36.73724365234375
2023-01-07 08:34:31,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,302 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 13.017545700073242
2023-01-07 08:34:31,302 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.2336273193359375
2023-01-07 08:34:31,303 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,303 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.783345639705658
2023-01-07 08:34:31,303 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,305 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 11247 orig size : 262144
2023-01-07 08:34:31,306 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 44.409175872802734
2023-01-07 08:34:31,306 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.07544241845607758
2023-01-07 08:34:31,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 64.50825500488281
2023-01-07 08:34:31,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.866815567016602
2023-01-07 08:34:31,308 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,308 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.4413652718067169
2023-01-07 08:34:31,308 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,309 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,309 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9776 orig size : 589824
2023-01-07 08:34:31,309 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 67.39849853515625
2023-01-07 08:34:31,309 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 16.759733200073242
2023-01-07 08:34:31,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,310 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -20.267467498779297
2023-01-07 08:34:31,310 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1993765830993652
2023-01-07 08:34:31,311 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,311 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -1.79478120803833
2023-01-07 08:34:31,311 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,312 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7698 orig size : 262144
2023-01-07 08:34:31,312 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -22.20250701904297
2023-01-07 08:34:31,312 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 56.13298034667969
2023-01-07 08:34:31,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,313 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 20.300569534301758
2023-01-07 08:34:31,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9220964908599854
2023-01-07 08:34:31,314 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,314 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.10708296298980713
2023-01-07 08:34:31,314 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,315 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6637 orig size : 262144
2023-01-07 08:34:31,315 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 7.316473007202148
2023-01-07 08:34:31,315 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,315 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.015634536743164062
2023-01-07 08:34:31,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,316 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -26.426406860351562
2023-01-07 08:34:31,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.7876996994018555
2023-01-07 08:34:31,317 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,317 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -1.3740596771240234
2023-01-07 08:34:31,317 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,318 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6836 orig size : 589824
2023-01-07 08:34:31,318 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -24.682357788085938
2023-01-07 08:34:31,318 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 38.4240608215332
2023-01-07 08:34:31,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,319 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -17.46262550354004
2023-01-07 08:34:31,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.418555736541748
2023-01-07 08:34:31,320 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,320 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -1.1732113361358643
2023-01-07 08:34:31,320 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,320 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,321 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7727 orig size : 262144
2023-01-07 08:34:31,321 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -19.357412338256836
2023-01-07 08:34:31,321 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -34.67656326293945
2023-01-07 08:34:31,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,322 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 14.188643455505371
2023-01-07 08:34:31,322 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.17855167388916
2023-01-07 08:34:31,323 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,323 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 1.6226242780685425
2023-01-07 08:34:31,323 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,324 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6486 orig size : 262144
2023-01-07 08:34:31,324 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 16.383989334106445
2023-01-07 08:34:31,324 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 16.781757354736328
2023-01-07 08:34:31,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,325 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -11.927812576293945
2023-01-07 08:34:31,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.386415958404541
2023-01-07 08:34:31,326 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,326 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.10673132538795471
2023-01-07 08:34:31,326 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,327 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,327 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -14.245523452758789
2023-01-07 08:34:31,327 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 30.224605560302734
2023-01-07 08:34:31,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -4.51885986328125
2023-01-07 08:34:31,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3295030891895294
2023-01-07 08:34:31,329 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,329 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.19253508746623993
2023-01-07 08:34:31,329 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,330 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,330 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7762 orig size : 262144
2023-01-07 08:34:31,330 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -6.746715545654297
2023-01-07 08:34:31,330 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,330 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,330 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 1.4394025802612305
2023-01-07 08:34:31,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 44.86396789550781
2023-01-07 08:34:31,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0617669820785522
2023-01-07 08:34:31,332 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,332 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.07744738459587097
2023-01-07 08:34:31,332 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,333 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6474 orig size : 262144
2023-01-07 08:34:31,333 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 20.063169479370117
2023-01-07 08:34:31,333 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.629247665405273
2023-01-07 08:34:31,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,334 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 14.773734092712402
2023-01-07 08:34:31,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7926940321922302
2023-01-07 08:34:31,335 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,335 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -0.1326310932636261
2023-01-07 08:34:31,335 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,336 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,336 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 11.013917922973633
2023-01-07 08:34:31,336 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.4688615798950195
2023-01-07 08:34:31,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 5.923546314239502
2023-01-07 08:34:31,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1829257011413574
2023-01-07 08:34:31,338 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,338 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.20773759484291077
2023-01-07 08:34:31,338 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,339 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7944 orig size : 262144
2023-01-07 08:34:31,339 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 4.532306671142578
2023-01-07 08:34:31,339 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,340 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,340 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.14300605654716492
2023-01-07 08:34:31,340 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,341 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9322 orig size : 262144
2023-01-07 08:34:31,341 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 8.254467964172363
2023-01-07 08:34:31,341 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,341 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,341 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 36.37263870239258
2023-01-07 08:34:31,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,342 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -4.119406700134277
2023-01-07 08:34:31,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.437819242477417
2023-01-07 08:34:31,343 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,343 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.056287430226802826
2023-01-07 08:34:31,343 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,344 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6449 orig size : 589824
2023-01-07 08:34:31,344 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 46.399147033691406
2023-01-07 08:34:31,344 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 1.565791130065918
2023-01-07 08:34:31,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,345 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.8314380645751953
2023-01-07 08:34:31,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0894907712936401
2023-01-07 08:34:31,346 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,346 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.11051824688911438
2023-01-07 08:34:31,346 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,347 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8980 orig size : 262144
2023-01-07 08:34:31,347 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 3.9826347827911377
2023-01-07 08:34:31,347 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,348 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,348 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.7536313533782959
2023-01-07 08:34:31,348 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,348 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,349 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9351 orig size : 524288
2023-01-07 08:34:31,349 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 10.85676097869873
2023-01-07 08:34:31,349 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,350 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,350 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.11055838316679001
2023-01-07 08:34:31,350 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,351 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6443 orig size : 2359296
2023-01-07 08:34:31,351 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 16.506258010864258
2023-01-07 08:34:31,351 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,351 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,352 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,352 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.188573956489563
2023-01-07 08:34:31,352 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,353 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,354 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 43557 orig size : 1048576
2023-01-07 08:34:31,354 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 11.066877365112305
2023-01-07 08:34:31,354 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 26.726709365844727
2023-01-07 08:34:31,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,354 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 0.3299076557159424
2023-01-07 08:34:31,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9146767854690552
2023-01-07 08:34:31,355 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,355 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.27962279319763184
2023-01-07 08:34:31,355 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,356 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6806 orig size : 2097152
2023-01-07 08:34:31,356 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1.9848792552947998
2023-01-07 08:34:31,357 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,357 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,357 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.9872260689735413
2023-01-07 08:34:31,357 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,358 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 23419 orig size : 1048576
2023-01-07 08:34:31,358 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 13.032210350036621
2023-01-07 08:34:31,359 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,359 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,359 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.1601058393716812
2023-01-07 08:34:31,359 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,360 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9267 orig size : 2359296
2023-01-07 08:34:31,360 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -10.213079452514648
2023-01-07 08:34:31,361 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.19667911529541
2023-01-07 08:34:31,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,361 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.461123466491699
2023-01-07 08:34:31,361 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8965590000152588
2023-01-07 08:34:31,362 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,362 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.13929814100265503
2023-01-07 08:34:31,362 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,363 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33875 orig size : 1048576
2023-01-07 08:34:31,363 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -8.763175964355469
2023-01-07 08:34:31,364 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,364 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,364 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.3143589496612549
2023-01-07 08:34:31,364 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,365 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6434 orig size : 1048576
2023-01-07 08:34:31,365 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 2.700779914855957
2023-01-07 08:34:31,366 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,366 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,366 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.03959869593381882
2023-01-07 08:34:31,366 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,367 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6363 orig size : 2359296
2023-01-07 08:34:31,367 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -4.758675575256348
2023-01-07 08:34:31,368 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,368 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,368 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.06825178116559982
2023-01-07 08:34:31,368 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,369 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33305 orig size : 1048576
2023-01-07 08:34:31,369 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -3.544999599456787
2023-01-07 08:34:31,370 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.103188514709473
2023-01-07 08:34:31,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,370 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0159912109375
2023-01-07 08:34:31,370 > [DEBUG] 0 :: before allreduce fusion buffer :: -343.2181091308594
2023-01-07 08:34:31,371 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,371 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.5219780206680298
2023-01-07 08:34:31,371 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,372 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6582 orig size : 2049000
2023-01-07 08:34:31,373 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -221.23880004882812
2023-01-07 08:34:31,373 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,374 > [DEBUG] 0 :: 7.408013343811035
2023-01-07 08:34:31,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,378 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.39158719778060913
2023-01-07 08:34:31,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39158719778060913
2023-01-07 08:34:31,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.068414106965065
2023-01-07 08:34:31,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.5365276336669922
2023-01-07 08:34:31,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6525895595550537
2023-01-07 08:34:31,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.02249033749103546
2023-01-07 08:34:31,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02249033749103546
2023-01-07 08:34:31,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.3598507046699524
2023-01-07 08:34:31,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3598507046699524
2023-01-07 08:34:31,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,392 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.027603797614574432
2023-01-07 08:34:31,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,392 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -3.158440113067627
2023-01-07 08:34:31,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8987486362457275
2023-01-07 08:34:31,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.041036803275346756
2023-01-07 08:34:31,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.041036803275346756
2023-01-07 08:34:31,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.24085265398025513
2023-01-07 08:34:31,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,397 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 7.529593467712402
2023-01-07 08:34:31,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3060557842254639
2023-01-07 08:34:31,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,399 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.34447145462036133
2023-01-07 08:34:31,399 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34447145462036133
2023-01-07 08:34:31,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,401 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.06989516317844391
2023-01-07 08:34:31,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,401 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -11.06027603149414
2023-01-07 08:34:31,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1599769592285156
2023-01-07 08:34:31,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,403 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.0324789434671402
2023-01-07 08:34:31,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0324789434671402
2023-01-07 08:34:31,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.013689868152141571
2023-01-07 08:34:31,405 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.013689868152141571
2023-01-07 08:34:31,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,407 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.01074877381324768
2023-01-07 08:34:31,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,407 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -27.63988494873047
2023-01-07 08:34:31,408 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.272133231163025
2023-01-07 08:34:31,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.014966808259487152
2023-01-07 08:34:31,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.014966808259487152
2023-01-07 08:34:31,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,412 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.10930072516202927
2023-01-07 08:34:31,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10930072516202927
2023-01-07 08:34:31,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,414 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.016885370016098022
2023-01-07 08:34:31,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,414 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 11.221363067626953
2023-01-07 08:34:31,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4671098291873932
2023-01-07 08:34:31,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,416 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.11053761839866638
2023-01-07 08:34:31,416 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11053761839866638
2023-01-07 08:34:31,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.03671666607260704
2023-01-07 08:34:31,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03671666607260704
2023-01-07 08:34:31,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.15912513434886932
2023-01-07 08:34:31,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,420 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -2.191741943359375
2023-01-07 08:34:31,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20536786317825317
2023-01-07 08:34:31,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,422 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.1645253598690033
2023-01-07 08:34:31,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1645253598690033
2023-01-07 08:34:31,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,424 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.09292745590209961
2023-01-07 08:34:31,425 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09292745590209961
2023-01-07 08:34:31,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,426 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.0437929630279541
2023-01-07 08:34:31,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0437929630279541
2023-01-07 08:34:31,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,428 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.17564907670021057
2023-01-07 08:34:31,428 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17564907670021057
2023-01-07 08:34:31,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.5001245141029358
2023-01-07 08:34:31,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5001245141029358
2023-01-07 08:34:31,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,432 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.11046579480171204
2023-01-07 08:34:31,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11046579480171204
2023-01-07 08:34:31,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,435 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 28.332509994506836
2023-01-07 08:34:31,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.371406555175781
2023-01-07 08:34:31,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,436 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.35395991802215576
2023-01-07 08:34:31,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,437 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 9.573492050170898
2023-01-07 08:34:31,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3978708982467651
2023-01-07 08:34:31,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,439 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 112.2157211303711
2023-01-07 08:34:31,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.742560386657715
2023-01-07 08:34:31,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,441 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.07177180051803589
2023-01-07 08:34:31,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,441 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 31.101625442504883
2023-01-07 08:34:31,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.740604400634766
2023-01-07 08:34:31,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,443 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -34.27574920654297
2023-01-07 08:34:31,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.34747314453125
2023-01-07 08:34:31,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,447 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -19.51243782043457
2023-01-07 08:34:31,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.346643447875977
2023-01-07 08:34:31,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 111.7321548461914
2023-01-07 08:34:31,449 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.0960750579834
2023-01-07 08:34:31,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,452 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.005920901894569397
2023-01-07 08:34:31,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005920901894569397
2023-01-07 08:34:31,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,454 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 73.90686798095703
2023-01-07 08:34:31,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.561326026916504
2023-01-07 08:34:31,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,457 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.207029342651367
2023-01-07 08:34:31,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6815624237060547
2023-01-07 08:34:31,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,459 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 57.29588317871094
2023-01-07 08:34:31,459 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.7639741897583
2023-01-07 08:34:31,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,461 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.1978790760040283
2023-01-07 08:34:31,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1978790760040283
2023-01-07 08:34:31,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -34.48186492919922
2023-01-07 08:34:31,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.393260955810547
2023-01-07 08:34:31,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,466 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1.5924646854400635
2023-01-07 08:34:31,467 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.529213905334473
2023-01-07 08:34:31,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,468 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.8024734258651733
2023-01-07 08:34:31,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,469 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.9863099455833435
2023-01-07 08:34:31,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7887831926345825
2023-01-07 08:34:31,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,471 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.203707218170166
2023-01-07 08:34:31,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.203707218170166
2023-01-07 08:34:31,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,474 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 1.8688719272613525
2023-01-07 08:34:31,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,474 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -21.040525436401367
2023-01-07 08:34:31,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.641536712646484
2023-01-07 08:34:31,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,476 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 25.194091796875
2023-01-07 08:34:31,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.208965301513672
2023-01-07 08:34:31,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,480 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.901540517807007
2023-01-07 08:34:31,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,480 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -3.6415579319000244
2023-01-07 08:34:31,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.205550193786621
2023-01-07 08:34:31,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,482 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.229278564453125
2023-01-07 08:34:31,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -64.64768981933594
2023-01-07 08:34:31,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,484 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -20.55031394958496
2023-01-07 08:34:31,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.53530502319336
2023-01-07 08:34:31,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,486 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.385267734527588
2023-01-07 08:34:31,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.385267734527588
2023-01-07 08:34:31,492 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 08:34:31,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,493 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 3639.824462890625
2023-01-07 08:34:31,493 > [DEBUG] 0 :: before allreduce fusion buffer :: 3639.824462890625
2023-01-07 08:34:31,494 > [DEBUG] 0 :: param conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,494 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 6058.8779296875
2023-01-07 08:34:31,495 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,495 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,496 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -2.050861358642578
2023-01-07 08:34:31,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.050861358642578
2023-01-07 08:34:31,498 > [DEBUG] 0 :: param bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,498 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 234.40118408203125
2023-01-07 08:34:31,498 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -1.1111645698547363
2023-01-07 08:34:31,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,499 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 43.43878173828125
2023-01-07 08:34:31,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.43878173828125
2023-01-07 08:34:31,500 > [DEBUG] 0 :: param layer1.0.conv1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,501 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 3231.107421875
2023-01-07 08:34:31,501 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 22.93356704711914
2023-01-07 08:34:31,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,501 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -32.161808013916016
2023-01-07 08:34:31,501 > [DEBUG] 0 :: before allreduce fusion buffer :: 90.3202133178711
2023-01-07 08:34:31,502 > [DEBUG] 0 :: param layer1.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,502 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -15.65223503112793
2023-01-07 08:34:31,502 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,503 > [DEBUG] 0 :: param layer1.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 4240 orig size : 36864
2023-01-07 08:34:31,504 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 2999.507568359375
2023-01-07 08:34:31,504 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,504 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,504 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -1.5273551940917969
2023-01-07 08:34:31,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,504 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 115.99990844726562
2023-01-07 08:34:31,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 114.47254943847656
2023-01-07 08:34:31,505 > [DEBUG] 0 :: param layer1.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,506 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -1.5273551940917969
2023-01-07 08:34:31,506 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,506 > [DEBUG] 0 :: param layer1.0.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,506 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 66.36036682128906
2023-01-07 08:34:31,507 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,507 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -7.608286380767822
2023-01-07 08:34:31,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,507 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.024370670318603516
2023-01-07 08:34:31,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,508 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 180.51568603515625
2023-01-07 08:34:31,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.60345458984375
2023-01-07 08:34:31,509 > [DEBUG] 0 :: param layer1.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,509 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -6.050563335418701
2023-01-07 08:34:31,509 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,510 > [DEBUG] 0 :: param layer1.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8824 orig size : 16384
2023-01-07 08:34:31,510 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2504.598876953125
2023-01-07 08:34:31,510 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,511 > [DEBUG] 0 :: param layer1.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,511 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 18.897808074951172
2023-01-07 08:34:31,511 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,512 > [DEBUG] 0 :: param layer1.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7699 orig size : 16384
2023-01-07 08:34:31,512 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 2662.2900390625
2023-01-07 08:34:31,512 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,513 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 3.5061216354370117
2023-01-07 08:34:31,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,513 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 62.900230407714844
2023-01-07 08:34:31,513 > [DEBUG] 0 :: before allreduce fusion buffer :: 206.04428100585938
2023-01-07 08:34:31,514 > [DEBUG] 0 :: param layer1.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,514 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 3.5061216354370117
2023-01-07 08:34:31,514 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,515 > [DEBUG] 0 :: param layer1.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 19439 orig size : 36864
2023-01-07 08:34:31,515 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 87.38042449951172
2023-01-07 08:34:31,516 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,516 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.9495849609375
2023-01-07 08:34:31,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,516 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 56.9929313659668
2023-01-07 08:34:31,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.81640625
2023-01-07 08:34:31,517 > [DEBUG] 0 :: param layer1.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,517 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 3.031635284423828
2023-01-07 08:34:31,518 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,519 > [DEBUG] 0 :: param layer1.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10946 orig size : 16384
2023-01-07 08:34:31,519 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 65.52598571777344
2023-01-07 08:34:31,519 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -14.240619659423828
2023-01-07 08:34:31,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,519 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 60.28628921508789
2023-01-07 08:34:31,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.467693328857422
2023-01-07 08:34:31,520 > [DEBUG] 0 :: param layer1.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,520 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 6.8494873046875
2023-01-07 08:34:31,521 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,521 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,522 > [DEBUG] 0 :: param layer1.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7683 orig size : 16384
2023-01-07 08:34:31,522 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 2743.4638671875
2023-01-07 08:34:31,522 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,522 > [DEBUG] 0 :: param layer1.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,522 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -8.64438533782959
2023-01-07 08:34:31,523 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,524 > [DEBUG] 0 :: param layer1.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10257 orig size : 36864
2023-01-07 08:34:31,524 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 144.39337158203125
2023-01-07 08:34:31,524 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 08:34:31,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,524 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 1.2041170597076416
2023-01-07 08:34:31,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,525 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -2.7697229385375977
2023-01-07 08:34:31,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,525 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -7.809534072875977
2023-01-07 08:34:31,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.317784309387207
2023-01-07 08:34:31,526 > [DEBUG] 0 :: param layer1.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,526 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 1.2041170597076416
2023-01-07 08:34:31,526 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,527 > [DEBUG] 0 :: param layer1.2.conv3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,527 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: -8.903305053710938
2023-01-07 08:34:31,527 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 4.267199516296387
2023-01-07 08:34:31,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,528 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -29.960594177246094
2023-01-07 08:34:31,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.623526096343994
2023-01-07 08:34:31,529 > [DEBUG] 0 :: param layer1.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,529 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 6.184322357177734
2023-01-07 08:34:31,529 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,529 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,530 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,531 > [DEBUG] 0 :: param layer2.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8069 orig size : 32768
2023-01-07 08:34:31,531 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -79.57917785644531
2023-01-07 08:34:31,531 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,531 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,532 > [DEBUG] 0 :: param layer2.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,532 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -1.8651129007339478
2023-01-07 08:34:31,532 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,533 > [DEBUG] 0 :: param layer2.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 10234 orig size : 147456
2023-01-07 08:34:31,533 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 37.732215881347656
2023-01-07 08:34:31,533 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,534 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.7223514914512634
2023-01-07 08:34:31,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7223514914512634
2023-01-07 08:34:31,535 > [DEBUG] 0 :: param layer2.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,535 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -1.905425786972046
2023-01-07 08:34:31,535 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,536 > [DEBUG] 0 :: param layer2.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7003 orig size : 65536
2023-01-07 08:34:31,536 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 81.86241912841797
2023-01-07 08:34:31,536 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:31,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,537 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 0.7193039655685425
2023-01-07 08:34:31,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7193039655685425
2023-01-07 08:34:31,537 > [DEBUG] 0 :: param layer2.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,538 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -1.6380634307861328
2023-01-07 08:34:31,538 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,539 > [DEBUG] 0 :: param layer2.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6105 orig size : 131072
2023-01-07 08:34:31,539 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 379.1293029785156
2023-01-07 08:34:31,539 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,539 > [DEBUG] 0 :: param layer2.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,540 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 6.416891574859619
2023-01-07 08:34:31,540 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,540 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,541 > [DEBUG] 0 :: param layer2.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6034 orig size : 65536
2023-01-07 08:34:31,541 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 676.8687744140625
2023-01-07 08:34:31,541 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.7486202716827393
2023-01-07 08:34:31,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7486202716827393
2023-01-07 08:34:31,542 > [DEBUG] 0 :: param layer2.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,542 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.7984195947647095
2023-01-07 08:34:31,543 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,544 > [DEBUG] 0 :: param layer2.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6601 orig size : 147456
2023-01-07 08:34:31,544 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 45.44288635253906
2023-01-07 08:34:31,544 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,544 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,544 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.4714389741420746
2023-01-07 08:34:31,544 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4714389741420746
2023-01-07 08:34:31,545 > [DEBUG] 0 :: param layer2.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,545 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 1.5807548761367798
2023-01-07 08:34:31,545 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,546 > [DEBUG] 0 :: param layer2.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8376 orig size : 65536
2023-01-07 08:34:31,547 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 2416.5947265625
2023-01-07 08:34:31,547 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,547 > [DEBUG] 0 :: param layer2.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,547 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 39.42854309082031
2023-01-07 08:34:31,547 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,549 > [DEBUG] 0 :: param layer2.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8290 orig size : 65536
2023-01-07 08:34:31,549 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 2559.958740234375
2023-01-07 08:34:31,549 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,549 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,549 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,549 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.27474308013916016
2023-01-07 08:34:31,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27474308013916016
2023-01-07 08:34:31,550 > [DEBUG] 0 :: param layer2.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,550 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 1.4008046388626099
2023-01-07 08:34:31,550 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,551 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,551 > [DEBUG] 0 :: param layer2.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6624 orig size : 147456
2023-01-07 08:34:31,552 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 116.52587127685547
2023-01-07 08:34:31,552 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,552 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,552 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.7493016719818115
2023-01-07 08:34:31,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7493016719818115
2023-01-07 08:34:31,553 > [DEBUG] 0 :: param layer2.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,553 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.16279244422912598
2023-01-07 08:34:31,553 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,554 > [DEBUG] 0 :: param layer2.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5265 orig size : 65536
2023-01-07 08:34:31,554 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 16.55736541748047
2023-01-07 08:34:31,555 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,555 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,555 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 18.716629028320312
2023-01-07 08:34:31,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 3.9687347412109375
2023-01-07 08:34:31,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2868536710739136
2023-01-07 08:34:31,556 > [DEBUG] 0 :: param layer2.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,556 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -1.4967108964920044
2023-01-07 08:34:31,556 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,557 > [DEBUG] 0 :: param layer2.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5001 orig size : 65536
2023-01-07 08:34:31,557 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -0.7234699726104736
2023-01-07 08:34:31,557 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.03144678473472595
2023-01-07 08:34:31,558 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03144678473472595
2023-01-07 08:34:31,559 > [DEBUG] 0 :: param layer2.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,559 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.3332328796386719
2023-01-07 08:34:31,559 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,561 > [DEBUG] 0 :: param layer2.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6593 orig size : 147456
2023-01-07 08:34:31,561 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 75.67549896240234
2023-01-07 08:34:31,561 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,561 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,561 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 08:34:31,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.0946747213602066
2023-01-07 08:34:31,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 4.2806243896484375
2023-01-07 08:34:31,562 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.977596759796143
2023-01-07 08:34:31,563 > [DEBUG] 0 :: param layer2.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,563 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: -0.0946747213602066
2023-01-07 08:34:31,563 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,563 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,564 > [DEBUG] 0 :: param layer2.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8226 orig size : 65536
2023-01-07 08:34:31,564 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 1.546189546585083
2023-01-07 08:34:31,564 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 08:34:31,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.891312837600708
2023-01-07 08:34:31,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.891312837600708
2023-01-07 08:34:31,565 > [DEBUG] 0 :: param layer2.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,566 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.5872597694396973
2023-01-07 08:34:31,566 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,566 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,567 > [DEBUG] 0 :: param layer3.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 5254 orig size : 131072
2023-01-07 08:34:31,567 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -38.04163360595703
2023-01-07 08:34:31,567 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,567 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.11789625883102417
2023-01-07 08:34:31,568 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11789625883102417
2023-01-07 08:34:31,568 > [DEBUG] 0 :: param layer3.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,569 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.09074720740318298
2023-01-07 08:34:31,569 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,570 > [DEBUG] 0 :: param layer3.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6696 orig size : 589824
2023-01-07 08:34:31,570 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -38.64586639404297
2023-01-07 08:34:31,570 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,570 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,570 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn3._dp_wrapped_module.flat_param_0 value:: 1024.0
2023-01-07 08:34:31,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.1486554741859436
2023-01-07 08:34:31,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1486554741859436
2023-01-07 08:34:31,571 > [DEBUG] 0 :: param layer3.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,571 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -0.6288579702377319
2023-01-07 08:34:31,572 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,572 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,573 > [DEBUG] 0 :: param layer3.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 12862 orig size : 262144
2023-01-07 08:34:31,573 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 8.169061660766602
2023-01-07 08:34:31,573 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,573 > [DEBUG] 0 :: param layer3.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,573 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.16394349932670593
2023-01-07 08:34:31,574 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,574 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,574 > [DEBUG] 0 :: param layer3.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7654 orig size : 524288
2023-01-07 08:34:31,575 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 80.12461853027344
2023-01-07 08:34:31,575 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -36.73724365234375
2023-01-07 08:34:31,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -52.550437927246094
2023-01-07 08:34:31,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7297571897506714
2023-01-07 08:34:31,576 > [DEBUG] 0 :: param layer3.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,576 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -13.675779342651367
2023-01-07 08:34:31,576 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,576 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,577 > [DEBUG] 0 :: param layer3.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 11247 orig size : 262144
2023-01-07 08:34:31,578 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 13.606168746948242
2023-01-07 08:34:31,578 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 08:34:31,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.013955891132354736
2023-01-07 08:34:31,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -71.3512954711914
2023-01-07 08:34:31,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.84177303314209
2023-01-07 08:34:31,579 > [DEBUG] 0 :: param layer3.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,580 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: -0.12556934356689453
2023-01-07 08:34:31,580 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,581 > [DEBUG] 0 :: param layer3.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9776 orig size : 589824
2023-01-07 08:34:31,581 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -64.19361114501953
2023-01-07 08:34:31,581 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,581 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 16.759733200073242
2023-01-07 08:34:31,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 9.764518737792969
2023-01-07 08:34:31,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011279404163360596
2023-01-07 08:34:31,582 > [DEBUG] 0 :: param layer3.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,583 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -1.527860164642334
2023-01-07 08:34:31,583 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,583 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,584 > [DEBUG] 0 :: param layer3.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7698 orig size : 262144
2023-01-07 08:34:31,584 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 15.734842300415039
2023-01-07 08:34:31,584 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,584 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,584 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 56.13298034667969
2023-01-07 08:34:31,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,584 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 2.8337502479553223
2023-01-07 08:34:31,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.412311851978302
2023-01-07 08:34:31,585 > [DEBUG] 0 :: param layer3.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,586 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -15.786031723022461
2023-01-07 08:34:31,586 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,586 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,587 > [DEBUG] 0 :: param layer3.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6637 orig size : 262144
2023-01-07 08:34:31,587 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 14.012372970581055
2023-01-07 08:34:31,587 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.015634536743164062
2023-01-07 08:34:31,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -23.267864227294922
2023-01-07 08:34:31,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.184299468994141
2023-01-07 08:34:31,588 > [DEBUG] 0 :: param layer3.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,589 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -0.6158100962638855
2023-01-07 08:34:31,589 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,589 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,590 > [DEBUG] 0 :: param layer3.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6836 orig size : 589824
2023-01-07 08:34:31,590 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -25.18545150756836
2023-01-07 08:34:31,590 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,590 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 38.4240608215332
2023-01-07 08:34:31,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 5.725343704223633
2023-01-07 08:34:31,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6024065613746643
2023-01-07 08:34:31,591 > [DEBUG] 0 :: param layer3.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,592 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -0.7914592623710632
2023-01-07 08:34:31,592 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,592 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,592 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,593 > [DEBUG] 0 :: param layer3.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7727 orig size : 262144
2023-01-07 08:34:31,593 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 7.436944007873535
2023-01-07 08:34:31,593 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -34.67656326293945
2023-01-07 08:34:31,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 27.222000122070312
2023-01-07 08:34:31,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5485636591911316
2023-01-07 08:34:31,594 > [DEBUG] 0 :: param layer3.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,594 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 5.23502779006958
2023-01-07 08:34:31,595 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,596 > [DEBUG] 0 :: param layer3.3.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6486 orig size : 262144
2023-01-07 08:34:31,596 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 25.39673614501953
2023-01-07 08:34:31,596 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 16.781757354736328
2023-01-07 08:34:31,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 45.442230224609375
2023-01-07 08:34:31,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9382879734039307
2023-01-07 08:34:31,597 > [DEBUG] 0 :: param layer3.3.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,597 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.23579612374305725
2023-01-07 08:34:31,598 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,598 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,598 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,599 > [DEBUG] 0 :: param layer3.3.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,599 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 38.830894470214844
2023-01-07 08:34:31,599 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,599 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,599 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 30.224605560302734
2023-01-07 08:34:31,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -14.930018424987793
2023-01-07 08:34:31,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.928523063659668
2023-01-07 08:34:31,600 > [DEBUG] 0 :: param layer3.3.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,600 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0712706595659256
2023-01-07 08:34:31,600 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,602 > [DEBUG] 0 :: param layer3.3.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7762 orig size : 262144
2023-01-07 08:34:31,602 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -16.307703018188477
2023-01-07 08:34:31,602 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,602 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,602 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: 1.4394025802612305
2023-01-07 08:34:31,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,602 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -17.284557342529297
2023-01-07 08:34:31,602 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4393537044525146
2023-01-07 08:34:31,603 > [DEBUG] 0 :: param layer3.3.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,603 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -0.07701733708381653
2023-01-07 08:34:31,603 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,605 > [DEBUG] 0 :: param layer3.4.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6474 orig size : 262144
2023-01-07 08:34:31,605 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 4.4261016845703125
2023-01-07 08:34:31,605 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 11.629247665405273
2023-01-07 08:34:31,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -7.940191268920898
2023-01-07 08:34:31,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0098685026168823
2023-01-07 08:34:31,606 > [DEBUG] 0 :: param layer3.4.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,606 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.15752968192100525
2023-01-07 08:34:31,606 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,607 > [DEBUG] 0 :: param layer3.4.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6809 orig size : 589824
2023-01-07 08:34:31,608 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -5.651244163513184
2023-01-07 08:34:31,608 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,608 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -6.4688615798950195
2023-01-07 08:34:31,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.699761390686035
2023-01-07 08:34:31,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20683595538139343
2023-01-07 08:34:31,609 > [DEBUG] 0 :: param layer3.4.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,609 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -0.1337394267320633
2023-01-07 08:34:31,609 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,610 > [DEBUG] 0 :: param layer3.4.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 7944 orig size : 262144
2023-01-07 08:34:31,611 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -3.189431667327881
2023-01-07 08:34:31,611 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,611 > [DEBUG] 0 :: param layer3.4.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,611 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -0.4574243724346161
2023-01-07 08:34:31,611 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,612 > [DEBUG] 0 :: param layer3.5.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9322 orig size : 262144
2023-01-07 08:34:31,613 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 11.267927169799805
2023-01-07 08:34:31,613 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,613 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,613 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 36.37263870239258
2023-01-07 08:34:31,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 4.599205493927002
2023-01-07 08:34:31,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26347166299819946
2023-01-07 08:34:31,614 > [DEBUG] 0 :: param layer3.5.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,614 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -0.15802019834518433
2023-01-07 08:34:31,614 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,615 > [DEBUG] 0 :: param layer3.5.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6449 orig size : 589824
2023-01-07 08:34:31,616 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -17.609909057617188
2023-01-07 08:34:31,616 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,616 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 1.565791130065918
2023-01-07 08:34:31,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,616 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -9.22635555267334
2023-01-07 08:34:31,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.22676868736743927
2023-01-07 08:34:31,617 > [DEBUG] 0 :: param layer3.5.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,617 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.10896271467208862
2023-01-07 08:34:31,617 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,618 > [DEBUG] 0 :: param layer3.5.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 8980 orig size : 262144
2023-01-07 08:34:31,619 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -8.52032470703125
2023-01-07 08:34:31,619 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,619 > [DEBUG] 0 :: param layer3.5.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,619 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.7810938954353333
2023-01-07 08:34:31,619 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,619 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,620 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,620 > [DEBUG] 0 :: param layer4.0.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9351 orig size : 524288
2023-01-07 08:34:31,621 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -26.0825252532959
2023-01-07 08:34:31,621 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,621 > [DEBUG] 0 :: param layer4.0.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,621 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -0.047200143337249756
2023-01-07 08:34:31,621 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,622 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,622 > [DEBUG] 0 :: param layer4.0.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6443 orig size : 2359296
2023-01-07 08:34:31,622 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -49.076236724853516
2023-01-07 08:34:31,623 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,623 > [DEBUG] 0 :: param layer4.0.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,623 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.1687455177307129
2023-01-07 08:34:31,623 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,625 > [DEBUG] 0 :: param layer4.0.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 43557 orig size : 1048576
2023-01-07 08:34:31,625 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -14.876680374145508
2023-01-07 08:34:31,625 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 26.726709365844727
2023-01-07 08:34:31,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,625 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 5.259452819824219
2023-01-07 08:34:31,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5945334434509277
2023-01-07 08:34:31,626 > [DEBUG] 0 :: param layer4.0.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,626 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -1.534977912902832
2023-01-07 08:34:31,626 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,627 > [DEBUG] 0 :: param layer4.0.downsample.0._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6806 orig size : 2097152
2023-01-07 08:34:31,627 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 6.251735687255859
2023-01-07 08:34:31,627 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,628 > [DEBUG] 0 :: param layer4.0.downsample.1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,628 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.06339414417743683
2023-01-07 08:34:31,628 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,629 > [DEBUG] 0 :: param layer4.1.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 23419 orig size : 1048576
2023-01-07 08:34:31,629 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 5.992007255554199
2023-01-07 08:34:31,630 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,630 > [DEBUG] 0 :: param layer4.1.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,630 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.14774611592292786
2023-01-07 08:34:31,630 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,631 > [DEBUG] 0 :: param layer4.1.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 9267 orig size : 2359296
2023-01-07 08:34:31,631 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -1.1830768585205078
2023-01-07 08:34:31,632 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,632 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 10.19667911529541
2023-01-07 08:34:31,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,632 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 1.1382921934127808
2023-01-07 08:34:31,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2266657054424286
2023-01-07 08:34:31,633 > [DEBUG] 0 :: param layer4.1.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,633 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.10670934617519379
2023-01-07 08:34:31,633 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,634 > [DEBUG] 0 :: param layer4.1.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33875 orig size : 1048576
2023-01-07 08:34:31,634 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -2.7011923789978027
2023-01-07 08:34:31,634 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,635 > [DEBUG] 0 :: param layer4.1.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,635 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -0.02194344997406006
2023-01-07 08:34:31,635 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,636 > [DEBUG] 0 :: param layer4.2.conv1._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6434 orig size : 1048576
2023-01-07 08:34:31,636 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.5440801382064819
2023-01-07 08:34:31,636 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,636 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,637 > [DEBUG] 0 :: param layer4.2.bn1._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,637 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.08900012820959091
2023-01-07 08:34:31,637 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,638 > [DEBUG] 0 :: param layer4.2.conv2._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6363 orig size : 2359296
2023-01-07 08:34:31,638 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 4.882701873779297
2023-01-07 08:34:31,638 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,639 > [DEBUG] 0 :: param layer4.2.bn2._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,639 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.08493626862764359
2023-01-07 08:34:31,639 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,639 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,640 > [DEBUG] 0 :: param layer4.2.conv3._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 33305 orig size : 1048576
2023-01-07 08:34:31,640 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -0.36782264709472656
2023-01-07 08:34:31,640 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,641 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.103188514709473
2023-01-07 08:34:31,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,641 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.008544921875
2023-01-07 08:34:31,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -332.2726745605469
2023-01-07 08:34:31,642 > [DEBUG] 0 :: param layer4.2.bn3._dp_wrapped_module.flat_param_0 is fully commnicated
2023-01-07 08:34:31,642 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.5684725046157837
2023-01-07 08:34:31,642 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 08:34:31,643 > [DEBUG] 0 :: param fc._dp_wrapped_module.flat_param_0 is not fully commnicated!!! communicated parameter : 6582 orig size : 2049000
2023-01-07 08:34:31,644 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -262.996826171875
2023-01-07 08:34:31,644 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 08:34:31,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 08:34:31,645 > [DEBUG] 0 :: 7.1959991455078125
2023-01-07 08:34:31,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,649 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.21346893906593323
2023-01-07 08:34:31,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21346893906593323
2023-01-07 08:34:31,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,654 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.03918786346912384
2023-01-07 08:34:31,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,655 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1.296055555343628
2023-01-07 08:34:31,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1393158435821533
2023-01-07 08:34:31,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,659 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.009320974349975586
2023-01-07 08:34:31,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.009320974349975586
2023-01-07 08:34:31,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,661 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.19422855973243713
2023-01-07 08:34:31,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19422855973243713
2023-01-07 08:34:31,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,663 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.06104674190282822
2023-01-07 08:34:31,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,663 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 4.875128746032715
2023-01-07 08:34:31,664 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.35781824588775635
2023-01-07 08:34:31,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.0422220304608345
2023-01-07 08:34:31,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0422220304608345
2023-01-07 08:34:31,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.10270204395055771
2023-01-07 08:34:31,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -6.393973350524902
2023-01-07 08:34:31,668 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4613461196422577
2023-01-07 08:34:31,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.039001449942588806
2023-01-07 08:34:31,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.039001449942588806
2023-01-07 08:34:31,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,672 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.04806680977344513
2023-01-07 08:34:31,672 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,672 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 1.3233306407928467
2023-01-07 08:34:31,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7502700686454773
2023-01-07 08:34:31,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.07862800359725952
2023-01-07 08:34:31,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07862800359725952
2023-01-07 08:34:31,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,676 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.23113614320755005
2023-01-07 08:34:31,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.23113614320755005
2023-01-07 08:34:31,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,679 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.0029866956174373627
2023-01-07 08:34:31,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,679 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -8.745075225830078
2023-01-07 08:34:31,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4780640602111816
2023-01-07 08:34:31,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,681 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.1304744929075241
2023-01-07 08:34:31,681 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1304744929075241
2023-01-07 08:34:31,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,683 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.03820104897022247
2023-01-07 08:34:31,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03820104897022247
2023-01-07 08:34:31,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,685 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.12859272956848145
2023-01-07 08:34:31,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,685 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 10.865318298339844
2023-01-07 08:34:31,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9881210327148438
2023-01-07 08:34:31,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,687 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.0745411366224289
2023-01-07 08:34:31,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0745411366224289
2023-01-07 08:34:31,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,689 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.26382187008857727
2023-01-07 08:34:31,690 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26382187008857727
2023-01-07 08:34:31,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,691 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.0015630722045898438
2023-01-07 08:34:31,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,692 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 8.102302551269531
2023-01-07 08:34:31,692 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5440163016319275
2023-01-07 08:34:31,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,694 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.0009139329195022583
2023-01-07 08:34:31,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0009139329195022583
2023-01-07 08:34:31,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,696 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.04720950499176979
2023-01-07 08:34:31,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04720950499176979
2023-01-07 08:34:31,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,698 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.11930105090141296
2023-01-07 08:34:31,698 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11930105090141296
2023-01-07 08:34:31,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,700 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.27150779962539673
2023-01-07 08:34:31,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27150779962539673
2023-01-07 08:34:31,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.08252805471420288
2023-01-07 08:34:31,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08252805471420288
2023-01-07 08:34:31,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,704 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.42601263523101807
2023-01-07 08:34:31,704 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42601263523101807
2023-01-07 08:34:31,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,706 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -1.5637283325195312
2023-01-07 08:34:31,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.424624443054199
2023-01-07 08:34:31,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.585198700428009
2023-01-07 08:34:31,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 18.897560119628906
2023-01-07 08:34:31,708 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.133800506591797
2023-01-07 08:34:31,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,710 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 20.100698471069336
2023-01-07 08:34:31,710 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.557503700256348
2023-01-07 08:34:31,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.2514652609825134
2023-01-07 08:34:31,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -8.217231750488281
2023-01-07 08:34:31,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.879554748535156
2023-01-07 08:34:31,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,715 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -12.272689819335938
2023-01-07 08:34:31,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7507203221321106
2023-01-07 08:34:31,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,718 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -7.621466159820557
2023-01-07 08:34:31,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.799383163452148
2023-01-07 08:34:31,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,720 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -90.96430969238281
2023-01-07 08:34:31,720 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.475104331970215
2023-01-07 08:34:31,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,723 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.82334303855896
2023-01-07 08:34:31,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.82334303855896
2023-01-07 08:34:31,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,725 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 117.60588073730469
2023-01-07 08:34:31,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.980649948120117
2023-01-07 08:34:31,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 22.283294677734375
2023-01-07 08:34:31,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.5941009521484375
2023-01-07 08:34:31,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 8.670110702514648
2023-01-07 08:34:31,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.390403747558594
2023-01-07 08:34:31,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -1.2275269031524658
2023-01-07 08:34:31,733 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2275269031524658
2023-01-07 08:34:31,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,736 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 95.24425506591797
2023-01-07 08:34:31,736 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.591033935546875
2023-01-07 08:34:31,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,738 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 28.091203689575195
2023-01-07 08:34:31,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.84026336669922
2023-01-07 08:34:31,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,740 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 0.5410323143005371
2023-01-07 08:34:31,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,740 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.64980149269104
2023-01-07 08:34:31,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10876882076263428
2023-01-07 08:34:31,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,742 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.1718921661376953
2023-01-07 08:34:31,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1718921661376953
2023-01-07 08:34:31,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,746 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -4.2949934005737305
2023-01-07 08:34:31,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,746 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 40.40473556518555
2023-01-07 08:34:31,746 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.153359413146973
2023-01-07 08:34:31,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,749 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -25.387983322143555
2023-01-07 08:34:31,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.850870132446289
2023-01-07 08:34:31,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,753 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 8.695283889770508
2023-01-07 08:34:31,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,753 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -14.025531768798828
2023-01-07 08:34:31,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.347223281860352
2023-01-07 08:34:31,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,755 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 132.2293243408203
2023-01-07 08:34:31,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.222579956054688
2023-01-07 08:34:31,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,757 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 54.33583068847656
2023-01-07 08:34:31,758 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.86395263671875
2023-01-07 08:34:31,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 08:34:31,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 08:34:31,759 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 1.5504255294799805
2023-01-07 08:34:31,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5504255294799805
