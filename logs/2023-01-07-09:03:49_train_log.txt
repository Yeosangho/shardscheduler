2023-01-07 09:03:56,341 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:03:56,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:56,378 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:56,378 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:56,378 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 09:03:56,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,071 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,071 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,071 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 09:03:57,071 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,073 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,073 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,073 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 09:03:57,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,074 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,074 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,074 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 09:03:57,074 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,075 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,075 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,075 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 09:03:57,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,111 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,111 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,111 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 09:03:57,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,112 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,112 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,113 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 09:03:57,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,114 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,114 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,114 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 09:03:57,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,115 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,115 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,115 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 09:03:57,115 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,115 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,116 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,116 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 09:03:57,116 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,116 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,116 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,117 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 09:03:57,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,117 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,117 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,117 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 09:03:57,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,118 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,118 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,118 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 09:03:57,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,119 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,119 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,119 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 09:03:57,119 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,120 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,120 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,120 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 09:03:57,120 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,121 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,121 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,121 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 09:03:57,121 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,122 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,122 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,122 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 09:03:57,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,123 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,123 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,123 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 09:03:57,123 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,124 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,124 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,124 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 09:03:57,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,125 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,125 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,125 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 09:03:57,125 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,126 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,126 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,126 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 09:03:57,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,127 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,127 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,127 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 09:03:57,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,128 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,128 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,128 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 09:03:57,128 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,128 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,129 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,129 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 09:03:57,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,130 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,130 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,130 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 09:03:57,130 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,131 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,131 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,131 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 09:03:57,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,132 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,132 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,132 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 09:03:57,132 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,132 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,132 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,133 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 09:03:57,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,133 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,133 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,133 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 09:03:57,134 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,135 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,135 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,135 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 09:03:57,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,136 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,136 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,136 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 09:03:57,136 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,136 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,137 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,137 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 09:03:57,137 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,137 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,137 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,138 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 09:03:57,138 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,139 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,139 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,139 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 09:03:57,139 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,139 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,139 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,140 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 09:03:57,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,140 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,140 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,140 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 09:03:57,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,141 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,141 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,141 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 09:03:57,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,142 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,142 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,142 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 09:03:57,142 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,143 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,143 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,143 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 09:03:57,143 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,144 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,144 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,144 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 09:03:57,144 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,145 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,145 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,145 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 09:03:57,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,146 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,146 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,146 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 09:03:57,146 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,147 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,147 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,147 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 09:03:57,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,148 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,148 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,148 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 09:03:57,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,149 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,149 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,149 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 09:03:57,149 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,150 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,150 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,150 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 09:03:57,150 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,151 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,151 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,151 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 09:03:57,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,151 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,152 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,152 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 09:03:57,152 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,153 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,153 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,153 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 09:03:57,153 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,153 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,154 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,154 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 09:03:57,154 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,154 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,155 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,155 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 09:03:57,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,155 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,156 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,156 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 09:03:57,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,157 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,157 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,157 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 09:03:57,157 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,157 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,158 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,158 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 09:03:57,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,158 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,158 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,158 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 09:03:57,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,159 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,159 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,159 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 09:03:57,159 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,160 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,160 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,160 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 09:03:57,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,161 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,161 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,161 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 09:03:57,161 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,162 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,162 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,162 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 09:03:57,162 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,163 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,163 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,163 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 09:03:57,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,164 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,164 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,164 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 09:03:57,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,165 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,165 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,165 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 09:03:57,165 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,166 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,166 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,166 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 09:03:57,166 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,167 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,167 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,167 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 09:03:57,167 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,168 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,168 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,168 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 09:03:57,168 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,169 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,169 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,169 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 09:03:57,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,169 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,170 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,170 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 09:03:57,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,170 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,170 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,170 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 09:03:57,170 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,171 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,171 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,171 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 09:03:57,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,172 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,172 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,172 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 09:03:57,172 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,173 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,173 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,173 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 09:03:57,173 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,174 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,174 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,174 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 09:03:57,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,175 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,175 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,175 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 09:03:57,175 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,176 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,176 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,176 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 09:03:57,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,177 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,177 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,177 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 09:03:57,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,178 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,178 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,178 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 09:03:57,178 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,179 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,179 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,179 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 09:03:57,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,179 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,179 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,179 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 09:03:57,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,180 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,180 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,180 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 09:03:57,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,181 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,181 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,181 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 09:03:57,181 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,182 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,182 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,182 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 09:03:57,182 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,183 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,183 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,183 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 09:03:57,183 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,184 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,184 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,184 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 09:03:57,184 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,185 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,185 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,185 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 09:03:57,185 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,186 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,186 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,186 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 09:03:57,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,186 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,186 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,187 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 09:03:57,187 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,188 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,188 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,188 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 09:03:57,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,188 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,188 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,188 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 09:03:57,189 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,189 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,189 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,189 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 09:03:57,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,190 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,190 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,190 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 09:03:57,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,191 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,191 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,191 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 09:03:57,191 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,192 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,192 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,192 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 09:03:57,192 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,193 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,193 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,193 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 09:03:57,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,194 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,194 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,194 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 09:03:57,194 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,195 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,195 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,195 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 09:03:57,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,196 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,196 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,196 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 09:03:57,196 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,197 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,197 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,197 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 09:03:57,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,198 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,198 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,198 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 09:03:57,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,199 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,199 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,199 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 09:03:57,199 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,200 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,200 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,200 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 09:03:57,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,200 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,201 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,201 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 09:03:57,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,201 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,201 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,201 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 09:03:57,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,202 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,202 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,202 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 09:03:57,202 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,203 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,203 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,203 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 09:03:57,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,204 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,204 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,204 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 09:03:57,204 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,205 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,205 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,205 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 09:03:57,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:57,206 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:57,206 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:03:57,206 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 09:03:57,207 > [DEBUG] 0 :: 7.271918773651123
2023-01-07 09:03:57,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,211 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,212 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00494384765625
2023-01-07 09:03:57,212 > [DEBUG] 0 :: before allreduce fusion buffer :: -362.6363220214844
2023-01-07 09:03:57,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,215 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.3321812152862549
2023-01-07 09:03:57,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,216 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -200.06951904296875
2023-01-07 09:03:57,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -359.5245666503906
2023-01-07 09:03:57,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,229 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,229 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 7.700539588928223
2023-01-07 09:03:57,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07276681810617447
2023-01-07 09:03:57,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,230 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,230 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.045686304569244385
2023-01-07 09:03:57,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,230 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -147.38514709472656
2023-01-07 09:03:57,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22086702287197113
2023-01-07 09:03:57,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,233 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,233 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 3.115769863128662
2023-01-07 09:03:57,233 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5707606673240662
2023-01-07 09:03:57,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,234 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,234 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.06256534904241562
2023-01-07 09:03:57,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,234 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -343.4295959472656
2023-01-07 09:03:57,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31578296422958374
2023-01-07 09:03:57,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,236 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,236 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.662905216217041
2023-01-07 09:03:57,236 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26591604948043823
2023-01-07 09:03:57,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,237 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.11949320137500763
2023-01-07 09:03:57,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,237 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -390.13140869140625
2023-01-07 09:03:57,237 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18167497217655182
2023-01-07 09:03:57,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,238 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,238 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2.6108756065368652
2023-01-07 09:03:57,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030969712883234024
2023-01-07 09:03:57,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,239 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,239 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.0867321789264679
2023-01-07 09:03:57,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,240 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -195.85340881347656
2023-01-07 09:03:57,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6029850244522095
2023-01-07 09:03:57,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,241 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,241 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 4.511486053466797
2023-01-07 09:03:57,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27986907958984375
2023-01-07 09:03:57,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,242 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,242 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.02662847191095352
2023-01-07 09:03:57,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,242 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -353.35479736328125
2023-01-07 09:03:57,242 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38916128873825073
2023-01-07 09:03:57,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,244 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,244 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 9.938959121704102
2023-01-07 09:03:57,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1208217516541481
2023-01-07 09:03:57,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,245 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,245 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.1670825183391571
2023-01-07 09:03:57,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,245 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -482.98876953125
2023-01-07 09:03:57,245 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7686563730239868
2023-01-07 09:03:57,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,247 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,247 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -15.240882873535156
2023-01-07 09:03:57,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7856565117835999
2023-01-07 09:03:57,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,248 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,248 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.14253383874893188
2023-01-07 09:03:57,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,248 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -520.9481201171875
2023-01-07 09:03:57,248 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7156568765640259
2023-01-07 09:03:57,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,249 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,249 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4.533536911010742
2023-01-07 09:03:57,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7484489679336548
2023-01-07 09:03:57,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,250 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,250 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.034228865057229996
2023-01-07 09:03:57,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,251 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -264.04644775390625
2023-01-07 09:03:57,251 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0303313732147217
2023-01-07 09:03:57,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,252 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,253 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 8.22718620300293
2023-01-07 09:03:57,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5711685419082642
2023-01-07 09:03:57,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,253 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,254 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.04079645872116089
2023-01-07 09:03:57,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,254 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -360.6214294433594
2023-01-07 09:03:57,254 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.47930109500885
2023-01-07 09:03:57,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,255 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,256 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.809592247009277
2023-01-07 09:03:57,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01848459243774414
2023-01-07 09:03:57,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,256 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,256 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.07061109691858292
2023-01-07 09:03:57,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,257 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -349.2743225097656
2023-01-07 09:03:57,257 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8336193561553955
2023-01-07 09:03:57,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,258 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -0.9478503465652466
2023-01-07 09:03:57,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5031716823577881
2023-01-07 09:03:57,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,259 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,259 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.032280221581459045
2023-01-07 09:03:57,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -255.66943359375
2023-01-07 09:03:57,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16027435660362244
2023-01-07 09:03:57,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,261 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 3.865402936935425
2023-01-07 09:03:57,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7113600969314575
2023-01-07 09:03:57,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,262 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 0.12426106631755829
2023-01-07 09:03:57,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -246.36158752441406
2023-01-07 09:03:57,263 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7280375361442566
2023-01-07 09:03:57,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,264 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -6.255719184875488
2023-01-07 09:03:57,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.022783100605010986
2023-01-07 09:03:57,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,265 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,265 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.06900322437286377
2023-01-07 09:03:57,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -361.3226623535156
2023-01-07 09:03:57,266 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2260841727256775
2023-01-07 09:03:57,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,267 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.477550506591797
2023-01-07 09:03:57,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02608778327703476
2023-01-07 09:03:57,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,268 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,268 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.036607563495635986
2023-01-07 09:03:57,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,268 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -257.83270263671875
2023-01-07 09:03:57,268 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.049390196800232
2023-01-07 09:03:57,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,270 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -12.24947738647461
2023-01-07 09:03:57,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2802605628967285
2023-01-07 09:03:57,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,271 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,271 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.14158792793750763
2023-01-07 09:03:57,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,271 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -257.9574279785156
2023-01-07 09:03:57,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4808578491210938
2023-01-07 09:03:57,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,272 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,272 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 5.3791303634643555
2023-01-07 09:03:57,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0502375364303589
2023-01-07 09:03:57,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,273 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.2848157584667206
2023-01-07 09:03:57,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -344.2310485839844
2023-01-07 09:03:57,274 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9341871738433838
2023-01-07 09:03:57,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,275 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,275 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -16.4060115814209
2023-01-07 09:03:57,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1829957515001297
2023-01-07 09:03:57,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,276 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,276 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.18864023685455322
2023-01-07 09:03:57,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,276 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -262.30596923828125
2023-01-07 09:03:57,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12828326225280762
2023-01-07 09:03:57,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,277 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -35.61121368408203
2023-01-07 09:03:57,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1971125602722168
2023-01-07 09:03:57,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,278 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,279 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.005960330367088318
2023-01-07 09:03:57,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,279 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -278.1466064453125
2023-01-07 09:03:57,279 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.815915107727051
2023-01-07 09:03:57,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,280 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,280 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -6.232307434082031
2023-01-07 09:03:57,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.855512857437134
2023-01-07 09:03:57,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,281 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.27504223585128784
2023-01-07 09:03:57,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -350.15911865234375
2023-01-07 09:03:57,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.8303210735321045
2023-01-07 09:03:57,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,283 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,283 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -5.370850086212158
2023-01-07 09:03:57,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0325762033462524
2023-01-07 09:03:57,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,284 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.1350707709789276
2023-01-07 09:03:57,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -229.3434600830078
2023-01-07 09:03:57,284 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6078552007675171
2023-01-07 09:03:57,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,285 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,285 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -52.478004455566406
2023-01-07 09:03:57,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6409733295440674
2023-01-07 09:03:57,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,286 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,286 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.30843403935432434
2023-01-07 09:03:57,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,287 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -260.5514831542969
2023-01-07 09:03:57,287 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.518444061279297
2023-01-07 09:03:57,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,288 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,288 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 9.638408660888672
2023-01-07 09:03:57,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3878365755081177
2023-01-07 09:03:57,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,289 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,289 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.4286588430404663
2023-01-07 09:03:57,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,289 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -293.2738342285156
2023-01-07 09:03:57,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.470228672027588
2023-01-07 09:03:57,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,291 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,291 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 7.916494369506836
2023-01-07 09:03:57,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4037399291992188
2023-01-07 09:03:57,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,292 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,292 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -0.025372624397277832
2023-01-07 09:03:57,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,292 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -165.60841369628906
2023-01-07 09:03:57,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.233478546142578
2023-01-07 09:03:57,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,294 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,294 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 43.377017974853516
2023-01-07 09:03:57,294 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.54461145401001
2023-01-07 09:03:57,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,295 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -125.18933868408203
2023-01-07 09:03:57,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1182475090026855
2023-01-07 09:03:57,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,296 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,296 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 54.17091751098633
2023-01-07 09:03:57,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4624240398406982
2023-01-07 09:03:57,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,297 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,297 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.44293755292892456
2023-01-07 09:03:57,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,297 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -198.42161560058594
2023-01-07 09:03:57,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.128805160522461
2023-01-07 09:03:57,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,299 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,299 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -6.791658401489258
2023-01-07 09:03:57,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9073485136032104
2023-01-07 09:03:57,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,300 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,300 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.8586989641189575
2023-01-07 09:03:57,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,300 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -107.39347076416016
2023-01-07 09:03:57,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.845447540283203
2023-01-07 09:03:57,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,302 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,302 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 10.995405197143555
2023-01-07 09:03:57,302 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09894271194934845
2023-01-07 09:03:57,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,303 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,303 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.3155372440814972
2023-01-07 09:03:57,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,303 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -111.56269836425781
2023-01-07 09:03:57,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4586997032165527
2023-01-07 09:03:57,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,305 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,305 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 82.96942901611328
2023-01-07 09:03:57,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.08026647567749
2023-01-07 09:03:57,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -34.40071487426758
2023-01-07 09:03:57,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.352020263671875
2023-01-07 09:03:57,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,307 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,307 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 5.4962663650512695
2023-01-07 09:03:57,307 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.501948356628418
2023-01-07 09:03:57,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,308 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -145.09095764160156
2023-01-07 09:03:57,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.135833740234375
2023-01-07 09:03:57,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,310 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 14.884422302246094
2023-01-07 09:03:57,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5203178524971008
2023-01-07 09:03:57,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -85.50321960449219
2023-01-07 09:03:57,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.741738319396973
2023-01-07 09:03:57,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,312 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,312 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -106.73635864257812
2023-01-07 09:03:57,312 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.85169792175293
2023-01-07 09:03:57,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -207.505859375
2023-01-07 09:03:57,313 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.95029067993164
2023-01-07 09:03:57,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,314 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -10.071788787841797
2023-01-07 09:03:57,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.324612617492676
2023-01-07 09:03:57,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,315 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -165.69676208496094
2023-01-07 09:03:57,316 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.640945434570312
2023-01-07 09:03:57,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,316 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,317 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 36.77994918823242
2023-01-07 09:03:57,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21609658002853394
2023-01-07 09:03:57,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,318 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -85.78438568115234
2023-01-07 09:03:57,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.159818649291992
2023-01-07 09:03:57,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,319 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,319 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -56.96835708618164
2023-01-07 09:03:57,319 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6481354236602783
2023-01-07 09:03:57,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,320 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -183.9002685546875
2023-01-07 09:03:57,320 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.551198959350586
2023-01-07 09:03:57,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,321 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,321 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 3.177095651626587
2023-01-07 09:03:57,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.502842426300049
2023-01-07 09:03:57,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,322 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.10694193840026855
2023-01-07 09:03:57,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,322 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -180.45697021484375
2023-01-07 09:03:57,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.666397571563721
2023-01-07 09:03:57,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,324 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 35.63512420654297
2023-01-07 09:03:57,324 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02619636058807373
2023-01-07 09:03:57,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -107.247802734375
2023-01-07 09:03:57,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.762214660644531
2023-01-07 09:03:57,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,326 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 76.32817840576172
2023-01-07 09:03:57,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.3277587890625
2023-01-07 09:03:57,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -67.5425796508789
2023-01-07 09:03:57,327 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.60124206542969
2023-01-07 09:03:57,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,328 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -52.99124526977539
2023-01-07 09:03:57,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.896399974822998
2023-01-07 09:03:57,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,329 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,329 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.5316014885902405
2023-01-07 09:03:57,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,329 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -213.30267333984375
2023-01-07 09:03:57,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.25247573852539
2023-01-07 09:03:57,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,331 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,331 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -40.55634307861328
2023-01-07 09:03:57,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4310688972473145
2023-01-07 09:03:57,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,332 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -106.23616790771484
2023-01-07 09:03:57,332 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8754987716674805
2023-01-07 09:03:57,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,333 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,333 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -47.845420837402344
2023-01-07 09:03:57,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9198293685913086
2023-01-07 09:03:57,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,334 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -176.599365234375
2023-01-07 09:03:57,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3545169830322266
2023-01-07 09:03:57,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,336 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,336 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 143.25733947753906
2023-01-07 09:03:57,336 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.796260833740234
2023-01-07 09:03:57,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,337 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.2019251585006714
2023-01-07 09:03:57,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,337 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.96049690246582
2023-01-07 09:03:57,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.198436737060547
2023-01-07 09:03:57,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,339 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -631.0013427734375
2023-01-07 09:03:57,339 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.37165069580078
2023-01-07 09:03:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,340 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -1.3648074865341187
2023-01-07 09:03:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,340 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 34.840309143066406
2023-01-07 09:03:57,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,340 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -846.20654296875
2023-01-07 09:03:57,341 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.971768379211426
2023-01-07 09:03:57,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,342 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -943.25927734375
2023-01-07 09:03:57,342 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.591583251953125
2023-01-07 09:03:57,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,343 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -709.0027465820312
2023-01-07 09:03:57,343 > [DEBUG] 0 :: before allreduce fusion buffer :: 33.49241256713867
2023-01-07 09:03:57,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,345 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,345 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -21.809267044067383
2023-01-07 09:03:57,345 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.857913970947266
2023-01-07 09:03:57,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,346 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.9107130765914917
2023-01-07 09:03:57,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,346 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -149.46490478515625
2023-01-07 09:03:57,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 38.48063278198242
2023-01-07 09:03:57,348 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,348 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,348 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -615.9933471679688
2023-01-07 09:03:57,348 > [DEBUG] 0 :: before allreduce fusion buffer :: 101.46855163574219
2023-01-07 09:03:57,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,349 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,349 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -1.113407015800476
2023-01-07 09:03:57,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,349 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,349 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 40.302188873291016
2023-01-07 09:03:57,349 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.99726104736328
2023-01-07 09:03:57,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,351 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -825.168212890625
2023-01-07 09:03:57,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.73551368713379
2023-01-07 09:03:57,351 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,352 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,352 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -14.048347473144531
2023-01-07 09:03:57,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,352 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1071.1910400390625
2023-01-07 09:03:57,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.539506912231445
2023-01-07 09:03:57,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -856.8954467773438
2023-01-07 09:03:57,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7377805709838867
2023-01-07 09:03:57,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,355 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,355 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -157.53387451171875
2023-01-07 09:03:57,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.337120056152344
2023-01-07 09:03:57,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,357 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -848.7294311523438
2023-01-07 09:03:57,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.97548294067383
2023-01-07 09:03:57,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,357 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,358 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 6.322926044464111
2023-01-07 09:03:57,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,358 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,358 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 21.992481231689453
2023-01-07 09:03:57,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,358 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -885.9097290039062
2023-01-07 09:03:57,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,358 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1282.7716064453125
2023-01-07 09:03:57,359 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.038177490234375
2023-01-07 09:03:57,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1029.3807373046875
2023-01-07 09:03:57,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.98640441894531
2023-01-07 09:03:57,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,361 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,361 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -50.52301788330078
2023-01-07 09:03:57,361 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.653533935546875
2023-01-07 09:03:57,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,362 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -964.505859375
2023-01-07 09:03:57,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.749692440032959
2023-01-07 09:03:57,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -706.79248046875
2023-01-07 09:03:57,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,363 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1302.0966796875
2023-01-07 09:03:57,363 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.32202911376953
2023-01-07 09:03:57,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,365 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,365 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 575.3872680664062
2023-01-07 09:03:57,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.494102478027344
2023-01-07 09:03:57,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,366 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -1.811539649963379
2023-01-07 09:03:57,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 519.3638916015625
2023-01-07 09:03:57,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 58.308998107910156
2023-01-07 09:03:57,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,368 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 159.2904052734375
2023-01-07 09:03:57,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.37548065185547
2023-01-07 09:03:57,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,369 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 169.38381958007812
2023-01-07 09:03:57,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.84748840332031
2023-01-07 09:03:57,373 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:03:57,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,373 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:57,373 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -225.73004150390625
2023-01-07 09:03:57,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,374 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 213.6671142578125
2023-01-07 09:03:57,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,374 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1324.8050537109375
2023-01-07 09:03:57,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,374 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -864.4030151367188
2023-01-07 09:03:57,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,374 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -598.3057861328125
2023-01-07 09:03:57,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -773.678466796875
2023-01-07 09:03:57,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -819.1748046875
2023-01-07 09:03:57,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -683.208984375
2023-01-07 09:03:57,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,375 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -578.6229858398438
2023-01-07 09:03:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -904.375244140625
2023-01-07 09:03:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -651.90771484375
2023-01-07 09:03:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -652.4856567382812
2023-01-07 09:03:57,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,376 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -879.3323364257812
2023-01-07 09:03:57,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,377 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -608.3359375
2023-01-07 09:03:57,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -602.6270141601562
2023-01-07 09:03:57,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,377 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -695.4144897460938
2023-01-07 09:03:57,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -943.0458374023438
2023-01-07 09:03:57,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -684.9143676757812
2023-01-07 09:03:57,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -927.632568359375
2023-01-07 09:03:57,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,378 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -799.9171142578125
2023-01-07 09:03:57,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -956.827880859375
2023-01-07 09:03:57,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -922.5643310546875
2023-01-07 09:03:57,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -944.9869384765625
2023-01-07 09:03:57,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,379 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1037.643310546875
2023-01-07 09:03:57,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1002.4468994140625
2023-01-07 09:03:57,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,380 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1019.6417236328125
2023-01-07 09:03:57,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -260.13720703125
2023-01-07 09:03:57,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1091.18701171875
2023-01-07 09:03:57,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1002.7005004882812
2023-01-07 09:03:57,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1003.9202880859375
2023-01-07 09:03:57,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,382 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1089.65771484375
2023-01-07 09:03:57,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1024.460205078125
2023-01-07 09:03:57,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -994.5306396484375
2023-01-07 09:03:57,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,383 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1086.2391357421875
2023-01-07 09:03:57,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,383 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1011.4735107421875
2023-01-07 09:03:57,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1119.36279296875
2023-01-07 09:03:57,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.927162170410156
2023-01-07 09:03:57,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -821.76611328125
2023-01-07 09:03:57,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1050.0711669921875
2023-01-07 09:03:57,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1011.6022338867188
2023-01-07 09:03:57,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1075.3455810546875
2023-01-07 09:03:57,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3661060333251953
2023-01-07 09:03:57,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -760.677978515625
2023-01-07 09:03:57,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,387 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -919.188720703125
2023-01-07 09:03:57,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,387 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1010.1158447265625
2023-01-07 09:03:57,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0738394260406494
2023-01-07 09:03:57,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -978.2213745117188
2023-01-07 09:03:57,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:57,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:57,388 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -369.32330322265625
2023-01-07 09:03:57,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 726.7819213867188
2023-01-07 09:03:58,239 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -630.631591796875 param sum :: 116.04783630371094
2023-01-07 09:03:58,239 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:03:58,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,239 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,240 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 3.3282527923583984
2023-01-07 09:03:58,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,240 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 261.2503967285156
2023-01-07 09:03:58,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 275.01715087890625
2023-01-07 09:03:58,241 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 4.101492404937744 param sum :: 63.599998474121094
2023-01-07 09:03:58,241 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -6.193335056304932
2023-01-07 09:03:58,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,241 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,242 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -44.8621826171875
2023-01-07 09:03:58,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,242 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 296.88897705078125
2023-01-07 09:03:58,242 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.798179626464844
2023-01-07 09:03:58,243 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 34.01851272583008 param sum :: -2.1933627128601074
2023-01-07 09:03:58,243 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,243 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,243 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 16.710491180419922
2023-01-07 09:03:58,243 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,243 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,244 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 308.2668762207031
2023-01-07 09:03:58,244 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.599220275878906
2023-01-07 09:03:58,245 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 14.91525936126709 param sum :: 62.20000076293945
2023-01-07 09:03:58,245 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,245 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 16.710491180419922
2023-01-07 09:03:58,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,245 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 226.06405639648438
2023-01-07 09:03:58,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 293.0078125
2023-01-07 09:03:58,246 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 249.16064453125 param sum :: 50.308475494384766
2023-01-07 09:03:58,246 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,246 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:03:58,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,246 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,246 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 1.7052104473114014
2023-01-07 09:03:58,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,247 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,247 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 106.04583740234375
2023-01-07 09:03:58,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,247 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -961.1448974609375
2023-01-07 09:03:58,247 > [DEBUG] 0 :: before allreduce fusion buffer :: 125.38177490234375
2023-01-07 09:03:58,248 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 2.456456184387207 param sum :: 63.59852600097656
2023-01-07 09:03:58,249 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -11.192665100097656
2023-01-07 09:03:58,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,249 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -905.505126953125
2023-01-07 09:03:58,249 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.07856369018555
2023-01-07 09:03:58,250 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 62.59648132324219 param sum :: -8.427115440368652
2023-01-07 09:03:58,250 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:03:58,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,250 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,250 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 2.8424482345581055
2023-01-07 09:03:58,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,250 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -535.5421752929688
2023-01-07 09:03:58,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.29465866088867
2023-01-07 09:03:58,252 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 2.5552496910095215 param sum :: 254.4000244140625
2023-01-07 09:03:58,252 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.8743114471435547
2023-01-07 09:03:58,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,252 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -392.73638916015625
2023-01-07 09:03:58,252 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.816688537597656
2023-01-07 09:03:58,253 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -1284.009033203125 param sum :: 175.6714324951172
2023-01-07 09:03:58,253 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,253 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,253 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.8743114471435547
2023-01-07 09:03:58,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,253 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -490.727783203125
2023-01-07 09:03:58,253 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.964683532714844
2023-01-07 09:03:58,254 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93.92008972167969 param sum :: 246.20004272460938
2023-01-07 09:03:58,254 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.8743114471435547
2023-01-07 09:03:58,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,255 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -453.88067626953125
2023-01-07 09:03:58,255 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.29877471923828
2023-01-07 09:03:58,256 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -537.109619140625 param sum :: 67.33262634277344
2023-01-07 09:03:58,256 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,256 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,256 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:03:58,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,256 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,256 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 4.725353240966797
2023-01-07 09:03:58,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,256 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -496.4700622558594
2023-01-07 09:03:58,257 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.985984802246094
2023-01-07 09:03:58,258 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 5.197014331817627 param sum :: 61.80000305175781
2023-01-07 09:03:58,258 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,258 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,258 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 2.8743114471435547
2023-01-07 09:03:58,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,258 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -690.5780639648438
2023-01-07 09:03:58,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,259 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1509.7071533203125
2023-01-07 09:03:58,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.965469360351562
2023-01-07 09:03:58,260 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -751.6302490234375 param sum :: 162.29132080078125
2023-01-07 09:03:58,260 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:03:58,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,260 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.5779800415039062
2023-01-07 09:03:58,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,260 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -377.043701171875
2023-01-07 09:03:58,260 > [DEBUG] 0 :: before allreduce fusion buffer :: -61.82715606689453
2023-01-07 09:03:58,262 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.4364604949951172 param sum :: 63.59999084472656
2023-01-07 09:03:58,262 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -11.192665100097656
2023-01-07 09:03:58,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,262 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -865.3140869140625
2023-01-07 09:03:58,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.02015686035156
2023-01-07 09:03:58,263 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -351.766845703125 param sum :: 66.93730163574219
2023-01-07 09:03:58,263 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,263 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,263 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 24.887630462646484
2023-01-07 09:03:58,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,263 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -670.2264404296875
2023-01-07 09:03:58,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.258270263671875
2023-01-07 09:03:58,264 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 21.629549026489258 param sum :: 253.19998168945312
2023-01-07 09:03:58,265 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,265 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 24.887630462646484
2023-01-07 09:03:58,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -694.3473510742188
2023-01-07 09:03:58,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,265 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -822.850830078125
2023-01-07 09:03:58,265 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.257827758789062
2023-01-07 09:03:58,266 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -679.478515625 param sum :: 221.4887237548828
2023-01-07 09:03:58,266 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,266 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -11.192665100097656
2023-01-07 09:03:58,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,267 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -820.0057983398438
2023-01-07 09:03:58,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -144.90185546875
2023-01-07 09:03:58,268 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -0.5438623428344727 param sum :: 64.40001678466797
2023-01-07 09:03:58,268 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,268 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,268 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: -11.192665100097656
2023-01-07 09:03:58,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,268 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -881.2047119140625
2023-01-07 09:03:58,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.733409881591797
2023-01-07 09:03:58,269 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -823.27587890625 param sum :: 153.61016845703125
2023-01-07 09:03:58,270 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:03:58,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,270 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,270 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.1428215503692627
2023-01-07 09:03:58,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,270 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,270 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 19.460865020751953
2023-01-07 09:03:58,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,270 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1534.916748046875
2023-01-07 09:03:58,270 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.163688659667969
2023-01-07 09:03:58,272 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -0.4267460107803345 param sum :: 64.60002899169922
2023-01-07 09:03:58,272 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.1209845542907715
2023-01-07 09:03:58,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,272 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1510.388916015625
2023-01-07 09:03:58,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.547821044921875
2023-01-07 09:03:58,273 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 19.63054084777832 param sum :: -32.00938415527344
2023-01-07 09:03:58,273 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,273 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 52.224029541015625
2023-01-07 09:03:58,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,274 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1522.43701171875
2023-01-07 09:03:58,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.480417728424072
2023-01-07 09:03:58,275 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -46.48298645019531 param sum :: 265.00006103515625
2023-01-07 09:03:58,275 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.1209845542907715
2023-01-07 09:03:58,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1522.8226318359375
2023-01-07 09:03:58,275 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.52345657348633
2023-01-07 09:03:58,276 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -1541.9580078125 param sum :: 565.6168212890625
2023-01-07 09:03:58,276 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.1209845542907715
2023-01-07 09:03:58,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1506.9466552734375
2023-01-07 09:03:58,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.5165958404541
2023-01-07 09:03:58,278 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -15.652889251708984 param sum :: 131.79995727539062
2023-01-07 09:03:58,278 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -7.1209845542907715
2023-01-07 09:03:58,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1481.9697265625
2023-01-07 09:03:58,278 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.41394305229187
2023-01-07 09:03:58,279 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -1478.0606689453125 param sum :: 554.7550659179688
2023-01-07 09:03:58,279 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,279 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,279 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,279 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 2.356146812438965
2023-01-07 09:03:58,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,280 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -819.049072265625
2023-01-07 09:03:58,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.144333839416504
2023-01-07 09:03:58,281 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 2.729888439178467 param sum :: 126.40007019042969
2023-01-07 09:03:58,281 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: -5.778161525726318
2023-01-07 09:03:58,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,281 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -803.660400390625
2023-01-07 09:03:58,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1510988473892212
2023-01-07 09:03:58,283 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -792.091552734375 param sum :: 299.9128112792969
2023-01-07 09:03:58,283 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:03:58,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,283 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -0.762007474899292
2023-01-07 09:03:58,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -536.4735107421875
2023-01-07 09:03:58,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.711694717407227
2023-01-07 09:03:58,284 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -0.40050792694091797 param sum :: 512.9993286132812
2023-01-07 09:03:58,284 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -11.524243354797363
2023-01-07 09:03:58,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -531.34814453125
2023-01-07 09:03:58,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8930606842041016
2023-01-07 09:03:58,286 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -520.9717407226562 param sum :: 171.76171875
2023-01-07 09:03:58,286 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -5.494458198547363
2023-01-07 09:03:58,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -749.751220703125
2023-01-07 09:03:58,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.602304458618164
2023-01-07 09:03:58,287 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -104.33479309082031 param sum :: 549.0001831054688
2023-01-07 09:03:58,287 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: -5.494458198547363
2023-01-07 09:03:58,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -742.762451171875
2023-01-07 09:03:58,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.224701404571533
2023-01-07 09:03:58,289 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -751.9654541015625 param sum :: 276.66925048828125
2023-01-07 09:03:58,289 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,289 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -0.2604999542236328
2023-01-07 09:03:58,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -831.2131958007812
2023-01-07 09:03:58,289 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.19920539855957
2023-01-07 09:03:58,291 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.10657191276550293 param sum :: 127.39991760253906
2023-01-07 09:03:58,291 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,291 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,291 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: -1.5839903354644775
2023-01-07 09:03:58,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,291 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -749.7420043945312
2023-01-07 09:03:58,291 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.074582576751709
2023-01-07 09:03:58,292 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -740.1195068359375 param sum :: 393.1651611328125
2023-01-07 09:03:58,293 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,293 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -0.4428354501724243
2023-01-07 09:03:58,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -709.9234619140625
2023-01-07 09:03:58,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.147735595703125
2023-01-07 09:03:58,294 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.16129672527313232 param sum :: 126.99992370605469
2023-01-07 09:03:58,294 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,294 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,294 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 22.040136337280273
2023-01-07 09:03:58,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -716.9139404296875
2023-01-07 09:03:58,295 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6715344190597534
2023-01-07 09:03:58,296 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -717.7855224609375 param sum :: 341.9408264160156
2023-01-07 09:03:58,296 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -4.062044143676758
2023-01-07 09:03:58,296 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,296 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,296 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -580.9373779296875
2023-01-07 09:03:58,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6233659982681274
2023-01-07 09:03:58,297 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -116.56083679199219 param sum :: 562.399169921875
2023-01-07 09:03:58,297 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,297 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,297 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -4.062044143676758
2023-01-07 09:03:58,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -572.60205078125
2023-01-07 09:03:58,298 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.351428985595703
2023-01-07 09:03:58,299 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -567.3114013671875 param sum :: 273.0256042480469
2023-01-07 09:03:58,299 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,299 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,299 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.06519818305969238
2023-01-07 09:03:58,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,299 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -904.0669555664062
2023-01-07 09:03:58,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5520100593566895
2023-01-07 09:03:58,301 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -0.45991891622543335 param sum :: 128.79994201660156
2023-01-07 09:03:58,301 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -0.13611793518066406
2023-01-07 09:03:58,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,301 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -905.2996826171875
2023-01-07 09:03:58,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.447834014892578
2023-01-07 09:03:58,302 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -863.947265625 param sum :: 479.416259765625
2023-01-07 09:03:58,302 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,303 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.587726354598999
2023-01-07 09:03:58,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,303 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -652.8848876953125
2023-01-07 09:03:58,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.735088348388672
2023-01-07 09:03:58,304 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -0.8776248693466187 param sum :: 128.9998321533203
2023-01-07 09:03:58,304 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: -26.43994903564453
2023-01-07 09:03:58,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,304 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -655.7330322265625
2023-01-07 09:03:58,305 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.384754180908203
2023-01-07 09:03:58,306 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -654.4406127929688 param sum :: 302.1866149902344
2023-01-07 09:03:58,306 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,306 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,306 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:03:58,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,306 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.8709179162979126
2023-01-07 09:03:58,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,306 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -653.5233154296875
2023-01-07 09:03:58,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1251895427703857
2023-01-07 09:03:58,307 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -1.1557279825210571 param sum :: 512.6011962890625
2023-01-07 09:03:58,307 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,308 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 36.40653991699219
2023-01-07 09:03:58,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,308 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -664.8770141601562
2023-01-07 09:03:58,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7782286405563354
2023-01-07 09:03:58,309 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -660.7296142578125 param sum :: 444.2287902832031
2023-01-07 09:03:58,309 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,309 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,309 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,309 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.5806365609169006
2023-01-07 09:03:58,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -880.1107788085938
2023-01-07 09:03:58,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.07137107849121
2023-01-07 09:03:58,311 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.3140973448753357 param sum :: 127.4000473022461
2023-01-07 09:03:58,311 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,311 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: -45.590980529785156
2023-01-07 09:03:58,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,311 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -904.2771606445312
2023-01-07 09:03:58,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.370929718017578
2023-01-07 09:03:58,312 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -882.0592651367188 param sum :: 649.9478149414062
2023-01-07 09:03:58,312 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,312 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:03:58,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,313 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.008348584175109863
2023-01-07 09:03:58,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,313 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -609.4710693359375
2023-01-07 09:03:58,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.8437910079956055
2023-01-07 09:03:58,314 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.4524657130241394 param sum :: 127.59977722167969
2023-01-07 09:03:58,314 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,314 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 18.42824363708496
2023-01-07 09:03:58,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -601.6376953125
2023-01-07 09:03:58,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.221252918243408
2023-01-07 09:03:58,316 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -602.873046875 param sum :: 432.947998046875
2023-01-07 09:03:58,316 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:03:58,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,316 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,316 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.2181980013847351
2023-01-07 09:03:58,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,316 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -613.4742431640625
2023-01-07 09:03:58,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.861834526062012
2023-01-07 09:03:58,317 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.5672047138214111 param sum :: 512.4271850585938
2023-01-07 09:03:58,317 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,317 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -5.295326232910156
2023-01-07 09:03:58,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,318 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -613.3843383789062
2023-01-07 09:03:58,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.83251953125
2023-01-07 09:03:58,319 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -610.5248413085938 param sum :: 509.96575927734375
2023-01-07 09:03:58,319 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:03:58,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,319 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,319 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.41866445541381836
2023-01-07 09:03:58,320 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,320 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,320 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1078.798583984375
2023-01-07 09:03:58,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.438637733459473
2023-01-07 09:03:58,321 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -0.8241823315620422 param sum :: 256.9985656738281
2023-01-07 09:03:58,321 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -16.095369338989258
2023-01-07 09:03:58,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,321 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1053.9501953125
2023-01-07 09:03:58,322 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.769700050354004
2023-01-07 09:03:58,323 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1036.008544921875 param sum :: -23646.03125
2023-01-07 09:03:58,323 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,323 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 7.22868537902832
2023-01-07 09:03:58,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,323 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -836.5494995117188
2023-01-07 09:03:58,323 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9042699337005615
2023-01-07 09:03:58,324 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: -45.39659881591797 param sum :: 290.5971374511719
2023-01-07 09:03:58,324 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 7.22868537902832
2023-01-07 09:03:58,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,325 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -837.1821899414062
2023-01-07 09:03:58,325 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.232179880142212
2023-01-07 09:03:58,326 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -839.8264770507812 param sum :: 361.54571533203125
2023-01-07 09:03:58,326 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 50.45468521118164
2023-01-07 09:03:58,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,326 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -251.1952667236328
2023-01-07 09:03:58,326 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.870326280593872
2023-01-07 09:03:58,327 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -161.3460693359375 param sum :: 1152.399658203125
2023-01-07 09:03:58,328 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,328 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,328 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 50.45468521118164
2023-01-07 09:03:58,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -251.28665161132812
2023-01-07 09:03:58,328 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25592169165611267
2023-01-07 09:03:58,329 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -253.601806640625 param sum :: -31671.548828125
2023-01-07 09:03:58,329 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 35.1424560546875
2023-01-07 09:03:58,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,329 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -724.9796142578125
2023-01-07 09:03:58,330 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.675256609916687
2023-01-07 09:03:58,330 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -160.29742431640625 param sum :: 1154.7998046875
2023-01-07 09:03:58,331 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,331 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,331 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 35.1424560546875
2023-01-07 09:03:58,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -725.7230224609375
2023-01-07 09:03:58,331 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.562763214111328
2023-01-07 09:03:58,332 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -726.3134765625 param sum :: -22759.0
2023-01-07 09:03:58,332 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,332 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:03:58,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,333 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:03:58,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.23062759637832642
2023-01-07 09:03:58,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -258.01519775390625
2023-01-07 09:03:58,333 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.597952842712402
2023-01-07 09:03:58,334 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.05675768852233887 param sum :: 256.4000244140625
2023-01-07 09:03:58,334 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,334 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,334 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: 3.9324073791503906
2023-01-07 09:03:58,334 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,334 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,334 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -267.22796630859375
2023-01-07 09:03:58,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.165316343307495
2023-01-07 09:03:58,335 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -267.25054931640625 param sum :: -55933.8359375
2023-01-07 09:03:58,336 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,336 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.925693511962891
2023-01-07 09:03:58,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -836.9380493164062
2023-01-07 09:03:58,336 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5566898584365845
2023-01-07 09:03:58,337 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -46.24199295043945 param sum :: 295.9986267089844
2023-01-07 09:03:58,337 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,337 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,337 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 5.925693511962891
2023-01-07 09:03:58,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -841.2039794921875
2023-01-07 09:03:58,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6377068758010864
2023-01-07 09:03:58,339 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -842.7817993164062 param sum :: -11538.310546875
2023-01-07 09:03:58,339 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,339 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,339 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 55.254329681396484
2023-01-07 09:03:58,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,339 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -940.8024291992188
2023-01-07 09:03:58,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0912799835205078
2023-01-07 09:03:58,340 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: -176.98944091796875 param sum :: 1189.2000732421875
2023-01-07 09:03:58,340 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 55.254329681396484
2023-01-07 09:03:58,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,341 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -940.1917114257812
2023-01-07 09:03:58,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.7064919471740723
2023-01-07 09:03:58,342 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -935.8928833007812 param sum :: 951.9287109375
2023-01-07 09:03:58,342 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,342 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,342 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -15.85328483581543
2023-01-07 09:03:58,342 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,342 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,342 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -874.2279663085938
2023-01-07 09:03:58,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.029350757598877
2023-01-07 09:03:58,343 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: -45.25498580932617 param sum :: 300.0376892089844
2023-01-07 09:03:58,344 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: -15.85328483581543
2023-01-07 09:03:58,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,344 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -866.0518798828125
2023-01-07 09:03:58,344 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4573156833648682
2023-01-07 09:03:58,345 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: -863.5779418945312 param sum :: 584.7568359375
2023-01-07 09:03:58,345 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,345 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,345 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2.711937427520752
2023-01-07 09:03:58,345 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,345 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,346 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1029.1160888671875
2023-01-07 09:03:58,346 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0090250968933105
2023-01-07 09:03:58,347 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: -45.35740661621094 param sum :: 301.199951171875
2023-01-07 09:03:58,347 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2.711937427520752
2023-01-07 09:03:58,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,347 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1032.241943359375
2023-01-07 09:03:58,347 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.651742696762085
2023-01-07 09:03:58,348 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -1034.484619140625 param sum :: 1024.7777099609375
2023-01-07 09:03:58,349 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 147.57513427734375
2023-01-07 09:03:58,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,349 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -997.1832885742188
2023-01-07 09:03:58,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7441822290420532
2023-01-07 09:03:58,350 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -181.6747283935547 param sum :: 1203.001953125
2023-01-07 09:03:58,350 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 147.57513427734375
2023-01-07 09:03:58,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,350 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,350 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1000.5280151367188
2023-01-07 09:03:58,351 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.4200143814086914
2023-01-07 09:03:58,352 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: -999.9580688476562 param sum :: 1190.582275390625
2023-01-07 09:03:58,352 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.881549835205078
2023-01-07 09:03:58,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,352 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2381.99609375
2023-01-07 09:03:58,352 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.672579288482666
2023-01-07 09:03:58,353 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -45.95441436767578 param sum :: 301.60003662109375
2023-01-07 09:03:58,353 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,353 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.881549835205078
2023-01-07 09:03:58,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,354 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -2381.549072265625
2023-01-07 09:03:58,354 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1954774856567383
2023-01-07 09:03:58,355 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -2379.78076171875 param sum :: 1417.218994140625
2023-01-07 09:03:58,355 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,355 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,355 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 34.930580139160156
2023-01-07 09:03:58,355 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,355 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,355 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1077.6209716796875
2023-01-07 09:03:58,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2119207382202148
2023-01-07 09:03:58,357 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -46.14988708496094 param sum :: 302.7981872558594
2023-01-07 09:03:58,357 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,357 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 34.930580139160156
2023-01-07 09:03:58,357 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,357 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,357 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1079.0574951171875
2023-01-07 09:03:58,357 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27556997537612915
2023-01-07 09:03:58,358 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -1081.27880859375 param sum :: 1195.000732421875
2023-01-07 09:03:58,358 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,358 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -34.36042785644531
2023-01-07 09:03:58,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,359 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1048.465087890625
2023-01-07 09:03:58,359 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08420620858669281
2023-01-07 09:03:58,360 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -186.89886474609375 param sum :: 1213.3975830078125
2023-01-07 09:03:58,360 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,360 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,360 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: -34.36042785644531
2023-01-07 09:03:58,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,360 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1043.6939697265625
2023-01-07 09:03:58,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09846784919500351
2023-01-07 09:03:58,362 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -1046.595947265625 param sum :: 1016.7935180664062
2023-01-07 09:03:58,362 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,362 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 13.596656799316406
2023-01-07 09:03:58,362 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,362 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,362 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1222.38232421875
2023-01-07 09:03:58,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.279064655303955
2023-01-07 09:03:58,363 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: -47.1038932800293 param sum :: 304.1996765136719
2023-01-07 09:03:58,363 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,363 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 13.596656799316406
2023-01-07 09:03:58,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,363 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1221.5604248046875
2023-01-07 09:03:58,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6137047410011292
2023-01-07 09:03:58,365 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1222.7244873046875 param sum :: 1177.0242919921875
2023-01-07 09:03:58,365 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,365 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,365 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 21.069107055664062
2023-01-07 09:03:58,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,365 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2207.81201171875
2023-01-07 09:03:58,365 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9198050498962402
2023-01-07 09:03:58,366 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -46.62939453125 param sum :: 304.5998840332031
2023-01-07 09:03:58,366 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 21.069107055664062
2023-01-07 09:03:58,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,367 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2210.992919921875
2023-01-07 09:03:58,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01428365707397461
2023-01-07 09:03:58,368 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2211.271240234375 param sum :: -1839.455322265625
2023-01-07 09:03:58,368 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -103.85577392578125
2023-01-07 09:03:58,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,368 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -438.6326904296875
2023-01-07 09:03:58,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.906592845916748
2023-01-07 09:03:58,369 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -186.74005126953125 param sum :: 1216.599365234375
2023-01-07 09:03:58,369 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,369 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,370 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -103.85577392578125
2023-01-07 09:03:58,370 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,370 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,370 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -436.0774841308594
2023-01-07 09:03:58,370 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22548705339431763
2023-01-07 09:03:58,371 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -438.9931640625 param sum :: -20248.6640625
2023-01-07 09:03:58,371 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,371 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -42.13187026977539
2023-01-07 09:03:58,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,372 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -450.118896484375
2023-01-07 09:03:58,372 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7840879559516907
2023-01-07 09:03:58,373 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -46.419166564941406 param sum :: 305.9996643066406
2023-01-07 09:03:58,373 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,373 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,373 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -42.13187026977539
2023-01-07 09:03:58,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,373 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -451.4798583984375
2023-01-07 09:03:58,373 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23783811926841736
2023-01-07 09:03:58,374 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -449.693603515625 param sum :: -7987.0791015625
2023-01-07 09:03:58,374 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 6.713089466094971
2023-01-07 09:03:58,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,375 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,375 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -584.16064453125
2023-01-07 09:03:58,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2612623870372772
2023-01-07 09:03:58,376 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -46.344459533691406 param sum :: 305.599853515625
2023-01-07 09:03:58,376 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,376 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,376 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: 6.713089466094971
2023-01-07 09:03:58,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,376 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -585.5985107421875
2023-01-07 09:03:58,376 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18762262165546417
2023-01-07 09:03:58,378 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -585.0994873046875 param sum :: -22529.15234375
2023-01-07 09:03:58,378 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,378 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,378 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 33.879722595214844
2023-01-07 09:03:58,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,378 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 17.232666015625
2023-01-07 09:03:58,378 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9363622665405273
2023-01-07 09:03:58,379 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: -184.62261962890625 param sum :: 1221.3975830078125
2023-01-07 09:03:58,379 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 33.879722595214844
2023-01-07 09:03:58,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,380 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 18.23394775390625
2023-01-07 09:03:58,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6038261651992798
2023-01-07 09:03:58,381 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 20.30096435546875 param sum :: -49372.80078125
2023-01-07 09:03:58,381 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 42.095611572265625
2023-01-07 09:03:58,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,381 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3008.46484375
2023-01-07 09:03:58,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9036376476287842
2023-01-07 09:03:58,382 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: -93.89265441894531 param sum :: 611.99658203125
2023-01-07 09:03:58,382 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,383 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,383 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: 42.095611572265625
2023-01-07 09:03:58,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,383 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -3011.0048828125
2023-01-07 09:03:58,383 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.592422604560852
2023-01-07 09:03:58,384 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -3010.2080078125 param sum :: -41439.58203125
2023-01-07 09:03:58,384 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.02186393737793
2023-01-07 09:03:58,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1323.0556640625
2023-01-07 09:03:58,385 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26484185457229614
2023-01-07 09:03:58,386 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.12179066985845566 param sum :: 513.40966796875
2023-01-07 09:03:58,386 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,386 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,386 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: -7.02186393737793
2023-01-07 09:03:58,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1323.89794921875
2023-01-07 09:03:58,386 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1438784897327423
2023-01-07 09:03:58,387 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -1324.509765625 param sum :: 1436.124267578125
2023-01-07 09:03:58,388 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.170249938964844
2023-01-07 09:03:58,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,388 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2619.789794921875
2023-01-07 09:03:58,388 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02009296417236328
2023-01-07 09:03:58,389 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: -392.9070739746094 param sum :: 2450.79736328125
2023-01-07 09:03:58,389 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,389 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 17.170249938964844
2023-01-07 09:03:58,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,389 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2617.7626953125
2023-01-07 09:03:58,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07957842200994492
2023-01-07 09:03:58,391 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 2613.6240234375 param sum :: -121787.234375
2023-01-07 09:03:58,391 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 49.99460983276367
2023-01-07 09:03:58,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,391 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 36.69964599609375
2023-01-07 09:03:58,391 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5759215950965881
2023-01-07 09:03:58,392 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -392.314208984375 param sum :: 2450.99755859375
2023-01-07 09:03:58,392 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,392 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,392 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: 49.99460983276367
2023-01-07 09:03:58,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,393 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 36.856201171875
2023-01-07 09:03:58,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4710036516189575
2023-01-07 09:03:58,394 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 37.95367431640625 param sum :: -25728.712890625
2023-01-07 09:03:58,394 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.78044891357422
2023-01-07 09:03:58,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,394 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7775.7998046875
2023-01-07 09:03:58,394 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8822935819625854
2023-01-07 09:03:58,395 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: -93.09986877441406 param sum :: 611.79931640625
2023-01-07 09:03:58,395 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,395 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -20.78044891357422
2023-01-07 09:03:58,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,396 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -7776.06103515625
2023-01-07 09:03:58,396 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11432935297489166
2023-01-07 09:03:58,397 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -7775.41162109375 param sum :: 1822.38916015625
2023-01-07 09:03:58,397 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 56.30255889892578
2023-01-07 09:03:58,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,398 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -616.59130859375
2023-01-07 09:03:58,398 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5785948038101196
2023-01-07 09:03:58,399 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.18606115877628326 param sum :: 510.3996887207031
2023-01-07 09:03:58,399 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: 56.30255889892578
2023-01-07 09:03:58,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,399 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -616.597412109375
2023-01-07 09:03:58,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.34198224544525146
2023-01-07 09:03:58,400 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -616.3961181640625 param sum :: -33268.6484375
2023-01-07 09:03:58,401 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,401 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,401 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 46.166690826416016
2023-01-07 09:03:58,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,401 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7599.318359375
2023-01-07 09:03:58,401 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.018738670274615288
2023-01-07 09:03:58,402 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -392.7873229980469 param sum :: 2452.798095703125
2023-01-07 09:03:58,402 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 46.166690826416016
2023-01-07 09:03:58,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,402 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7599.50341796875
2023-01-07 09:03:58,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31369006633758545
2023-01-07 09:03:58,404 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 7599.814453125 param sum :: -101844.5078125
2023-01-07 09:03:58,404 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.692312240600586
2023-01-07 09:03:58,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,404 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -11236.2373046875
2023-01-07 09:03:58,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.153486967086792
2023-01-07 09:03:58,405 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -93.25038146972656 param sum :: 612.3992919921875
2023-01-07 09:03:58,405 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 6.692312240600586
2023-01-07 09:03:58,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,405 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -11236.3076171875
2023-01-07 09:03:58,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.029906053096055984
2023-01-07 09:03:58,407 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -11236.833984375 param sum :: -11515.291015625
2023-01-07 09:03:58,407 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -62.85820770263672
2023-01-07 09:03:58,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,407 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3141.83544921875
2023-01-07 09:03:58,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19682949781417847
2023-01-07 09:03:58,408 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -93.3763656616211 param sum :: 612.399169921875
2023-01-07 09:03:58,408 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,409 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,409 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -62.85820770263672
2023-01-07 09:03:58,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,409 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3143.01953125
2023-01-07 09:03:58,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.00543212890625e-05
2023-01-07 09:03:58,410 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -3143.251953125 param sum :: 1413.353271484375
2023-01-07 09:03:58,410 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.998942852020264
2023-01-07 09:03:58,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,411 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 18290.8984375
2023-01-07 09:03:58,411 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9938395023345947
2023-01-07 09:03:58,412 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -393.82440185546875 param sum :: 2452.99951171875
2023-01-07 09:03:58,412 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -4.998942852020264
2023-01-07 09:03:58,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,412 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 18294.51171875
2023-01-07 09:03:58,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9355870485305786
2023-01-07 09:03:58,414 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 18295.2109375 param sum :: -149775.90625
2023-01-07 09:03:58,414 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:58,414 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:58,415 > [DEBUG] 0 :: 11.4038724899292
2023-01-07 09:03:58,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,417 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.06516264379024506
2023-01-07 09:03:58,417 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 195.716552734375
2023-01-07 09:03:58,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,419 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 3.9656436443328857
2023-01-07 09:03:58,419 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,420 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 6.590812683105469
2023-01-07 09:03:58,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -233.9779052734375
2023-01-07 09:03:58,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,423 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -5.065169334411621
2023-01-07 09:03:58,424 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.174373265821487e-05
2023-01-07 09:03:58,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,426 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.008647896349430084
2023-01-07 09:03:58,426 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,426 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 85.30754852294922
2023-01-07 09:03:58,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.020761013031006
2023-01-07 09:03:58,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,428 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 12.74899673461914
2023-01-07 09:03:58,428 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00017255410784855485
2023-01-07 09:03:58,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,429 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.11669433116912842
2023-01-07 09:03:58,429 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,430 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 12.865692138671875
2023-01-07 09:03:58,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11672983318567276
2023-01-07 09:03:58,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,431 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.2976231575012207
2023-01-07 09:03:58,431 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,431 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004215382970869541
2023-01-07 09:03:58,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 2.8872764110565186
2023-01-07 09:03:58,432 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,432 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.4410288333892822
2023-01-07 09:03:58,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.310539960861206
2023-01-07 09:03:58,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,434 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 9.825220108032227
2023-01-07 09:03:58,434 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,434 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.008253287523984909
2023-01-07 09:03:58,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.07598169147968292
2023-01-07 09:03:58,435 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,435 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 10.61469554901123
2023-01-07 09:03:58,435 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.46423912048339844
2023-01-07 09:03:58,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,436 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 233.67123413085938
2023-01-07 09:03:58,436 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,436 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0020350655540823936
2023-01-07 09:03:58,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,437 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.07670251280069351
2023-01-07 09:03:58,437 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,437 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 234.8981475830078
2023-01-07 09:03:58,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07743264734745026
2023-01-07 09:03:58,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,439 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -75.72320556640625
2023-01-07 09:03:58,439 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,439 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0005595041438937187
2023-01-07 09:03:58,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.4480366706848145
2023-01-07 09:03:58,440 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,440 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -73.3336410522461
2023-01-07 09:03:58,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.373801827430725
2023-01-07 09:03:58,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,441 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -4.169840335845947
2023-01-07 09:03:58,442 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0012533394619822502
2023-01-07 09:03:58,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,442 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 2.8308000564575195
2023-01-07 09:03:58,443 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,443 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 0.9303545951843262
2023-01-07 09:03:58,443 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.830580949783325
2023-01-07 09:03:58,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,444 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -49.83186340332031
2023-01-07 09:03:58,444 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,444 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007373228669166565
2023-01-07 09:03:58,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.1653440296649933
2023-01-07 09:03:58,445 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,445 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -46.67213439941406
2023-01-07 09:03:58,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.170200824737549
2023-01-07 09:03:58,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,447 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 160.70843505859375
2023-01-07 09:03:58,447 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,447 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.965746145695448e-06
2023-01-07 09:03:58,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,448 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.1751287877559662
2023-01-07 09:03:58,448 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,448 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 160.70843505859375
2023-01-07 09:03:58,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.964569091796875
2023-01-07 09:03:58,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,449 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -5.152712345123291
2023-01-07 09:03:58,449 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11060288548469543
2023-01-07 09:03:58,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,450 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.4840695261955261
2023-01-07 09:03:58,450 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,451 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.059905052185058594
2023-01-07 09:03:58,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6991657018661499
2023-01-07 09:03:58,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.7095155715942383
2023-01-07 09:03:58,452 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007318504154682159
2023-01-07 09:03:58,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,453 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.6782823204994202
2023-01-07 09:03:58,453 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.6658754348754883
2023-01-07 09:03:58,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6762217283248901
2023-01-07 09:03:58,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 284.8526611328125
2023-01-07 09:03:58,455 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.003941208589822054
2023-01-07 09:03:58,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,456 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.3842260241508484
2023-01-07 09:03:58,456 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,456 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 289.5894470214844
2023-01-07 09:03:58,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44253987073898315
2023-01-07 09:03:58,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,457 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.343055248260498
2023-01-07 09:03:58,458 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.022858140990138054
2023-01-07 09:03:58,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,458 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.10060301423072815
2023-01-07 09:03:58,459 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,459 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.651652336120605
2023-01-07 09:03:58,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6213933229446411
2023-01-07 09:03:58,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,460 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -27.831684112548828
2023-01-07 09:03:58,460 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,460 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0031301043927669525
2023-01-07 09:03:58,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,461 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.06286462396383286
2023-01-07 09:03:58,461 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,461 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -23.5032901763916
2023-01-07 09:03:58,461 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.025498056784272194
2023-01-07 09:03:58,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,463 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 1.2000789642333984
2023-01-07 09:03:58,463 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0003490068484097719
2023-01-07 09:03:58,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,464 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.16650015115737915
2023-01-07 09:03:58,464 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,464 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 4.937000274658203
2023-01-07 09:03:58,464 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16519129276275635
2023-01-07 09:03:58,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,465 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -9.864909172058105
2023-01-07 09:03:58,465 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,465 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.008023285306990147
2023-01-07 09:03:58,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -0.1772436797618866
2023-01-07 09:03:58,466 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,467 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -5.647952079772949
2023-01-07 09:03:58,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.241943821310997
2023-01-07 09:03:58,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -89.81048583984375
2023-01-07 09:03:58,468 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.015835504978895187
2023-01-07 09:03:58,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,469 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.05258449912071228
2023-01-07 09:03:58,469 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,469 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -86.17073059082031
2023-01-07 09:03:58,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0555586963891983
2023-01-07 09:03:58,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -91.3370590209961
2023-01-07 09:03:58,471 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004326608031988144
2023-01-07 09:03:58,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.12486770749092102
2023-01-07 09:03:58,472 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,472 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -87.53327178955078
2023-01-07 09:03:58,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12848268449306488
2023-01-07 09:03:58,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,473 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 43.31383514404297
2023-01-07 09:03:58,473 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,473 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01894790306687355
2023-01-07 09:03:58,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.14400121569633484
2023-01-07 09:03:58,474 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 47.08477020263672
2023-01-07 09:03:58,475 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1526026725769043
2023-01-07 09:03:58,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,476 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -144.1116180419922
2023-01-07 09:03:58,476 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,476 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.026789918541908264
2023-01-07 09:03:58,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.15271243453025818
2023-01-07 09:03:58,477 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -140.52908325195312
2023-01-07 09:03:58,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2599126696586609
2023-01-07 09:03:58,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 9.943438529968262
2023-01-07 09:03:58,478 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,479 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.027837853878736496
2023-01-07 09:03:58,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,479 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.24239525198936462
2023-01-07 09:03:58,479 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 13.589396476745605
2023-01-07 09:03:58,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24892526865005493
2023-01-07 09:03:58,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,481 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -88.56654357910156
2023-01-07 09:03:58,481 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03657050430774689
2023-01-07 09:03:58,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,482 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.16338369250297546
2023-01-07 09:03:58,482 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,482 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -85.25257873535156
2023-01-07 09:03:58,482 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13451963663101196
2023-01-07 09:03:58,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,484 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -394.1698913574219
2023-01-07 09:03:58,484 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,484 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.003437843406572938
2023-01-07 09:03:58,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,485 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -2.2492690086364746
2023-01-07 09:03:58,485 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,485 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -391.90838623046875
2023-01-07 09:03:58,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2380104064941406
2023-01-07 09:03:58,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,487 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -4.401466369628906
2023-01-07 09:03:58,487 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,487 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02876594290137291
2023-01-07 09:03:58,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,488 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -5.458049774169922
2023-01-07 09:03:58,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26249703764915466
2023-01-07 09:03:58,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,489 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -8.339310646057129
2023-01-07 09:03:58,489 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9891290068626404
2023-01-07 09:03:58,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -19.559375762939453
2023-01-07 09:03:58,490 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,490 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,490 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,490 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -10.119081497192383
2023-01-07 09:03:58,490 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.921092987060547
2023-01-07 09:03:58,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,491 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 272.7008056640625
2023-01-07 09:03:58,491 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,492 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.019333653151988983
2023-01-07 09:03:58,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,492 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -1.4021036624908447
2023-01-07 09:03:58,492 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,493 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 272.26068115234375
2023-01-07 09:03:58,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.253617525100708
2023-01-07 09:03:58,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,494 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -752.60546875
2023-01-07 09:03:58,494 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,494 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2500222325325012
2023-01-07 09:03:58,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.3205537796020508
2023-01-07 09:03:58,495 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,495 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -743.473876953125
2023-01-07 09:03:58,495 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.766883850097656
2023-01-07 09:03:58,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,497 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -101.81387329101562
2023-01-07 09:03:58,497 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06879720091819763
2023-01-07 09:03:58,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -123.86618041992188
2023-01-07 09:03:58,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15966689586639404
2023-01-07 09:03:58,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 161.90956115722656
2023-01-07 09:03:58,499 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,499 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3364355564117432
2023-01-07 09:03:58,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 138.8568572998047
2023-01-07 09:03:58,500 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4401949942111969
2023-01-07 09:03:58,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,501 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 74.66556549072266
2023-01-07 09:03:58,501 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4370357394218445
2023-01-07 09:03:58,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,502 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 68.19496154785156
2023-01-07 09:03:58,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1205837726593018
2023-01-07 09:03:58,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,503 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -79.95802307128906
2023-01-07 09:03:58,503 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,503 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2397596687078476
2023-01-07 09:03:58,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,504 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -80.0819091796875
2023-01-07 09:03:58,504 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1658315807580948
2023-01-07 09:03:58,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,505 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -4.511287212371826
2023-01-07 09:03:58,505 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,505 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4799583852291107
2023-01-07 09:03:58,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,506 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -21.0355281829834
2023-01-07 09:03:58,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0189642906188965
2023-01-07 09:03:58,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,507 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -34.387367248535156
2023-01-07 09:03:58,507 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10571885854005814
2023-01-07 09:03:58,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,508 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -25.895240783691406
2023-01-07 09:03:58,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46985435485839844
2023-01-07 09:03:58,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,509 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 70.23343658447266
2023-01-07 09:03:58,510 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,510 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33875206112861633
2023-01-07 09:03:58,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,510 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 75.84764862060547
2023-01-07 09:03:58,511 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1379680633544922
2023-01-07 09:03:58,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,512 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 42.46092224121094
2023-01-07 09:03:58,512 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7939015626907349
2023-01-07 09:03:58,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,513 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -2.0523316860198975
2023-01-07 09:03:58,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,513 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 21.622051239013672
2023-01-07 09:03:58,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8668758869171143
2023-01-07 09:03:58,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,515 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -196.9837646484375
2023-01-07 09:03:58,515 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,515 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.78336763381958
2023-01-07 09:03:58,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,516 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -142.3124542236328
2023-01-07 09:03:58,516 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2195830345153809
2023-01-07 09:03:58,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,517 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 149.52774047851562
2023-01-07 09:03:58,517 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2723536491394043
2023-01-07 09:03:58,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,518 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 209.7498779296875
2023-01-07 09:03:58,518 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3435747623443604
2023-01-07 09:03:58,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,519 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -302.8185119628906
2023-01-07 09:03:58,519 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5906145572662354
2023-01-07 09:03:58,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,520 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.2763543128967285
2023-01-07 09:03:58,520 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,520 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 14.586994171142578
2023-01-07 09:03:58,520 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.8457770347595215
2023-01-07 09:03:58,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,521 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -162.05511474609375
2023-01-07 09:03:58,522 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,522 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.17102283239364624
2023-01-07 09:03:58,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,523 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -164.21624755859375
2023-01-07 09:03:58,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.522939682006836
2023-01-07 09:03:58,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,524 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 66.25438690185547
2023-01-07 09:03:58,524 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.217610359191895
2023-01-07 09:03:58,525 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,525 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,525 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 65.62612915039062
2023-01-07 09:03:58,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.780760288238525
2023-01-07 09:03:58,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,526 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 44.8780517578125
2023-01-07 09:03:58,526 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.190216064453125
2023-01-07 09:03:58,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,527 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.7096576690673828
2023-01-07 09:03:58,527 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,527 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 37.999969482421875
2023-01-07 09:03:58,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.465074062347412
2023-01-07 09:03:58,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,529 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 55.670806884765625
2023-01-07 09:03:58,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7004485130310059
2023-01-07 09:03:58,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,530 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -2.655322313308716
2023-01-07 09:03:58,530 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -51.372562408447266
2023-01-07 09:03:58,530 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,530 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 53.4908447265625
2023-01-07 09:03:58,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.07831573486328
2023-01-07 09:03:58,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 53.490081787109375
2023-01-07 09:03:58,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.135864019393921
2023-01-07 09:03:58,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,533 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -44.20846939086914
2023-01-07 09:03:58,533 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.33750915527344
2023-01-07 09:03:58,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,534 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 14.786172866821289
2023-01-07 09:03:58,534 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.41566276550293
2023-01-07 09:03:58,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,535 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 2.046661376953125
2023-01-07 09:03:58,535 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,535 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 29.159652709960938
2023-01-07 09:03:58,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8923826217651367
2023-01-07 09:03:58,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,537 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 74.00682830810547
2023-01-07 09:03:58,537 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.981136322021484
2023-01-07 09:03:58,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,537 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -2.5646069049835205
2023-01-07 09:03:58,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,538 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 45.106815338134766
2023-01-07 09:03:58,538 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.09783935546875
2023-01-07 09:03:58,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,539 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 132.98663330078125
2023-01-07 09:03:58,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0891056060791016
2023-01-07 09:03:58,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,540 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -28.320453643798828
2023-01-07 09:03:58,540 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,541 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 55.07135009765625
2023-01-07 09:03:58,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.883766174316406
2023-01-07 09:03:58,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,542 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 174.60110473632812
2023-01-07 09:03:58,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.111343383789062
2023-01-07 09:03:58,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,543 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 70.987060546875
2023-01-07 09:03:58,543 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,543 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.078316688537598
2023-01-07 09:03:58,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,544 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 223.91307067871094
2023-01-07 09:03:58,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.51951789855957
2023-01-07 09:03:58,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,545 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 1.7804107666015625
2023-01-07 09:03:58,545 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,545 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 99.43448638916016
2023-01-07 09:03:58,546 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,546 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 220.66885375976562
2023-01-07 09:03:58,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 57.254425048828125
2023-01-07 09:03:58,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.14176082611084
2023-01-07 09:03:58,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,547 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 222.60043334960938
2023-01-07 09:03:58,548 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.05332088470459
2023-01-07 09:03:58,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,548 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,548 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -125.22360229492188
2023-01-07 09:03:58,548 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 86.58026123046875
2023-01-07 09:03:58,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,550 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 227.14227294921875
2023-01-07 09:03:58,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.353479385375977
2023-01-07 09:03:58,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,550 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -87.6630859375
2023-01-07 09:03:58,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,551 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 61.963134765625
2023-01-07 09:03:58,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -212.0577392578125
2023-01-07 09:03:58,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,552 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 90.22429656982422
2023-01-07 09:03:58,552 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.717609405517578
2023-01-07 09:03:58,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,553 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 6.028093338012695
2023-01-07 09:03:58,553 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,553 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 13.764049530029297
2023-01-07 09:03:58,553 > [DEBUG] 0 :: before allreduce fusion buffer :: 67.55770111083984
2023-01-07 09:03:58,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,555 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -164.2353057861328
2023-01-07 09:03:58,555 > [DEBUG] 0 :: before allreduce fusion buffer :: 51.98225784301758
2023-01-07 09:03:58,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,556 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -164.2353057861328
2023-01-07 09:03:58,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.0362548828125
2023-01-07 09:03:58,558 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:03:58,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,559 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 41.122344970703125
2023-01-07 09:03:58,559 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:58,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,559 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -242.9883575439453
2023-01-07 09:03:58,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 57.51362609863281
2023-01-07 09:03:58,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,560 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 65.62290954589844
2023-01-07 09:03:58,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -175.13967895507812
2023-01-07 09:03:58,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 57.82575607299805
2023-01-07 09:03:58,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 289.7275695800781
2023-01-07 09:03:58,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1.9130706787109375
2023-01-07 09:03:58,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 22.85315704345703
2023-01-07 09:03:58,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 73.96685791015625
2023-01-07 09:03:58,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 45.04823684692383
2023-01-07 09:03:58,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -54.23944854736328
2023-01-07 09:03:58,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,565 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -121.61174774169922
2023-01-07 09:03:58,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 46.62542724609375
2023-01-07 09:03:58,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,566 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 85.52613830566406
2023-01-07 09:03:58,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -155.37977600097656
2023-01-07 09:03:58,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -739.676513671875
2023-01-07 09:03:58,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 274.2515869140625
2023-01-07 09:03:58,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -2.348048686981201
2023-01-07 09:03:58,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -6.191061973571777
2023-01-07 09:03:58,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -389.2657165527344
2023-01-07 09:03:58,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -84.32622528076172
2023-01-07 09:03:58,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 16.736976623535156
2023-01-07 09:03:58,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -136.79293823242188
2023-01-07 09:03:58,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 46.996421813964844
2023-01-07 09:03:58,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -84.2055892944336
2023-01-07 09:03:58,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -1112.62890625
2023-01-07 09:03:58,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -82.89383697509766
2023-01-07 09:03:58,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -5.573603630065918
2023-01-07 09:03:58,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 7.999199867248535
2023-01-07 09:03:58,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -20.0116024017334
2023-01-07 09:03:58,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 8.626193046569824
2023-01-07 09:03:58,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 293.0712890625
2023-01-07 09:03:58,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.046694278717041
2023-01-07 09:03:58,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.2575373649597168
2023-01-07 09:03:58,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,575 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 163.23695373535156
2023-01-07 09:03:58,575 > [DEBUG] 0 :: before allreduce fusion buffer :: 305.71673583984375
2023-01-07 09:03:58,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,576 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -57.746368408203125
2023-01-07 09:03:58,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,576 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.3904962539672852
2023-01-07 09:03:58,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,576 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -84.53684997558594
2023-01-07 09:03:58,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 235.3422393798828
2023-01-07 09:03:58,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.241634368896484
2023-01-07 09:03:58,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 11.809326171875
2023-01-07 09:03:58,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,578 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.603433609008789
2023-01-07 09:03:58,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,578 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 12.988353729248047
2023-01-07 09:03:58,578 > [DEBUG] 0 :: before allreduce fusion buffer :: 64.2911605834961
2023-01-07 09:03:58,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,579 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 85.6264877319336
2023-01-07 09:03:58,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:58,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:58,579 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 165.21798706054688
2023-01-07 09:03:58,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.00672912597656
2023-01-07 09:03:59,422 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: -26.710594177246094 param sum :: 182.06719970703125
2023-01-07 09:03:59,422 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 63.599998474121094
2023-01-07 09:03:59,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,422 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 4.9038004875183105
2023-01-07 09:03:59,422 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,423 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -430.0625
2023-01-07 09:03:59,423 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.175565719604492
2023-01-07 09:03:59,424 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 9.398231506347656 param sum :: 62.65672302246094
2023-01-07 09:03:59,425 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -2.1933627128601074
2023-01-07 09:03:59,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,425 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 324.85626220703125
2023-01-07 09:03:59,425 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,425 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -325.2891845703125
2023-01-07 09:03:59,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 332.605712890625
2023-01-07 09:03:59,426 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 421.0263671875 param sum :: -39.066436767578125
2023-01-07 09:03:59,427 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,427 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 50.308475494384766
2023-01-07 09:03:59,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,427 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -327.36761474609375
2023-01-07 09:03:59,427 > [DEBUG] 0 :: before allreduce fusion buffer :: -110.4222640991211
2023-01-07 09:03:59,428 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 45.061302185058594 param sum :: 59.680908203125
2023-01-07 09:03:59,428 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 50.308475494384766
2023-01-07 09:03:59,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,428 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -319.3075256347656
2023-01-07 09:03:59,428 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.20562744140625
2023-01-07 09:03:59,429 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -391.85638427734375 param sum :: 114.8167953491211
2023-01-07 09:03:59,429 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,429 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,430 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 63.59852600097656
2023-01-07 09:03:59,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,430 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 1.1814610958099365
2023-01-07 09:03:59,430 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,430 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 68.30587768554688
2023-01-07 09:03:59,430 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,430 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 198.61175537109375
2023-01-07 09:03:59,430 > [DEBUG] 0 :: before allreduce fusion buffer :: 89.28079986572266
2023-01-07 09:03:59,432 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -0.7195320129394531 param sum :: 64.3573989868164
2023-01-07 09:03:59,432 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,432 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,432 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 153.61016845703125
2023-01-07 09:03:59,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,432 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 145.31625366210938
2023-01-07 09:03:59,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.05510330200195
2023-01-07 09:03:59,433 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 112.25863647460938 param sum :: -33.20974349975586
2023-01-07 09:03:59,433 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.4000244140625
2023-01-07 09:03:59,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,433 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -1.4927053451538086
2023-01-07 09:03:59,433 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,434 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 116.78082275390625
2023-01-07 09:03:59,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 119.14215087890625
2023-01-07 09:03:59,435 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 3.0027413368225098 param sum :: 254.38912963867188
2023-01-07 09:03:59,435 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 162.29132080078125
2023-01-07 09:03:59,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,435 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 95.6279296875
2023-01-07 09:03:59,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 34.86191940307617
2023-01-07 09:03:59,436 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: -16.510833740234375 param sum :: 282.1540222167969
2023-01-07 09:03:59,436 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,436 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 162.29132080078125
2023-01-07 09:03:59,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,437 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 81.07991027832031
2023-01-07 09:03:59,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.949109077453613
2023-01-07 09:03:59,438 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 31.55788803100586 param sum :: 237.54432678222656
2023-01-07 09:03:59,438 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 162.29132080078125
2023-01-07 09:03:59,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,438 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 59.01810073852539
2023-01-07 09:03:59,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.42548942565918
2023-01-07 09:03:59,439 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 105.37776184082031 param sum :: 82.91890716552734
2023-01-07 09:03:59,439 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,439 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.80000305175781
2023-01-07 09:03:59,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,439 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.2395269870758057
2023-01-07 09:03:59,439 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,440 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 88.26500701904297
2023-01-07 09:03:59,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4068841934204102
2023-01-07 09:03:59,441 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -1.8127288818359375 param sum :: 61.284759521484375
2023-01-07 09:03:59,441 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 162.29132080078125
2023-01-07 09:03:59,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,441 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 122.09010314941406
2023-01-07 09:03:59,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,441 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 10.06544303894043
2023-01-07 09:03:59,442 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.279664993286133
2023-01-07 09:03:59,443 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 146.43309020996094 param sum :: 221.1082000732422
2023-01-07 09:03:59,443 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,443 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,443 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 63.59999084472656
2023-01-07 09:03:59,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,443 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -2.4564027786254883
2023-01-07 09:03:59,443 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,443 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -21.920124053955078
2023-01-07 09:03:59,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.14971160888672
2023-01-07 09:03:59,444 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 1.7796275615692139 param sum :: 63.397705078125
2023-01-07 09:03:59,444 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,445 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 153.61016845703125
2023-01-07 09:03:59,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,445 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 145.69943237304688
2023-01-07 09:03:59,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 88.90225219726562
2023-01-07 09:03:59,446 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: -99.66285705566406 param sum :: 125.23143005371094
2023-01-07 09:03:59,446 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,446 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,446 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 221.4887237548828
2023-01-07 09:03:59,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,446 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 34.476768493652344
2023-01-07 09:03:59,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.049373626708984
2023-01-07 09:03:59,447 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 10.532218933105469 param sum :: 250.94189453125
2023-01-07 09:03:59,447 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 221.4887237548828
2023-01-07 09:03:59,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,448 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -61.621620178222656
2023-01-07 09:03:59,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,448 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 170.91586303710938
2023-01-07 09:03:59,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -69.56608581542969
2023-01-07 09:03:59,449 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -58.20949172973633 param sum :: 360.6060485839844
2023-01-07 09:03:59,449 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 153.61016845703125
2023-01-07 09:03:59,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,449 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 111.44242095947266
2023-01-07 09:03:59,449 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.22796630859375
2023-01-07 09:03:59,450 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.5534505844116211 param sum :: 64.65766143798828
2023-01-07 09:03:59,450 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,451 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 153.61016845703125
2023-01-07 09:03:59,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,451 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -7.642856597900391
2023-01-07 09:03:59,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.044637680053711
2023-01-07 09:03:59,452 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -2.5836563110351562 param sum :: 276.6053466796875
2023-01-07 09:03:59,452 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.60002899169922
2023-01-07 09:03:59,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,452 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.04102194309234619
2023-01-07 09:03:59,452 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,453 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -35.9864387512207
2023-01-07 09:03:59,453 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -56.663997650146484
2023-01-07 09:03:59,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -88.49142456054688
2023-01-07 09:03:59,454 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 4.282737731933594 param sum :: 64.03046417236328
2023-01-07 09:03:59,454 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,454 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 554.7550659179688
2023-01-07 09:03:59,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,455 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -0.9815583229064941
2023-01-07 09:03:59,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9529286623001099
2023-01-07 09:03:59,456 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 98.0782241821289 param sum :: -40.91401672363281
2023-01-07 09:03:59,456 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,456 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,456 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 565.6168212890625
2023-01-07 09:03:59,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,456 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -47.293846130371094
2023-01-07 09:03:59,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.485149383544922
2023-01-07 09:03:59,457 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -0.017346501350402832 param sum :: 269.386474609375
2023-01-07 09:03:59,457 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 554.7550659179688
2023-01-07 09:03:59,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,458 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4.2636590003967285
2023-01-07 09:03:59,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.915096640586853
2023-01-07 09:03:59,459 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -118.57783508300781 param sum :: 909.7977905273438
2023-01-07 09:03:59,459 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,459 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,459 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 554.7550659179688
2023-01-07 09:03:59,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,459 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 11.931425094604492
2023-01-07 09:03:59,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.699954986572266
2023-01-07 09:03:59,460 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -3.3090920448303223 param sum :: 133.8304443359375
2023-01-07 09:03:59,460 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 554.7550659179688
2023-01-07 09:03:59,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,460 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 55.27978515625
2023-01-07 09:03:59,461 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.263729095458984
2023-01-07 09:03:59,462 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 15.839962005615234 param sum :: 783.8806762695312
2023-01-07 09:03:59,462 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,462 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,462 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 126.40007019042969
2023-01-07 09:03:59,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,462 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -0.13349127769470215
2023-01-07 09:03:59,462 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,462 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,462 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,462 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1.3261241912841797
2023-01-07 09:03:59,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 47.8740348815918
2023-01-07 09:03:59,463 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -0.4598875045776367 param sum :: 125.51715850830078
2023-01-07 09:03:59,463 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 299.9128112792969
2023-01-07 09:03:59,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -13.975844383239746
2023-01-07 09:03:59,464 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.348798751831055
2023-01-07 09:03:59,465 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -8.753799438476562 param sum :: 539.8554077148438
2023-01-07 09:03:59,465 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,465 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,465 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.9993286132812
2023-01-07 09:03:59,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,465 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 6.0651021003723145
2023-01-07 09:03:59,465 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,465 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -34.000308990478516
2023-01-07 09:03:59,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 20.27294158935547
2023-01-07 09:03:59,467 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 7.423041343688965 param sum :: 509.7526550292969
2023-01-07 09:03:59,467 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,467 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 171.76171875
2023-01-07 09:03:59,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,467 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1.0149154663085938
2023-01-07 09:03:59,467 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.466390609741211
2023-01-07 09:03:59,468 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 9.64083480834961 param sum :: 302.08636474609375
2023-01-07 09:03:59,468 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,468 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 276.66925048828125
2023-01-07 09:03:59,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,468 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 372.63079833984375
2023-01-07 09:03:59,469 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.716970443725586
2023-01-07 09:03:59,469 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 26.609054565429688 param sum :: 560.8987426757812
2023-01-07 09:03:59,470 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,470 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 276.66925048828125
2023-01-07 09:03:59,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,470 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 400.46319580078125
2023-01-07 09:03:59,470 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.693201065063477
2023-01-07 09:03:59,471 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 412.9900207519531 param sum :: 382.18975830078125
2023-01-07 09:03:59,471 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,471 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.39991760253906
2023-01-07 09:03:59,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,471 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.41114580631256104
2023-01-07 09:03:59,472 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,472 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 117.13481903076172
2023-01-07 09:03:59,472 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.737017631530762
2023-01-07 09:03:59,473 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.0761947631835938 param sum :: 127.72686767578125
2023-01-07 09:03:59,473 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,473 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,473 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 393.1651611328125
2023-01-07 09:03:59,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 116.5013198852539
2023-01-07 09:03:59,473 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.957518577575684
2023-01-07 09:03:59,474 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 113.591552734375 param sum :: 709.8131103515625
2023-01-07 09:03:59,474 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,474 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,475 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 126.99992370605469
2023-01-07 09:03:59,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,475 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.40671491622924805
2023-01-07 09:03:59,475 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,475 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 118.779541015625
2023-01-07 09:03:59,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.8851318359375
2023-01-07 09:03:59,476 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.6001167297363281 param sum :: 127.32525634765625
2023-01-07 09:03:59,476 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,476 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,476 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 341.9408264160156
2023-01-07 09:03:59,476 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,476 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,476 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 133.83935546875
2023-01-07 09:03:59,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.129721641540527
2023-01-07 09:03:59,478 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 129.3603515625 param sum :: 555.1539306640625
2023-01-07 09:03:59,478 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 273.0256042480469
2023-01-07 09:03:59,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,478 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 24.592063903808594
2023-01-07 09:03:59,478 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9844865798950195
2023-01-07 09:03:59,479 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 9.960419654846191 param sum :: 589.9212646484375
2023-01-07 09:03:59,479 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,479 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,479 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 273.0256042480469
2023-01-07 09:03:59,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,479 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,479 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 22.995067596435547
2023-01-07 09:03:59,480 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.316292762756348
2023-01-07 09:03:59,481 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 26.766334533691406 param sum :: 391.7188720703125
2023-01-07 09:03:59,481 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.79994201660156
2023-01-07 09:03:59,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,481 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -1.2495110034942627
2023-01-07 09:03:59,481 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,481 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 67.84407806396484
2023-01-07 09:03:59,481 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.456815719604492
2023-01-07 09:03:59,482 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: -1.14188551902771 param sum :: 129.12490844726562
2023-01-07 09:03:59,482 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,482 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 479.416259765625
2023-01-07 09:03:59,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,483 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 86.81926727294922
2023-01-07 09:03:59,483 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.122677803039551
2023-01-07 09:03:59,484 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 84.97415924072266 param sum :: 588.640625
2023-01-07 09:03:59,484 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.9998321533203
2023-01-07 09:03:59,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,484 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.14501500129699707
2023-01-07 09:03:59,484 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,485 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 55.95179748535156
2023-01-07 09:03:59,485 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.749584674835205
2023-01-07 09:03:59,486 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.9104738235473633 param sum :: 129.92193603515625
2023-01-07 09:03:59,486 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 302.1866149902344
2023-01-07 09:03:59,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,486 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 70.02603149414062
2023-01-07 09:03:59,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.359871864318848
2023-01-07 09:03:59,487 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 38.759788513183594 param sum :: 439.91705322265625
2023-01-07 09:03:59,487 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,487 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.6011962890625
2023-01-07 09:03:59,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,488 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -1.2227394580841064
2023-01-07 09:03:59,488 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,488 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -46.43408966064453
2023-01-07 09:03:59,488 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.00829029083252
2023-01-07 09:03:59,489 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -1.3803842067718506 param sum :: 513.538818359375
2023-01-07 09:03:59,489 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 444.2287902832031
2023-01-07 09:03:59,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,489 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -57.629310607910156
2023-01-07 09:03:59,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.34195327758789
2023-01-07 09:03:59,490 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -41.43608474731445 param sum :: 729.795654296875
2023-01-07 09:03:59,490 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 127.4000473022461
2023-01-07 09:03:59,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,491 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 1.0661368370056152
2023-01-07 09:03:59,491 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,491 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -119.65769958496094
2023-01-07 09:03:59,491 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0353546142578125
2023-01-07 09:03:59,492 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: -1.3344303369522095 param sum :: 127.15335083007812
2023-01-07 09:03:59,492 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,492 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 649.9478149414062
2023-01-07 09:03:59,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,493 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -107.16118621826172
2023-01-07 09:03:59,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3572139739990234
2023-01-07 09:03:59,494 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -100.25899505615234 param sum :: 1132.4073486328125
2023-01-07 09:03:59,494 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 127.59977722167969
2023-01-07 09:03:59,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,494 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 2.5406765937805176
2023-01-07 09:03:59,494 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,494 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 43.78641891479492
2023-01-07 09:03:59,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.289262771606445
2023-01-07 09:03:59,496 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 3.124025821685791 param sum :: 126.24015808105469
2023-01-07 09:03:59,496 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,496 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,496 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 432.947998046875
2023-01-07 09:03:59,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,496 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 42.64340591430664
2023-01-07 09:03:59,496 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.913006782531738
2023-01-07 09:03:59,497 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 51.68474578857422 param sum :: 548.50048828125
2023-01-07 09:03:59,497 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.4271850585938
2023-01-07 09:03:59,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,498 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.09325364232063293
2023-01-07 09:03:59,498 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 28.882705688476562
2023-01-07 09:03:59,498 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.328739166259766
2023-01-07 09:03:59,499 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.40668654441833496 param sum :: 510.72210693359375
2023-01-07 09:03:59,499 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,499 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,499 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 509.96575927734375
2023-01-07 09:03:59,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -6.2329864501953125
2023-01-07 09:03:59,499 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.19432830810547
2023-01-07 09:03:59,500 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -37.178916931152344 param sum :: 830.7462768554688
2023-01-07 09:03:59,500 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.9985656738281
2023-01-07 09:03:59,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -0.5557582974433899
2023-01-07 09:03:59,501 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -282.224609375
2023-01-07 09:03:59,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.978287696838379
2023-01-07 09:03:59,502 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -2.447993516921997 param sum :: 256.1586608886719
2023-01-07 09:03:59,502 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,502 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -23646.03125
2023-01-07 09:03:59,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -278.48931884765625
2023-01-07 09:03:59,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.107246398925781
2023-01-07 09:03:59,504 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -273.65496826171875 param sum :: -39147.5546875
2023-01-07 09:03:59,504 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,504 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 361.54571533203125
2023-01-07 09:03:59,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -739.676513671875
2023-01-07 09:03:59,504 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.43958854675293
2023-01-07 09:03:59,505 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.683234453201294 param sum :: 307.2815246582031
2023-01-07 09:03:59,505 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,506 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 361.54571533203125
2023-01-07 09:03:59,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,506 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -717.5294799804688
2023-01-07 09:03:59,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6450905799865723
2023-01-07 09:03:59,507 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: -714.841552734375 param sum :: 4431.8740234375
2023-01-07 09:03:59,507 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,507 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,507 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31671.548828125
2023-01-07 09:03:59,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,507 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 274.3668212890625
2023-01-07 09:03:59,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.821165561676025
2023-01-07 09:03:59,508 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -0.909308910369873 param sum :: 1232.1962890625
2023-01-07 09:03:59,508 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31671.548828125
2023-01-07 09:03:59,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,509 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 264.4803771972656
2023-01-07 09:03:59,509 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.919742584228516
2023-01-07 09:03:59,510 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 277.6953125 param sum :: -53778.53125
2023-01-07 09:03:59,510 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,510 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,510 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22759.0
2023-01-07 09:03:59,510 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,510 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,510 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 3.695535182952881
2023-01-07 09:03:59,510 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24261534214019775
2023-01-07 09:03:59,511 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -21.878259658813477 param sum :: 1248.59814453125
2023-01-07 09:03:59,511 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22759.0
2023-01-07 09:03:59,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,512 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 4.026035308837891
2023-01-07 09:03:59,512 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2977361679077148
2023-01-07 09:03:59,513 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 6.281965255737305 param sum :: -38102.03125
2023-01-07 09:03:59,513 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,513 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.4000244140625
2023-01-07 09:03:59,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,513 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 6.00634765625
2023-01-07 09:03:59,513 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:03:59,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,514 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -6.366111755371094
2023-01-07 09:03:59,514 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7248873710632324
2023-01-07 09:03:59,515 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 4.119303226470947 param sum :: 250.20465087890625
2023-01-07 09:03:59,515 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -55933.8359375
2023-01-07 09:03:59,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,515 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 5.056447982788086
2023-01-07 09:03:59,515 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0613245964050293
2023-01-07 09:03:59,516 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 9.931999206542969 param sum :: -93456.5625
2023-01-07 09:03:59,516 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,516 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,516 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11538.310546875
2023-01-07 09:03:59,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,517 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -11.763320922851562
2023-01-07 09:03:59,517 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.062735080718994
2023-01-07 09:03:59,518 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -2.290107011795044 param sum :: 320.89764404296875
2023-01-07 09:03:59,518 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11538.310546875
2023-01-07 09:03:59,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,518 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 0.7415726780891418
2023-01-07 09:03:59,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.253568649291992
2023-01-07 09:03:59,519 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -0.940320611000061 param sum :: -19297.83984375
2023-01-07 09:03:59,520 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 951.9287109375
2023-01-07 09:03:59,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,520 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -33.500213623046875
2023-01-07 09:03:59,520 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3749014139175415
2023-01-07 09:03:59,521 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 3.619257688522339 param sum :: 1290.698486328125
2023-01-07 09:03:59,521 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,521 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,521 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 951.9287109375
2023-01-07 09:03:59,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,521 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -24.226181030273438
2023-01-07 09:03:59,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4108083248138428
2023-01-07 09:03:59,522 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: -24.089874267578125 param sum :: 1450.72216796875
2023-01-07 09:03:59,523 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,523 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 584.7568359375
2023-01-07 09:03:59,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,523 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 15.372982025146484
2023-01-07 09:03:59,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.351433753967285
2023-01-07 09:03:59,524 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.20146828889846802 param sum :: 325.9282531738281
2023-01-07 09:03:59,524 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 584.7568359375
2023-01-07 09:03:59,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,524 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 15.633523941040039
2023-01-07 09:03:59,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14514531195163727
2023-01-07 09:03:59,526 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 14.923321723937988 param sum :: 1021.884521484375
2023-01-07 09:03:59,526 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1024.7777099609375
2023-01-07 09:03:59,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,526 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 11.277375221252441
2023-01-07 09:03:59,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8280996680259705
2023-01-07 09:03:59,527 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.44114500284194946 param sum :: 328.01544189453125
2023-01-07 09:03:59,527 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1024.7777099609375
2023-01-07 09:03:59,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,527 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 10.321813583374023
2023-01-07 09:03:59,528 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5020880699157715
2023-01-07 09:03:59,529 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 10.523300170898438 param sum :: 1677.99609375
2023-01-07 09:03:59,529 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,529 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,529 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1190.582275390625
2023-01-07 09:03:59,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,529 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 57.71076965332031
2023-01-07 09:03:59,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3131046295166016
2023-01-07 09:03:59,530 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 4.090689659118652 param sum :: 1312.083251953125
2023-01-07 09:03:59,530 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1190.582275390625
2023-01-07 09:03:59,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,530 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 52.749755859375
2023-01-07 09:03:59,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3539979457855225
2023-01-07 09:03:59,532 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 56.411598205566406 param sum :: 1788.3935546875
2023-01-07 09:03:59,532 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1417.218994140625
2023-01-07 09:03:59,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,532 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -46.086692810058594
2023-01-07 09:03:59,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17551004886627197
2023-01-07 09:03:59,533 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.4602600932121277 param sum :: 329.2872009277344
2023-01-07 09:03:59,533 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,533 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,533 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1417.218994140625
2023-01-07 09:03:59,533 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,533 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,534 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -45.39722442626953
2023-01-07 09:03:59,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14047761261463165
2023-01-07 09:03:59,535 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: -45.07737731933594 param sum :: 2217.423828125
2023-01-07 09:03:59,535 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1195.000732421875
2023-01-07 09:03:59,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,535 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -103.23741149902344
2023-01-07 09:03:59,535 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011798292398452759
2023-01-07 09:03:59,536 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.5983496308326721 param sum :: 331.1861267089844
2023-01-07 09:03:59,536 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,536 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,536 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1195.000732421875
2023-01-07 09:03:59,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,537 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -110.53912353515625
2023-01-07 09:03:59,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2858453392982483
2023-01-07 09:03:59,538 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: -110.71478271484375 param sum :: 1993.9332275390625
2023-01-07 09:03:59,538 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1016.7935180664062
2023-01-07 09:03:59,538 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,538 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,538 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 46.83638000488281
2023-01-07 09:03:59,538 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5233403444290161
2023-01-07 09:03:59,539 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 3.9760324954986572 param sum :: 1328.95849609375
2023-01-07 09:03:59,539 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,539 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,539 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1016.7935180664062
2023-01-07 09:03:59,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,539 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 46.63591003417969
2023-01-07 09:03:59,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49791476130485535
2023-01-07 09:03:59,541 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 45.40924072265625 param sum :: 1804.408935546875
2023-01-07 09:03:59,541 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,541 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,541 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1177.0242919921875
2023-01-07 09:03:59,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,541 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 252.2193603515625
2023-01-07 09:03:59,541 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8518503904342651
2023-01-07 09:03:59,542 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.8425127267837524 param sum :: 332.46905517578125
2023-01-07 09:03:59,542 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1177.0242919921875
2023-01-07 09:03:59,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,543 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 252.33551025390625
2023-01-07 09:03:59,543 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00872674211859703
2023-01-07 09:03:59,544 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 252.40875244140625 param sum :: 1818.0093994140625
2023-01-07 09:03:59,544 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,544 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,544 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1839.455322265625
2023-01-07 09:03:59,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,544 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -364.19757080078125
2023-01-07 09:03:59,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9981107711791992
2023-01-07 09:03:59,545 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.6737470030784607 param sum :: 333.4615478515625
2023-01-07 09:03:59,545 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,545 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,545 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1839.455322265625
2023-01-07 09:03:59,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,546 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -365.5909118652344
2023-01-07 09:03:59,546 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30352216958999634
2023-01-07 09:03:59,547 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -366.2146911621094 param sum :: -2944.5654296875
2023-01-07 09:03:59,547 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -20248.6640625
2023-01-07 09:03:59,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,547 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 13.88657283782959
2023-01-07 09:03:59,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4497263431549072
2023-01-07 09:03:59,548 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 4.220771789550781 param sum :: 1334.619384765625
2023-01-07 09:03:59,548 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,548 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -20248.6640625
2023-01-07 09:03:59,548 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,549 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 11.397184371948242
2023-01-07 09:03:59,549 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2839251756668091
2023-01-07 09:03:59,550 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 6.425002574920654 param sum :: -33744.6796875
2023-01-07 09:03:59,550 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -7987.0791015625
2023-01-07 09:03:59,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,550 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -989.7091064453125
2023-01-07 09:03:59,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.053799476474523544
2023-01-07 09:03:59,551 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.8968438506126404 param sum :: 335.16168212890625
2023-01-07 09:03:59,551 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,551 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -7987.0791015625
2023-01-07 09:03:59,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -990.2916259765625
2023-01-07 09:03:59,552 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45091885328292847
2023-01-07 09:03:59,553 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -990.435791015625 param sum :: -10119.3359375
2023-01-07 09:03:59,553 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22529.15234375
2023-01-07 09:03:59,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,553 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.078139781951904
2023-01-07 09:03:59,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3324425220489502
2023-01-07 09:03:59,554 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 1.2785180807113647 param sum :: 334.8500061035156
2023-01-07 09:03:59,554 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,554 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22529.15234375
2023-01-07 09:03:59,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.7484025955200195
2023-01-07 09:03:59,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35500645637512207
2023-01-07 09:03:59,556 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 8.091014862060547 param sum :: -37641.328125
2023-01-07 09:03:59,556 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49372.80078125
2023-01-07 09:03:59,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,556 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 0.5190596580505371
2023-01-07 09:03:59,556 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.411312103271484
2023-01-07 09:03:59,557 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 5.702248573303223 param sum :: 1340.069580078125
2023-01-07 09:03:59,557 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,557 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,557 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49372.80078125
2023-01-07 09:03:59,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,557 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 4.383061408996582
2023-01-07 09:03:59,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.41562870144844055
2023-01-07 09:03:59,559 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -5.814169406890869 param sum :: -82426.2890625
2023-01-07 09:03:59,559 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41439.58203125
2023-01-07 09:03:59,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,559 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -61.815765380859375
2023-01-07 09:03:59,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.046316128224134445
2023-01-07 09:03:59,560 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 4.311570644378662 param sum :: 667.746826171875
2023-01-07 09:03:59,560 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,560 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,560 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41439.58203125
2023-01-07 09:03:59,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,561 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -61.80580139160156
2023-01-07 09:03:59,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014478009194135666
2023-01-07 09:03:59,562 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: -61.837608337402344 param sum :: -69446.125
2023-01-07 09:03:59,562 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1436.124267578125
2023-01-07 09:03:59,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,562 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -230.052978515625
2023-01-07 09:03:59,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2305359840393066
2023-01-07 09:03:59,563 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.0002580378786660731 param sum :: 514.3963623046875
2023-01-07 09:03:59,563 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,563 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,563 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1436.124267578125
2023-01-07 09:03:59,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,564 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -232.37417602539062
2023-01-07 09:03:59,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35346975922584534
2023-01-07 09:03:59,565 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -232.2606201171875 param sum :: 2675.8544921875
2023-01-07 09:03:59,565 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,565 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121787.234375
2023-01-07 09:03:59,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,565 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2129.96728515625
2023-01-07 09:03:59,565 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.216533899307251
2023-01-07 09:03:59,566 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 6.6754560470581055 param sum :: 2709.65771484375
2023-01-07 09:03:59,567 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,567 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121787.234375
2023-01-07 09:03:59,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,567 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2129.345703125
2023-01-07 09:03:59,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09234440326690674
2023-01-07 09:03:59,568 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -2130.35009765625 param sum :: -200224.90625
2023-01-07 09:03:59,568 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,568 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,568 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25728.712890625
2023-01-07 09:03:59,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,568 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -113.44181060791016
2023-01-07 09:03:59,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.038914442062378
2023-01-07 09:03:59,569 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 5.12143611907959 param sum :: 2711.02587890625
2023-01-07 09:03:59,569 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25728.712890625
2023-01-07 09:03:59,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,570 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -108.55431365966797
2023-01-07 09:03:59,570 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6216055154800415
2023-01-07 09:03:59,571 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -117.72942352294922 param sum :: -42901.421875
2023-01-07 09:03:59,571 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,571 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,571 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1822.38916015625
2023-01-07 09:03:59,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,571 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 661.3816528320312
2023-01-07 09:03:59,571 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16804267466068268
2023-01-07 09:03:59,572 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.9988154768943787 param sum :: 675.9112548828125
2023-01-07 09:03:59,572 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1822.38916015625
2023-01-07 09:03:59,573 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,573 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 662.1426391601562
2023-01-07 09:03:59,573 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.037530772387981415
2023-01-07 09:03:59,574 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 662.0346069335938 param sum :: 2924.210205078125
2023-01-07 09:03:59,574 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,574 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,574 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -33268.6484375
2023-01-07 09:03:59,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,574 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -5382.61572265625
2023-01-07 09:03:59,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31512942910194397
2023-01-07 09:03:59,575 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: -0.0011475541396066546 param sum :: 509.2591247558594
2023-01-07 09:03:59,575 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -33268.6484375
2023-01-07 09:03:59,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,576 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -5382.29736328125
2023-01-07 09:03:59,576 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08827191591262817
2023-01-07 09:03:59,577 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: -5382.2216796875 param sum :: -52362.96875
2023-01-07 09:03:59,577 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101844.5078125
2023-01-07 09:03:59,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,577 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.453763723373413
2023-01-07 09:03:59,577 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.499709129333496
2023-01-07 09:03:59,578 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 3.2267892360687256 param sum :: 2718.776123046875
2023-01-07 09:03:59,578 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,579 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101844.5078125
2023-01-07 09:03:59,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,579 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 2.6884870529174805
2023-01-07 09:03:59,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7708039283752441
2023-01-07 09:03:59,580 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 2.1160731315612793 param sum :: -170135.96875
2023-01-07 09:03:59,580 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11515.291015625
2023-01-07 09:03:59,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,580 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1075.497802734375
2023-01-07 09:03:59,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.013647175393998623
2023-01-07 09:03:59,581 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -0.04734804481267929 param sum :: 677.567626953125
2023-01-07 09:03:59,582 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,582 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11515.291015625
2023-01-07 09:03:59,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,582 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1075.50244140625
2023-01-07 09:03:59,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006425921805202961
2023-01-07 09:03:59,583 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: -1075.484375 param sum :: -19454.23828125
2023-01-07 09:03:59,583 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1413.353271484375
2023-01-07 09:03:59,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,583 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2139.857177734375
2023-01-07 09:03:59,584 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39882761240005493
2023-01-07 09:03:59,585 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.00044308294309303164 param sum :: 679.672119140625
2023-01-07 09:03:59,585 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1413.353271484375
2023-01-07 09:03:59,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,585 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 2140.614501953125
2023-01-07 09:03:59,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8576368689537048
2023-01-07 09:03:59,586 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 2142.4130859375 param sum :: 2539.8095703125
2023-01-07 09:03:59,586 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,586 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,586 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149775.90625
2023-01-07 09:03:59,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,587 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -10509.2294921875
2023-01-07 09:03:59,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:03:59,588 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 199.9153289794922 param sum :: 2563.724609375
2023-01-07 09:03:59,588 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149775.90625
2023-01-07 09:03:59,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,588 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -10509.2294921875
2023-01-07 09:03:59,588 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1175870895385742e-08
2023-01-07 09:03:59,589 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: -10509.2294921875 param sum :: -247223.78125
2023-01-07 09:03:59,589 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:03:59,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:03:59,590 > [DEBUG] 0 :: 121.77950286865234
2023-01-07 09:03:59,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,593 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.010009735822677612
2023-01-07 09:03:59,593 > [DEBUG] 0 :: before allreduce fusion buffer :: -421.0264892578125
2023-01-07 09:03:59,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,595 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 43.75677490234375
2023-01-07 09:03:59,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,595 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 798.2936401367188
2023-01-07 09:03:59,596 > [DEBUG] 0 :: before allreduce fusion buffer :: 959.4923706054688
2023-01-07 09:03:59,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,599 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 46.05046081542969
2023-01-07 09:03:59,599 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.003145884722471237
2023-01-07 09:03:59,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,601 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.3154921233654022
2023-01-07 09:03:59,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,602 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -55.24650573730469
2023-01-07 09:03:59,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3276657462120056
2023-01-07 09:03:59,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,604 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 44.81452178955078
2023-01-07 09:03:59,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.346951754996553e-05
2023-01-07 09:03:59,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,605 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.8341495990753174
2023-01-07 09:03:59,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,605 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -56.8048095703125
2023-01-07 09:03:59,605 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8365063667297363
2023-01-07 09:03:59,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,606 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -1.8736472129821777
2023-01-07 09:03:59,607 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0043857283890247345
2023-01-07 09:03:59,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,607 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 35.518192291259766
2023-01-07 09:03:59,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,608 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -73.83645629882812
2023-01-07 09:03:59,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.2749137878418
2023-01-07 09:03:59,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,609 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 14.570712089538574
2023-01-07 09:03:59,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.011472581885755062
2023-01-07 09:03:59,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.5616291165351868
2023-01-07 09:03:59,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,610 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -75.42048645019531
2023-01-07 09:03:59,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.546414852142334
2023-01-07 09:03:59,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,611 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 405.8489990234375
2023-01-07 09:03:59,612 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07684701681137085
2023-01-07 09:03:59,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,612 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.06321132183074951
2023-01-07 09:03:59,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,613 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 438.21148681640625
2023-01-07 09:03:59,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04794907569885254
2023-01-07 09:03:59,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,614 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 90.37997436523438
2023-01-07 09:03:59,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.013418206945061684
2023-01-07 09:03:59,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 25.99003028869629
2023-01-07 09:03:59,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,615 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 129.150634765625
2023-01-07 09:03:59,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.27614212036133
2023-01-07 09:03:59,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,617 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -58.58485794067383
2023-01-07 09:03:59,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03311222419142723
2023-01-07 09:03:59,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,617 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 33.36390686035156
2023-01-07 09:03:59,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,618 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -58.58485794067383
2023-01-07 09:03:59,618 > [DEBUG] 0 :: before allreduce fusion buffer :: 32.61472702026367
2023-01-07 09:03:59,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,619 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -354.2950439453125
2023-01-07 09:03:59,619 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03930492699146271
2023-01-07 09:03:59,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -1.781015396118164
2023-01-07 09:03:59,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,620 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -263.90277099609375
2023-01-07 09:03:59,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.320554733276367
2023-01-07 09:03:59,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,622 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -77.93071746826172
2023-01-07 09:03:59,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0027946336194872856
2023-01-07 09:03:59,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -4.427663803100586
2023-01-07 09:03:59,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,623 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -77.93071746826172
2023-01-07 09:03:59,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.984911918640137
2023-01-07 09:03:59,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,624 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.6200343370437622
2023-01-07 09:03:59,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01835554838180542
2023-01-07 09:03:59,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 5.027371406555176
2023-01-07 09:03:59,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,625 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.5856503248214722
2023-01-07 09:03:59,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.178592681884766
2023-01-07 09:03:59,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,627 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -3.9905593395233154
2023-01-07 09:03:59,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005002163350582123
2023-01-07 09:03:59,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -4.694428443908691
2023-01-07 09:03:59,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 0.10710406303405762
2023-01-07 09:03:59,628 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.7227888107299805
2023-01-07 09:03:59,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 245.37255859375
2023-01-07 09:03:59,630 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08518233895301819
2023-01-07 09:03:59,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,630 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -8.721025466918945
2023-01-07 09:03:59,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 244.78521728515625
2023-01-07 09:03:59,631 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.105772018432617
2023-01-07 09:03:59,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,632 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -10.871692657470703
2023-01-07 09:03:59,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16649246215820312
2023-01-07 09:03:59,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -4.5647382736206055
2023-01-07 09:03:59,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -19.990291595458984
2023-01-07 09:03:59,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.29078483581543
2023-01-07 09:03:59,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -113.82637023925781
2023-01-07 09:03:59,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012299653142690659
2023-01-07 09:03:59,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,635 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.7876720428466797
2023-01-07 09:03:59,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,636 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -116.9127197265625
2023-01-07 09:03:59,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7778451442718506
2023-01-07 09:03:59,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 148.9423828125
2023-01-07 09:03:59,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.001185835339128971
2023-01-07 09:03:59,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 3.3918821811676025
2023-01-07 09:03:59,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,638 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 145.26866149902344
2023-01-07 09:03:59,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4091243743896484
2023-01-07 09:03:59,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -25.148906707763672
2023-01-07 09:03:59,640 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16386190056800842
2023-01-07 09:03:59,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -7.064281463623047
2023-01-07 09:03:59,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,641 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -25.50570297241211
2023-01-07 09:03:59,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.798720359802246
2023-01-07 09:03:59,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -627.0841064453125
2023-01-07 09:03:59,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1274309754371643
2023-01-07 09:03:59,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 3.924199104309082
2023-01-07 09:03:59,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -632.8050537109375
2023-01-07 09:03:59,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.846231460571289
2023-01-07 09:03:59,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,644 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -567.052001953125
2023-01-07 09:03:59,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.027475304901599884
2023-01-07 09:03:59,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -0.7637958526611328
2023-01-07 09:03:59,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -567.9796142578125
2023-01-07 09:03:59,646 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9179277420043945
2023-01-07 09:03:59,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,647 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 751.5335693359375
2023-01-07 09:03:59,647 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20572082698345184
2023-01-07 09:03:59,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,648 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 1.817530632019043
2023-01-07 09:03:59,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,648 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 749.5887451171875
2023-01-07 09:03:59,648 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5544514656066895
2023-01-07 09:03:59,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,649 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -528.7815551757812
2023-01-07 09:03:59,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.014587447047233582
2023-01-07 09:03:59,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 1.7159446477890015
2023-01-07 09:03:59,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,650 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -529.0238037109375
2023-01-07 09:03:59,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2177385091781616
2023-01-07 09:03:59,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,652 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -49.289459228515625
2023-01-07 09:03:59,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10156719386577606
2023-01-07 09:03:59,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -1.4343671798706055
2023-01-07 09:03:59,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,653 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -47.384674072265625
2023-01-07 09:03:59,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2395358085632324
2023-01-07 09:03:59,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,654 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 633.1040649414062
2023-01-07 09:03:59,654 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16401521861553192
2023-01-07 09:03:59,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 18.13502311706543
2023-01-07 09:03:59,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,655 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 633.62158203125
2023-01-07 09:03:59,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.04281997680664
2023-01-07 09:03:59,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,657 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2488.84521484375
2023-01-07 09:03:59,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45315003395080566
2023-01-07 09:03:59,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -33.392066955566406
2023-01-07 09:03:59,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,658 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2475.5126953125
2023-01-07 09:03:59,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.643470764160156
2023-01-07 09:03:59,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,659 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 3.9043900966644287
2023-01-07 09:03:59,660 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05688612908124924
2023-01-07 09:03:59,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -17.20442008972168
2023-01-07 09:03:59,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20508922636508942
2023-01-07 09:03:59,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -14.373208999633789
2023-01-07 09:03:59,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2649292945861816
2023-01-07 09:03:59,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -141.62562561035156
2023-01-07 09:03:59,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -35.58675003051758
2023-01-07 09:03:59,663 > [DEBUG] 0 :: before allreduce fusion buffer :: -142.5008544921875
2023-01-07 09:03:59,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -85.7421646118164
2023-01-07 09:03:59,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04747864231467247
2023-01-07 09:03:59,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 21.668426513671875
2023-01-07 09:03:59,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -141.1439666748047
2023-01-07 09:03:59,665 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.809833526611328
2023-01-07 09:03:59,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,666 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1151.58740234375
2023-01-07 09:03:59,666 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.35936981439590454
2023-01-07 09:03:59,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -16.527591705322266
2023-01-07 09:03:59,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1170.4400634765625
2023-01-07 09:03:59,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.695953369140625
2023-01-07 09:03:59,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,669 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 164.68142700195312
2023-01-07 09:03:59,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.687082827091217
2023-01-07 09:03:59,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 177.06838989257812
2023-01-07 09:03:59,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.246306896209717
2023-01-07 09:03:59,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,671 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -5386.33203125
2023-01-07 09:03:59,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1425897777080536
2023-01-07 09:03:59,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,672 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,672 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -5376.4130859375
2023-01-07 09:03:59,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.093561172485352
2023-01-07 09:03:59,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,673 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 484.03411865234375
2023-01-07 09:03:59,673 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6425997018814087
2023-01-07 09:03:59,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,674 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 575.1707153320312
2023-01-07 09:03:59,674 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.566575050354004
2023-01-07 09:03:59,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,675 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 114.66439819335938
2023-01-07 09:03:59,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8930319547653198
2023-01-07 09:03:59,675 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,675 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,676 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -60.125308990478516
2023-01-07 09:03:59,676 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.497819423675537
2023-01-07 09:03:59,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,677 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 111.28218841552734
2023-01-07 09:03:59,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.049236685037612915
2023-01-07 09:03:59,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,678 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 225.2408905029297
2023-01-07 09:03:59,678 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.213036298751831
2023-01-07 09:03:59,678 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,678 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,679 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 759.462646484375
2023-01-07 09:03:59,679 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.9090704917907715
2023-01-07 09:03:59,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,679 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 921.9149169921875
2023-01-07 09:03:59,680 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.92428207397461
2023-01-07 09:03:59,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,680 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -258.3384094238281
2023-01-07 09:03:59,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3304320573806763
2023-01-07 09:03:59,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,681 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -416.9216003417969
2023-01-07 09:03:59,682 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1651254892349243
2023-01-07 09:03:59,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,682 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -1040.970458984375
2023-01-07 09:03:59,683 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.078680515289307
2023-01-07 09:03:59,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,683 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,683 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -59.68624496459961
2023-01-07 09:03:59,683 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,684 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -919.3226318359375
2023-01-07 09:03:59,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.880001068115234
2023-01-07 09:03:59,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,685 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -74.72540283203125
2023-01-07 09:03:59,686 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.466456413269043
2023-01-07 09:03:59,686 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,686 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,686 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -74.32048034667969
2023-01-07 09:03:59,686 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.91710662841797
2023-01-07 09:03:59,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,687 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 658.5057373046875
2023-01-07 09:03:59,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.136760711669922
2023-01-07 09:03:59,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,688 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 708.7958984375
2023-01-07 09:03:59,688 > [DEBUG] 0 :: before allreduce fusion buffer :: 298.0982666015625
2023-01-07 09:03:59,689 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,689 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,689 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1333.36376953125
2023-01-07 09:03:59,689 > [DEBUG] 0 :: before allreduce fusion buffer :: -148.27584838867188
2023-01-07 09:03:59,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -44.85185241699219
2023-01-07 09:03:59,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,690 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -1135.922607421875
2023-01-07 09:03:59,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -456.9031982421875
2023-01-07 09:03:59,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,692 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -8958.947265625
2023-01-07 09:03:59,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.170171737670898
2023-01-07 09:03:59,692 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,692 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,693 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -8943.236328125
2023-01-07 09:03:59,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.408263206481934
2023-01-07 09:03:59,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -827.2271118164062
2023-01-07 09:03:59,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.83469009399414
2023-01-07 09:03:59,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,694 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1356.2767333984375
2023-01-07 09:03:59,695 > [DEBUG] 0 :: before allreduce fusion buffer :: 162.92596435546875
2023-01-07 09:03:59,695 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,695 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4397.33740234375
2023-01-07 09:03:59,696 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.198543548583984
2023-01-07 09:03:59,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,696 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -3.43408203125
2023-01-07 09:03:59,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,697 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3895.330322265625
2023-01-07 09:03:59,697 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.899104118347168
2023-01-07 09:03:59,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,698 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,698 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 4379.71630859375
2023-01-07 09:03:59,698 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.699684143066406
2023-01-07 09:03:59,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,699 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 36.110897064208984
2023-01-07 09:03:59,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1083.141357421875
2023-01-07 09:03:59,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,699 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3962.668212890625
2023-01-07 09:03:59,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 105.80125427246094
2023-01-07 09:03:59,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,701 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3962.667236328125
2023-01-07 09:03:59,701 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0432301759719849
2023-01-07 09:03:59,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,702 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1462.416015625
2023-01-07 09:03:59,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.2568473815918
2023-01-07 09:03:59,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,703 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -547.4649047851562
2023-01-07 09:03:59,703 > [DEBUG] 0 :: before allreduce fusion buffer :: 156.7969970703125
2023-01-07 09:03:59,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,704 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 19.588668823242188
2023-01-07 09:03:59,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,704 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -979.6574096679688
2023-01-07 09:03:59,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 419.71051025390625
2023-01-07 09:03:59,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,705 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1359.14697265625
2023-01-07 09:03:59,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 145.85870361328125
2023-01-07 09:03:59,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: 32.15375900268555
2023-01-07 09:03:59,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,706 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -385.4136962890625
2023-01-07 09:03:59,706 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.09484100341797
2023-01-07 09:03:59,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,708 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1546.318603515625
2023-01-07 09:03:59,708 > [DEBUG] 0 :: before allreduce fusion buffer :: -339.6510314941406
2023-01-07 09:03:59,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,708 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 283.3067321777344
2023-01-07 09:03:59,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,709 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3571.881103515625
2023-01-07 09:03:59,709 > [DEBUG] 0 :: before allreduce fusion buffer :: -274.96435546875
2023-01-07 09:03:59,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,710 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1706.2442626953125
2023-01-07 09:03:59,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 98.8162841796875
2023-01-07 09:03:59,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,711 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 20.87078857421875
2023-01-07 09:03:59,711 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.50267791748047
2023-01-07 09:03:59,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,712 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2364.0615234375
2023-01-07 09:03:59,712 > [DEBUG] 0 :: before allreduce fusion buffer :: -184.23797607421875
2023-01-07 09:03:59,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 14.57400131225586
2023-01-07 09:03:59,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -144.4110565185547
2023-01-07 09:03:59,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,713 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2922.27880859375
2023-01-07 09:03:59,713 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,713 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3629.622314453125
2023-01-07 09:03:59,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 213.16981506347656
2023-01-07 09:03:59,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,715 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2801.9638671875
2023-01-07 09:03:59,715 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.53724670410156
2023-01-07 09:03:59,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,716 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1011.01611328125
2023-01-07 09:03:59,716 > [DEBUG] 0 :: before allreduce fusion buffer :: 682.7265014648438
2023-01-07 09:03:59,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,717 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2545.904052734375
2023-01-07 09:03:59,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -123.20893096923828
2023-01-07 09:03:59,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,717 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 897.89404296875
2023-01-07 09:03:59,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,718 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3567.042236328125
2023-01-07 09:03:59,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 337.9484558105469
2023-01-07 09:03:59,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,719 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 478.83123779296875
2023-01-07 09:03:59,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -122.81178283691406
2023-01-07 09:03:59,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,720 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -41.3023681640625
2023-01-07 09:03:59,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,720 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 2.552398681640625
2023-01-07 09:03:59,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -182.99632263183594
2023-01-07 09:03:59,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,721 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 263.1190185546875
2023-01-07 09:03:59,721 > [DEBUG] 0 :: before allreduce fusion buffer :: -213.89340209960938
2023-01-07 09:03:59,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,722 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 263.1190185546875
2023-01-07 09:03:59,722 > [DEBUG] 0 :: before allreduce fusion buffer :: -190.39419555664062
2023-01-07 09:03:59,726 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:03:59,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,727 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -2269.498046875
2023-01-07 09:03:59,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,727 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1040.733642578125
2023-01-07 09:03:59,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,728 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 3574.456298828125
2023-01-07 09:03:59,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,728 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1036.9664306640625
2023-01-07 09:03:59,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -7954.849609375
2023-01-07 09:03:59,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,729 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 480.40283203125
2023-01-07 09:03:59,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,730 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 940.936279296875
2023-01-07 09:03:59,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,730 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 120.80902862548828
2023-01-07 09:03:59,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -497.68304443359375
2023-01-07 09:03:59,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,731 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -626.6201782226562
2023-01-07 09:03:59,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,732 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 1093.204345703125
2023-01-07 09:03:59,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,732 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 162.03793334960938
2023-01-07 09:03:59,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -156.9062042236328
2023-01-07 09:03:59,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,733 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 504.603515625
2023-01-07 09:03:59,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -5701.37890625
2023-01-07 09:03:59,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,734 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 170.14834594726562
2023-01-07 09:03:59,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,734 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1162.5478515625
2023-01-07 09:03:59,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -132.6344451904297
2023-01-07 09:03:59,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -43.81349182128906
2023-01-07 09:03:59,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -38.253787994384766
2023-01-07 09:03:59,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2478.158203125
2023-01-07 09:03:59,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 633.2850341796875
2023-01-07 09:03:59,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -46.371002197265625
2023-01-07 09:03:59,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -528.33447265625
2023-01-07 09:03:59,736 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,736 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,737 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 749.7861328125
2023-01-07 09:03:59,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,737 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -567.3140258789062
2023-01-07 09:03:59,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -12608.2734375
2023-01-07 09:03:59,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -634.6781005859375
2023-01-07 09:03:59,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.929706573486328
2023-01-07 09:03:59,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 144.06161499023438
2023-01-07 09:03:59,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -117.16313171386719
2023-01-07 09:03:59,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -20.331153869628906
2023-01-07 09:03:59,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 246.1929168701172
2023-01-07 09:03:59,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.4761576652526855
2023-01-07 09:03:59,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,740 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1.7148524522781372
2023-01-07 09:03:59,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,740 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -21.02911376953125
2023-01-07 09:03:59,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -580.559326171875
2023-01-07 09:03:59,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,741 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -254.10223388671875
2023-01-07 09:03:59,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,741 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.105350494384766
2023-01-07 09:03:59,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,741 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 132.7244873046875
2023-01-07 09:03:59,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,742 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 474.1297302246094
2023-01-07 09:03:59,742 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.29727935791016
2023-01-07 09:03:59,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,742 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -67.41059875488281
2023-01-07 09:03:59,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,743 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -73.73590087890625
2023-01-07 09:03:59,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,743 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -56.80577850341797
2023-01-07 09:03:59,743 > [DEBUG] 0 :: before allreduce fusion buffer :: 212.55441284179688
2023-01-07 09:03:59,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,743 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -56.326255798339844
2023-01-07 09:03:59,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:03:59,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:03:59,744 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 1618.70361328125
2023-01-07 09:03:59,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -302.6845703125
2023-01-07 09:04:00,589 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 88.92398071289062 param sum :: 205.45994567871094
2023-01-07 09:04:00,589 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,589 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,589 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 62.65672302246094
2023-01-07 09:04:00,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,589 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -12.770135879516602
2023-01-07 09:04:00,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,590 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1836.823486328125
2023-01-07 09:04:00,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 72.60539245605469
2023-01-07 09:04:00,591 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: -61.014007568359375 param sum :: 62.4830322265625
2023-01-07 09:04:00,591 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,591 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,591 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -39.066436767578125
2023-01-07 09:04:00,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,591 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -618.5233154296875
2023-01-07 09:04:00,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,592 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2531.384033203125
2023-01-07 09:04:00,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -614.1826782226562
2023-01-07 09:04:00,593 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -1192.042236328125 param sum :: -38.85418701171875
2023-01-07 09:04:00,593 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,593 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 114.8167953491211
2023-01-07 09:04:00,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,593 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2541.1181640625
2023-01-07 09:04:00,593 > [DEBUG] 0 :: before allreduce fusion buffer :: 137.5189971923828
2023-01-07 09:04:00,594 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -18.433204650878906 param sum :: 59.12098693847656
2023-01-07 09:04:00,594 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 114.8167953491211
2023-01-07 09:04:00,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,595 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2569.26953125
2023-01-07 09:04:00,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 124.00421905517578
2023-01-07 09:04:00,596 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: -2587.6962890625 param sum :: 166.16482543945312
2023-01-07 09:04:00,596 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,596 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,596 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.3573989868164
2023-01-07 09:04:00,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,596 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 14.187658309936523
2023-01-07 09:04:00,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,597 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -253.689208984375
2023-01-07 09:04:00,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,597 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -3132.022705078125
2023-01-07 09:04:00,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -186.8107452392578
2023-01-07 09:04:00,598 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -4.012451171875 param sum :: 64.2445297241211
2023-01-07 09:04:00,598 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,598 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,598 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 276.6053466796875
2023-01-07 09:04:00,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,599 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2527.424072265625
2023-01-07 09:04:00,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 185.9796142578125
2023-01-07 09:04:00,599 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -279.1527099609375 param sum :: -42.06477737426758
2023-01-07 09:04:00,599 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,600 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.38912963867188
2023-01-07 09:04:00,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,600 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -32.471805572509766
2023-01-07 09:04:00,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,600 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -1113.95849609375
2023-01-07 09:04:00,600 > [DEBUG] 0 :: before allreduce fusion buffer :: -402.42095947265625
2023-01-07 09:04:00,601 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: -29.863597869873047 param sum :: 254.69332885742188
2023-01-07 09:04:00,601 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 221.1082000732422
2023-01-07 09:04:00,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,602 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -272.3481140136719
2023-01-07 09:04:00,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.517581939697266
2023-01-07 09:04:00,602 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1081.969970703125 param sum :: 298.9599609375
2023-01-07 09:04:00,603 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 221.1082000732422
2023-01-07 09:04:00,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,603 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -48.9375
2023-01-07 09:04:00,603 > [DEBUG] 0 :: before allreduce fusion buffer :: -359.7166748046875
2023-01-07 09:04:00,604 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 642.2392578125 param sum :: 229.53749084472656
2023-01-07 09:04:00,604 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,604 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,604 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 221.1082000732422
2023-01-07 09:04:00,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,604 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -259.39019775390625
2023-01-07 09:04:00,604 > [DEBUG] 0 :: before allreduce fusion buffer :: -197.63348388671875
2023-01-07 09:04:00,605 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -338.4713134765625 param sum :: 54.6307373046875
2023-01-07 09:04:00,605 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,605 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,605 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.284759521484375
2023-01-07 09:04:00,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,606 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -31.902761459350586
2023-01-07 09:04:00,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,606 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -264.6147155761719
2023-01-07 09:04:00,606 > [DEBUG] 0 :: before allreduce fusion buffer :: 238.5076446533203
2023-01-07 09:04:00,607 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: -32.8891716003418 param sum :: 61.35986328125
2023-01-07 09:04:00,607 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,607 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 221.1082000732422
2023-01-07 09:04:00,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,608 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -199.72329711914062
2023-01-07 09:04:00,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -346.4526672363281
2023-01-07 09:04:00,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 149.91366577148438
2023-01-07 09:04:00,609 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -170.4554901123047 param sum :: 237.05767822265625
2023-01-07 09:04:00,609 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 63.397705078125
2023-01-07 09:04:00,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,609 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -18.490657806396484
2023-01-07 09:04:00,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,610 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 208.49388122558594
2023-01-07 09:04:00,610 > [DEBUG] 0 :: before allreduce fusion buffer :: 534.7493896484375
2023-01-07 09:04:00,611 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -62.98999786376953 param sum :: 63.850425720214844
2023-01-07 09:04:00,611 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 276.6053466796875
2023-01-07 09:04:00,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,611 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2485.944091796875
2023-01-07 09:04:00,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -159.38656616210938
2023-01-07 09:04:00,612 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 110.9969482421875 param sum :: 147.274658203125
2023-01-07 09:04:00,612 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 360.6060485839844
2023-01-07 09:04:00,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,612 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -570.1365966796875
2023-01-07 09:04:00,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -247.77273559570312
2023-01-07 09:04:00,614 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 577.0580444335938 param sum :: 244.48977661132812
2023-01-07 09:04:00,614 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,614 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 360.6060485839844
2023-01-07 09:04:00,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,614 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -617.0631103515625
2023-01-07 09:04:00,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,614 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2363.50048828125
2023-01-07 09:04:00,614 > [DEBUG] 0 :: before allreduce fusion buffer :: -564.7490234375
2023-01-07 09:04:00,615 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -595.3912963867188 param sum :: 446.3773498535156
2023-01-07 09:04:00,615 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,616 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 276.6053466796875
2023-01-07 09:04:00,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,616 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2367.209716796875
2023-01-07 09:04:00,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -128.91310119628906
2023-01-07 09:04:00,617 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 120.49760437011719 param sum :: 64.17237091064453
2023-01-07 09:04:00,617 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,617 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 276.6053466796875
2023-01-07 09:04:00,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,617 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2454.72021484375
2023-01-07 09:04:00,617 > [DEBUG] 0 :: before allreduce fusion buffer :: -135.72096252441406
2023-01-07 09:04:00,618 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -2424.03955078125 param sum :: 395.94403076171875
2023-01-07 09:04:00,618 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.03046417236328
2023-01-07 09:04:00,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 11.653524398803711
2023-01-07 09:04:00,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,619 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 506.6433410644531
2023-01-07 09:04:00,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,619 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1767.90771484375
2023-01-07 09:04:00,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -237.77099609375
2023-01-07 09:04:00,621 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19.73723602294922 param sum :: 64.21100616455078
2023-01-07 09:04:00,621 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,621 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 783.8806762695312
2023-01-07 09:04:00,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -279.11846923828125
2023-01-07 09:04:00,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.125005722045898
2023-01-07 09:04:00,622 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 370.1302490234375 param sum :: -73.8224868774414
2023-01-07 09:04:00,622 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,622 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 909.7977905273438
2023-01-07 09:04:00,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,622 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1964.329833984375
2023-01-07 09:04:00,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -338.30755615234375
2023-01-07 09:04:00,623 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 2.6134986877441406 param sum :: 272.7147216796875
2023-01-07 09:04:00,624 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,624 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 783.8806762695312
2023-01-07 09:04:00,624 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -252.66265869140625
2023-01-07 09:04:00,624 > [DEBUG] 0 :: before allreduce fusion buffer :: 301.0567626953125
2023-01-07 09:04:00,625 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: -2391.70166015625 param sum :: 1173.3895263671875
2023-01-07 09:04:00,625 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,625 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 783.8806762695312
2023-01-07 09:04:00,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,625 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -302.8144226074219
2023-01-07 09:04:00,625 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.279386520385742
2023-01-07 09:04:00,626 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -8.990346908569336 param sum :: 134.72830200195312
2023-01-07 09:04:00,626 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,626 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 783.8806762695312
2023-01-07 09:04:00,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,627 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,627 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -161.2683868408203
2023-01-07 09:04:00,627 > [DEBUG] 0 :: before allreduce fusion buffer :: 130.10711669921875
2023-01-07 09:04:00,628 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: -120.92620849609375 param sum :: 748.0128784179688
2023-01-07 09:04:00,628 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 125.51715850830078
2023-01-07 09:04:00,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,628 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: -77.34521484375
2023-01-07 09:04:00,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,628 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 15.37042236328125
2023-01-07 09:04:00,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 97.41780853271484
2023-01-07 09:04:00,629 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -51.417808532714844 param sum :: 125.86052703857422
2023-01-07 09:04:00,630 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,630 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,630 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 539.8554077148438
2023-01-07 09:04:00,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,630 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 7.2193145751953125
2023-01-07 09:04:00,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -103.26006317138672
2023-01-07 09:04:00,631 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 53.06126403808594 param sum :: 629.4091796875
2023-01-07 09:04:00,631 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 509.7526550292969
2023-01-07 09:04:00,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,631 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -14.91903305053711
2023-01-07 09:04:00,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,632 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 873.5939331054688
2023-01-07 09:04:00,632 > [DEBUG] 0 :: before allreduce fusion buffer :: -80.65335083007812
2023-01-07 09:04:00,633 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -29.64699935913086 param sum :: 510.36932373046875
2023-01-07 09:04:00,633 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,633 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,633 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 302.08636474609375
2023-01-07 09:04:00,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,633 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1079.9814453125
2023-01-07 09:04:00,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.84297180175781
2023-01-07 09:04:00,634 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1338.883544921875 param sum :: 313.4145812988281
2023-01-07 09:04:00,634 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 382.18975830078125
2023-01-07 09:04:00,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,634 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 847.11962890625
2023-01-07 09:04:00,635 > [DEBUG] 0 :: before allreduce fusion buffer :: -339.4811706542969
2023-01-07 09:04:00,636 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 52.35411071777344 param sum :: 563.646484375
2023-01-07 09:04:00,636 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,636 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,636 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 382.18975830078125
2023-01-07 09:04:00,636 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 514.8802490234375
2023-01-07 09:04:00,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -63.876670837402344
2023-01-07 09:04:00,637 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 426.327392578125 param sum :: 548.2166748046875
2023-01-07 09:04:00,637 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.72686767578125
2023-01-07 09:04:00,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,637 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 2.551510810852051
2023-01-07 09:04:00,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,638 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,638 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 579.1322021484375
2023-01-07 09:04:00,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 121.5587158203125
2023-01-07 09:04:00,639 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 2.503354072570801 param sum :: 127.26235961914062
2023-01-07 09:04:00,639 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,639 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,639 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 709.8131103515625
2023-01-07 09:04:00,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,639 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 594.3856201171875
2023-01-07 09:04:00,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -148.29539489746094
2023-01-07 09:04:00,640 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 580.2015991210938 param sum :: 814.1490478515625
2023-01-07 09:04:00,640 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 127.32525634765625
2023-01-07 09:04:00,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,641 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 49.817386627197266
2023-01-07 09:04:00,641 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,641 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,641 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -257.00238037109375
2023-01-07 09:04:00,641 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.554611206054688
2023-01-07 09:04:00,642 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 58.42691421508789 param sum :: 126.06207275390625
2023-01-07 09:04:00,642 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,642 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 555.1539306640625
2023-01-07 09:04:00,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,642 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -105.69779968261719
2023-01-07 09:04:00,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -82.19929504394531
2023-01-07 09:04:00,644 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -134.92333984375 param sum :: 733.5897216796875
2023-01-07 09:04:00,644 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,644 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,644 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 391.7188720703125
2023-01-07 09:04:00,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,644 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -506.0287170410156
2023-01-07 09:04:00,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9135117530822754
2023-01-07 09:04:00,645 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -185.02783203125 param sum :: 607.460205078125
2023-01-07 09:04:00,645 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 391.7188720703125
2023-01-07 09:04:00,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,645 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -511.71026611328125
2023-01-07 09:04:00,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.272331476211548
2023-01-07 09:04:00,646 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -515.1015625 param sum :: 631.8576049804688
2023-01-07 09:04:00,647 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,647 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,647 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 129.12490844726562
2023-01-07 09:04:00,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,647 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 36.02159881591797
2023-01-07 09:04:00,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,647 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -633.2693481445312
2023-01-07 09:04:00,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.658477783203125
2023-01-07 09:04:00,648 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 83.73419189453125 param sum :: 127.07914733886719
2023-01-07 09:04:00,648 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 588.640625
2023-01-07 09:04:00,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,649 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -836.563232421875
2023-01-07 09:04:00,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 109.58529663085938
2023-01-07 09:04:00,650 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -975.4808959960938 param sum :: 772.0977783203125
2023-01-07 09:04:00,650 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,650 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,650 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 129.92193603515625
2023-01-07 09:04:00,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,650 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 16.04299545288086
2023-01-07 09:04:00,650 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,650 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,650 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 1096.5147705078125
2023-01-07 09:04:00,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -43.75672149658203
2023-01-07 09:04:00,652 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: -11.16085433959961 param sum :: 130.95982360839844
2023-01-07 09:04:00,652 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 439.91705322265625
2023-01-07 09:04:00,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,652 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 1280.4658203125
2023-01-07 09:04:00,652 > [DEBUG] 0 :: before allreduce fusion buffer :: 54.467918395996094
2023-01-07 09:04:00,653 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 1381.4342041015625 param sum :: 271.1986083984375
2023-01-07 09:04:00,653 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,653 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,653 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 513.538818359375
2023-01-07 09:04:00,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,653 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -9.407621383666992
2023-01-07 09:04:00,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 163.80242919921875
2023-01-07 09:04:00,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -133.57809448242188
2023-01-07 09:04:00,655 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 10.588993072509766 param sum :: 512.82666015625
2023-01-07 09:04:00,655 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,655 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 729.795654296875
2023-01-07 09:04:00,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 167.64163208007812
2023-01-07 09:04:00,655 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.533199310302734
2023-01-07 09:04:00,656 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 116.36111450195312 param sum :: 743.75537109375
2023-01-07 09:04:00,656 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,656 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 127.15335083007812
2023-01-07 09:04:00,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,657 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 3.801682472229004
2023-01-07 09:04:00,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,657 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -162.30389404296875
2023-01-07 09:04:00,657 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.870622634887695
2023-01-07 09:04:00,658 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 2.8741455078125 param sum :: 127.96773529052734
2023-01-07 09:04:00,658 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,658 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,658 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1132.4073486328125
2023-01-07 09:04:00,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,658 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -111.99224090576172
2023-01-07 09:04:00,658 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.5330681800842285
2023-01-07 09:04:00,660 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: -99.6817398071289 param sum :: 1292.0877685546875
2023-01-07 09:04:00,660 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 126.24015808105469
2023-01-07 09:04:00,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,660 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 8.498638153076172
2023-01-07 09:04:00,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,660 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 503.9151611328125
2023-01-07 09:04:00,661 > [DEBUG] 0 :: before allreduce fusion buffer :: 77.26344299316406
2023-01-07 09:04:00,661 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 21.670921325683594 param sum :: 124.71186828613281
2023-01-07 09:04:00,661 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 548.50048828125
2023-01-07 09:04:00,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,662 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 653.215576171875
2023-01-07 09:04:00,662 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.117916107177734
2023-01-07 09:04:00,663 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 754.2914428710938 param sum :: 388.5810546875
2023-01-07 09:04:00,663 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 510.72210693359375
2023-01-07 09:04:00,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,663 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -16.849315643310547
2023-01-07 09:04:00,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -2713.9150390625
2023-01-07 09:04:00,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -622.3272094726562
2023-01-07 09:04:00,665 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: -24.59806251525879 param sum :: 511.1766662597656
2023-01-07 09:04:00,665 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 830.7462768554688
2023-01-07 09:04:00,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -2539.2587890625
2023-01-07 09:04:00,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9774971008300781
2023-01-07 09:04:00,666 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: -2555.153076171875 param sum :: 1216.6943359375
2023-01-07 09:04:00,666 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.1586608886719
2023-01-07 09:04:00,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 29.98467254638672
2023-01-07 09:04:00,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,667 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1116.926025390625
2023-01-07 09:04:00,667 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.5582218170166
2023-01-07 09:04:00,668 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: -64.26002502441406 param sum :: 257.77374267578125
2023-01-07 09:04:00,668 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -39147.5546875
2023-01-07 09:04:00,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,668 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1208.18701171875
2023-01-07 09:04:00,669 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.291533946990967
2023-01-07 09:04:00,670 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: -1214.781005859375 param sum :: -50186.609375
2023-01-07 09:04:00,670 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,670 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,670 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 4431.8740234375
2023-01-07 09:04:00,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,670 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1162.5478515625
2023-01-07 09:04:00,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.094738006591797
2023-01-07 09:04:00,671 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 6.345844268798828 param sum :: 317.44110107421875
2023-01-07 09:04:00,671 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 4431.8740234375
2023-01-07 09:04:00,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,671 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1241.725830078125
2023-01-07 09:04:00,672 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.499992370605469
2023-01-07 09:04:00,673 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 1237.472900390625 param sum :: 5233.37109375
2023-01-07 09:04:00,673 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,673 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -53778.53125
2023-01-07 09:04:00,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,673 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -134.78883361816406
2023-01-07 09:04:00,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.274070739746094
2023-01-07 09:04:00,674 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: -1.1042757034301758 param sum :: 1263.4041748046875
2023-01-07 09:04:00,674 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -53778.53125
2023-01-07 09:04:00,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,674 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -128.43849182128906
2023-01-07 09:04:00,675 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.959171295166016
2023-01-07 09:04:00,676 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: -118.19891357421875 param sum :: -70469.6484375
2023-01-07 09:04:00,676 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -38102.03125
2023-01-07 09:04:00,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,676 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -46.01704406738281
2023-01-07 09:04:00,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.36442565917969
2023-01-07 09:04:00,677 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: -161.89251708984375 param sum :: 1324.130859375
2023-01-07 09:04:00,677 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,677 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -38102.03125
2023-01-07 09:04:00,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,677 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -47.962852478027344
2023-01-07 09:04:00,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.39797019958496
2023-01-07 09:04:00,678 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: -89.03720092773438 param sum :: -49930.1875
2023-01-07 09:04:00,679 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,679 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,679 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 250.20465087890625
2023-01-07 09:04:00,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,679 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 103.66447448730469
2023-01-07 09:04:00,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,679 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -36.02011489868164
2023-01-07 09:04:00,679 > [DEBUG] 0 :: before allreduce fusion buffer :: 128.7874755859375
2023-01-07 09:04:00,680 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 71.19627380371094 param sum :: 246.5004425048828
2023-01-07 09:04:00,680 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,681 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -93456.5625
2023-01-07 09:04:00,681 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,681 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,681 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -26.23611068725586
2023-01-07 09:04:00,681 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.836746215820312
2023-01-07 09:04:00,682 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: -26.14291763305664 param sum :: -122560.671875
2023-01-07 09:04:00,682 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,682 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19297.83984375
2023-01-07 09:04:00,682 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,682 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,682 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -38.257232666015625
2023-01-07 09:04:00,682 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.2185001373291
2023-01-07 09:04:00,683 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: -16.354774475097656 param sum :: 329.04144287109375
2023-01-07 09:04:00,683 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,683 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,684 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19297.83984375
2023-01-07 09:04:00,684 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,684 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,684 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -48.65416717529297
2023-01-07 09:04:00,684 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.64081382751465
2023-01-07 09:04:00,685 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: -54.18592071533203 param sum :: -25361.796875
2023-01-07 09:04:00,685 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,685 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,685 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1450.72216796875
2023-01-07 09:04:00,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,685 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 30.46759033203125
2023-01-07 09:04:00,685 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.293390274047852
2023-01-07 09:04:00,686 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 19.004913330078125 param sum :: 1337.9375
2023-01-07 09:04:00,686 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,686 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,687 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1450.72216796875
2023-01-07 09:04:00,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,687 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 26.390649795532227
2023-01-07 09:04:00,687 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.231727600097656
2023-01-07 09:04:00,688 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 24.75153923034668 param sum :: 1775.9720458984375
2023-01-07 09:04:00,688 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,688 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,688 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1021.884521484375
2023-01-07 09:04:00,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,688 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 50.22108840942383
2023-01-07 09:04:00,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.634981632232666
2023-01-07 09:04:00,689 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.3658428192138672 param sum :: 335.48590087890625
2023-01-07 09:04:00,689 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,689 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,690 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1021.884521484375
2023-01-07 09:04:00,690 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,690 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,690 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 46.254024505615234
2023-01-07 09:04:00,690 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.656899929046631
2023-01-07 09:04:00,691 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 47.936580657958984 param sum :: 1231.5350341796875
2023-01-07 09:04:00,691 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,691 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,691 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1677.99609375
2023-01-07 09:04:00,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,691 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -34.86506271362305
2023-01-07 09:04:00,691 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.462198257446289
2023-01-07 09:04:00,692 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 1.6989160776138306 param sum :: 340.94891357421875
2023-01-07 09:04:00,692 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,692 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,693 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1677.99609375
2023-01-07 09:04:00,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,693 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,693 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -30.206403732299805
2023-01-07 09:04:00,693 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.3492431640625
2023-01-07 09:04:00,694 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: -35.620887756347656 param sum :: 2260.162109375
2023-01-07 09:04:00,694 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,694 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,694 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1788.3935546875
2023-01-07 09:04:00,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,694 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 19.743345260620117
2023-01-07 09:04:00,694 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.811636447906494
2023-01-07 09:04:00,695 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: -0.05106830596923828 param sum :: 1370.0869140625
2023-01-07 09:04:00,695 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,695 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,696 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1788.3935546875
2023-01-07 09:04:00,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,696 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 20.293779373168945
2023-01-07 09:04:00,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.335704803466797
2023-01-07 09:04:00,697 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 27.56587028503418 param sum :: 2123.7626953125
2023-01-07 09:04:00,697 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,697 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,697 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2217.423828125
2023-01-07 09:04:00,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,697 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 190.99368286132812
2023-01-07 09:04:00,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.50297737121582
2023-01-07 09:04:00,698 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: -0.9782674312591553 param sum :: 340.86737060546875
2023-01-07 09:04:00,698 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,699 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,699 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2217.423828125
2023-01-07 09:04:00,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,699 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 191.37442016601562
2023-01-07 09:04:00,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2646255493164062
2023-01-07 09:04:00,700 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 190.74929809570312 param sum :: 2779.666259765625
2023-01-07 09:04:00,700 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,700 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,700 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1993.9332275390625
2023-01-07 09:04:00,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,700 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 104.20947265625
2023-01-07 09:04:00,700 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.463564872741699
2023-01-07 09:04:00,701 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: -0.24330759048461914 param sum :: 344.3209228515625
2023-01-07 09:04:00,702 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,702 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,702 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1993.9332275390625
2023-01-07 09:04:00,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,702 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 102.22139739990234
2023-01-07 09:04:00,702 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.36271595954895
2023-01-07 09:04:00,703 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 97.25590515136719 param sum :: 2574.779296875
2023-01-07 09:04:00,703 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,703 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,703 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1804.408935546875
2023-01-07 09:04:00,703 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,703 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,703 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1846.6378173828125
2023-01-07 09:04:00,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.926403045654297
2023-01-07 09:04:00,704 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: -7.327849388122559 param sum :: 1399.84619140625
2023-01-07 09:04:00,705 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,705 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,705 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1804.408935546875
2023-01-07 09:04:00,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,705 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1846.976806640625
2023-01-07 09:04:00,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.678426742553711
2023-01-07 09:04:00,706 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -1837.7957763671875 param sum :: 2534.858642578125
2023-01-07 09:04:00,706 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,706 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,706 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1818.0093994140625
2023-01-07 09:04:00,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,706 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1945.48193359375
2023-01-07 09:04:00,707 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.81616473197937
2023-01-07 09:04:00,707 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.9453344345092773 param sum :: 346.09454345703125
2023-01-07 09:04:00,707 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,708 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,708 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1818.0093994140625
2023-01-07 09:04:00,708 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,708 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,708 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1947.30859375
2023-01-07 09:04:00,708 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8481438755989075
2023-01-07 09:04:00,709 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -1947.64501953125 param sum :: 2114.90869140625
2023-01-07 09:04:00,709 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,709 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,709 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -2944.5654296875
2023-01-07 09:04:00,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,709 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2130.056396484375
2023-01-07 09:04:00,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.460348606109619
2023-01-07 09:04:00,710 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: -2.447035551071167 param sum :: 350.65283203125
2023-01-07 09:04:00,711 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,711 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,711 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -2944.5654296875
2023-01-07 09:04:00,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,711 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2135.66357421875
2023-01-07 09:04:00,711 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.577052593231201
2023-01-07 09:04:00,712 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: -2134.824462890625 param sum :: -3039.75830078125
2023-01-07 09:04:00,712 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,712 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,712 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -33744.6796875
2023-01-07 09:04:00,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,712 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -44.2682991027832
2023-01-07 09:04:00,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -42.07038879394531
2023-01-07 09:04:00,713 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: -2.894754648208618 param sum :: 1415.4630126953125
2023-01-07 09:04:00,714 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,714 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,714 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -33744.6796875
2023-01-07 09:04:00,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,714 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -29.46853256225586
2023-01-07 09:04:00,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.0591390132904053
2023-01-07 09:04:00,715 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -29.12936782836914 param sum :: -44065.0703125
2023-01-07 09:04:00,715 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,715 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,715 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -10119.3359375
2023-01-07 09:04:00,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,716 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2191.31689453125
2023-01-07 09:04:00,716 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12031510472297668
2023-01-07 09:04:00,717 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: -10.59432315826416 param sum :: 345.49072265625
2023-01-07 09:04:00,717 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,717 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,717 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -10119.3359375
2023-01-07 09:04:00,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2193.66455078125
2023-01-07 09:04:00,717 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9819672107696533
2023-01-07 09:04:00,718 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 2197.68798828125 param sum :: -13627.1650390625
2023-01-07 09:04:00,718 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,718 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,718 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37641.328125
2023-01-07 09:04:00,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.9309287071228027
2023-01-07 09:04:00,719 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3878684043884277
2023-01-07 09:04:00,720 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: -2.0139620304107666 param sum :: 350.23944091796875
2023-01-07 09:04:00,720 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,720 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,720 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37641.328125
2023-01-07 09:04:00,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 0.06930685043334961
2023-01-07 09:04:00,720 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1245626211166382
2023-01-07 09:04:00,721 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: -1.5461149215698242 param sum :: -49282.3359375
2023-01-07 09:04:00,721 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,721 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,721 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82426.2890625
2023-01-07 09:04:00,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,722 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -2.602681875228882
2023-01-07 09:04:00,722 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.962843418121338
2023-01-07 09:04:00,723 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 5.072500228881836 param sum :: 1425.1075439453125
2023-01-07 09:04:00,723 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,723 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,723 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82426.2890625
2023-01-07 09:04:00,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,723 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -0.2013411521911621
2023-01-07 09:04:00,723 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.285403251647949
2023-01-07 09:04:00,724 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: -1.1523072719573975 param sum :: -107978.390625
2023-01-07 09:04:00,724 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,724 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,724 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -69446.125
2023-01-07 09:04:00,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,725 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 573.6842651367188
2023-01-07 09:04:00,725 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007149088196456432
2023-01-07 09:04:00,726 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 18.302858352661133 param sum :: 685.466796875
2023-01-07 09:04:00,726 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,726 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,726 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -69446.125
2023-01-07 09:04:00,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,726 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 573.443359375
2023-01-07 09:04:00,726 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.026010049507021904
2023-01-07 09:04:00,727 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 572.9769897460938 param sum :: -91399.6484375
2023-01-07 09:04:00,727 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,727 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,728 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 2675.8544921875
2023-01-07 09:04:00,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,728 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -950.19482421875
2023-01-07 09:04:00,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2568881511688232
2023-01-07 09:04:00,729 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -6.495136767625809e-05 param sum :: 515.3056640625
2023-01-07 09:04:00,729 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,729 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,729 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 2675.8544921875
2023-01-07 09:04:00,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,729 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -951.727783203125
2023-01-07 09:04:00,729 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9520159363746643
2023-01-07 09:04:00,730 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -952.74462890625 param sum :: 3375.02490234375
2023-01-07 09:04:00,730 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,731 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,731 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -200224.90625
2023-01-07 09:04:00,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,731 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -6125.0380859375
2023-01-07 09:04:00,731 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.300453245639801
2023-01-07 09:04:00,732 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 134.44717407226562 param sum :: 2803.0068359375
2023-01-07 09:04:00,732 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,732 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,732 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -200224.90625
2023-01-07 09:04:00,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,732 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -6125.9052734375
2023-01-07 09:04:00,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07472442090511322
2023-01-07 09:04:00,733 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: -6126.923828125 param sum :: -261682.15625
2023-01-07 09:04:00,733 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,734 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,734 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -42901.421875
2023-01-07 09:04:00,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,734 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 242.76197814941406
2023-01-07 09:04:00,734 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.671005368232727
2023-01-07 09:04:00,735 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: -0.029314424842596054 param sum :: 2911.94091796875
2023-01-07 09:04:00,735 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,735 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,735 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -42901.421875
2023-01-07 09:04:00,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,735 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 245.43084716796875
2023-01-07 09:04:00,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05770394951105118
2023-01-07 09:04:00,736 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 246.06295776367188 param sum :: -56544.53125
2023-01-07 09:04:00,737 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,737 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,737 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 2924.210205078125
2023-01-07 09:04:00,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,737 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -13881.994140625
2023-01-07 09:04:00,737 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9696455597877502
2023-01-07 09:04:00,738 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 2.730647325515747 param sum :: 717.2111206054688
2023-01-07 09:04:00,738 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,738 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,738 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 2924.210205078125
2023-01-07 09:04:00,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,738 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -13884.8828125
2023-01-07 09:04:00,739 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3509245216846466
2023-01-07 09:04:00,740 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: -13884.5 param sum :: 3618.21240234375
2023-01-07 09:04:00,740 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,740 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,740 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -52362.96875
2023-01-07 09:04:00,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,740 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 211.48828125
2023-01-07 09:04:00,740 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45252755284309387
2023-01-07 09:04:00,741 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.1196894496679306 param sum :: 506.9314880371094
2023-01-07 09:04:00,741 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,741 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,741 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -52362.96875
2023-01-07 09:04:00,741 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,741 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,741 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 212.474853515625
2023-01-07 09:04:00,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31568995118141174
2023-01-07 09:04:00,742 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 212.29736328125 param sum :: -68043.28125
2023-01-07 09:04:00,743 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,743 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,743 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -170135.96875
2023-01-07 09:04:00,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,743 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -73.66021728515625
2023-01-07 09:04:00,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11145029217004776
2023-01-07 09:04:00,744 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: -52.907371520996094 param sum :: 2906.61669921875
2023-01-07 09:04:00,744 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,744 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,744 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -170135.96875
2023-01-07 09:04:00,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,744 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -72.37602996826172
2023-01-07 09:04:00,744 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.447629690170288
2023-01-07 09:04:00,746 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -74.16979217529297 param sum :: -222913.0
2023-01-07 09:04:00,746 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,746 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,746 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -19454.23828125
2023-01-07 09:04:00,746 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,746 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,746 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2866.404296875
2023-01-07 09:04:00,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.027969522401690483
2023-01-07 09:04:00,747 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -100.78270721435547 param sum :: 753.0001220703125
2023-01-07 09:04:00,747 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,747 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,747 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -19454.23828125
2023-01-07 09:04:00,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,747 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2866.35302734375
2023-01-07 09:04:00,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01265418715775013
2023-01-07 09:04:00,749 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 2866.349365234375 param sum :: -25910.55859375
2023-01-07 09:04:00,749 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,749 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,749 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 2539.8095703125
2023-01-07 09:04:00,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,749 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -30836.46875
2023-01-07 09:04:00,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07636360079050064
2023-01-07 09:04:00,750 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -0.00010443390783620998 param sum :: 731.67529296875
2023-01-07 09:04:00,750 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,750 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,750 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 2539.8095703125
2023-01-07 09:04:00,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,751 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -30836.43359375
2023-01-07 09:04:00,751 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.369356632232666
2023-01-07 09:04:00,752 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -30836.640625 param sum :: 4075.20654296875
2023-01-07 09:04:00,752 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,752 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,752 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -247223.78125
2023-01-07 09:04:00,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,752 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 2938.2626953125
2023-01-07 09:04:00,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:04:00,753 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -568.7911987304688 param sum :: 2788.854736328125
2023-01-07 09:04:00,753 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,753 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,753 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -247223.78125
2023-01-07 09:04:00,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,754 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 2938.2626953125
2023-01-07 09:04:00,754 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9802322387695312e-08
2023-01-07 09:04:00,755 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 2938.2626953125 param sum :: -324071.15625
2023-01-07 09:04:00,755 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:00,755 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:00,756 > [DEBUG] 0 :: 148.3055419921875
2023-01-07 09:04:00,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,758 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.00860595703125
2023-01-07 09:04:00,759 > [DEBUG] 0 :: before allreduce fusion buffer :: -509.9604187011719
2023-01-07 09:04:00,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,761 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 45.49591064453125
2023-01-07 09:04:00,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,761 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -514.8961791992188
2023-01-07 09:04:00,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 533.6781005859375
2023-01-07 09:04:00,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,764 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -57.274349212646484
2023-01-07 09:04:00,765 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00013800407759845257
2023-01-07 09:04:00,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,767 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.2387043982744217
2023-01-07 09:04:00,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,767 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -166.74508666992188
2023-01-07 09:04:00,767 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.135269045829773
2023-01-07 09:04:00,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,770 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -13.288302421569824
2023-01-07 09:04:00,771 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.267617133446038e-05
2023-01-07 09:04:00,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,772 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 1.746679425239563
2023-01-07 09:04:00,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,772 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -93.88475036621094
2023-01-07 09:04:00,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.745277762413025
2023-01-07 09:04:00,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,773 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.7995477914810181
2023-01-07 09:04:00,773 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005442025605589151
2023-01-07 09:04:00,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,774 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 41.134803771972656
2023-01-07 09:04:00,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,775 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -94.67362976074219
2023-01-07 09:04:00,775 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.151004791259766
2023-01-07 09:04:00,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,776 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 32.125343322753906
2023-01-07 09:04:00,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.020539119839668274
2023-01-07 09:04:00,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,777 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.24048662185668945
2023-01-07 09:04:00,777 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,777 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,777 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 41.340118408203125
2023-01-07 09:04:00,777 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3734254837036133
2023-01-07 09:04:00,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,778 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3651.296875
2023-01-07 09:04:00,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.038267552852630615
2023-01-07 09:04:00,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,779 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.2361164093017578
2023-01-07 09:04:00,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,780 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3687.08740234375
2023-01-07 09:04:00,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2407483458518982
2023-01-07 09:04:00,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,781 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -60.56866455078125
2023-01-07 09:04:00,781 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02287689596414566
2023-01-07 09:04:00,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,782 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 37.35638427734375
2023-01-07 09:04:00,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,782 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -52.336097717285156
2023-01-07 09:04:00,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.87078857421875
2023-01-07 09:04:00,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,784 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -56.05998992919922
2023-01-07 09:04:00,784 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.003663577139377594
2023-01-07 09:04:00,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,784 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 38.03159713745117
2023-01-07 09:04:00,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,785 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -26.16046142578125
2023-01-07 09:04:00,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.75084686279297
2023-01-07 09:04:00,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,786 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -176.3185272216797
2023-01-07 09:04:00,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0012548845261335373
2023-01-07 09:04:00,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,787 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.547866702079773
2023-01-07 09:04:00,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,787 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -155.92974853515625
2023-01-07 09:04:00,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.35506272315979
2023-01-07 09:04:00,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,789 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -303.2220458984375
2023-01-07 09:04:00,789 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0006530550890602171
2023-01-07 09:04:00,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,789 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -1.8165570497512817
2023-01-07 09:04:00,790 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,790 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,790 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -303.2220458984375
2023-01-07 09:04:00,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9364206790924072
2023-01-07 09:04:00,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,791 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -6.819231986999512
2023-01-07 09:04:00,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03308826684951782
2023-01-07 09:04:00,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,792 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 15.901595115661621
2023-01-07 09:04:00,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,792 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 11.504682540893555
2023-01-07 09:04:00,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.982770919799805
2023-01-07 09:04:00,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,794 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -8.511953353881836
2023-01-07 09:04:00,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05913589149713516
2023-01-07 09:04:00,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,795 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.6282665133476257
2023-01-07 09:04:00,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,795 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 24.769994735717773
2023-01-07 09:04:00,795 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7592988610267639
2023-01-07 09:04:00,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,797 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -510.0169677734375
2023-01-07 09:04:00,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01796524040400982
2023-01-07 09:04:00,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,797 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 5.2349324226379395
2023-01-07 09:04:00,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,798 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -477.4554138183594
2023-01-07 09:04:00,798 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.562440872192383
2023-01-07 09:04:00,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,799 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 23.519105911254883
2023-01-07 09:04:00,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2965411841869354
2023-01-07 09:04:00,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,800 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.6965553760528564
2023-01-07 09:04:00,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,800 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 62.252685546875
2023-01-07 09:04:00,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2001904845237732
2023-01-07 09:04:00,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,801 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -196.26889038085938
2023-01-07 09:04:00,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.035212449729442596
2023-01-07 09:04:00,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,802 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 2.700545310974121
2023-01-07 09:04:00,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,803 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -159.74853515625
2023-01-07 09:04:00,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7675983905792236
2023-01-07 09:04:00,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,804 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -100.13128662109375
2023-01-07 09:04:00,804 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.001751515083014965
2023-01-07 09:04:00,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,805 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.3985286355018616
2023-01-07 09:04:00,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,805 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -68.04335021972656
2023-01-07 09:04:00,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2006700038909912
2023-01-07 09:04:00,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,806 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -65.591064453125
2023-01-07 09:04:00,806 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0355086512863636
2023-01-07 09:04:00,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,807 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -1.9041067361831665
2023-01-07 09:04:00,807 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,807 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,808 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -33.25188064575195
2023-01-07 09:04:00,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.904997706413269
2023-01-07 09:04:00,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,809 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 19.376773834228516
2023-01-07 09:04:00,809 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.024146154522895813
2023-01-07 09:04:00,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: 0.39979198575019836
2023-01-07 09:04:00,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,810 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 49.09619140625
2023-01-07 09:04:00,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3955855369567871
2023-01-07 09:04:00,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,811 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -537.214599609375
2023-01-07 09:04:00,811 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.014673419296741486
2023-01-07 09:04:00,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,812 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 2.2331769466400146
2023-01-07 09:04:00,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,812 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -539.0598754882812
2023-01-07 09:04:00,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1931192874908447
2023-01-07 09:04:00,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,814 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -665.826904296875
2023-01-07 09:04:00,814 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.18204933404922485
2023-01-07 09:04:00,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,815 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -2.911015510559082
2023-01-07 09:04:00,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,815 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -659.7201538085938
2023-01-07 09:04:00,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4202256202697754
2023-01-07 09:04:00,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,816 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -908.1060791015625
2023-01-07 09:04:00,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03319460526108742
2023-01-07 09:04:00,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,817 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.38565897941589355
2023-01-07 09:04:00,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,817 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -880.5914306640625
2023-01-07 09:04:00,817 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4473375678062439
2023-01-07 09:04:00,818 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,819 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 12.52859115600586
2023-01-07 09:04:00,819 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03550101816654205
2023-01-07 09:04:00,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,820 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.24685418605804443
2023-01-07 09:04:00,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,820 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 39.086944580078125
2023-01-07 09:04:00,820 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3910846710205078
2023-01-07 09:04:00,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,821 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -154.8574676513672
2023-01-07 09:04:00,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04532599821686745
2023-01-07 09:04:00,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,822 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -6.131295204162598
2023-01-07 09:04:00,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,822 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -114.72755432128906
2023-01-07 09:04:00,822 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.129859447479248
2023-01-07 09:04:00,823 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,823 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,824 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 974.03369140625
2023-01-07 09:04:00,824 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012581228278577328
2023-01-07 09:04:00,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,824 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: -1.7614877223968506
2023-01-07 09:04:00,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,825 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 984.2984619140625
2023-01-07 09:04:00,825 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8146693706512451
2023-01-07 09:04:00,826 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,826 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,826 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 0.7572612762451172
2023-01-07 09:04:00,827 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2262757122516632
2023-01-07 09:04:00,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,827 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 31.629865646362305
2023-01-07 09:04:00,827 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3205473721027374
2023-01-07 09:04:00,828 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,828 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 0.5873489379882812
2023-01-07 09:04:00,828 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.013674681074917316
2023-01-07 09:04:00,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 5.806806564331055
2023-01-07 09:04:00,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,829 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 32.5140266418457
2023-01-07 09:04:00,830 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.815038681030273
2023-01-07 09:04:00,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -35.78056335449219
2023-01-07 09:04:00,831 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0017449186416342854
2023-01-07 09:04:00,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 6.262538909912109
2023-01-07 09:04:00,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -35.449127197265625
2023-01-07 09:04:00,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.987329959869385
2023-01-07 09:04:00,833 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,833 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,833 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 1.8724045753479004
2023-01-07 09:04:00,833 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0990079939365387
2023-01-07 09:04:00,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,834 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 1.285524845123291
2023-01-07 09:04:00,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,834 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 7.221385955810547
2023-01-07 09:04:00,834 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.85379695892334
2023-01-07 09:04:00,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -36.210655212402344
2023-01-07 09:04:00,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05451636016368866
2023-01-07 09:04:00,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,837 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -28.091169357299805
2023-01-07 09:04:00,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.011452633887529373
2023-01-07 09:04:00,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,838 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 127.8764419555664
2023-01-07 09:04:00,838 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.071481853723526
2023-01-07 09:04:00,838 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,838 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,839 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 155.2987060546875
2023-01-07 09:04:00,839 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07348115742206573
2023-01-07 09:04:00,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,840 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -24.34537124633789
2023-01-07 09:04:00,840 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19648917019367218
2023-01-07 09:04:00,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,841 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -75.69601440429688
2023-01-07 09:04:00,841 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39206209778785706
2023-01-07 09:04:00,841 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,841 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -15.274022102355957
2023-01-07 09:04:00,842 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06190139800310135
2023-01-07 09:04:00,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,842 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 3.435274362564087
2023-01-07 09:04:00,843 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.046503543853759766
2023-01-07 09:04:00,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,844 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 33.29631042480469
2023-01-07 09:04:00,844 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.016993321478366852
2023-01-07 09:04:00,844 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,844 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,844 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -82.06767272949219
2023-01-07 09:04:00,845 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12971137464046478
2023-01-07 09:04:00,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,845 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -21.661121368408203
2023-01-07 09:04:00,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1355795860290527
2023-01-07 09:04:00,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,846 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -99.45486450195312
2023-01-07 09:04:00,846 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5666117072105408
2023-01-07 09:04:00,847 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,847 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,847 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -0.5752843618392944
2023-01-07 09:04:00,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.17635080218315125
2023-01-07 09:04:00,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,848 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -96.90760803222656
2023-01-07 09:04:00,848 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04684580862522125
2023-01-07 09:04:00,849 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,849 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,849 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -7.148571968078613
2023-01-07 09:04:00,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.029097527265548706
2023-01-07 09:04:00,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,850 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -4.012554168701172
2023-01-07 09:04:00,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,850 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -22.986530303955078
2023-01-07 09:04:00,851 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.91129732131958
2023-01-07 09:04:00,852 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,852 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,852 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -29.02332878112793
2023-01-07 09:04:00,852 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4493623971939087
2023-01-07 09:04:00,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,853 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -53.45494079589844
2023-01-07 09:04:00,853 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.6248064041137695
2023-01-07 09:04:00,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,854 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 181.5232391357422
2023-01-07 09:04:00,854 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7181028127670288
2023-01-07 09:04:00,855 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,855 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,855 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 153.2990264892578
2023-01-07 09:04:00,855 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.227859497070312
2023-01-07 09:04:00,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,856 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -105.01191711425781
2023-01-07 09:04:00,856 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3646373748779297
2023-01-07 09:04:00,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,857 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.7925602793693542
2023-01-07 09:04:00,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,857 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -177.08590698242188
2023-01-07 09:04:00,857 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4602296352386475
2023-01-07 09:04:00,858 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,858 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,858 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 407.46490478515625
2023-01-07 09:04:00,858 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11297745257616043
2023-01-07 09:04:00,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,859 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 407.90972900390625
2023-01-07 09:04:00,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33251050114631653
2023-01-07 09:04:00,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,860 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 154.17164611816406
2023-01-07 09:04:00,860 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.10959243774414
2023-01-07 09:04:00,861 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,861 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,861 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 151.33221435546875
2023-01-07 09:04:00,861 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.89372730255127
2023-01-07 09:04:00,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,862 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -127.19001770019531
2023-01-07 09:04:00,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6863546371459961
2023-01-07 09:04:00,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,863 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -2.6342251300811768
2023-01-07 09:04:00,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,863 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -122.32560729980469
2023-01-07 09:04:00,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.334325790405273
2023-01-07 09:04:00,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,865 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -119.76844787597656
2023-01-07 09:04:00,865 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4642856121063232
2023-01-07 09:04:00,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,866 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -4.056044101715088
2023-01-07 09:04:00,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,866 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 9.616474151611328
2023-01-07 09:04:00,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,866 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -119.19915771484375
2023-01-07 09:04:00,866 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.099895477294922
2023-01-07 09:04:00,867 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,867 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,867 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -119.19837951660156
2023-01-07 09:04:00,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.034999147057533264
2023-01-07 09:04:00,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,868 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 9.952152252197266
2023-01-07 09:04:00,868 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.984310150146484
2023-01-07 09:04:00,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,869 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2.7387938499450684
2023-01-07 09:04:00,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.972843170166016
2023-01-07 09:04:00,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,870 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -2.2784371376037598
2023-01-07 09:04:00,870 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,870 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,871 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -14.556788444519043
2023-01-07 09:04:00,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.0296549797058105
2023-01-07 09:04:00,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,872 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -29.627586364746094
2023-01-07 09:04:00,872 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.429346084594727
2023-01-07 09:04:00,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,873 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -4.90744686126709
2023-01-07 09:04:00,873 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,873 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,873 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 42.84165954589844
2023-01-07 09:04:00,873 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5701088905334473
2023-01-07 09:04:00,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,874 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -55.62895202636719
2023-01-07 09:04:00,874 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1634953022003174
2023-01-07 09:04:00,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,875 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 0.9168596267700195
2023-01-07 09:04:00,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,875 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -123.02638244628906
2023-01-07 09:04:00,875 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.046588897705078
2023-01-07 09:04:00,876 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,876 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,876 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -92.77879333496094
2023-01-07 09:04:00,877 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1313862800598145
2023-01-07 09:04:00,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,877 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -109.52772521972656
2023-01-07 09:04:00,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.907855987548828
2023-01-07 09:04:00,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,878 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -134.30606079101562
2023-01-07 09:04:00,879 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6406891345977783
2023-01-07 09:04:00,879 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,880 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 9.342876434326172
2023-01-07 09:04:00,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,880 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 49.968658447265625
2023-01-07 09:04:00,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,880 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -168.41961669921875
2023-01-07 09:04:00,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,880 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -104.35063171386719
2023-01-07 09:04:00,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.69869613647461
2023-01-07 09:04:00,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,882 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -167.26565551757812
2023-01-07 09:04:00,882 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6051864624023438
2023-01-07 09:04:00,882 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,882 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,883 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 152.60287475585938
2023-01-07 09:04:00,883 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.80535888671875
2023-01-07 09:04:00,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,884 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -204.77987670898438
2023-01-07 09:04:00,884 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.05457305908203
2023-01-07 09:04:00,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,884 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 145.63002014160156
2023-01-07 09:04:00,885 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,885 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,885 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -109.43089294433594
2023-01-07 09:04:00,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 63.42559814453125
2023-01-07 09:04:00,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,886 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 1.9696879386901855
2023-01-07 09:04:00,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.270545959472656
2023-01-07 09:04:00,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,887 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -1.1687769889831543
2023-01-07 09:04:00,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,887 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 28.797515869140625
2023-01-07 09:04:00,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.938068389892578
2023-01-07 09:04:00,888 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,888 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,888 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 187.28201293945312
2023-01-07 09:04:00,888 > [DEBUG] 0 :: before allreduce fusion buffer :: -38.62920379638672
2023-01-07 09:04:00,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,889 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 347.01611328125
2023-01-07 09:04:00,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.017409324645996
2023-01-07 09:04:00,893 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:04:00,893 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,894 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 201.8699951171875
2023-01-07 09:04:00,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,894 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 391.94805908203125
2023-01-07 09:04:00,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,895 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -107.93016052246094
2023-01-07 09:04:00,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,896 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 156.3936767578125
2023-01-07 09:04:00,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,896 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 427.447509765625
2023-01-07 09:04:00,896 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,896 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,897 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -950.2134399414062
2023-01-07 09:04:00,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,897 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1087.0797119140625
2023-01-07 09:04:00,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,898 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -559.9365234375
2023-01-07 09:04:00,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,898 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -265.7223815917969
2023-01-07 09:04:00,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,899 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -402.6703796386719
2023-01-07 09:04:00,899 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,899 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,899 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -403.04083251953125
2023-01-07 09:04:00,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,900 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -325.983642578125
2023-01-07 09:04:00,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,901 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 44.179901123046875
2023-01-07 09:04:00,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,901 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -130.59799194335938
2023-01-07 09:04:00,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 182.24307250976562
2023-01-07 09:04:00,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,902 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,902 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 162.9008331298828
2023-01-07 09:04:00,902 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 51.32305908203125
2023-01-07 09:04:00,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -25.438804626464844
2023-01-07 09:04:00,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,903 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 54.1045036315918
2023-01-07 09:04:00,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 57.37346649169922
2023-01-07 09:04:00,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 981.3170166015625
2023-01-07 09:04:00,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -115.9919204711914
2023-01-07 09:04:00,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 46.939632415771484
2023-01-07 09:04:00,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,904 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -871.5964965820312
2023-01-07 09:04:00,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -658.9990234375
2023-01-07 09:04:00,905 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,905 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,905 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -564.5194702148438
2023-01-07 09:04:00,905 > [DEBUG] 0 :: before allreduce fusion buffer :: -629.9459838867188
2023-01-07 09:04:00,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,906 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 62.63256072998047
2023-01-07 09:04:00,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -33.3918571472168
2023-01-07 09:04:00,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -54.2083740234375
2023-01-07 09:04:00,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -138.4888916015625
2023-01-07 09:04:00,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,907 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 62.14813995361328
2023-01-07 09:04:00,907 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,907 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -456.0127868652344
2023-01-07 09:04:00,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,908 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 45.62579345703125
2023-01-07 09:04:00,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 11.414787292480469
2023-01-07 09:04:00,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -288.3314208984375
2023-01-07 09:04:00,908 > [DEBUG] 0 :: before allreduce fusion buffer :: -1106.62255859375
2023-01-07 09:04:00,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -142.59413146972656
2023-01-07 09:04:00,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -23.773681640625
2023-01-07 09:04:00,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,910 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -48.224510192871094
2023-01-07 09:04:00,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,910 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 3693.73046875
2023-01-07 09:04:00,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 2201.223876953125
2023-01-07 09:04:00,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,910 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 47.37239074707031
2023-01-07 09:04:00,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -94.89798736572266
2023-01-07 09:04:00,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -48.7571907043457
2023-01-07 09:04:00,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 1171.776123046875
2023-01-07 09:04:00,911 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,911 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,912 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -168.6572723388672
2023-01-07 09:04:00,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:00,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:00,912 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -593.8226928710938
2023-01-07 09:04:00,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 148.92820739746094
2023-01-07 09:04:01,755 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 173.1581573486328 param sum :: 220.78936767578125
2023-01-07 09:04:01,756 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,756 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,756 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 62.4830322265625
2023-01-07 09:04:01,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,756 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -3.7926955223083496
2023-01-07 09:04:01,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,756 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 390.692138671875
2023-01-07 09:04:01,756 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.635519027709961
2023-01-07 09:04:01,758 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 5.483483791351318 param sum :: 62.26506805419922
2023-01-07 09:04:01,758 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,758 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,758 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -38.85418701171875
2023-01-07 09:04:01,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,758 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 116.24102783203125
2023-01-07 09:04:01,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,759 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 375.4030456542969
2023-01-07 09:04:01,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 115.93611145019531
2023-01-07 09:04:01,760 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: -57.348480224609375 param sum :: -37.41791534423828
2023-01-07 09:04:01,760 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,760 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,760 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 166.16482543945312
2023-01-07 09:04:01,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,760 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 373.631591796875
2023-01-07 09:04:01,761 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.598275184631348
2023-01-07 09:04:01,761 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: -2.9943830966949463 param sum :: 58.714195251464844
2023-01-07 09:04:01,761 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,762 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,762 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 166.16482543945312
2023-01-07 09:04:01,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,762 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 403.11724853515625
2023-01-07 09:04:01,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.721733093261719
2023-01-07 09:04:01,763 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 355.0000915527344 param sum :: 207.88958740234375
2023-01-07 09:04:01,763 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,763 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,763 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.2445297241211
2023-01-07 09:04:01,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,763 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -1.267012357711792
2023-01-07 09:04:01,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,763 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -64.06254577636719
2023-01-07 09:04:01,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,764 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -221.43017578125
2023-01-07 09:04:01,764 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.17286682128906
2023-01-07 09:04:01,765 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: -9.387392044067383 param sum :: 63.89923095703125
2023-01-07 09:04:01,765 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,765 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,765 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 395.94403076171875
2023-01-07 09:04:01,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,766 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -207.18258666992188
2023-01-07 09:04:01,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.514118194580078
2023-01-07 09:04:01,766 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: -8.098724365234375 param sum :: -51.710655212402344
2023-01-07 09:04:01,767 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,767 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,767 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.69332885742188
2023-01-07 09:04:01,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,767 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 1.8667042255401611
2023-01-07 09:04:01,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,767 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 64.0263671875
2023-01-07 09:04:01,767 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.60308074951172
2023-01-07 09:04:01,768 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 68.58427429199219 param sum :: 253.86215209960938
2023-01-07 09:04:01,768 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,768 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,768 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 237.05767822265625
2023-01-07 09:04:01,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,769 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -114.34226989746094
2023-01-07 09:04:01,769 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7214431762695312
2023-01-07 09:04:01,769 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 255.02700805664062 param sum :: 303.05767822265625
2023-01-07 09:04:01,770 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,770 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,770 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 237.05767822265625
2023-01-07 09:04:01,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,770 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -127.978515625
2023-01-07 09:04:01,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.9401273727417
2023-01-07 09:04:01,771 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: -30.021183013916016 param sum :: 223.68997192382812
2023-01-07 09:04:01,771 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,771 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,771 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 237.05767822265625
2023-01-07 09:04:01,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,771 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -92.532470703125
2023-01-07 09:04:01,772 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.732156753540039
2023-01-07 09:04:01,772 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: -250.58343505859375 param sum :: 39.1148567199707
2023-01-07 09:04:01,772 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,772 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,772 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 61.35986328125
2023-01-07 09:04:01,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,773 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: 1.9482474327087402
2023-01-07 09:04:01,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,773 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -25.644744873046875
2023-01-07 09:04:01,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.89767837524414
2023-01-07 09:04:01,774 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 10.784521102905273 param sum :: 60.838958740234375
2023-01-07 09:04:01,774 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,774 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,774 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 237.05767822265625
2023-01-07 09:04:01,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,775 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -187.8992462158203
2023-01-07 09:04:01,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,775 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1.9751405715942383
2023-01-07 09:04:01,775 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.69805908203125
2023-01-07 09:04:01,776 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: -212.65484619140625 param sum :: 256.31451416015625
2023-01-07 09:04:01,776 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,776 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,776 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 63.850425720214844
2023-01-07 09:04:01,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,776 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 2.420440196990967
2023-01-07 09:04:01,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,777 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -32.62836837768555
2023-01-07 09:04:01,777 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.315909385681152
2023-01-07 09:04:01,778 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: -6.139185905456543 param sum :: 64.37870788574219
2023-01-07 09:04:01,778 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,778 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,778 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 395.94403076171875
2023-01-07 09:04:01,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,778 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -130.68348693847656
2023-01-07 09:04:01,778 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.926237106323242
2023-01-07 09:04:01,779 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 97.971435546875 param sum :: 160.5785675048828
2023-01-07 09:04:01,779 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,779 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,779 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 446.3773498535156
2023-01-07 09:04:01,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,779 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 48.109527587890625
2023-01-07 09:04:01,780 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.840240478515625
2023-01-07 09:04:01,781 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: -33.717987060546875 param sum :: 240.30645751953125
2023-01-07 09:04:01,781 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,781 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,781 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 446.3773498535156
2023-01-07 09:04:01,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,781 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -73.92391967773438
2023-01-07 09:04:01,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,781 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -81.00511169433594
2023-01-07 09:04:01,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.43111801147461
2023-01-07 09:04:01,782 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: -80.00155639648438 param sum :: 522.849853515625
2023-01-07 09:04:01,782 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,782 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,783 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 395.94403076171875
2023-01-07 09:04:01,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,783 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -79.33540344238281
2023-01-07 09:04:01,783 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.601671814918518
2023-01-07 09:04:01,784 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: -7.66754150390625 param sum :: 63.97801971435547
2023-01-07 09:04:01,784 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,784 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,784 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 395.94403076171875
2023-01-07 09:04:01,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,784 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -117.36450958251953
2023-01-07 09:04:01,784 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.621725082397461
2023-01-07 09:04:01,785 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: -64.22178649902344 param sum :: 483.58404541015625
2023-01-07 09:04:01,785 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,785 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,785 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.21100616455078
2023-01-07 09:04:01,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,786 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 0.29359936714172363
2023-01-07 09:04:01,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,786 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 4.9598236083984375
2023-01-07 09:04:01,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -5.017904281616211
2023-01-07 09:04:01,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.062990188598633
2023-01-07 09:04:01,788 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: -6.049376487731934 param sum :: 64.31751251220703
2023-01-07 09:04:01,788 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,788 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,788 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 748.0128784179688
2023-01-07 09:04:01,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 14.632161140441895
2023-01-07 09:04:01,788 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.495344638824463
2023-01-07 09:04:01,789 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 169.7544403076172 param sum :: -101.66377258300781
2023-01-07 09:04:01,789 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,789 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,789 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 1173.3895263671875
2023-01-07 09:04:01,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,789 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 85.72364807128906
2023-01-07 09:04:01,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.148874282836914
2023-01-07 09:04:01,790 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: -5.3527936935424805 param sum :: 275.33892822265625
2023-01-07 09:04:01,791 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,791 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,791 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 748.0128784179688
2023-01-07 09:04:01,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,791 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 25.55642318725586
2023-01-07 09:04:01,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.233487129211426
2023-01-07 09:04:01,792 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 27.41638946533203 param sum :: 1378.18212890625
2023-01-07 09:04:01,792 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,792 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,792 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 748.0128784179688
2023-01-07 09:04:01,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,792 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 51.055667877197266
2023-01-07 09:04:01,792 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01766413450241089
2023-01-07 09:04:01,793 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: -3.283123016357422 param sum :: 135.22610473632812
2023-01-07 09:04:01,793 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,793 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,794 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 748.0128784179688
2023-01-07 09:04:01,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,794 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 139.53292846679688
2023-01-07 09:04:01,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.494405746459961
2023-01-07 09:04:01,795 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 179.5232696533203 param sum :: 635.539794921875
2023-01-07 09:04:01,795 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,795 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,795 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 125.86052703857422
2023-01-07 09:04:01,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,795 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 4.9902520179748535
2023-01-07 09:04:01,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,795 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 50.10698699951172
2023-01-07 09:04:01,796 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.26413345336914
2023-01-07 09:04:01,797 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: -16.019821166992188 param sum :: 126.52082824707031
2023-01-07 09:04:01,797 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,797 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,797 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 629.4091796875
2023-01-07 09:04:01,797 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,797 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,797 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -73.20409393310547
2023-01-07 09:04:01,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6924389600753784
2023-01-07 09:04:01,798 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: -77.21412658691406 param sum :: 713.5400390625
2023-01-07 09:04:01,798 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,798 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,798 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 510.36932373046875
2023-01-07 09:04:01,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,798 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: -1.1910600662231445
2023-01-07 09:04:01,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,799 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 25.02642822265625
2023-01-07 09:04:01,799 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.248096466064453
2023-01-07 09:04:01,800 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: -51.33228302001953 param sum :: 511.1273193359375
2023-01-07 09:04:01,800 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,800 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,800 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 313.4145812988281
2023-01-07 09:04:01,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,800 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -89.82254791259766
2023-01-07 09:04:01,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.533994674682617
2023-01-07 09:04:01,801 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: -185.13502502441406 param sum :: 335.389892578125
2023-01-07 09:04:01,801 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,801 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,801 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 548.2166748046875
2023-01-07 09:04:01,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,802 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -840.11181640625
2023-01-07 09:04:01,802 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7665901184082031
2023-01-07 09:04:01,803 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: -23.929513931274414 param sum :: 567.0537109375
2023-01-07 09:04:01,803 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,803 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,803 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 548.2166748046875
2023-01-07 09:04:01,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,803 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -971.0420532226562
2023-01-07 09:04:01,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.778242588043213
2023-01-07 09:04:01,804 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: -806.739013671875 param sum :: 810.4393310546875
2023-01-07 09:04:01,804 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,804 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,804 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.26235961914062
2023-01-07 09:04:01,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,805 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -3.6789958477020264
2023-01-07 09:04:01,805 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,805 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,805 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1292.02001953125
2023-01-07 09:04:01,805 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.3043975830078125
2023-01-07 09:04:01,806 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: -1.8116912841796875 param sum :: 127.29035949707031
2023-01-07 09:04:01,806 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,806 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,806 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 814.1490478515625
2023-01-07 09:04:01,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,806 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1300.8363037109375
2023-01-07 09:04:01,806 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.937955856323242
2023-01-07 09:04:01,807 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: -1339.31884765625 param sum :: 1259.9278564453125
2023-01-07 09:04:01,807 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,807 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,808 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 126.06207275390625
2023-01-07 09:04:01,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,808 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -1.49151611328125
2023-01-07 09:04:01,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,808 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -553.7702026367188
2023-01-07 09:04:01,808 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0568342208862305
2023-01-07 09:04:01,809 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: -23.988378524780273 param sum :: 125.55400085449219
2023-01-07 09:04:01,809 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,809 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,809 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 733.5897216796875
2023-01-07 09:04:01,809 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,809 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -553.7196655273438
2023-01-07 09:04:01,810 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15938273072242737
2023-01-07 09:04:01,811 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: -589.9470825195312 param sum :: 984.3148193359375
2023-01-07 09:04:01,811 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,811 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,811 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 631.8576049804688
2023-01-07 09:04:01,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,811 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -272.5020446777344
2023-01-07 09:04:01,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.24177885055542
2023-01-07 09:04:01,812 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: -23.92024803161621 param sum :: 622.5020141601562
2023-01-07 09:04:01,812 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,812 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,812 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 631.8576049804688
2023-01-07 09:04:01,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,812 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -286.9855041503906
2023-01-07 09:04:01,813 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.048708915710449
2023-01-07 09:04:01,814 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: -295.1437683105469 param sum :: 853.7710571289062
2023-01-07 09:04:01,814 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,814 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,814 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 127.07914733886719
2023-01-07 09:04:01,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,814 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.2896363139152527
2023-01-07 09:04:01,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,814 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -417.0350646972656
2023-01-07 09:04:01,814 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5610880851745605
2023-01-07 09:04:01,815 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 8.794153213500977 param sum :: 124.88383483886719
2023-01-07 09:04:01,815 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,815 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,816 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 772.0977783203125
2023-01-07 09:04:01,816 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,816 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,816 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -428.10626220703125
2023-01-07 09:04:01,816 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1697019338607788
2023-01-07 09:04:01,817 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: -415.3497009277344 param sum :: 926.1548461914062
2023-01-07 09:04:01,817 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,817 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,817 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 130.95982360839844
2023-01-07 09:04:01,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,817 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.8991392850875854
2023-01-07 09:04:01,817 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,817 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,817 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -429.1583251953125
2023-01-07 09:04:01,818 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12554824352264404
2023-01-07 09:04:01,819 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 1.8640170097351074 param sum :: 132.00997924804688
2023-01-07 09:04:01,819 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,819 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,819 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 271.1986083984375
2023-01-07 09:04:01,819 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,819 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,819 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -436.8624572753906
2023-01-07 09:04:01,819 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.1101975440979004
2023-01-07 09:04:01,820 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: -459.4334716796875 param sum :: 172.90499877929688
2023-01-07 09:04:01,820 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,820 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,820 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.82666015625
2023-01-07 09:04:01,820 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,820 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,820 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.5599215030670166
2023-01-07 09:04:01,821 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,821 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,821 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -332.57684326171875
2023-01-07 09:04:01,821 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.981819152832031
2023-01-07 09:04:01,822 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: -3.2147109508514404 param sum :: 512.5633544921875
2023-01-07 09:04:01,822 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,822 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,822 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 743.75537109375
2023-01-07 09:04:01,822 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,822 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,822 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -316.78082275390625
2023-01-07 09:04:01,822 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39878785610198975
2023-01-07 09:04:01,823 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: -338.3595275878906 param sum :: 734.9613037109375
2023-01-07 09:04:01,823 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,823 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,824 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 127.96773529052734
2023-01-07 09:04:01,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,824 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -2.429636001586914
2023-01-07 09:04:01,824 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,824 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,824 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 51.06650161743164
2023-01-07 09:04:01,824 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.1188530921936035
2023-01-07 09:04:01,825 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 16.13372802734375 param sum :: 127.44928741455078
2023-01-07 09:04:01,825 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,825 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,825 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1292.0877685546875
2023-01-07 09:04:01,825 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,825 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,826 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 52.323734283447266
2023-01-07 09:04:01,826 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.057681385427713394
2023-01-07 09:04:01,827 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 43.96989440917969 param sum :: 1434.851318359375
2023-01-07 09:04:01,827 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,827 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,827 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 124.71186828613281
2023-01-07 09:04:01,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,827 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,827 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 1.4998023509979248
2023-01-07 09:04:01,827 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,828 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,828 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -127.70454406738281
2023-01-07 09:04:01,828 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3344780206680298
2023-01-07 09:04:01,829 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 4.766529560089111 param sum :: 123.01582336425781
2023-01-07 09:04:01,829 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,829 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,829 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 388.5810546875
2023-01-07 09:04:01,829 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,829 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,829 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -82.45570373535156
2023-01-07 09:04:01,829 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.137093186378479
2023-01-07 09:04:01,830 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: -79.99169921875 param sum :: 249.02255249023438
2023-01-07 09:04:01,830 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,830 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,830 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 511.1766662597656
2023-01-07 09:04:01,830 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,830 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,831 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.707289457321167
2023-01-07 09:04:01,831 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,831 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,831 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 95.29345703125
2023-01-07 09:04:01,831 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.3804097175598145
2023-01-07 09:04:01,832 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 1.3176465034484863 param sum :: 511.9466552734375
2023-01-07 09:04:01,832 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,832 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,832 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 1216.6943359375
2023-01-07 09:04:01,832 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,832 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,832 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 118.09141540527344
2023-01-07 09:04:01,832 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.261102557182312
2023-01-07 09:04:01,833 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 122.57482147216797 param sum :: 1481.61669921875
2023-01-07 09:04:01,833 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,834 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,834 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 257.77374267578125
2023-01-07 09:04:01,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,834 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -6.140436172485352
2023-01-07 09:04:01,834 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,834 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,834 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 482.68634033203125
2023-01-07 09:04:01,834 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.18671989440918
2023-01-07 09:04:01,835 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 32.53158950805664 param sum :: 258.7412109375
2023-01-07 09:04:01,835 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,835 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,835 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -50186.609375
2023-01-07 09:04:01,836 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,836 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,836 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 450.02020263671875
2023-01-07 09:04:01,836 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2951607406139374
2023-01-07 09:04:01,837 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 452.11505126953125 param sum :: -59227.34765625
2023-01-07 09:04:01,837 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,837 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,837 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 5233.37109375
2023-01-07 09:04:01,837 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,837 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,837 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 51.32305908203125
2023-01-07 09:04:01,837 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3876590728759766
2023-01-07 09:04:01,838 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 2.5119571685791016 param sum :: 325.0548095703125
2023-01-07 09:04:01,838 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,838 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,839 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 5233.37109375
2023-01-07 09:04:01,839 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,839 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,839 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 63.73616027832031
2023-01-07 09:04:01,839 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.308871030807495
2023-01-07 09:04:01,840 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 58.988380432128906 param sum :: 5828.60205078125
2023-01-07 09:04:01,840 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,840 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,840 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -70469.6484375
2023-01-07 09:04:01,840 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,840 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,840 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -25.398134231567383
2023-01-07 09:04:01,840 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.20310288667678833
2023-01-07 09:04:01,841 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 1.1968852281570435 param sum :: 1286.74072265625
2023-01-07 09:04:01,842 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,842 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,842 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -70469.6484375
2023-01-07 09:04:01,842 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,842 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,842 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 47.702171325683594
2023-01-07 09:04:01,842 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27327871322631836
2023-01-07 09:04:01,843 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 49.274024963378906 param sum :: -84133.0859375
2023-01-07 09:04:01,843 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,843 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,843 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49930.1875
2023-01-07 09:04:01,843 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,843 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,843 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 54.09493637084961
2023-01-07 09:04:01,843 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7714036107063293
2023-01-07 09:04:01,844 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 30.021896362304688 param sum :: 1372.250732421875
2023-01-07 09:04:01,844 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,844 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,845 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49930.1875
2023-01-07 09:04:01,845 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,845 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,845 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 46.38488006591797
2023-01-07 09:04:01,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.327277660369873
2023-01-07 09:04:01,846 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 45.25368881225586 param sum :: -59635.23828125
2023-01-07 09:04:01,846 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,846 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,846 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 246.5004425048828
2023-01-07 09:04:01,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,846 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.5383757948875427
2023-01-07 09:04:01,846 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,846 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,847 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 57.43596267700195
2023-01-07 09:04:01,847 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.469949245452881
2023-01-07 09:04:01,848 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 29.768573760986328 param sum :: 239.18930053710938
2023-01-07 09:04:01,848 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,848 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,848 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -122560.671875
2023-01-07 09:04:01,848 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,848 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,848 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -25.39077377319336
2023-01-07 09:04:01,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.922364711761475
2023-01-07 09:04:01,849 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 84.70431518554688 param sum :: -146440.421875
2023-01-07 09:04:01,849 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,849 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,850 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25361.796875
2023-01-07 09:04:01,850 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,850 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,850 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 30.465225219726562
2023-01-07 09:04:01,850 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.951117515563965
2023-01-07 09:04:01,851 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 10.623879432678223 param sum :: 333.1470947265625
2023-01-07 09:04:01,851 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,851 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,851 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25361.796875
2023-01-07 09:04:01,851 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,851 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,851 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 25.311527252197266
2023-01-07 09:04:01,851 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.44430160522461
2023-01-07 09:04:01,852 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 161.8115997314453 param sum :: -30334.03515625
2023-01-07 09:04:01,852 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,852 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,853 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1775.9720458984375
2023-01-07 09:04:01,853 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,853 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,853 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 39.783660888671875
2023-01-07 09:04:01,853 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.384004592895508
2023-01-07 09:04:01,854 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 35.31024169921875 param sum :: 1363.1322021484375
2023-01-07 09:04:01,854 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,854 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,854 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1775.9720458984375
2023-01-07 09:04:01,854 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,854 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,854 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 58.50596618652344
2023-01-07 09:04:01,854 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.249871253967285
2023-01-07 09:04:01,855 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 70.00924682617188 param sum :: 1975.2696533203125
2023-01-07 09:04:01,855 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,855 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,856 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1231.5350341796875
2023-01-07 09:04:01,856 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,856 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,856 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 22.89533042907715
2023-01-07 09:04:01,856 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6226296424865723
2023-01-07 09:04:01,857 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 19.19830322265625 param sum :: 338.5970764160156
2023-01-07 09:04:01,857 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,857 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,857 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1231.5350341796875
2023-01-07 09:04:01,857 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,857 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,857 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 20.028995513916016
2023-01-07 09:04:01,857 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.285053253173828
2023-01-07 09:04:01,858 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 17.33091163635254 param sum :: 1392.617919921875
2023-01-07 09:04:01,858 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,858 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,859 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2260.162109375
2023-01-07 09:04:01,859 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,859 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,859 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 37.70359802246094
2023-01-07 09:04:01,859 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3265652656555176
2023-01-07 09:04:01,860 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 18.953811645507812 param sum :: 345.7467041015625
2023-01-07 09:04:01,860 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,860 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,860 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2260.162109375
2023-01-07 09:04:01,860 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,860 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,860 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 61.4027214050293
2023-01-07 09:04:01,861 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9142705202102661
2023-01-07 09:04:01,862 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 84.07904815673828 param sum :: 2557.76611328125
2023-01-07 09:04:01,862 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,862 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,862 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2123.7626953125
2023-01-07 09:04:01,862 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,862 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,862 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 4.682776927947998
2023-01-07 09:04:01,862 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.43991172313690186
2023-01-07 09:04:01,863 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 2.8906044960021973 param sum :: 1404.9393310546875
2023-01-07 09:04:01,863 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,863 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,863 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2123.7626953125
2023-01-07 09:04:01,863 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,863 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,863 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 3.8919196128845215
2023-01-07 09:04:01,864 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6727734804153442
2023-01-07 09:04:01,865 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 2.382862091064453 param sum :: 2411.333740234375
2023-01-07 09:04:01,865 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,865 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,865 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2779.666259765625
2023-01-07 09:04:01,865 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,865 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,865 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 300.00726318359375
2023-01-07 09:04:01,865 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4688239097595215
2023-01-07 09:04:01,866 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 21.060768127441406 param sum :: 344.9244384765625
2023-01-07 09:04:01,866 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,866 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,866 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2779.666259765625
2023-01-07 09:04:01,866 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,866 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,866 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 306.7889404296875
2023-01-07 09:04:01,866 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.486608862876892
2023-01-07 09:04:01,867 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 306.9696350097656 param sum :: 3192.60791015625
2023-01-07 09:04:01,868 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,868 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,868 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2574.779296875
2023-01-07 09:04:01,868 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,868 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,868 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 40.385353088378906
2023-01-07 09:04:01,868 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.179025173187256
2023-01-07 09:04:01,869 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 18.8174991607666 param sum :: 348.33709716796875
2023-01-07 09:04:01,869 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,869 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,869 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2574.779296875
2023-01-07 09:04:01,869 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,869 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,869 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 41.03547286987305
2023-01-07 09:04:01,870 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2354769706726074
2023-01-07 09:04:01,871 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 52.650352478027344 param sum :: 2989.1533203125
2023-01-07 09:04:01,871 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,871 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,871 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2534.858642578125
2023-01-07 09:04:01,871 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,871 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,871 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -72.8510971069336
2023-01-07 09:04:01,871 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5944925546646118
2023-01-07 09:04:01,872 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 30.423538208007812 param sum :: 1442.501953125
2023-01-07 09:04:01,872 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,872 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,872 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2534.858642578125
2023-01-07 09:04:01,872 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,872 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,872 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -74.66339111328125
2023-01-07 09:04:01,873 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6253172159194946
2023-01-07 09:04:01,874 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: -75.75687408447266 param sum :: 3680.31103515625
2023-01-07 09:04:01,874 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,874 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,874 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2114.90869140625
2023-01-07 09:04:01,874 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,874 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,874 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -37.50044631958008
2023-01-07 09:04:01,874 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9156835079193115
2023-01-07 09:04:01,875 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 18.77407455444336 param sum :: 350.16766357421875
2023-01-07 09:04:01,875 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,875 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,875 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2114.90869140625
2023-01-07 09:04:01,875 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,875 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,875 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -40.564849853515625
2023-01-07 09:04:01,876 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07244298607110977
2023-01-07 09:04:01,877 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: -38.51140594482422 param sum :: 2345.051513671875
2023-01-07 09:04:01,877 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,877 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,877 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3039.75830078125
2023-01-07 09:04:01,877 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,877 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,877 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 661.3497314453125
2023-01-07 09:04:01,877 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.368027687072754
2023-01-07 09:04:01,878 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 18.374910354614258 param sum :: 356.4070739746094
2023-01-07 09:04:01,878 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,878 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,878 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3039.75830078125
2023-01-07 09:04:01,878 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,878 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,878 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 646.7354125976562
2023-01-07 09:04:01,879 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.394662618637085
2023-01-07 09:04:01,880 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 630.3123779296875 param sum :: -3112.450927734375
2023-01-07 09:04:01,880 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,880 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,880 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -44065.0703125
2023-01-07 09:04:01,880 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,880 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,880 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 30.26904296875
2023-01-07 09:04:01,880 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.121109008789062
2023-01-07 09:04:01,881 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 37.38466262817383 param sum :: 1457.3328857421875
2023-01-07 09:04:01,881 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,881 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,881 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -44065.0703125
2023-01-07 09:04:01,881 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,881 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,881 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 24.64051055908203
2023-01-07 09:04:01,882 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.04415512084961
2023-01-07 09:04:01,883 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: -33.004173278808594 param sum :: -52494.7265625
2023-01-07 09:04:01,883 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,883 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,883 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -13627.1650390625
2023-01-07 09:04:01,883 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,883 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,883 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -49.79503631591797
2023-01-07 09:04:01,883 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13854233920574188
2023-01-07 09:04:01,884 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 16.5613956451416 param sum :: 349.8087158203125
2023-01-07 09:04:01,884 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,884 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,884 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -13627.1650390625
2023-01-07 09:04:01,884 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,884 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,884 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -54.668304443359375
2023-01-07 09:04:01,885 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6296809911727905
2023-01-07 09:04:01,886 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: -59.703887939453125 param sum :: -16418.576171875
2023-01-07 09:04:01,886 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,886 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,886 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49282.3359375
2023-01-07 09:04:01,886 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,886 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,886 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 45.90715408325195
2023-01-07 09:04:01,886 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.552317142486572
2023-01-07 09:04:01,887 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 11.356849670410156 param sum :: 357.63482666015625
2023-01-07 09:04:01,887 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,887 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,887 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49282.3359375
2023-01-07 09:04:01,887 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,887 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,887 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 43.98240661621094
2023-01-07 09:04:01,888 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4881038665771484
2023-01-07 09:04:01,889 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 41.882293701171875 param sum :: -58803.328125
2023-01-07 09:04:01,889 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,889 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,889 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107978.390625
2023-01-07 09:04:01,889 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,889 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,889 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 11.392311096191406
2023-01-07 09:04:01,889 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.59107780456543
2023-01-07 09:04:01,890 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 34.20494842529297 param sum :: 1469.123291015625
2023-01-07 09:04:01,890 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,890 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,890 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107978.390625
2023-01-07 09:04:01,890 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,891 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,891 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 12.257064819335938
2023-01-07 09:04:01,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7012802362442017
2023-01-07 09:04:01,892 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 15.004417419433594 param sum :: -128914.75
2023-01-07 09:04:01,892 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,892 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,892 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -91399.6484375
2023-01-07 09:04:01,892 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,892 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,892 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 626.2190551757812
2023-01-07 09:04:01,892 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.45147785544395447
2023-01-07 09:04:01,893 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 6.283047676086426 param sum :: 695.7022705078125
2023-01-07 09:04:01,893 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,893 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,894 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -91399.6484375
2023-01-07 09:04:01,894 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,894 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,894 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 626.794189453125
2023-01-07 09:04:01,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4476373195648193
2023-01-07 09:04:01,895 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 630.845458984375 param sum :: -109417.03125
2023-01-07 09:04:01,895 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,895 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,895 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 3375.02490234375
2023-01-07 09:04:01,895 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,895 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,895 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -374.04132080078125
2023-01-07 09:04:01,895 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0777108669281006
2023-01-07 09:04:01,896 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: -0.0007110199658200145 param sum :: 516.0040283203125
2023-01-07 09:04:01,896 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,897 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,897 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 3375.02490234375
2023-01-07 09:04:01,897 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,897 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,897 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -372.33270263671875
2023-01-07 09:04:01,897 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8386214971542358
2023-01-07 09:04:01,898 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: -372.71856689453125 param sum :: 4445.603515625
2023-01-07 09:04:01,898 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,898 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,898 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -261682.15625
2023-01-07 09:04:01,898 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,898 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,898 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1100.5955810546875
2023-01-07 09:04:01,899 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.141744613647461
2023-01-07 09:04:01,900 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 61.37376403808594 param sum :: 2841.6708984375
2023-01-07 09:04:01,900 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,900 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,900 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -261682.15625
2023-01-07 09:04:01,900 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,900 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,900 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1101.63525390625
2023-01-07 09:04:01,900 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11582820117473602
2023-01-07 09:04:01,901 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 1101.253173828125 param sum :: -311962.90625
2023-01-07 09:04:01,901 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,901 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,901 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -56544.53125
2023-01-07 09:04:01,901 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,901 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,901 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -17.490373611450195
2023-01-07 09:04:01,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05581910163164139
2023-01-07 09:04:01,903 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 45.60935974121094 param sum :: 3025.60400390625
2023-01-07 09:04:01,903 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,903 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,903 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -56544.53125
2023-01-07 09:04:01,903 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,903 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,903 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -18.273927688598633
2023-01-07 09:04:01,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14194272458553314
2023-01-07 09:04:01,904 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: -17.237939834594727 param sum :: -67691.1015625
2023-01-07 09:04:01,904 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,904 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,904 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3618.21240234375
2023-01-07 09:04:01,904 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,904 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,905 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 638.9595336914062
2023-01-07 09:04:01,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03652887046337128
2023-01-07 09:04:01,906 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 1.749173879623413 param sum :: 742.78662109375
2023-01-07 09:04:01,906 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,906 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,906 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3618.21240234375
2023-01-07 09:04:01,906 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,906 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,906 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 637.8782958984375
2023-01-07 09:04:01,906 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07687593251466751
2023-01-07 09:04:01,907 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 639.17822265625 param sum :: 4678.6220703125
2023-01-07 09:04:01,907 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,907 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,907 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -68043.28125
2023-01-07 09:04:01,908 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,908 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,908 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2013.896484375
2023-01-07 09:04:01,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.39986157417297363
2023-01-07 09:04:01,909 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.09000232070684433 param sum :: 504.07513427734375
2023-01-07 09:04:01,909 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,909 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,909 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -68043.28125
2023-01-07 09:04:01,909 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,909 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,909 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2013.055419921875
2023-01-07 09:04:01,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3921068012714386
2023-01-07 09:04:01,910 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 2011.8037109375 param sum :: -80737.7578125
2023-01-07 09:04:01,910 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,910 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,910 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222913.0
2023-01-07 09:04:01,910 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,910 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,911 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -94.96359252929688
2023-01-07 09:04:01,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.25990331172943115
2023-01-07 09:04:01,912 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 40.366920471191406 param sum :: 3021.030029296875
2023-01-07 09:04:01,912 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,912 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,912 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222913.0
2023-01-07 09:04:01,912 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,912 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,912 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -94.82966613769531
2023-01-07 09:04:01,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.45241037011146545
2023-01-07 09:04:01,913 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: -94.75993347167969 param sum :: -266154.53125
2023-01-07 09:04:01,913 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,913 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,913 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -25910.55859375
2023-01-07 09:04:01,913 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,913 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,914 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2582.309814453125
2023-01-07 09:04:01,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31534460186958313
2023-01-07 09:04:01,915 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: -107.485595703125 param sum :: 831.2928466796875
2023-01-07 09:04:01,915 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,915 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,915 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -25910.55859375
2023-01-07 09:04:01,915 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,915 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,915 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2582.33544921875
2023-01-07 09:04:01,915 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0006333681521937251
2023-01-07 09:04:01,916 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 2582.337890625 param sum :: -31256.41015625
2023-01-07 09:04:01,916 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,916 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,917 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4075.20654296875
2023-01-07 09:04:01,917 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,917 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,917 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1690.7733154296875
2023-01-07 09:04:01,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1600089967250824
2023-01-07 09:04:01,918 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: -109.23239135742188 param sum :: 797.4921875
2023-01-07 09:04:01,918 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,918 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,918 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4075.20654296875
2023-01-07 09:04:01,918 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,918 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,918 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1691.2828369140625
2023-01-07 09:04:01,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09414039552211761
2023-01-07 09:04:01,919 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: -1691.6278076171875 param sum :: 6042.4765625
2023-01-07 09:04:01,919 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,919 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,920 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -324071.15625
2023-01-07 09:04:01,920 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,920 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,920 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 3470.642578125
2023-01-07 09:04:01,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:04:01,921 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: -567.91552734375 param sum :: 3054.3759765625
2023-01-07 09:04:01,921 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,921 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,921 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -324071.15625
2023-01-07 09:04:01,921 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,921 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,921 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 3470.642578125
2023-01-07 09:04:01,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:04:01,923 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 3470.642578125 param sum :: -386857.0625
2023-01-07 09:04:01,923 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:04:01,923 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:04:01,923 > [DEBUG] 0 :: 419.33544921875
2023-01-07 09:04:01,925 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,926 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,926 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.059814453125
2023-01-07 09:04:01,926 > [DEBUG] 0 :: before allreduce fusion buffer :: -581.1031494140625
2023-01-07 09:04:01,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,928 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,928 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 132.42074584960938
2023-01-07 09:04:01,928 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,929 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,929 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -334.4075927734375
2023-01-07 09:04:01,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 202.56448364257812
2023-01-07 09:04:01,932 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,932 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,932 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -103.45928955078125
2023-01-07 09:04:01,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002483250806108117
2023-01-07 09:04:01,934 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,934 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,935 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 1.3839995861053467
2023-01-07 09:04:01,935 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,935 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,935 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -258.38916015625
2023-01-07 09:04:01,936 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6180408000946045
2023-01-07 09:04:01,938 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,939 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,939 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -8.331986427307129
2023-01-07 09:04:01,939 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00020829983986914158
2023-01-07 09:04:01,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,940 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.9339320063591003
2023-01-07 09:04:01,940 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,940 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,940 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -102.23943328857422
2023-01-07 09:04:01,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9317972660064697
2023-01-07 09:04:01,941 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,941 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,942 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.023812882602214813
2023-01-07 09:04:01,942 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0027470439672470093
2023-01-07 09:04:01,942 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,942 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,942 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 120.11698150634766
2023-01-07 09:04:01,943 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,943 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,943 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -74.38587951660156
2023-01-07 09:04:01,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 120.17253875732422
2023-01-07 09:04:01,944 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,944 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,944 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -113.15357208251953
2023-01-07 09:04:01,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.024922920390963554
2023-01-07 09:04:01,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,945 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.2658904790878296
2023-01-07 09:04:01,945 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,945 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,945 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -237.65988159179688
2023-01-07 09:04:01,945 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.939415216445923
2023-01-07 09:04:01,946 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,946 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,947 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2333.35400390625
2023-01-07 09:04:01,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0320003405213356
2023-01-07 09:04:01,947 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,947 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,948 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.0003769397735595703
2023-01-07 09:04:01,948 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,948 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,948 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 2410.62939453125
2023-01-07 09:04:01,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03308326005935669
2023-01-07 09:04:01,949 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,949 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,949 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -482.91326904296875
2023-01-07 09:04:01,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07585693895816803
2023-01-07 09:04:01,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 112.20897674560547
2023-01-07 09:04:01,950 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,950 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,950 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -439.424560546875
2023-01-07 09:04:01,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 107.98792266845703
2023-01-07 09:04:01,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,952 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,952 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -332.4407958984375
2023-01-07 09:04:01,952 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.002917054109275341
2023-01-07 09:04:01,952 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 104.27833557128906
2023-01-07 09:04:01,953 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,953 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,953 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -276.5521240234375
2023-01-07 09:04:01,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.80162811279297
2023-01-07 09:04:01,954 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,954 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,954 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -1033.20654296875
2023-01-07 09:04:01,954 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0759400725364685
2023-01-07 09:04:01,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,955 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 4.009939193725586
2023-01-07 09:04:01,955 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,955 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,955 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -984.13330078125
2023-01-07 09:04:01,956 > [DEBUG] 0 :: before allreduce fusion buffer :: -28.878156661987305
2023-01-07 09:04:01,957 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,957 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,957 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -91.98416137695312
2023-01-07 09:04:01,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.001546788844279945
2023-01-07 09:04:01,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,958 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -1.8705408573150635
2023-01-07 09:04:01,958 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,958 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,958 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -83.84378051757812
2023-01-07 09:04:01,958 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5632258653640747
2023-01-07 09:04:01,959 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,959 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,959 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -12.164318084716797
2023-01-07 09:04:01,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19191128015518188
2023-01-07 09:04:01,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,960 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -23.528459548950195
2023-01-07 09:04:01,960 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,960 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,960 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 36.32112121582031
2023-01-07 09:04:01,961 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.43128204345703
2023-01-07 09:04:01,962 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,962 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,962 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.4387340545654297
2023-01-07 09:04:01,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012276291847229004
2023-01-07 09:04:01,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,963 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -9.317068099975586
2023-01-07 09:04:01,963 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,963 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,963 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 3.438138008117676
2023-01-07 09:04:01,964 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.957330703735352
2023-01-07 09:04:01,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,965 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,965 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2782.33837890625
2023-01-07 09:04:01,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04520229995250702
2023-01-07 09:04:01,965 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,966 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -13.566232681274414
2023-01-07 09:04:01,966 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,966 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,966 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2773.08642578125
2023-01-07 09:04:01,966 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.396162033081055
2023-01-07 09:04:01,967 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,967 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,967 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.31683349609375
2023-01-07 09:04:01,967 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4317171573638916
2023-01-07 09:04:01,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -4.7574687004089355
2023-01-07 09:04:01,968 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,968 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,968 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.818168640136719
2023-01-07 09:04:01,968 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5404536724090576
2023-01-07 09:04:01,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,970 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 224.7960205078125
2023-01-07 09:04:01,970 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12050719559192657
2023-01-07 09:04:01,970 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,970 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,971 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -7.7736334800720215
2023-01-07 09:04:01,971 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,971 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,971 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 221.76165771484375
2023-01-07 09:04:01,971 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.841139316558838
2023-01-07 09:04:01,972 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,972 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,972 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 259.0042724609375
2023-01-07 09:04:01,972 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06164681538939476
2023-01-07 09:04:01,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,973 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 3.902617931365967
2023-01-07 09:04:01,973 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,973 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,973 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 248.46315002441406
2023-01-07 09:04:01,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.940065860748291
2023-01-07 09:04:01,974 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,974 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,975 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 34.62992858886719
2023-01-07 09:04:01,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07879339903593063
2023-01-07 09:04:01,975 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,975 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: -1.647963285446167
2023-01-07 09:04:01,976 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,976 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,976 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 27.987411499023438
2023-01-07 09:04:01,976 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.397867202758789
2023-01-07 09:04:01,977 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,977 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,977 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1031.1607666015625
2023-01-07 09:04:01,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3783116340637207
2023-01-07 09:04:01,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,978 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -1.1638621091842651
2023-01-07 09:04:01,978 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,978 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,978 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1039.62158203125
2023-01-07 09:04:01,978 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.288830041885376
2023-01-07 09:04:01,979 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,979 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,980 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 741.074462890625
2023-01-07 09:04:01,980 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.035674937069416046
2023-01-07 09:04:01,980 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,980 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,980 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.40236330032348633
2023-01-07 09:04:01,981 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,981 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,981 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 731.9105224609375
2023-01-07 09:04:01,981 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3650754690170288
2023-01-07 09:04:01,982 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,982 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,982 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 474.9889831542969
2023-01-07 09:04:01,982 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004600003361701965
2023-01-07 09:04:01,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,983 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 14.403154373168945
2023-01-07 09:04:01,983 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,983 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,983 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 466.3050842285156
2023-01-07 09:04:01,983 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.57540512084961
2023-01-07 09:04:01,984 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,984 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,984 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 2521.33203125
2023-01-07 09:04:01,985 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.44486069679260254
2023-01-07 09:04:01,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,985 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,985 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.6703863143920898
2023-01-07 09:04:01,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,986 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,986 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 2531.0087890625
2023-01-07 09:04:01,986 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.281071662902832
2023-01-07 09:04:01,987 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,987 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,987 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -1504.6884765625
2023-01-07 09:04:01,987 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3356611132621765
2023-01-07 09:04:01,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 15.086448669433594
2023-01-07 09:04:01,988 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,988 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,988 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -1493.3675537109375
2023-01-07 09:04:01,988 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.225457191467285
2023-01-07 09:04:01,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,989 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,989 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -985.0177001953125
2023-01-07 09:04:01,989 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7759745121002197
2023-01-07 09:04:01,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,990 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 26.513586044311523
2023-01-07 09:04:01,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,991 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -970.4287109375
2023-01-07 09:04:01,991 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.76327896118164
2023-01-07 09:04:01,992 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,992 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,992 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8479.998046875
2023-01-07 09:04:01,992 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05386461317539215
2023-01-07 09:04:01,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 1.6315784454345703
2023-01-07 09:04:01,993 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,993 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,993 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -8450.212890625
2023-01-07 09:04:01,993 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7888736724853516
2023-01-07 09:04:01,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,995 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -27.702877044677734
2023-01-07 09:04:01,995 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09936219453811646
2023-01-07 09:04:01,995 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,995 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,996 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 6.431770324707031
2023-01-07 09:04:01,996 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3538879156112671
2023-01-07 09:04:01,996 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,996 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,997 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -10.691475868225098
2023-01-07 09:04:01,997 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12439008802175522
2023-01-07 09:04:01,997 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,997 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,997 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -10.725372314453125
2023-01-07 09:04:01,998 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,998 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,998 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 24.437641143798828
2023-01-07 09:04:01,998 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.225908279418945
2023-01-07 09:04:01,999 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:01,999 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:01,999 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.56867980957031
2023-01-07 09:04:01,999 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.000934826210141182
2023-01-07 09:04:02,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,000 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -11.368532180786133
2023-01-07 09:04:02,000 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,000 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,000 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -18.305866241455078
2023-01-07 09:04:02,000 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.013633728027344
2023-01-07 09:04:02,001 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,001 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,001 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 0.3514251708984375
2023-01-07 09:04:02,002 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42361849546432495
2023-01-07 09:04:02,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,002 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,002 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -1.7974505424499512
2023-01-07 09:04:02,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,003 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 24.246883392333984
2023-01-07 09:04:02,003 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5846247673034668
2023-01-07 09:04:02,004 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,004 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,004 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 80.6624755859375
2023-01-07 09:04:02,004 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09095446765422821
2023-01-07 09:04:02,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,005 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,005 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 102.30868530273438
2023-01-07 09:04:02,005 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.031813740730285645
2023-01-07 09:04:02,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,006 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,006 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 226.4133758544922
2023-01-07 09:04:02,006 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.058910369873046875
2023-01-07 09:04:02,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,007 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 247.7777862548828
2023-01-07 09:04:02,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4683997333049774
2023-01-07 09:04:02,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,008 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -0.26955604553222656
2023-01-07 09:04:02,008 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7379359006881714
2023-01-07 09:04:02,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,009 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,009 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 46.75419616699219
2023-01-07 09:04:02,009 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.458232879638672
2023-01-07 09:04:02,010 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,010 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 59.41962814331055
2023-01-07 09:04:02,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.02634778991341591
2023-01-07 09:04:02,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,011 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 52.01332092285156
2023-01-07 09:04:02,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2136029154062271
2023-01-07 09:04:02,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,012 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -2.0440540313720703
2023-01-07 09:04:02,012 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.43995237350463867
2023-01-07 09:04:02,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,013 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 19.686416625976562
2023-01-07 09:04:02,013 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30114543437957764
2023-01-07 09:04:02,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,014 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -10.071151733398438
2023-01-07 09:04:02,014 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.902944564819336
2023-01-07 09:04:02,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,015 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.212180137634277
2023-01-07 09:04:02,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4636681079864502
2023-01-07 09:04:02,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,016 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 5.714530944824219
2023-01-07 09:04:02,016 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7659820914268494
2023-01-07 09:04:02,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,017 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 30.929061889648438
2023-01-07 09:04:02,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6460723876953125
2023-01-07 09:04:02,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,018 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 156.4761962890625
2023-01-07 09:04:02,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2421301007270813
2023-01-07 09:04:02,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,019 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -16.250633239746094
2023-01-07 09:04:02,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,019 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 140.30355834960938
2023-01-07 09:04:02,019 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.32197380065918
2023-01-07 09:04:02,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,020 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,021 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 64.98739624023438
2023-01-07 09:04:02,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.808115482330322
2023-01-07 09:04:02,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,021 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 52.73694610595703
2023-01-07 09:04:02,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.565547943115234
2023-01-07 09:04:02,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,022 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 926.8388671875
2023-01-07 09:04:02,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2407592236995697
2023-01-07 09:04:02,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,023 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 898.0227661132812
2023-01-07 09:04:02,024 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.347564697265625
2023-01-07 09:04:02,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,024 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -293.92279052734375
2023-01-07 09:04:02,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.172687530517578
2023-01-07 09:04:02,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,025 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -4.5153703689575195
2023-01-07 09:04:02,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,026 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -375.09881591796875
2023-01-07 09:04:02,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.560073852539062
2023-01-07 09:04:02,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,027 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2310.22802734375
2023-01-07 09:04:02,027 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7980073690414429
2023-01-07 09:04:02,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,028 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,028 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2317.5751953125
2023-01-07 09:04:02,028 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.687462329864502
2023-01-07 09:04:02,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,029 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 205.96365356445312
2023-01-07 09:04:02,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.364151000976562
2023-01-07 09:04:02,030 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,030 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,030 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 167.88235473632812
2023-01-07 09:04:02,030 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8777828216552734
2023-01-07 09:04:02,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,031 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1161.684326171875
2023-01-07 09:04:02,031 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.560419082641602
2023-01-07 09:04:02,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,032 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 12.618539810180664
2023-01-07 09:04:02,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,032 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1116.1097412109375
2023-01-07 09:04:02,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.18619155883789
2023-01-07 09:04:02,033 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,033 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,033 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 1039.919189453125
2023-01-07 09:04:02,033 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8446810245513916
2023-01-07 09:04:02,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -15.88620376586914
2023-01-07 09:04:02,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,034 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -478.46514892578125
2023-01-07 09:04:02,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,035 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 949.0716552734375
2023-01-07 09:04:02,035 > [DEBUG] 0 :: before allreduce fusion buffer :: -101.47718048095703
2023-01-07 09:04:02,036 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,036 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,036 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 969.3773193359375
2023-01-07 09:04:02,036 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.611929416656494
2023-01-07 09:04:02,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,037 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -549.2610473632812
2023-01-07 09:04:02,037 > [DEBUG] 0 :: before allreduce fusion buffer :: -363.3219299316406
2023-01-07 09:04:02,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,038 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -23.420936584472656
2023-01-07 09:04:02,038 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.077442169189453
2023-01-07 09:04:02,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 9.248075485229492
2023-01-07 09:04:02,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -418.97283935546875
2023-01-07 09:04:02,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 76.29331970214844
2023-01-07 09:04:02,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,040 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,040 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -418.97283935546875
2023-01-07 09:04:02,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -47.71308898925781
2023-01-07 09:04:02,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,041 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -14.082157135009766
2023-01-07 09:04:02,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,041 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 39.436622619628906
2023-01-07 09:04:02,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.01031494140625
2023-01-07 09:04:02,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,043 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1286.337890625
2023-01-07 09:04:02,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.521167755126953
2023-01-07 09:04:02,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,044 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 146.13265991210938
2023-01-07 09:04:02,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 885.3502197265625
2023-01-07 09:04:02,044 > [DEBUG] 0 :: before allreduce fusion buffer :: 142.23605346679688
2023-01-07 09:04:02,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,045 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1678.26806640625
2023-01-07 09:04:02,045 > [DEBUG] 0 :: before allreduce fusion buffer :: -39.939151763916016
2023-01-07 09:04:02,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,046 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -206.34661865234375
2023-01-07 09:04:02,046 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5189132690429688
2023-01-07 09:04:02,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,047 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1990.3211669921875
2023-01-07 09:04:02,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.850460529327393
2023-01-07 09:04:02,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,048 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 59.68280792236328
2023-01-07 09:04:02,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,048 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 26.711223602294922
2023-01-07 09:04:02,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,049 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2317.26123046875
2023-01-07 09:04:02,049 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,049 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,049 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 930.2147216796875
2023-01-07 09:04:02,049 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.7734146118164
2023-01-07 09:04:02,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,050 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2333.99462890625
2023-01-07 09:04:02,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.5267333984375
2023-01-07 09:04:02,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,051 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -102.48777770996094
2023-01-07 09:04:02,051 > [DEBUG] 0 :: before allreduce fusion buffer :: -118.81318664550781
2023-01-07 09:04:02,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,052 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2325.56640625
2023-01-07 09:04:02,052 > [DEBUG] 0 :: before allreduce fusion buffer :: 25.7126522064209
2023-01-07 09:04:02,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,053 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -374.9106140136719
2023-01-07 09:04:02,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 910.2161865234375
2023-01-07 09:04:02,053 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.495063781738281
2023-01-07 09:04:02,054 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,054 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,054 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -545.4725952148438
2023-01-07 09:04:02,055 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.263218879699707
2023-01-07 09:04:02,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,055 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 2.552682876586914
2023-01-07 09:04:02,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,056 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -822.987060546875
2023-01-07 09:04:02,056 > [DEBUG] 0 :: before allreduce fusion buffer :: 105.56998443603516
2023-01-07 09:04:02,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,057 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,057 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -822.987060546875
2023-01-07 09:04:02,057 > [DEBUG] 0 :: before allreduce fusion buffer :: -120.8963394165039
2023-01-07 09:04:02,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:04:02,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:04:02,058 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1144.591796875
2023-01-07 09:04:02,058 > [DEBUG] 0 :: before allreduce fusion buffer :: -134.33946228027344
