2023-01-07 09:18:05,056 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:18:05,057 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,094 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,094 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,094 > [DEBUG] 0 :: scheduled task in conv1._dp_wrapped_module.flat_param_0 :: 0, FW, [AR, [[1, torch.Size([128]) 0 0], [4, torch.Size([36864]) 24374 24374]]]
2023-01-07 09:18:05,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,845 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,845 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,845 > [DEBUG] 0 :: scheduled task in bn1._dp_wrapped_module.flat_param_0 :: 1, FW, [AR, [[2, torch.Size([4096]) 0 0], [4, torch.Size([36864]) 29806 29806]]]
2023-01-07 09:18:05,845 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,847 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,847 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,847 > [DEBUG] 0 :: scheduled task in layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 2, FW, [AR, [[4, torch.Size([36864]) 29895 29895]]]
2023-01-07 09:18:05,847 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,848 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,848 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,848 > [DEBUG] 0 :: scheduled task in layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 3, FW, [AR, [[4, torch.Size([36864]) 34135 34135]]]
2023-01-07 09:18:05,848 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,849 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,849 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,849 > [DEBUG] 0 :: scheduled task in layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 4, FW, [AR, [[5, torch.Size([128]) 0 0], [6, torch.Size([16384]) 0 0], [18, torch.Size([36864]) 16953 16953]]]
2023-01-07 09:18:05,849 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,884 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,884 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,884 > [DEBUG] 0 :: scheduled task in layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 5, FW, [AR, [[18, torch.Size([36864]) 19948 19948]]]
2023-01-07 09:18:05,885 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,886 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,886 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,886 > [DEBUG] 0 :: scheduled task in layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 6, FW, [AR, [[7, torch.Size([512]) 0 0], [10, torch.Size([16384]) 5999 5999]]]
2023-01-07 09:18:05,886 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,887 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,887 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,887 > [DEBUG] 0 :: scheduled task in layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 7, FW, [AR, [[12, torch.Size([36864]) 4826 4826]]]
2023-01-07 09:18:05,887 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,888 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,888 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,888 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 8, FW, [AR, [[12, torch.Size([36864]) 9052 9052]]]
2023-01-07 09:18:05,888 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,889 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,889 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,889 > [DEBUG] 0 :: scheduled task in layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 9, FW, [AR, [[12, torch.Size([36864]) 13245 13245]]]
2023-01-07 09:18:05,889 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,890 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,890 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,890 > [DEBUG] 0 :: scheduled task in layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 10, FW, [AR, [[11, torch.Size([128]) 0 0], [12, torch.Size([36864]) 15961 15961]]]
2023-01-07 09:18:05,890 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,891 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,891 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,891 > [DEBUG] 0 :: scheduled task in layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 11, FW, [AR, [[12, torch.Size([36864]) 35400 35400], [24, torch.Size([147456]) 133638 133638]]]
2023-01-07 09:18:05,891 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,892 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,892 > [DEBUG] 0 :: scheduled task in layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 12, FW, [AR, [[13, torch.Size([128]) 0 0], [14, torch.Size([16384]) 5438 5438]]]
2023-01-07 09:18:05,892 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,892 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,892 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,892 > [DEBUG] 0 :: scheduled task in layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 13, FW, [AR, [[18, torch.Size([36864]) 22600 22600]]]
2023-01-07 09:18:05,893 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,893 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,893 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,894 > [DEBUG] 0 :: scheduled task in layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 14, FW, [AR, [[16, torch.Size([16384]) 8376 8376]]]
2023-01-07 09:18:05,894 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,894 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,894 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,894 > [DEBUG] 0 :: scheduled task in layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 15, FW, [AR, [[16, torch.Size([16384]) 15379 15379], [18, torch.Size([36864]) 26760 26760]]]
2023-01-07 09:18:05,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,895 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,895 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,895 > [DEBUG] 0 :: scheduled task in layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 16, FW, [AR, [[18, torch.Size([36864]) 29950 29950]]]
2023-01-07 09:18:05,895 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,896 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,896 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,896 > [DEBUG] 0 :: scheduled task in layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 17, FW, [AR, [[18, torch.Size([36864]) 34171 34171]]]
2023-01-07 09:18:05,896 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,897 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,897 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,897 > [DEBUG] 0 :: scheduled task in layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 18, FW, [AR, [[19, torch.Size([128]) 0 0], [20, torch.Size([16384]) 0 0], [22, torch.Size([32768]) 14465 14465]]]
2023-01-07 09:18:05,897 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,898 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,898 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,898 > [DEBUG] 0 :: scheduled task in layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 19, FW, [AR, [[24, torch.Size([147456]) 134884 134884]]]
2023-01-07 09:18:05,898 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,899 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,899 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,899 > [DEBUG] 0 :: scheduled task in layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 20, FW, [AR, [[22, torch.Size([32768]) 24699 24699]]]
2023-01-07 09:18:05,899 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,900 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,900 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,900 > [DEBUG] 0 :: scheduled task in layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 21, FW, [AR, [[24, torch.Size([147456]) 137525 137525]]]
2023-01-07 09:18:05,900 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,901 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,901 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,901 > [DEBUG] 0 :: scheduled task in layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 22, FW, [AR, [[24, torch.Size([147456]) 140496 140496]]]
2023-01-07 09:18:05,901 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,902 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,902 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,902 > [DEBUG] 0 :: scheduled task in layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 23, FW, [AR, [[24, torch.Size([147456]) 144875 144875]]]
2023-01-07 09:18:05,902 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,903 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,903 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,903 > [DEBUG] 0 :: scheduled task in layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 24, FW, [AR, [[25, torch.Size([256]) 0 0], [26, torch.Size([65536]) 55354 55354]]]
2023-01-07 09:18:05,903 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,904 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,904 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,904 > [DEBUG] 0 :: scheduled task in layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 25, FW, [AR, [[26, torch.Size([65536]) 62932 62932]]]
2023-01-07 09:18:05,904 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,905 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,905 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,905 > [DEBUG] 0 :: scheduled task in layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 26, FW, [AR, [[27, torch.Size([1024]) 0 0], [28, torch.Size([131072]) 121409 121409]]]
2023-01-07 09:18:05,905 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,906 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,906 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,906 > [DEBUG] 0 :: scheduled task in layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 27, FW, [AR, [[28, torch.Size([131072]) 128101 128101]]]
2023-01-07 09:18:05,906 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,907 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,907 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,907 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 28, FW, [AR, [[30, torch.Size([65536]) 58518 58518]]]
2023-01-07 09:18:05,907 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,908 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,908 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,908 > [DEBUG] 0 :: scheduled task in layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 29, FW, [AR, [[30, torch.Size([65536]) 62926 62926]]]
2023-01-07 09:18:05,908 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,909 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,909 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,909 > [DEBUG] 0 :: scheduled task in layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 30, FW, [AR, [[31, torch.Size([256]) 0 0], [32, torch.Size([147456]) 137276 137276]]]
2023-01-07 09:18:05,909 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,910 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,910 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,910 > [DEBUG] 0 :: scheduled task in layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 31, FW, [AR, [[32, torch.Size([147456]) 144843 144843]]]
2023-01-07 09:18:05,910 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,911 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,911 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,911 > [DEBUG] 0 :: scheduled task in layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 32, FW, [AR, [[33, torch.Size([256]) 0 0], [34, torch.Size([65536]) 55010 55010]]]
2023-01-07 09:18:05,911 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,912 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,912 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,912 > [DEBUG] 0 :: scheduled task in layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 33, FW, [AR, [[34, torch.Size([65536]) 62519 62519]]]
2023-01-07 09:18:05,912 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,913 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,913 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,913 > [DEBUG] 0 :: scheduled task in layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 34, FW, [AR, [[36, torch.Size([65536]) 58337 58337]]]
2023-01-07 09:18:05,913 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,914 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,914 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,914 > [DEBUG] 0 :: scheduled task in layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 35, FW, [AR, [[36, torch.Size([65536]) 62837 62837]]]
2023-01-07 09:18:05,914 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,915 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,915 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,915 > [DEBUG] 0 :: scheduled task in layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 36, FW, [AR, [[37, torch.Size([256]) 0 0], [38, torch.Size([147456]) 136838 136838]]]
2023-01-07 09:18:05,915 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,916 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,916 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,916 > [DEBUG] 0 :: scheduled task in layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 37, FW, [AR, [[38, torch.Size([147456]) 144611 144611]]]
2023-01-07 09:18:05,916 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,917 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,917 > [DEBUG] 0 :: scheduled task in layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 38, FW, [AR, [[39, torch.Size([256]) 0 0], [40, torch.Size([65536]) 54391 54391]]]
2023-01-07 09:18:05,917 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,917 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,917 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,917 > [DEBUG] 0 :: scheduled task in layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 39, FW, [AR, [[40, torch.Size([65536]) 62211 62211]]]
2023-01-07 09:18:05,918 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,918 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,918 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,918 > [DEBUG] 0 :: scheduled task in layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 40, FW, [AR, [[41, torch.Size([1024]) 0 0], [42, torch.Size([65536]) 58457 58457]]]
2023-01-07 09:18:05,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,919 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,919 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,919 > [DEBUG] 0 :: scheduled task in layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 41, FW, [AR, [[42, torch.Size([65536]) 62434 62434]]]
2023-01-07 09:18:05,919 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,920 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,920 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,920 > [DEBUG] 0 :: scheduled task in layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 42, FW, [AR, [[43, torch.Size([256]) 0 0], [44, torch.Size([147456]) 136250 136250]]]
2023-01-07 09:18:05,920 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,921 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,921 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,921 > [DEBUG] 0 :: scheduled task in layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 43, FW, [AR, [[44, torch.Size([147456]) 144343 144343]]]
2023-01-07 09:18:05,921 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,922 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,922 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,922 > [DEBUG] 0 :: scheduled task in layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 44, FW, [AR, [[45, torch.Size([256]) 0 0], [46, torch.Size([65536]) 53786 53786]]]
2023-01-07 09:18:05,922 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,923 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,923 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,923 > [DEBUG] 0 :: scheduled task in layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 45, FW, [AR, [[46, torch.Size([65536]) 62012 62012]]]
2023-01-07 09:18:05,923 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,924 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,924 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,924 > [DEBUG] 0 :: scheduled task in layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 46, FW, [AR, [[47, torch.Size([1024]) 0 0], [48, torch.Size([131072]) 123183 123183]]]
2023-01-07 09:18:05,924 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,925 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,925 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,925 > [DEBUG] 0 :: scheduled task in layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 47, FW, [AR, [[48, torch.Size([131072]) 127567 127567]]]
2023-01-07 09:18:05,925 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,926 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,926 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,926 > [DEBUG] 0 :: scheduled task in layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 48, FW, [AR, [[49, torch.Size([512]) 0 0], [50, torch.Size([589824]) 577505 577505]]]
2023-01-07 09:18:05,926 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,927 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,927 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,927 > [DEBUG] 0 :: scheduled task in layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 49, FW, [AR, [[50, torch.Size([589824]) 586102 586102]]]
2023-01-07 09:18:05,927 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,928 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,928 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,928 > [DEBUG] 0 :: scheduled task in layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 50, FW, [AR, [[52, torch.Size([262144]) 248915 248915]]]
2023-01-07 09:18:05,928 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,929 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,929 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,929 > [DEBUG] 0 :: scheduled task in layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 51, FW, [AR, [[52, torch.Size([262144]) 258136 258136]]]
2023-01-07 09:18:05,929 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,930 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,930 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,930 > [DEBUG] 0 :: scheduled task in layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 52, FW, [AR, [[54, torch.Size([524288]) 514056 514056]]]
2023-01-07 09:18:05,930 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,931 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,931 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,931 > [DEBUG] 0 :: scheduled task in layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 53, FW, [AR, [[54, torch.Size([524288]) 520083 520083]]]
2023-01-07 09:18:05,931 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,932 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,932 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 54, FW, [AR, [[56, torch.Size([262144]) 251612 251612]]]
2023-01-07 09:18:05,932 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,932 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,932 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,933 > [DEBUG] 0 :: scheduled task in layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 55, FW, [AR, [[56, torch.Size([262144]) 258055 258055]]]
2023-01-07 09:18:05,933 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,933 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,933 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,934 > [DEBUG] 0 :: scheduled task in layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 56, FW, [AR, [[57, torch.Size([512]) 0 0], [58, torch.Size([589824]) 575870 575870]]]
2023-01-07 09:18:05,934 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,934 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,934 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,934 > [DEBUG] 0 :: scheduled task in layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 57, FW, [AR, [[58, torch.Size([589824]) 585646 585646]]]
2023-01-07 09:18:05,935 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,935 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,935 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,935 > [DEBUG] 0 :: scheduled task in layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 58, FW, [AR, [[60, torch.Size([262144]) 250298 250298]]]
2023-01-07 09:18:05,936 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,936 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,936 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,936 > [DEBUG] 0 :: scheduled task in layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 59, FW, [AR, [[60, torch.Size([262144]) 257996 257996]]]
2023-01-07 09:18:05,937 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,937 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,937 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,937 > [DEBUG] 0 :: scheduled task in layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 60, FW, [AR, [[62, torch.Size([262144]) 251486 251486]]]
2023-01-07 09:18:05,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,938 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,938 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,938 > [DEBUG] 0 :: scheduled task in layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 61, FW, [AR, [[62, torch.Size([262144]) 257960 257960]]]
2023-01-07 09:18:05,938 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,939 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,939 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,939 > [DEBUG] 0 :: scheduled task in layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 62, FW, [AR, [[64, torch.Size([589824]) 578797 578797]]]
2023-01-07 09:18:05,939 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,940 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,940 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,940 > [DEBUG] 0 :: scheduled task in layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 63, FW, [AR, [[64, torch.Size([589824]) 585633 585633]]]
2023-01-07 09:18:05,940 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,941 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,941 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,941 > [DEBUG] 0 :: scheduled task in layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 64, FW, [AR, [[66, torch.Size([262144]) 250265 250265]]]
2023-01-07 09:18:05,941 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,942 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,942 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,942 > [DEBUG] 0 :: scheduled task in layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 65, FW, [AR, [[66, torch.Size([262144]) 257992 257992]]]
2023-01-07 09:18:05,942 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,943 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,943 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,943 > [DEBUG] 0 :: scheduled task in layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 66, FW, [AR, [[68, torch.Size([262144]) 251489 251489]]]
2023-01-07 09:18:05,943 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,944 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,944 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,944 > [DEBUG] 0 :: scheduled task in layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 67, FW, [AR, [[68, torch.Size([262144]) 257975 257975]]]
2023-01-07 09:18:05,944 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,945 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,945 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,945 > [DEBUG] 0 :: scheduled task in layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 68, FW, [AR, [[70, torch.Size([589824]) 578834 578834]]]
2023-01-07 09:18:05,945 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,945 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,946 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,946 > [DEBUG] 0 :: scheduled task in layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 69, FW, [AR, [[70, torch.Size([589824]) 585643 585643]]]
2023-01-07 09:18:05,946 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,946 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,947 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,947 > [DEBUG] 0 :: scheduled task in layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 70, FW, [AR, [[72, torch.Size([262144]) 250221 250221]]]
2023-01-07 09:18:05,947 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,947 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,947 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,947 > [DEBUG] 0 :: scheduled task in layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 71, FW, [AR, [[72, torch.Size([262144]) 257983 257983]]]
2023-01-07 09:18:05,948 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,948 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,948 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,948 > [DEBUG] 0 :: scheduled task in layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 72, FW, [AR, [[74, torch.Size([262144]) 251508 251508]]]
2023-01-07 09:18:05,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,949 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,949 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,949 > [DEBUG] 0 :: scheduled task in layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 73, FW, [AR, [[74, torch.Size([262144]) 257957 257957]]]
2023-01-07 09:18:05,949 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,950 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,950 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,950 > [DEBUG] 0 :: scheduled task in layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 74, FW, [AR, [[76, torch.Size([589824]) 578817 578817]]]
2023-01-07 09:18:05,950 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,951 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,951 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,951 > [DEBUG] 0 :: scheduled task in layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 75, FW, [AR, [[76, torch.Size([589824]) 585626 585626]]]
2023-01-07 09:18:05,951 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,952 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,952 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,952 > [DEBUG] 0 :: scheduled task in layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 76, FW, [AR, [[78, torch.Size([262144]) 250146 250146]]]
2023-01-07 09:18:05,952 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,953 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,953 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,953 > [DEBUG] 0 :: scheduled task in layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 77, FW, [AR, [[78, torch.Size([262144]) 258090 258090]]]
2023-01-07 09:18:05,953 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,954 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,954 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,954 > [DEBUG] 0 :: scheduled task in layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 78, FW, [AR, [[80, torch.Size([262144]) 252751 252751]]]
2023-01-07 09:18:05,954 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,955 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,955 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,955 > [DEBUG] 0 :: scheduled task in layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 79, FW, [AR, [[80, torch.Size([262144]) 258004 258004]]]
2023-01-07 09:18:05,955 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,956 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,956 > [DEBUG] 0 :: scheduled task in layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 80, FW, [AR, [[82, torch.Size([589824]) 579303 579303]]]
2023-01-07 09:18:05,956 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,956 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,956 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,956 > [DEBUG] 0 :: scheduled task in layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 81, FW, [AR, [[82, torch.Size([589824]) 585666 585666]]]
2023-01-07 09:18:05,957 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,957 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,957 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,957 > [DEBUG] 0 :: scheduled task in layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 82, FW, [AR, [[84, torch.Size([262144]) 249049 249049]]]
2023-01-07 09:18:05,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,958 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,958 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,958 > [DEBUG] 0 :: scheduled task in layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 83, FW, [AR, [[84, torch.Size([262144]) 258029 258029]]]
2023-01-07 09:18:05,958 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,959 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,959 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,959 > [DEBUG] 0 :: scheduled task in layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 84, FW, [AR, [[86, torch.Size([524288]) 513996 513996]]]
2023-01-07 09:18:05,959 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,960 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,960 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,960 > [DEBUG] 0 :: scheduled task in layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 85, FW, [AR, [[86, torch.Size([524288]) 520257 520257]]]
2023-01-07 09:18:05,960 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,961 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,961 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,961 > [DEBUG] 0 :: scheduled task in layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 86, FW, [AR, [[88, torch.Size([2359296]) 2348986 2348986]]]
2023-01-07 09:18:05,961 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,962 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,962 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,962 > [DEBUG] 0 :: scheduled task in layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 87, FW, [AR, [[88, torch.Size([2359296]) 2355264 2355264]]]
2023-01-07 09:18:05,962 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,963 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,963 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,963 > [DEBUG] 0 :: scheduled task in layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 88, FW, [AR, [[90, torch.Size([1048576]) 1038739 1038739]]]
2023-01-07 09:18:05,963 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,964 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,964 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,964 > [DEBUG] 0 :: scheduled task in layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 89, FW, [AR, [[90, torch.Size([1048576]) 1044678 1044678]]]
2023-01-07 09:18:05,964 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,965 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,965 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,965 > [DEBUG] 0 :: scheduled task in layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 90, FW, [AR, [[92, torch.Size([2097152]) 2086529 2086529]]]
2023-01-07 09:18:05,965 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,966 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,966 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,966 > [DEBUG] 0 :: scheduled task in layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 91, FW, [AR, [[92, torch.Size([2097152]) 2093335 2093335]]]
2023-01-07 09:18:05,966 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,966 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,966 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,967 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 92, FW, [AR, [[94, torch.Size([1048576]) 1039183 1039183]]]
2023-01-07 09:18:05,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,967 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,967 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,967 > [DEBUG] 0 :: scheduled task in layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 93, FW, [AR, [[94, torch.Size([1048576]) 1044932 1044932]]]
2023-01-07 09:18:05,967 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,968 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,968 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,968 > [DEBUG] 0 :: scheduled task in layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 94, FW, [AR, [[96, torch.Size([2359296]) 2350332 2350332]]]
2023-01-07 09:18:05,968 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,969 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,969 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,969 > [DEBUG] 0 :: scheduled task in layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 95, FW, [AR, [[96, torch.Size([2359296]) 2355789 2355789]]]
2023-01-07 09:18:05,969 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,970 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,970 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,970 > [DEBUG] 0 :: scheduled task in layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 96, FW, [AR, [[98, torch.Size([1048576]) 1038824 1038824]]]
2023-01-07 09:18:05,970 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,971 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,971 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,971 > [DEBUG] 0 :: scheduled task in layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 97, FW, [AR, [[98, torch.Size([1048576]) 1045258 1045258]]]
2023-01-07 09:18:05,971 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,972 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,972 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,972 > [DEBUG] 0 :: scheduled task in layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 98, FW, [AR, [[100, torch.Size([1048576]) 1041045 1041045]]]
2023-01-07 09:18:05,972 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,973 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,973 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,973 > [DEBUG] 0 :: scheduled task in layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 99, FW, [AR, [[100, torch.Size([1048576]) 1045245 1045245]]]
2023-01-07 09:18:05,973 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,974 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,974 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,974 > [DEBUG] 0 :: scheduled task in layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 100, FW, [AR, [[102, torch.Size([2359296]) 2351012 2351012]]]
2023-01-07 09:18:05,974 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,975 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,975 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,975 > [DEBUG] 0 :: scheduled task in layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 101, FW, [AR, [[102, torch.Size([2359296]) 2356170 2356170]]]
2023-01-07 09:18:05,975 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,976 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,976 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,976 > [DEBUG] 0 :: scheduled task in layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 102, FW, [AR, [[104, torch.Size([1048576]) 1040523 1040523]]]
2023-01-07 09:18:05,976 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,977 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,977 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,977 > [DEBUG] 0 :: scheduled task in layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 103, FW, [AR, [[104, torch.Size([1048576]) 1045407 1045407]]]
2023-01-07 09:18:05,977 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,977 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,978 > [DEBUG] 0 :: scheduled task in layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 104, FW, [AR, [[106, torch.Size([2049000]) 2039302 2039302]]]
2023-01-07 09:18:05,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,978 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,978 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,978 > [DEBUG] 0 :: scheduled task in layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 105, FW, [AR, [[106, torch.Size([2049000]) 2045884 2045884]]]
2023-01-07 09:18:05,978 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:05,980 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:05,980 > [DEBUG] 0 :: ########### task is not assigned to module############
2023-01-07 09:18:05,980 > [DEBUG] 0 :: scheduled task in fc._dp_wrapped_module.flat_param_0 :: No scheduled
2023-01-07 09:18:05,981 > [DEBUG] 0 :: 7.068889617919922
2023-01-07 09:18:05,985 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:05,985 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:05,985 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.01983642578125
2023-01-07 09:18:05,986 > [DEBUG] 0 :: before allreduce fusion buffer :: -365.0428466796875
2023-01-07 09:18:05,989 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:05,989 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:05,989 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 0.1814342439174652
2023-01-07 09:18:05,990 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:05,990 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:05,990 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -191.6834716796875
2023-01-07 09:18:05,990 > [DEBUG] 0 :: before allreduce fusion buffer :: -371.9093322753906
2023-01-07 09:18:06,002 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,002 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,002 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1.7785413265228271
2023-01-07 09:18:06,002 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.013921493664383888
2023-01-07 09:18:06,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,003 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,003 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.02526206150650978
2023-01-07 09:18:06,003 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,003 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,003 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -148.10604858398438
2023-01-07 09:18:06,003 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24035003781318665
2023-01-07 09:18:06,005 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,005 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,006 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -10.921355247497559
2023-01-07 09:18:06,006 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11977285891771317
2023-01-07 09:18:06,006 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,006 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,006 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.0064726099371910095
2023-01-07 09:18:06,007 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,007 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,007 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -353.4886169433594
2023-01-07 09:18:06,007 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3340623378753662
2023-01-07 09:18:06,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,008 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,008 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -5.141512393951416
2023-01-07 09:18:06,009 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.041252411901950836
2023-01-07 09:18:06,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,009 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,009 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 0.18831630051136017
2023-01-07 09:18:06,009 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,010 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,010 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -396.2547302246094
2023-01-07 09:18:06,010 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24015814065933228
2023-01-07 09:18:06,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,011 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,011 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -3.5295581817626953
2023-01-07 09:18:06,011 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1699191927909851
2023-01-07 09:18:06,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,012 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,012 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.05349007248878479
2023-01-07 09:18:06,012 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,012 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,012 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -194.22947692871094
2023-01-07 09:18:06,012 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8998618125915527
2023-01-07 09:18:06,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,014 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,014 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -0.7829976081848145
2023-01-07 09:18:06,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.12118324637413025
2023-01-07 09:18:06,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,015 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,015 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.04600333422422409
2023-01-07 09:18:06,015 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,015 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,015 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -355.9126281738281
2023-01-07 09:18:06,015 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.293740451335907
2023-01-07 09:18:06,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,016 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,017 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -2.0598936080932617
2023-01-07 09:18:06,017 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.47396695613861084
2023-01-07 09:18:06,017 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,017 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,018 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.23653700947761536
2023-01-07 09:18:06,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,018 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -492.2345886230469
2023-01-07 09:18:06,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42981764674186707
2023-01-07 09:18:06,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,019 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,020 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -14.751577377319336
2023-01-07 09:18:06,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.42149221897125244
2023-01-07 09:18:06,020 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,020 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,020 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 0.015306826680898666
2023-01-07 09:18:06,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,021 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -517.9241943359375
2023-01-07 09:18:06,021 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.27250373363494873
2023-01-07 09:18:06,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,022 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,022 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -5.217815399169922
2023-01-07 09:18:06,022 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.007021404802799225
2023-01-07 09:18:06,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,023 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,023 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.01986505836248398
2023-01-07 09:18:06,023 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,023 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,023 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -259.7485656738281
2023-01-07 09:18:06,023 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.8241937160491943
2023-01-07 09:18:06,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,025 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,025 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 19.239084243774414
2023-01-07 09:18:06,025 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14554566144943237
2023-01-07 09:18:06,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,026 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,026 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 0.1479029357433319
2023-01-07 09:18:06,026 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,026 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,026 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -347.3098449707031
2023-01-07 09:18:06,027 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5048809051513672
2023-01-07 09:18:06,028 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,028 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,028 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 1.644303321838379
2023-01-07 09:18:06,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0660734176635742
2023-01-07 09:18:06,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,029 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,029 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -0.1315252035856247
2023-01-07 09:18:06,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,029 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -359.4178466796875
2023-01-07 09:18:06,029 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1974328756332397
2023-01-07 09:18:06,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,031 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,031 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 4.023860931396484
2023-01-07 09:18:06,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09175518155097961
2023-01-07 09:18:06,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,032 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,032 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.029185503721237183
2023-01-07 09:18:06,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,032 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -244.5192108154297
2023-01-07 09:18:06,032 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25515955686569214
2023-01-07 09:18:06,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,034 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,034 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1.7008531093597412
2023-01-07 09:18:06,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.033218830823898315
2023-01-07 09:18:06,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,035 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,035 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -0.06918813288211823
2023-01-07 09:18:06,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,035 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -241.6321563720703
2023-01-07 09:18:06,035 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9234627485275269
2023-01-07 09:18:06,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,037 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,037 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.0072526931762695
2023-01-07 09:18:06,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.346946120262146
2023-01-07 09:18:06,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,038 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,038 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.09104357659816742
2023-01-07 09:18:06,038 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,038 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,038 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -341.55657958984375
2023-01-07 09:18:06,038 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7248129844665527
2023-01-07 09:18:06,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,040 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 8.882233619689941
2023-01-07 09:18:06,040 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.40188586711883545
2023-01-07 09:18:06,040 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,040 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,040 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.04630015045404434
2023-01-07 09:18:06,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,041 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -235.50003051757812
2023-01-07 09:18:06,041 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3147420287132263
2023-01-07 09:18:06,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,042 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,042 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -26.39142417907715
2023-01-07 09:18:06,042 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6934288740158081
2023-01-07 09:18:06,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,043 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,043 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.029283437877893448
2023-01-07 09:18:06,043 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,043 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,043 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -262.3347473144531
2023-01-07 09:18:06,043 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3678540289402008
2023-01-07 09:18:06,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,045 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,045 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -6.118171215057373
2023-01-07 09:18:06,045 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.304156541824341
2023-01-07 09:18:06,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,045 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,046 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.06603194773197174
2023-01-07 09:18:06,046 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,046 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,046 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -347.17657470703125
2023-01-07 09:18:06,046 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9806963801383972
2023-01-07 09:18:06,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,047 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,047 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 2.891214370727539
2023-01-07 09:18:06,047 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09981980174779892
2023-01-07 09:18:06,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,048 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,048 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.05464857816696167
2023-01-07 09:18:06,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,048 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -234.84609985351562
2023-01-07 09:18:06,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1699039936065674
2023-01-07 09:18:06,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,050 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,050 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -6.369050025939941
2023-01-07 09:18:06,050 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2184913158416748
2023-01-07 09:18:06,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,051 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,051 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.058009564876556396
2023-01-07 09:18:06,051 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,051 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,051 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -242.2650909423828
2023-01-07 09:18:06,051 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13990938663482666
2023-01-07 09:18:06,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,052 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,052 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 7.380273342132568
2023-01-07 09:18:06,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6293652057647705
2023-01-07 09:18:06,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,053 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,053 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -0.108970507979393
2023-01-07 09:18:06,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,053 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -340.4832458496094
2023-01-07 09:18:06,054 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21191459894180298
2023-01-07 09:18:06,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,055 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,055 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 10.097972869873047
2023-01-07 09:18:06,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7694236636161804
2023-01-07 09:18:06,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,056 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,056 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.07853192090988159
2023-01-07 09:18:06,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,056 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -215.308349609375
2023-01-07 09:18:06,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.3145666122436523
2023-01-07 09:18:06,057 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,057 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,057 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 8.375307083129883
2023-01-07 09:18:06,058 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.28795868158340454
2023-01-07 09:18:06,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,058 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.21558541059494019
2023-01-07 09:18:06,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,059 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -202.45982360839844
2023-01-07 09:18:06,059 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8971940875053406
2023-01-07 09:18:06,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,060 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,060 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 31.547142028808594
2023-01-07 09:18:06,060 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.38078978657722473
2023-01-07 09:18:06,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,061 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,061 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.04931734502315521
2023-01-07 09:18:06,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,061 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,061 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -286.4425048828125
2023-01-07 09:18:06,061 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5735580921173096
2023-01-07 09:18:06,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,063 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,063 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2.0078043937683105
2023-01-07 09:18:06,063 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8241387009620667
2023-01-07 09:18:06,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,064 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,064 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.01794809103012085
2023-01-07 09:18:06,064 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,064 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,064 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -180.54527282714844
2023-01-07 09:18:06,064 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.825043797492981
2023-01-07 09:18:06,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,065 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,065 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -2.8176193237304688
2023-01-07 09:18:06,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.41375732421875
2023-01-07 09:18:06,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,066 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -177.58682250976562
2023-01-07 09:18:06,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4196171760559082
2023-01-07 09:18:06,067 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,067 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,067 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 0.07417106628417969
2023-01-07 09:18:06,067 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9435343742370605
2023-01-07 09:18:06,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,068 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,068 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.3119434118270874
2023-01-07 09:18:06,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,068 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -275.3594055175781
2023-01-07 09:18:06,069 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7443739175796509
2023-01-07 09:18:06,070 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,070 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,070 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 18.897979736328125
2023-01-07 09:18:06,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5576711297035217
2023-01-07 09:18:06,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,071 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,071 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -0.07920993864536285
2023-01-07 09:18:06,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,071 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -87.3387451171875
2023-01-07 09:18:06,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9666311740875244
2023-01-07 09:18:06,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,073 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,073 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 33.60717010498047
2023-01-07 09:18:06,073 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9038197994232178
2023-01-07 09:18:06,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,074 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,074 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -0.3006247878074646
2023-01-07 09:18:06,074 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,074 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,074 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -93.85706329345703
2023-01-07 09:18:06,074 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8988760113716125
2023-01-07 09:18:06,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,076 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,076 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 95.0353012084961
2023-01-07 09:18:06,076 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.452746868133545
2023-01-07 09:18:06,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,077 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -28.76062774658203
2023-01-07 09:18:06,077 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.10357666015625
2023-01-07 09:18:06,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,078 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,078 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 21.818931579589844
2023-01-07 09:18:06,078 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.449876308441162
2023-01-07 09:18:06,079 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,079 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,079 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -143.20343017578125
2023-01-07 09:18:06,079 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3972668647766113
2023-01-07 09:18:06,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,080 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -3.8948235511779785
2023-01-07 09:18:06,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0097311735153198
2023-01-07 09:18:06,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,081 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,081 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -118.44132232666016
2023-01-07 09:18:06,081 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.783923625946045
2023-01-07 09:18:06,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,082 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 66.93804931640625
2023-01-07 09:18:06,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.324502944946289
2023-01-07 09:18:06,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,083 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -49.08933639526367
2023-01-07 09:18:06,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -26.628482818603516
2023-01-07 09:18:06,084 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,084 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,085 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 7.4987592697143555
2023-01-07 09:18:06,085 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.619192361831665
2023-01-07 09:18:06,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,085 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -151.22994995117188
2023-01-07 09:18:06,086 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40003132820129395
2023-01-07 09:18:06,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,086 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,086 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 2.329376459121704
2023-01-07 09:18:06,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.0013816356658936
2023-01-07 09:18:06,087 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,087 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,087 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -95.9586181640625
2023-01-07 09:18:06,088 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.907141923904419
2023-01-07 09:18:06,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,088 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,088 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 187.14276123046875
2023-01-07 09:18:06,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.698657035827637
2023-01-07 09:18:06,089 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,089 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,089 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 90.5972671508789
2023-01-07 09:18:06,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.242077827453613
2023-01-07 09:18:06,091 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,091 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,091 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -17.89864158630371
2023-01-07 09:18:06,091 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4878222942352295
2023-01-07 09:18:06,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,092 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,092 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: -0.04077249765396118
2023-01-07 09:18:06,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,092 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -178.0911865234375
2023-01-07 09:18:06,092 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4588310718536377
2023-01-07 09:18:06,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,093 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,093 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -1.6102910041809082
2023-01-07 09:18:06,093 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1810725927352905
2023-01-07 09:18:06,094 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,094 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,094 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -103.16607666015625
2023-01-07 09:18:06,094 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.559081792831421
2023-01-07 09:18:06,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,095 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,095 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -146.3595428466797
2023-01-07 09:18:06,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.16714096069336
2023-01-07 09:18:06,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,096 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -240.80458068847656
2023-01-07 09:18:06,096 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.3183822631836
2023-01-07 09:18:06,097 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,097 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,097 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -60.582176208496094
2023-01-07 09:18:06,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1717751026153564
2023-01-07 09:18:06,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,098 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,098 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.29660236835479736
2023-01-07 09:18:06,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,098 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -236.65110778808594
2023-01-07 09:18:06,099 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.0373759269714355
2023-01-07 09:18:06,100 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,100 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,100 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.63117980957031
2023-01-07 09:18:06,100 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2853519320487976
2023-01-07 09:18:06,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,101 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -102.61490631103516
2023-01-07 09:18:06,101 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.430604934692383
2023-01-07 09:18:06,102 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,102 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,102 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -44.90237808227539
2023-01-07 09:18:06,102 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.825663566589355
2023-01-07 09:18:06,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,103 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -155.73800659179688
2023-01-07 09:18:06,103 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.139808654785156
2023-01-07 09:18:06,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,104 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,104 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -124.82363891601562
2023-01-07 09:18:06,105 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.54524230957031
2023-01-07 09:18:06,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,105 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,105 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.699043869972229
2023-01-07 09:18:06,105 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,106 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -229.2354278564453
2023-01-07 09:18:06,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.53813934326172
2023-01-07 09:18:06,107 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,107 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,107 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -962.5260009765625
2023-01-07 09:18:06,107 > [DEBUG] 0 :: before allreduce fusion buffer :: -35.77016830444336
2023-01-07 09:18:06,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,108 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,108 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.9267786145210266
2023-01-07 09:18:06,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,108 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,108 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -44.524085998535156
2023-01-07 09:18:06,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,109 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1243.6207275390625
2023-01-07 09:18:06,109 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.07930755615234
2023-01-07 09:18:06,110 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,110 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,110 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1345.287841796875
2023-01-07 09:18:06,111 > [DEBUG] 0 :: before allreduce fusion buffer :: 24.095417022705078
2023-01-07 09:18:06,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,111 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -898.76220703125
2023-01-07 09:18:06,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.500587463378906
2023-01-07 09:18:06,113 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,113 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,113 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -93.85716247558594
2023-01-07 09:18:06,113 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.6611478328704834
2023-01-07 09:18:06,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,114 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,114 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 0.44838690757751465
2023-01-07 09:18:06,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,114 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -268.29083251953125
2023-01-07 09:18:06,114 > [DEBUG] 0 :: before allreduce fusion buffer :: -33.92246627807617
2023-01-07 09:18:06,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,116 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -923.42138671875
2023-01-07 09:18:06,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -59.761253356933594
2023-01-07 09:18:06,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,117 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,117 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -2.24580979347229
2023-01-07 09:18:06,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,117 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,117 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 30.8289794921875
2023-01-07 09:18:06,117 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.046689987182617
2023-01-07 09:18:06,118 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,118 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,118 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1236.50341796875
2023-01-07 09:18:06,118 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.380943298339844
2023-01-07 09:18:06,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,119 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,119 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -30.607866287231445
2023-01-07 09:18:06,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,119 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1553.755126953125
2023-01-07 09:18:06,120 > [DEBUG] 0 :: before allreduce fusion buffer :: -51.471160888671875
2023-01-07 09:18:06,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,122 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1435.3587646484375
2023-01-07 09:18:06,122 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.068443298339844
2023-01-07 09:18:06,123 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,123 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,123 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -66.94563293457031
2023-01-07 09:18:06,123 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.960586547851562
2023-01-07 09:18:06,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,124 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -1714.109619140625
2023-01-07 09:18:06,124 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.850006103515625
2023-01-07 09:18:06,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,125 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,125 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.11709880828857422
2023-01-07 09:18:06,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,125 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,125 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -39.46266174316406
2023-01-07 09:18:06,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,125 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2139.40576171875
2023-01-07 09:18:06,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,125 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1798.9222412109375
2023-01-07 09:18:06,126 > [DEBUG] 0 :: before allreduce fusion buffer :: 53.55767059326172
2023-01-07 09:18:06,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,127 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2284.174072265625
2023-01-07 09:18:06,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.742084503173828
2023-01-07 09:18:06,128 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,128 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,128 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -43.64178466796875
2023-01-07 09:18:06,128 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.661949157714844
2023-01-07 09:18:06,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,129 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2503.2587890625
2023-01-07 09:18:06,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1804111003875732
2023-01-07 09:18:06,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,130 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1250.04833984375
2023-01-07 09:18:06,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,130 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1815.2279052734375
2023-01-07 09:18:06,130 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.235816955566406
2023-01-07 09:18:06,131 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,131 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,131 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -119.15579223632812
2023-01-07 09:18:06,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 70.60809326171875
2023-01-07 09:18:06,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,132 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,132 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 0.7885708808898926
2023-01-07 09:18:06,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,133 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -394.77734375
2023-01-07 09:18:06,133 > [DEBUG] 0 :: before allreduce fusion buffer :: 146.9222869873047
2023-01-07 09:18:06,134 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,134 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,134 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1322.9779052734375
2023-01-07 09:18:06,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -206.18812561035156
2023-01-07 09:18:06,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,135 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1758.4268798828125
2023-01-07 09:18:06,135 > [DEBUG] 0 :: before allreduce fusion buffer :: -116.03817749023438
2023-01-07 09:18:06,143 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:18:06,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,143 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:06,143 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -987.4298095703125
2023-01-07 09:18:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,144 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2194.02587890625
2023-01-07 09:18:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,144 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1830.9007568359375
2023-01-07 09:18:06,144 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,144 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,144 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -871.9088134765625
2023-01-07 09:18:06,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,145 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -643.1917114257812
2023-01-07 09:18:06,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,145 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -879.82080078125
2023-01-07 09:18:06,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,145 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -948.20166015625
2023-01-07 09:18:06,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -688.6766357421875
2023-01-07 09:18:06,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -627.0828857421875
2023-01-07 09:18:06,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,146 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -642.435546875
2023-01-07 09:18:06,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,147 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -579.465087890625
2023-01-07 09:18:06,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,147 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -654.587646484375
2023-01-07 09:18:06,147 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,147 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,147 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -742.7691650390625
2023-01-07 09:18:06,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,148 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -696.2572021484375
2023-01-07 09:18:06,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -640.0065307617188
2023-01-07 09:18:06,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,148 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -737.6246337890625
2023-01-07 09:18:06,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,149 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -978.3790283203125
2023-01-07 09:18:06,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,149 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -691.0252685546875
2023-01-07 09:18:06,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,149 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -1036.4547119140625
2023-01-07 09:18:06,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -881.2537231445312
2023-01-07 09:18:06,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -1004.792236328125
2023-01-07 09:18:06,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,150 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -938.5936279296875
2023-01-07 09:18:06,150 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,150 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -902.215576171875
2023-01-07 09:18:06,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1048.096435546875
2023-01-07 09:18:06,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,151 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1013.2490234375
2023-01-07 09:18:06,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,152 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -990.294921875
2023-01-07 09:18:06,152 > [DEBUG] 0 :: before allreduce fusion buffer :: -781.8548583984375
2023-01-07 09:18:06,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1073.1087646484375
2023-01-07 09:18:06,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1022.30224609375
2023-01-07 09:18:06,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,154 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -1007.0294189453125
2023-01-07 09:18:06,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1070.3206787109375
2023-01-07 09:18:06,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -1015.0984497070312
2023-01-07 09:18:06,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -990.501220703125
2023-01-07 09:18:06,155 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,155 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,155 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1080.3685302734375
2023-01-07 09:18:06,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,156 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -1033.7698974609375
2023-01-07 09:18:06,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,156 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -1113.0072021484375
2023-01-07 09:18:06,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.105515480041504
2023-01-07 09:18:06,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,157 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -825.4328002929688
2023-01-07 09:18:06,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,157 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1055.8865966796875
2023-01-07 09:18:06,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,157 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -1030.8199462890625
2023-01-07 09:18:06,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,157 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,158 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -1087.11865234375
2023-01-07 09:18:06,158 > [DEBUG] 0 :: before allreduce fusion buffer :: -19.607044219970703
2023-01-07 09:18:06,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,158 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -768.4135131835938
2023-01-07 09:18:06,158 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -932.97705078125
2023-01-07 09:18:06,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -1024.0740966796875
2023-01-07 09:18:06,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.442604064941406
2023-01-07 09:18:06,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -989.3323364257812
2023-01-07 09:18:06,160 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:06,160 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:06,160 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -356.442626953125
2023-01-07 09:18:06,160 > [DEBUG] 0 :: before allreduce fusion buffer :: 729.1058349609375
2023-01-07 09:18:07,008 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 148.90597534179688
2023-01-07 09:18:07,008 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,008 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,008 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:07,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,008 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,008 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 5.788139343261719
2023-01-07 09:18:07,008 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,008 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,008 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2148.6923828125
2023-01-07 09:18:07,008 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.000944137573242
2023-01-07 09:18:07,010 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 62.20000076293945
2023-01-07 09:18:07,010 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,011 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,011 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 18.332433700561523
2023-01-07 09:18:07,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,011 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,011 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -120.15834045410156
2023-01-07 09:18:07,011 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,011 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,011 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2336.786376953125
2023-01-07 09:18:07,011 > [DEBUG] 0 :: before allreduce fusion buffer :: -123.01310729980469
2023-01-07 09:18:07,012 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 18.132328033447266
2023-01-07 09:18:07,012 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,013 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,013 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -3.8703083992004395
2023-01-07 09:18:07,013 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,013 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,013 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2351.191650390625
2023-01-07 09:18:07,013 > [DEBUG] 0 :: before allreduce fusion buffer :: 89.05776977539062
2023-01-07 09:18:07,014 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 62.40001678466797
2023-01-07 09:18:07,014 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,014 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,014 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: -3.8703083992004395
2023-01-07 09:18:07,014 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,014 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,014 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -2275.7763671875
2023-01-07 09:18:07,014 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.185678482055664
2023-01-07 09:18:07,015 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 287.67596435546875
2023-01-07 09:18:07,015 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,015 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,016 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:07,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,016 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,016 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: 3.6202685832977295
2023-01-07 09:18:07,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,016 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,016 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 28.342304229736328
2023-01-07 09:18:07,016 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,016 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,016 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2761.3935546875
2023-01-07 09:18:07,016 > [DEBUG] 0 :: before allreduce fusion buffer :: 36.52586364746094
2023-01-07 09:18:07,018 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 64.0
2023-01-07 09:18:07,018 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,018 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,018 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.475543022155762
2023-01-07 09:18:07,018 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,018 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,018 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2764.2392578125
2023-01-07 09:18:07,018 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49529480934143066
2023-01-07 09:18:07,019 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -24.181324005126953
2023-01-07 09:18:07,019 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,019 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,019 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:18:07,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,019 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,019 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 2.580808162689209
2023-01-07 09:18:07,019 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,019 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,020 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -983.9701538085938
2023-01-07 09:18:07,020 > [DEBUG] 0 :: before allreduce fusion buffer :: -34.10049819946289
2023-01-07 09:18:07,021 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 256.4000244140625
2023-01-07 09:18:07,021 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,021 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,021 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.345697402954102
2023-01-07 09:18:07,021 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,021 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,021 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -914.875244140625
2023-01-07 09:18:07,021 > [DEBUG] 0 :: before allreduce fusion buffer :: -123.64053344726562
2023-01-07 09:18:07,022 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 264.40533447265625
2023-01-07 09:18:07,022 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,022 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,022 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.345697402954102
2023-01-07 09:18:07,022 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,022 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,022 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -912.9434204101562
2023-01-07 09:18:07,023 > [DEBUG] 0 :: before allreduce fusion buffer :: 70.22491455078125
2023-01-07 09:18:07,024 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 270.2000427246094
2023-01-07 09:18:07,024 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,024 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,024 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.345697402954102
2023-01-07 09:18:07,024 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,024 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,024 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -812.2198486328125
2023-01-07 09:18:07,024 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.940092086791992
2023-01-07 09:18:07,025 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 165.970458984375
2023-01-07 09:18:07,025 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,025 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,025 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:07,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,025 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -1.4825384616851807
2023-01-07 09:18:07,025 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,025 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,025 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -826.2867431640625
2023-01-07 09:18:07,026 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.830678939819336
2023-01-07 09:18:07,027 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 64.79994201660156
2023-01-07 09:18:07,027 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,027 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,027 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -7.345697402954102
2023-01-07 09:18:07,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,027 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -626.9830322265625
2023-01-07 09:18:07,027 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,027 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,028 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1963.6077880859375
2023-01-07 09:18:07,028 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0385589599609375
2023-01-07 09:18:07,029 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 62.452510833740234
2023-01-07 09:18:07,029 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,029 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,029 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:07,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,029 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -3.3745415210723877
2023-01-07 09:18:07,029 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,029 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,029 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -923.2017822265625
2023-01-07 09:18:07,029 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.199100494384766
2023-01-07 09:18:07,031 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.0
2023-01-07 09:18:07,031 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,031 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,031 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.475543022155762
2023-01-07 09:18:07,031 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,031 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,031 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2723.56396484375
2023-01-07 09:18:07,031 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.47007751464844
2023-01-07 09:18:07,032 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 169.00030517578125
2023-01-07 09:18:07,032 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,032 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,032 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 1.7482953071594238
2023-01-07 09:18:07,032 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,032 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,032 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -1067.85498046875
2023-01-07 09:18:07,032 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.226718902587891
2023-01-07 09:18:07,033 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 269.99993896484375
2023-01-07 09:18:07,033 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,033 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,034 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 1.7482953071594238
2023-01-07 09:18:07,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -1046.70751953125
2023-01-07 09:18:07,034 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,034 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,034 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2808.22216796875
2023-01-07 09:18:07,034 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.08473205566406
2023-01-07 09:18:07,035 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 238.3463592529297
2023-01-07 09:18:07,035 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,035 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,035 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.475543022155762
2023-01-07 09:18:07,035 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,035 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,036 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2732.83642578125
2023-01-07 09:18:07,036 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.92872619628906
2023-01-07 09:18:07,037 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 64.59999084472656
2023-01-07 09:18:07,037 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,037 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,037 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 5.475543022155762
2023-01-07 09:18:07,037 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,037 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,037 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -2698.103515625
2023-01-07 09:18:07,037 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.05068039894104
2023-01-07 09:18:07,038 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 412.8675231933594
2023-01-07 09:18:07,038 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,038 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,039 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:07,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -1.3989343643188477
2023-01-07 09:18:07,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,039 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,039 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 9.75149917602539
2023-01-07 09:18:07,039 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,039 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,039 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1908.8509521484375
2023-01-07 09:18:07,039 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.535036087036133
2023-01-07 09:18:07,041 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.599910736083984
2023-01-07 09:18:07,041 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,041 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,041 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.220293045043945
2023-01-07 09:18:07,041 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,041 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,041 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1926.0177001953125
2023-01-07 09:18:07,041 > [DEBUG] 0 :: before allreduce fusion buffer :: -46.252906799316406
2023-01-07 09:18:07,042 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 31.949403762817383
2023-01-07 09:18:07,042 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,042 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,042 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: -17.515911102294922
2023-01-07 09:18:07,042 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,042 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,043 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -1969.0291748046875
2023-01-07 09:18:07,043 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.221989631652832
2023-01-07 09:18:07,044 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 272.4000244140625
2023-01-07 09:18:07,044 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,044 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,044 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.220293045043945
2023-01-07 09:18:07,044 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,044 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,044 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1944.0980224609375
2023-01-07 09:18:07,044 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.91722106933594
2023-01-07 09:18:07,045 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 588.2679443359375
2023-01-07 09:18:07,045 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,045 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,045 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.220293045043945
2023-01-07 09:18:07,045 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,045 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,045 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1975.792724609375
2023-01-07 09:18:07,045 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9858124256134033
2023-01-07 09:18:07,046 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 135.19992065429688
2023-01-07 09:18:07,046 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,047 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,047 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: -24.220293045043945
2023-01-07 09:18:07,047 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,047 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,047 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -1914.8829345703125
2023-01-07 09:18:07,047 > [DEBUG] 0 :: before allreduce fusion buffer :: -40.534706115722656
2023-01-07 09:18:07,048 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 583.39892578125
2023-01-07 09:18:07,048 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,048 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,048 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,048 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,048 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 1.6823949813842773
2023-01-07 09:18:07,048 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,048 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,049 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -823.91650390625
2023-01-07 09:18:07,049 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5631237030029297
2023-01-07 09:18:07,050 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 125.79998779296875
2023-01-07 09:18:07,050 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,050 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,050 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 21.353090286254883
2023-01-07 09:18:07,050 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,050 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,050 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -822.4822998046875
2023-01-07 09:18:07,050 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.850827693939209
2023-01-07 09:18:07,051 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 342.8404846191406
2023-01-07 09:18:07,051 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,051 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,052 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:18:07,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,052 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,052 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 1.2429702281951904
2023-01-07 09:18:07,052 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,052 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,052 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -590.837646484375
2023-01-07 09:18:07,052 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.614156723022461
2023-01-07 09:18:07,053 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 507.9998779296875
2023-01-07 09:18:07,053 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,053 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,053 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -9.866982460021973
2023-01-07 09:18:07,053 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,053 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,053 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -621.1199951171875
2023-01-07 09:18:07,054 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7866753339767456
2023-01-07 09:18:07,054 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 233.4884033203125
2023-01-07 09:18:07,055 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,055 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,055 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.398221969604492
2023-01-07 09:18:07,055 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,055 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,055 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -826.2083129882812
2023-01-07 09:18:07,055 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.178707122802734
2023-01-07 09:18:07,056 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 541.998779296875
2023-01-07 09:18:07,056 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,056 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,056 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 16.398221969604492
2023-01-07 09:18:07,056 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,056 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,056 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -834.7080078125
2023-01-07 09:18:07,056 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.69741439819336
2023-01-07 09:18:07,058 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 413.2260437011719
2023-01-07 09:18:07,058 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,058 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,058 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,058 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.7521995306015015
2023-01-07 09:18:07,058 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,058 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,058 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -879.3930053710938
2023-01-07 09:18:07,059 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.22510528564453
2023-01-07 09:18:07,060 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.0
2023-01-07 09:18:07,060 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,060 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,060 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 0.7270157337188721
2023-01-07 09:18:07,060 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,060 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,060 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -868.968994140625
2023-01-07 09:18:07,060 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.951826095581055
2023-01-07 09:18:07,061 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 445.1630859375
2023-01-07 09:18:07,061 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,061 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,061 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,061 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,062 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.6884649991989136
2023-01-07 09:18:07,062 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,062 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,062 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -680.14990234375
2023-01-07 09:18:07,062 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.662393093109131
2023-01-07 09:18:07,063 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 126.60018157958984
2023-01-07 09:18:07,063 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,063 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,063 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: -12.923073768615723
2023-01-07 09:18:07,063 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,063 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,063 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -694.343017578125
2023-01-07 09:18:07,063 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6174286007881165
2023-01-07 09:18:07,064 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 420.2672424316406
2023-01-07 09:18:07,065 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,065 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,065 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -35.56856155395508
2023-01-07 09:18:07,065 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,065 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,065 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -625.173828125
2023-01-07 09:18:07,065 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.647572040557861
2023-01-07 09:18:07,066 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 548.1997680664062
2023-01-07 09:18:07,066 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,066 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,066 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: -35.56856155395508
2023-01-07 09:18:07,066 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,066 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,066 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -639.9593505859375
2023-01-07 09:18:07,066 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.318026542663574
2023-01-07 09:18:07,067 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 366.466796875
2023-01-07 09:18:07,068 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,068 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,068 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,068 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,068 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: 0.36184313893318176
2023-01-07 09:18:07,068 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,068 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,068 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -640.8290405273438
2023-01-07 09:18:07,068 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.237529754638672
2023-01-07 09:18:07,069 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 127.00009155273438
2023-01-07 09:18:07,069 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,069 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,069 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: -10.998025894165039
2023-01-07 09:18:07,069 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,070 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,070 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -637.4632568359375
2023-01-07 09:18:07,070 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.947952270507812
2023-01-07 09:18:07,071 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 129.55148315429688
2023-01-07 09:18:07,071 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,071 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,071 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,071 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,071 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: 0.14720235764980316
2023-01-07 09:18:07,071 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,071 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,072 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -577.62158203125
2023-01-07 09:18:07,072 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.890167236328125
2023-01-07 09:18:07,073 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.40000915527344
2023-01-07 09:18:07,073 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,073 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,073 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 16.837451934814453
2023-01-07 09:18:07,073 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,073 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,073 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -582.0831298828125
2023-01-07 09:18:07,073 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.449840545654297
2023-01-07 09:18:07,074 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 381.4235534667969
2023-01-07 09:18:07,074 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,074 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,074 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:18:07,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,075 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: 0.39994585514068604
2023-01-07 09:18:07,075 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,075 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,075 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -656.29638671875
2023-01-07 09:18:07,075 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4484241008758545
2023-01-07 09:18:07,076 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 511.20037841796875
2023-01-07 09:18:07,076 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,076 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,076 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: -37.401859283447266
2023-01-07 09:18:07,076 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,076 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,076 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -649.2752685546875
2023-01-07 09:18:07,077 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2586429715156555
2023-01-07 09:18:07,078 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 364.59735107421875
2023-01-07 09:18:07,078 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,078 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,078 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,078 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -0.20445673167705536
2023-01-07 09:18:07,078 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,078 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,078 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -740.77490234375
2023-01-07 09:18:07,078 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.424755096435547
2023-01-07 09:18:07,079 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.19921875
2023-01-07 09:18:07,080 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,080 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,080 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 0.13738298416137695
2023-01-07 09:18:07,080 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,080 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,080 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -775.7508544921875
2023-01-07 09:18:07,080 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.414641380310059
2023-01-07 09:18:07,081 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 506.47265625
2023-01-07 09:18:07,081 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,081 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,081 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:07,081 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,081 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,081 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -0.018117010593414307
2023-01-07 09:18:07,082 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,082 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,082 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -696.9007568359375
2023-01-07 09:18:07,082 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.1025543212890625
2023-01-07 09:18:07,083 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 129.79989624023438
2023-01-07 09:18:07,083 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,083 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,083 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 7.649838447570801
2023-01-07 09:18:07,083 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,083 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,083 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -698.36328125
2023-01-07 09:18:07,083 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4080027341842651
2023-01-07 09:18:07,084 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 518.67138671875
2023-01-07 09:18:07,084 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,084 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,085 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 512.0
2023-01-07 09:18:07,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,085 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,085 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.1836847960948944
2023-01-07 09:18:07,085 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,085 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,085 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -653.5860595703125
2023-01-07 09:18:07,085 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.5322182178497314
2023-01-07 09:18:07,086 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 515.3980712890625
2023-01-07 09:18:07,086 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,086 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,086 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: -55.30311584472656
2023-01-07 09:18:07,086 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,086 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,086 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -667.8060302734375
2023-01-07 09:18:07,087 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.135951042175293
2023-01-07 09:18:07,088 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 460.4952087402344
2023-01-07 09:18:07,088 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,088 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,088 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:18:07,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,088 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.03308391571044922
2023-01-07 09:18:07,088 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,088 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,088 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1193.471923828125
2023-01-07 09:18:07,089 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.464967727661133
2023-01-07 09:18:07,090 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 255.19952392578125
2023-01-07 09:18:07,090 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,090 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,090 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: 10.84982681274414
2023-01-07 09:18:07,090 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,090 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,090 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -1202.816650390625
2023-01-07 09:18:07,090 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.985728740692139
2023-01-07 09:18:07,091 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -23131.078125
2023-01-07 09:18:07,091 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,091 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,092 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -9.35628604888916
2023-01-07 09:18:07,092 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,092 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,092 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -866.6409301757812
2023-01-07 09:18:07,092 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.8927905559539795
2023-01-07 09:18:07,093 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 291.79986572265625
2023-01-07 09:18:07,093 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,093 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,093 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -9.35628604888916
2023-01-07 09:18:07,093 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,093 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,093 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: -864.6931762695312
2023-01-07 09:18:07,093 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.094123601913452
2023-01-07 09:18:07,095 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 428.1945495605469
2023-01-07 09:18:07,095 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,095 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,095 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -9.802867889404297
2023-01-07 09:18:07,095 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,095 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,095 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -347.30914306640625
2023-01-07 09:18:07,095 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4711335897445679
2023-01-07 09:18:07,096 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1181.3988037109375
2023-01-07 09:18:07,096 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,096 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,096 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -9.802867889404297
2023-01-07 09:18:07,096 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,096 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,097 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -350.03619384765625
2023-01-07 09:18:07,097 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.470963478088379
2023-01-07 09:18:07,098 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -31405.92578125
2023-01-07 09:18:07,098 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,098 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,098 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 59.33494186401367
2023-01-07 09:18:07,098 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,098 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,098 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -803.7595825195312
2023-01-07 09:18:07,098 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3398260474205017
2023-01-07 09:18:07,099 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1187.3990478515625
2023-01-07 09:18:07,099 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,099 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,099 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: 59.33494186401367
2023-01-07 09:18:07,099 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,099 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,099 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -806.492919921875
2023-01-07 09:18:07,100 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5175741314888
2023-01-07 09:18:07,101 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -22592.25390625
2023-01-07 09:18:07,101 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,101 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,101 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 256.0
2023-01-07 09:18:07,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,101 > [DEBUG] 0 :: add param_num to self.comm_param_size_dict
2023-01-07 09:18:07,101 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -0.114469513297081
2023-01-07 09:18:07,101 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,101 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,101 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -303.415771484375
2023-01-07 09:18:07,101 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.056129455566406
2023-01-07 09:18:07,103 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 257.5999755859375
2023-01-07 09:18:07,103 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,103 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,103 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -8.409623146057129
2023-01-07 09:18:07,103 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,103 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,103 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -307.94659423828125
2023-01-07 09:18:07,103 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.0172576904296875
2023-01-07 09:18:07,104 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -55966.29296875
2023-01-07 09:18:07,104 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,104 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,104 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 21.27646827697754
2023-01-07 09:18:07,104 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,104 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,105 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -874.7286376953125
2023-01-07 09:18:07,105 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1436388492584229
2023-01-07 09:18:07,106 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 298.9997253417969
2023-01-07 09:18:07,106 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,106 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,106 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: 21.27646827697754
2023-01-07 09:18:07,106 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,106 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,106 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -873.7169189453125
2023-01-07 09:18:07,106 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49304306507110596
2023-01-07 09:18:07,107 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -11585.720703125
2023-01-07 09:18:07,107 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,107 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,108 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.92848205566406
2023-01-07 09:18:07,108 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,108 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,108 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -971.6567993164062
2023-01-07 09:18:07,108 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0798158645629883
2023-01-07 09:18:07,109 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1200.0
2023-01-07 09:18:07,109 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,109 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,109 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: -62.92848205566406
2023-01-07 09:18:07,109 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,109 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,109 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -975.05126953125
2023-01-07 09:18:07,109 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24180178344249725
2023-01-07 09:18:07,111 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 830.00048828125
2023-01-07 09:18:07,111 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,111 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,111 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1.5430946350097656
2023-01-07 09:18:07,111 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,111 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,111 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -912.4847412109375
2023-01-07 09:18:07,111 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.593118190765381
2023-01-07 09:18:07,112 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 300.80181884765625
2023-01-07 09:18:07,112 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,112 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,112 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1.5430946350097656
2023-01-07 09:18:07,112 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,112 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,112 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -911.2492065429688
2023-01-07 09:18:07,113 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8014342784881592
2023-01-07 09:18:07,114 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 845.1528930664062
2023-01-07 09:18:07,114 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,114 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,114 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 19.50229263305664
2023-01-07 09:18:07,114 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,114 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,114 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1055.6456298828125
2023-01-07 09:18:07,114 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.86445152759552
2023-01-07 09:18:07,115 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 300.8105773925781
2023-01-07 09:18:07,115 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,115 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,115 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 19.50229263305664
2023-01-07 09:18:07,116 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,116 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,116 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1056.311767578125
2023-01-07 09:18:07,116 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8610488176345825
2023-01-07 09:18:07,117 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1056.15185546875
2023-01-07 09:18:07,117 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,117 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,117 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -121.1282958984375
2023-01-07 09:18:07,117 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,117 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,117 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1030.9144287109375
2023-01-07 09:18:07,118 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5032066702842712
2023-01-07 09:18:07,118 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1209.79931640625
2023-01-07 09:18:07,118 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,119 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,119 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: -121.1282958984375
2023-01-07 09:18:07,119 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,119 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,119 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -1030.0308837890625
2023-01-07 09:18:07,119 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.349252700805664
2023-01-07 09:18:07,120 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 964.4322509765625
2023-01-07 09:18:07,120 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,120 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,120 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.476484298706055
2023-01-07 09:18:07,120 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,121 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,121 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3870.98486328125
2023-01-07 09:18:07,121 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2955303192138672
2023-01-07 09:18:07,122 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 301.9991455078125
2023-01-07 09:18:07,122 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,122 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,122 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 6.476484298706055
2023-01-07 09:18:07,122 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,122 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,122 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -3870.71533203125
2023-01-07 09:18:07,122 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4158192276954651
2023-01-07 09:18:07,123 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1360.0450439453125
2023-01-07 09:18:07,124 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,124 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,124 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 6.021664619445801
2023-01-07 09:18:07,124 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,124 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,124 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1070.821533203125
2023-01-07 09:18:07,124 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0787101984024048
2023-01-07 09:18:07,125 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 303.7996826171875
2023-01-07 09:18:07,125 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,125 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,125 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 6.021664619445801
2023-01-07 09:18:07,125 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,125 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,125 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -1071.07177734375
2023-01-07 09:18:07,125 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33501672744750977
2023-01-07 09:18:07,127 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1148.284423828125
2023-01-07 09:18:07,127 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,127 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,127 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 97.85881042480469
2023-01-07 09:18:07,127 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,127 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,127 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1005.4402465820312
2023-01-07 09:18:07,127 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6947283744812012
2023-01-07 09:18:07,128 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1214.3990478515625
2023-01-07 09:18:07,128 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,128 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,128 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 97.85881042480469
2023-01-07 09:18:07,129 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,129 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,129 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -1007.4180908203125
2023-01-07 09:18:07,129 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1666688919067383
2023-01-07 09:18:07,130 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1130.82763671875
2023-01-07 09:18:07,130 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,130 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,130 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -22.873336791992188
2023-01-07 09:18:07,130 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,130 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,130 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -623.2616577148438
2023-01-07 09:18:07,131 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6857037544250488
2023-01-07 09:18:07,131 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 303.59967041015625
2023-01-07 09:18:07,131 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,132 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,132 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: -22.873336791992188
2023-01-07 09:18:07,132 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,132 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,132 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -626.1242065429688
2023-01-07 09:18:07,132 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9912927150726318
2023-01-07 09:18:07,133 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 607.5145263671875
2023-01-07 09:18:07,133 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,133 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,133 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 11.098684310913086
2023-01-07 09:18:07,133 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,133 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,133 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2173.311767578125
2023-01-07 09:18:07,134 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.165738582611084
2023-01-07 09:18:07,135 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 304.80126953125
2023-01-07 09:18:07,135 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,135 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,135 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: 11.098684310913086
2023-01-07 09:18:07,135 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,135 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,135 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2172.404541015625
2023-01-07 09:18:07,135 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21657370030879974
2023-01-07 09:18:07,136 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -1943.7474365234375
2023-01-07 09:18:07,136 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,136 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,137 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19.015560150146484
2023-01-07 09:18:07,137 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,137 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,137 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -628.5949096679688
2023-01-07 09:18:07,137 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.454437255859375
2023-01-07 09:18:07,138 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1215.799072265625
2023-01-07 09:18:07,138 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,138 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,138 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19.015560150146484
2023-01-07 09:18:07,138 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,138 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,138 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -628.4482421875
2023-01-07 09:18:07,138 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6715608835220337
2023-01-07 09:18:07,140 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -19707.310546875
2023-01-07 09:18:07,140 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,140 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,140 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -30.976947784423828
2023-01-07 09:18:07,140 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,140 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,140 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -459.39971923828125
2023-01-07 09:18:07,140 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15308484435081482
2023-01-07 09:18:07,141 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 304.59979248046875
2023-01-07 09:18:07,141 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,141 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,141 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -30.976947784423828
2023-01-07 09:18:07,141 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,141 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,142 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -456.32574462890625
2023-01-07 09:18:07,142 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30840635299682617
2023-01-07 09:18:07,143 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -7868.537109375
2023-01-07 09:18:07,143 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,143 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,143 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -15.626333236694336
2023-01-07 09:18:07,143 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,143 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,143 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -600.5647583007812
2023-01-07 09:18:07,143 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7539935111999512
2023-01-07 09:18:07,144 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 305.3997802734375
2023-01-07 09:18:07,144 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,144 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,144 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -15.626333236694336
2023-01-07 09:18:07,145 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,145 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,145 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -599.21142578125
2023-01-07 09:18:07,145 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4545774459838867
2023-01-07 09:18:07,146 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -22593.80859375
2023-01-07 09:18:07,146 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,146 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,146 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 52.55424118041992
2023-01-07 09:18:07,146 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,146 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,146 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -50.167724609375
2023-01-07 09:18:07,147 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7453192472457886
2023-01-07 09:18:07,148 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1220.1982421875
2023-01-07 09:18:07,148 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,148 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,148 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: 52.55424118041992
2023-01-07 09:18:07,148 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,148 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,148 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -53.045196533203125
2023-01-07 09:18:07,148 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2953870296478271
2023-01-07 09:18:07,149 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -49228.3359375
2023-01-07 09:18:07,149 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,149 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,149 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -22.807003021240234
2023-01-07 09:18:07,149 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,149 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,150 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -5973.69384765625
2023-01-07 09:18:07,150 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6449873447418213
2023-01-07 09:18:07,151 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 609.7994384765625
2023-01-07 09:18:07,151 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,151 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,151 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -22.807003021240234
2023-01-07 09:18:07,151 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,151 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,151 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -5976.48095703125
2023-01-07 09:18:07,151 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.425987720489502
2023-01-07 09:18:07,153 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -41334.5546875
2023-01-07 09:18:07,153 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,153 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,153 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.955009460449219
2023-01-07 09:18:07,153 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,153 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,153 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -168.44248962402344
2023-01-07 09:18:07,153 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07525545358657837
2023-01-07 09:18:07,154 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 513.8024291992188
2023-01-07 09:18:07,154 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,154 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,154 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 15.955009460449219
2023-01-07 09:18:07,154 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,154 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,154 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -168.7329864501953
2023-01-07 09:18:07,155 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.07486185431480408
2023-01-07 09:18:07,156 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 964.8635864257812
2023-01-07 09:18:07,156 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,156 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,156 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.87965202331543
2023-01-07 09:18:07,156 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,156 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,156 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2295.290771484375
2023-01-07 09:18:07,156 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6550952196121216
2023-01-07 09:18:07,157 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2450.00146484375
2023-01-07 09:18:07,157 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,157 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,157 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -17.87965202331543
2023-01-07 09:18:07,157 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,158 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,158 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 2294.795654296875
2023-01-07 09:18:07,158 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7783174514770508
2023-01-07 09:18:07,159 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -121582.1875
2023-01-07 09:18:07,159 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,159 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,159 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -38.847801208496094
2023-01-07 09:18:07,159 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,159 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,159 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 74.39877319335938
2023-01-07 09:18:07,159 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2635166049003601
2023-01-07 09:18:07,160 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2450.98876953125
2023-01-07 09:18:07,160 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,161 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,161 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -38.847801208496094
2023-01-07 09:18:07,161 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,161 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,161 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 74.25265502929688
2023-01-07 09:18:07,161 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02481704205274582
2023-01-07 09:18:07,162 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -25675.189453125
2023-01-07 09:18:07,162 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,162 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,162 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.304698944091797
2023-01-07 09:18:07,162 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,162 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,162 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11108.6396484375
2023-01-07 09:18:07,163 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11026675999164581
2023-01-07 09:18:07,164 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 611.3995971679688
2023-01-07 09:18:07,164 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,164 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,164 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: -12.304698944091797
2023-01-07 09:18:07,164 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,164 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,164 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -11107.4658203125
2023-01-07 09:18:07,164 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0032175295054912567
2023-01-07 09:18:07,165 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1175.573974609375
2023-01-07 09:18:07,166 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,166 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,166 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -19.654443740844727
2023-01-07 09:18:07,166 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,166 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,166 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1353.07080078125
2023-01-07 09:18:07,166 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.30671536922454834
2023-01-07 09:18:07,167 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 511.7997741699219
2023-01-07 09:18:07,167 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,167 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,167 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -19.654443740844727
2023-01-07 09:18:07,167 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,167 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,167 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -1353.352294921875
2023-01-07 09:18:07,167 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09964323043823242
2023-01-07 09:18:07,169 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -32613.185546875
2023-01-07 09:18:07,169 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,169 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,169 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 18.076797485351562
2023-01-07 09:18:07,169 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,169 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,169 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7357.64013671875
2023-01-07 09:18:07,169 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.019719593226909637
2023-01-07 09:18:07,170 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2452.7978515625
2023-01-07 09:18:07,170 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,170 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,170 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: 18.076797485351562
2023-01-07 09:18:07,170 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,171 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,171 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 7356.68896484375
2023-01-07 09:18:07,171 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08865901827812195
2023-01-07 09:18:07,172 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -101711.0703125
2023-01-07 09:18:07,172 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,172 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,172 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8975181579589844
2023-01-07 09:18:07,172 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,172 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,172 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -23003.94140625
2023-01-07 09:18:07,172 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10835769772529602
2023-01-07 09:18:07,173 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 612.1990966796875
2023-01-07 09:18:07,173 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,173 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,174 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: 0.8975181579589844
2023-01-07 09:18:07,174 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,174 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,174 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -23003.96484375
2023-01-07 09:18:07,174 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.22326280176639557
2023-01-07 09:18:07,175 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -11509.0078125
2023-01-07 09:18:07,175 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,175 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,175 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -51.32390594482422
2023-01-07 09:18:07,175 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,176 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,176 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1996.798828125
2023-01-07 09:18:07,176 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2528264820575714
2023-01-07 09:18:07,177 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 611.8009033203125
2023-01-07 09:18:07,177 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,177 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,177 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: -51.32390594482422
2023-01-07 09:18:07,177 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,177 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,177 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1996.49072265625
2023-01-07 09:18:07,177 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0013174954801797867
2023-01-07 09:18:07,178 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 868.9623413085938
2023-01-07 09:18:07,179 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,179 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,179 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.22209453582763672
2023-01-07 09:18:07,179 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,179 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,179 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 16509.26171875
2023-01-07 09:18:07,179 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.862367153167725
2023-01-07 09:18:07,180 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2452.5986328125
2023-01-07 09:18:07,180 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,180 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,180 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -0.22209453582763672
2023-01-07 09:18:07,180 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,180 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,180 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 16513.775390625
2023-01-07 09:18:07,180 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0432746410369873
2023-01-07 09:18:07,182 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -149172.28125
2023-01-07 09:18:07,182 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:07,182 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:07,183 > [DEBUG] 0 :: 11.356731414794922
2023-01-07 09:18:07,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,185 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.017303451895713806
2023-01-07 09:18:07,186 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,186 > [DEBUG] 0 :: before allreduce fusion buffer :: 326.4469909667969
2023-01-07 09:18:07,187 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,188 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 4.0646843910217285
2023-01-07 09:18:07,188 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,188 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -209.07826232910156
2023-01-07 09:18:07,189 > [DEBUG] 0 :: before allreduce fusion buffer :: -290.350830078125
2023-01-07 09:18:07,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,192 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 12.805233001708984
2023-01-07 09:18:07,192 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.546097135171294e-05
2023-01-07 09:18:07,194 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,194 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,195 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.07603016495704651
2023-01-07 09:18:07,195 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,195 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -51.36730194091797
2023-01-07 09:18:07,195 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.0151147842407227
2023-01-07 09:18:07,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,197 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 11.47409439086914
2023-01-07 09:18:07,197 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,197 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.103086004965007e-05
2023-01-07 09:18:07,197 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,197 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,198 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -0.4892770051956177
2023-01-07 09:18:07,198 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,198 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 11.462787628173828
2023-01-07 09:18:07,198 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.49018529057502747
2023-01-07 09:18:07,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,199 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.5246773362159729
2023-01-07 09:18:07,199 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,199 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005616775713860989
2023-01-07 09:18:07,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,200 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 3.3678128719329834
2023-01-07 09:18:07,200 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,200 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,200 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,200 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.024765849113464355
2023-01-07 09:18:07,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3443803787231445
2023-01-07 09:18:07,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,202 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -31.331937789916992
2023-01-07 09:18:07,202 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,202 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004161704331636429
2023-01-07 09:18:07,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,203 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.06329590082168579
2023-01-07 09:18:07,203 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,203 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,203 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,203 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -31.092464447021484
2023-01-07 09:18:07,203 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2691142559051514
2023-01-07 09:18:07,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,204 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 239.45700073242188
2023-01-07 09:18:07,205 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.008913186378777027
2023-01-07 09:18:07,205 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,205 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,205 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 0.329828679561615
2023-01-07 09:18:07,206 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,206 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 241.83758544921875
2023-01-07 09:18:07,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31539618968963623
2023-01-07 09:18:07,207 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,207 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,207 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -93.97384643554688
2023-01-07 09:18:07,207 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,207 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.014664871618151665
2023-01-07 09:18:07,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,208 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 2.072488784790039
2023-01-07 09:18:07,208 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,208 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -93.17288208007812
2023-01-07 09:18:07,208 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4420504570007324
2023-01-07 09:18:07,210 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,210 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,210 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.144981384277344
2023-01-07 09:18:07,210 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,210 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.010238487273454666
2023-01-07 09:18:07,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,211 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 2.9166693687438965
2023-01-07 09:18:07,211 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,211 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -58.36090850830078
2023-01-07 09:18:07,211 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.272751808166504
2023-01-07 09:18:07,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,212 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 41.939754486083984
2023-01-07 09:18:07,213 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,213 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007923252880573273
2023-01-07 09:18:07,213 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,213 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,213 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 0.22950291633605957
2023-01-07 09:18:07,213 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,214 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 44.207401275634766
2023-01-07 09:18:07,214 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2686576545238495
2023-01-07 09:18:07,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,215 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 153.12158203125
2023-01-07 09:18:07,215 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,215 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0003206100082024932
2023-01-07 09:18:07,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: -0.21604537963867188
2023-01-07 09:18:07,216 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,216 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 153.12158203125
2023-01-07 09:18:07,217 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.26832032203674316
2023-01-07 09:18:07,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,218 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.768183708190918
2023-01-07 09:18:07,218 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,218 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.015683874487876892
2023-01-07 09:18:07,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,219 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 0.8896965980529785
2023-01-07 09:18:07,219 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,219 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.390872955322266
2023-01-07 09:18:07,219 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6521484851837158
2023-01-07 09:18:07,220 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,220 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,220 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.680547833442688
2023-01-07 09:18:07,220 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,221 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.930662155151367e-05
2023-01-07 09:18:07,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,222 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 0.05319586396217346
2023-01-07 09:18:07,222 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,222 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.4743276834487915
2023-01-07 09:18:07,222 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.030478090047836304
2023-01-07 09:18:07,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,223 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -132.43331909179688
2023-01-07 09:18:07,223 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,224 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004453253000974655
2023-01-07 09:18:07,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,224 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 1.794252634048462
2023-01-07 09:18:07,224 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,224 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,224 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,225 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -132.14730834960938
2023-01-07 09:18:07,225 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7801804542541504
2023-01-07 09:18:07,226 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,226 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,226 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 4.042580604553223
2023-01-07 09:18:07,226 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,226 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05320816859602928
2023-01-07 09:18:07,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,227 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -0.012565620243549347
2023-01-07 09:18:07,227 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,227 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.190918922424316
2023-01-07 09:18:07,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.32099419832229614
2023-01-07 09:18:07,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,228 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -3.394655227661133
2023-01-07 09:18:07,229 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,229 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.009626131504774094
2023-01-07 09:18:07,229 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,229 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,229 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.12056474387645721
2023-01-07 09:18:07,229 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,230 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1.4219995737075806
2023-01-07 09:18:07,230 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13425512611865997
2023-01-07 09:18:07,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,231 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -18.627674102783203
2023-01-07 09:18:07,231 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,231 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005686655640602112
2023-01-07 09:18:07,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,232 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.05517154932022095
2023-01-07 09:18:07,232 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,232 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,232 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,232 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -16.4150447845459
2023-01-07 09:18:07,232 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06082810461521149
2023-01-07 09:18:07,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,234 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -2.550849437713623
2023-01-07 09:18:07,234 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,234 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.016492215916514397
2023-01-07 09:18:07,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,235 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.005369707942008972
2023-01-07 09:18:07,235 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,235 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.23215913772583008
2023-01-07 09:18:07,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.019341975450515747
2023-01-07 09:18:07,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,236 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 9.639421463012695
2023-01-07 09:18:07,236 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.015016665682196617
2023-01-07 09:18:07,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.2333584427833557
2023-01-07 09:18:07,237 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,237 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,237 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,237 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 8.34688949584961
2023-01-07 09:18:07,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.23448434472084045
2023-01-07 09:18:07,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,239 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 4.519176483154297
2023-01-07 09:18:07,239 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,239 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005090630147606134
2023-01-07 09:18:07,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.27304375171661377
2023-01-07 09:18:07,240 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,240 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,240 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,240 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 3.564577102661133
2023-01-07 09:18:07,240 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2518422603607178
2023-01-07 09:18:07,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,241 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -64.0052490234375
2023-01-07 09:18:07,241 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,241 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03531212359666824
2023-01-07 09:18:07,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: 0.373394638299942
2023-01-07 09:18:07,242 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,242 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -63.543697357177734
2023-01-07 09:18:07,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.37443944811820984
2023-01-07 09:18:07,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,244 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 53.29701614379883
2023-01-07 09:18:07,244 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04307897761464119
2023-01-07 09:18:07,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 0.25171786546707153
2023-01-07 09:18:07,245 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,245 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,245 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,245 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 55.7786750793457
2023-01-07 09:18:07,245 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24723535776138306
2023-01-07 09:18:07,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,246 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -74.9451675415039
2023-01-07 09:18:07,246 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,247 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.010065421462059021
2023-01-07 09:18:07,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,247 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 0.07578058540821075
2023-01-07 09:18:07,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,248 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,248 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,248 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -72.11940002441406
2023-01-07 09:18:07,248 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11421509087085724
2023-01-07 09:18:07,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,249 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -23.548845291137695
2023-01-07 09:18:07,249 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,249 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04034891724586487
2023-01-07 09:18:07,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -2.1789281368255615
2023-01-07 09:18:07,250 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,250 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -20.6992130279541
2023-01-07 09:18:07,250 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.2199325561523438
2023-01-07 09:18:07,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,252 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 96.94331359863281
2023-01-07 09:18:07,252 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,252 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0013291072100400925
2023-01-07 09:18:07,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,253 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 0.14589641988277435
2023-01-07 09:18:07,253 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,253 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,253 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,253 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 99.22323608398438
2023-01-07 09:18:07,253 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1483025997877121
2023-01-07 09:18:07,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,254 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 0.14174792170524597
2023-01-07 09:18:07,255 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,255 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06038803234696388
2023-01-07 09:18:07,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,255 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.337259292602539
2023-01-07 09:18:07,256 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.053882159292697906
2023-01-07 09:18:07,256 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,256 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,256 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5.356401443481445
2023-01-07 09:18:07,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.06336653977632523
2023-01-07 09:18:07,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,257 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.22413784265518188
2023-01-07 09:18:07,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,258 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,258 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,258 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 8.95815658569336
2023-01-07 09:18:07,258 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6383590698242188
2023-01-07 09:18:07,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,259 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -63.15589141845703
2023-01-07 09:18:07,259 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,259 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.009053956717252731
2023-01-07 09:18:07,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: 0.73177170753479
2023-01-07 09:18:07,260 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,260 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,260 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -60.45985412597656
2023-01-07 09:18:07,260 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.74324631690979
2023-01-07 09:18:07,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 73.38533020019531
2023-01-07 09:18:07,262 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,262 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.025999199599027634
2023-01-07 09:18:07,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.3449540436267853
2023-01-07 09:18:07,263 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,263 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,263 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,263 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 77.00859069824219
2023-01-07 09:18:07,263 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4668750762939453
2023-01-07 09:18:07,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 88.46063232421875
2023-01-07 09:18:07,264 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,264 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05936550348997116
2023-01-07 09:18:07,265 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,265 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,265 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 96.17936706542969
2023-01-07 09:18:07,265 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03707529976963997
2023-01-07 09:18:07,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -33.91801834106445
2023-01-07 09:18:07,266 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.021805576980113983
2023-01-07 09:18:07,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -33.87489318847656
2023-01-07 09:18:07,267 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03440108522772789
2023-01-07 09:18:07,268 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,268 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,268 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -14.192402839660645
2023-01-07 09:18:07,268 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14467108249664307
2023-01-07 09:18:07,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,269 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -25.30702018737793
2023-01-07 09:18:07,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05237354338169098
2023-01-07 09:18:07,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,270 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -29.6801815032959
2023-01-07 09:18:07,270 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,270 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07097335904836655
2023-01-07 09:18:07,271 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,271 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,271 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -14.87864875793457
2023-01-07 09:18:07,271 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.013577762991189957
2023-01-07 09:18:07,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,272 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -9.998900413513184
2023-01-07 09:18:07,272 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,272 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0657932236790657
2023-01-07 09:18:07,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,273 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -30.467628479003906
2023-01-07 09:18:07,273 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03150685504078865
2023-01-07 09:18:07,274 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,274 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,274 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -50.95781326293945
2023-01-07 09:18:07,274 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5591045022010803
2023-01-07 09:18:07,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,275 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -23.417823791503906
2023-01-07 09:18:07,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5551435947418213
2023-01-07 09:18:07,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,276 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -11.84206485748291
2023-01-07 09:18:07,276 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,276 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11912044137716293
2023-01-07 09:18:07,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,277 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -3.338200807571411
2023-01-07 09:18:07,277 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.20491012930870056
2023-01-07 09:18:07,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,278 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 17.45065689086914
2023-01-07 09:18:07,278 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,278 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10717476159334183
2023-01-07 09:18:07,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 0.14527224004268646
2023-01-07 09:18:07,279 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,279 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,279 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,279 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 45.52185821533203
2023-01-07 09:18:07,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21729767322540283
2023-01-07 09:18:07,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,281 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -116.98795318603516
2023-01-07 09:18:07,281 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,281 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0523703396320343
2023-01-07 09:18:07,282 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,282 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,282 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -60.69172668457031
2023-01-07 09:18:07,282 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8663480281829834
2023-01-07 09:18:07,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,283 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -33.223793029785156
2023-01-07 09:18:07,283 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,283 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.015321929007768631
2023-01-07 09:18:07,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,284 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 23.55935287475586
2023-01-07 09:18:07,284 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11577332019805908
2023-01-07 09:18:07,285 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,285 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,285 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 2.3403494358062744
2023-01-07 09:18:07,285 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3861542344093323
2023-01-07 09:18:07,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.3832716941833496
2023-01-07 09:18:07,286 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,286 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 29.275070190429688
2023-01-07 09:18:07,286 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4818558096885681
2023-01-07 09:18:07,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,287 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 27.92507553100586
2023-01-07 09:18:07,287 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,288 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2786353528499603
2023-01-07 09:18:07,288 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,288 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,288 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 33.14657211303711
2023-01-07 09:18:07,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05669897794723511
2023-01-07 09:18:07,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,289 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -40.93250274658203
2023-01-07 09:18:07,289 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.62429141998291
2023-01-07 09:18:07,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,290 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -32.88870620727539
2023-01-07 09:18:07,290 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07408475875854492
2023-01-07 09:18:07,291 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,291 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,291 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 31.345718383789062
2023-01-07 09:18:07,291 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,292 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06800238788127899
2023-01-07 09:18:07,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,292 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 1.7621381282806396
2023-01-07 09:18:07,292 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,293 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,293 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 32.21991729736328
2023-01-07 09:18:07,293 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0372886657714844
2023-01-07 09:18:07,294 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,294 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 38.63667297363281
2023-01-07 09:18:07,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.25251102447509766
2023-01-07 09:18:07,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,295 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.6416682600975037
2023-01-07 09:18:07,295 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -17.625429153442383
2023-01-07 09:18:07,295 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,295 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 42.06139373779297
2023-01-07 09:18:07,296 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.237053394317627
2023-01-07 09:18:07,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,297 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 42.06340408325195
2023-01-07 09:18:07,297 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2156480848789215
2023-01-07 09:18:07,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,298 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -14.408161163330078
2023-01-07 09:18:07,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.684179306030273
2023-01-07 09:18:07,299 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,299 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,299 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 13.464482307434082
2023-01-07 09:18:07,299 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,299 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.579106330871582
2023-01-07 09:18:07,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,300 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.9560123682022095
2023-01-07 09:18:07,300 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,300 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 5.733121395111084
2023-01-07 09:18:07,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9002792239189148
2023-01-07 09:18:07,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,301 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -0.5248723030090332
2023-01-07 09:18:07,301 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.3974345922470093
2023-01-07 09:18:07,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,302 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -2.249708652496338
2023-01-07 09:18:07,302 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,302 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,302 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,302 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 20.434967041015625
2023-01-07 09:18:07,302 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.756917953491211
2023-01-07 09:18:07,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,304 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -14.998132705688477
2023-01-07 09:18:07,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.699280738830566
2023-01-07 09:18:07,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,305 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 57.68656539916992
2023-01-07 09:18:07,305 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,305 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,305 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,305 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 40.89497375488281
2023-01-07 09:18:07,305 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.992983341217041
2023-01-07 09:18:07,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,306 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -22.94664192199707
2023-01-07 09:18:07,306 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.75654411315918
2023-01-07 09:18:07,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,307 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 69.13119506835938
2023-01-07 09:18:07,307 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.860420227050781
2023-01-07 09:18:07,308 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,308 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,308 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -36.231422424316406
2023-01-07 09:18:07,308 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6967582702636719
2023-01-07 09:18:07,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,309 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -0.6410870552062988
2023-01-07 09:18:07,309 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,310 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 19.244285583496094
2023-01-07 09:18:07,310 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,310 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -39.32652282714844
2023-01-07 09:18:07,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,310 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 46.525177001953125
2023-01-07 09:18:07,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.557652473449707
2023-01-07 09:18:07,311 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,311 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,311 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -36.9769287109375
2023-01-07 09:18:07,311 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4421484470367432
2023-01-07 09:18:07,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,312 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 9.446852684020996
2023-01-07 09:18:07,312 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.089061737060547
2023-01-07 09:18:07,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,313 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -50.32118606567383
2023-01-07 09:18:07,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.982336044311523
2023-01-07 09:18:07,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,314 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 22.97028923034668
2023-01-07 09:18:07,314 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,314 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,314 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 42.786128997802734
2023-01-07 09:18:07,315 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.427793025970459
2023-01-07 09:18:07,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,316 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 0.2589564323425293
2023-01-07 09:18:07,316 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,316 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.448501586914062
2023-01-07 09:18:07,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -0.17409634590148926
2023-01-07 09:18:07,317 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,317 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,317 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,317 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 2.301253318786621
2023-01-07 09:18:07,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.90719223022461
2023-01-07 09:18:07,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,318 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 70.15873718261719
2023-01-07 09:18:07,318 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5639476776123047
2023-01-07 09:18:07,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,319 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 118.40727233886719
2023-01-07 09:18:07,319 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.406810998916626
2023-01-07 09:18:07,321 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:18:07,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,322 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: 349.67657470703125
2023-01-07 09:18:07,322 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:07,322 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,322 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,323 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 127.98456573486328
2023-01-07 09:18:07,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,323 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 37.97898483276367
2023-01-07 09:18:07,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -30.362749099731445
2023-01-07 09:18:07,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 41.40583419799805
2023-01-07 09:18:07,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,324 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 29.45615577697754
2023-01-07 09:18:07,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 68.0147705078125
2023-01-07 09:18:07,325 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,325 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,325 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -16.46180534362793
2023-01-07 09:18:07,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 31.51365089416504
2023-01-07 09:18:07,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,326 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 58.70866012573242
2023-01-07 09:18:07,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 21.240236282348633
2023-01-07 09:18:07,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,327 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -32.90898132324219
2023-01-07 09:18:07,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -7.8230977058410645
2023-01-07 09:18:07,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,328 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -15.15788745880127
2023-01-07 09:18:07,328 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,328 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,328 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -58.489784240722656
2023-01-07 09:18:07,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,329 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 112.86090850830078
2023-01-07 09:18:07,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,329 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 81.60134887695312
2023-01-07 09:18:07,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -63.81591796875
2023-01-07 09:18:07,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,330 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 8.636781692504883
2023-01-07 09:18:07,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 5.965944766998291
2023-01-07 09:18:07,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 98.8332748413086
2023-01-07 09:18:07,331 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,331 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,331 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -20.569150924682617
2023-01-07 09:18:07,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,332 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -73.19953918457031
2023-01-07 09:18:07,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,332 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 54.29323959350586
2023-01-07 09:18:07,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -63.64873504638672
2023-01-07 09:18:07,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,333 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -0.30657958984375
2023-01-07 09:18:07,333 > [DEBUG] 0 :: before allreduce fusion buffer :: 257.3471374511719
2023-01-07 09:18:07,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,335 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 3.946043014526367
2023-01-07 09:18:07,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -0.1586153507232666
2023-01-07 09:18:07,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -17.121356964111328
2023-01-07 09:18:07,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2.276477336883545
2023-01-07 09:18:07,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,336 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 6.292241096496582
2023-01-07 09:18:07,336 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,336 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -132.6905517578125
2023-01-07 09:18:07,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,337 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -2.077162504196167
2023-01-07 09:18:07,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,337 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.32112979888916
2023-01-07 09:18:07,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,337 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 152.7891845703125
2023-01-07 09:18:07,337 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.450489044189453
2023-01-07 09:18:07,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,338 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: 51.41852569580078
2023-01-07 09:18:07,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,338 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -56.540428161621094
2023-01-07 09:18:07,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,339 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -88.99293518066406
2023-01-07 09:18:07,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,339 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 243.76329040527344
2023-01-07 09:18:07,339 > [DEBUG] 0 :: before allreduce fusion buffer :: 64.53414154052734
2023-01-07 09:18:07,339 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,339 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,340 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -19.857528686523438
2023-01-07 09:18:07,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,340 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.01967841386795044
2023-01-07 09:18:07,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,340 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 11.457344055175781
2023-01-07 09:18:07,340 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.91403579711914
2023-01-07 09:18:07,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,341 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -60.00347137451172
2023-01-07 09:18:07,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:07,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:07,341 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -164.55718994140625
2023-01-07 09:18:07,341 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.45851993560791
2023-01-07 09:18:08,185 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 246.9463653564453
2023-01-07 09:18:08,185 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,185 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,185 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 62.20000076293945
2023-01-07 09:18:08,185 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,185 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,186 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -0.6896286010742188
2023-01-07 09:18:08,186 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,186 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,186 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,186 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 128.82374572753906
2023-01-07 09:18:08,186 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.9217000007629395
2023-01-07 09:18:08,187 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 61.35772705078125
2023-01-07 09:18:08,188 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,188 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,188 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 18.132328033447266
2023-01-07 09:18:08,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,188 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: 64.52091217041016
2023-01-07 09:18:08,188 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,188 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,188 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,188 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 167.30825805664062
2023-01-07 09:18:08,188 > [DEBUG] 0 :: before allreduce fusion buffer :: 66.18141174316406
2023-01-07 09:18:08,189 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -3.0653915405273438
2023-01-07 09:18:08,190 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,190 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,190 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 287.67596435546875
2023-01-07 09:18:08,190 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,190 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,190 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 168.9610595703125
2023-01-07 09:18:08,190 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9931156635284424
2023-01-07 09:18:08,191 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 61.995445251464844
2023-01-07 09:18:08,191 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,191 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,191 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 287.67596435546875
2023-01-07 09:18:08,191 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,191 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,191 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 210.07354736328125
2023-01-07 09:18:08,191 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5394289493560791
2023-01-07 09:18:08,192 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 424.54644775390625
2023-01-07 09:18:08,192 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,192 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,192 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.0
2023-01-07 09:18:08,192 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,192 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,193 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -0.14942950010299683
2023-01-07 09:18:08,193 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,193 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 35.353416442871094
2023-01-07 09:18:08,193 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,193 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,193 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,193 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -61.37498092651367
2023-01-07 09:18:08,193 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.95803451538086
2023-01-07 09:18:08,195 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 64.17098999023438
2023-01-07 09:18:08,195 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,195 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,195 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 412.8675231933594
2023-01-07 09:18:08,195 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,195 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,195 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -26.32525634765625
2023-01-07 09:18:08,195 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.547847747802734
2023-01-07 09:18:08,196 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -48.376220703125
2023-01-07 09:18:08,196 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,196 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,196 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 256.4000244140625
2023-01-07 09:18:08,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,196 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: -0.09072297811508179
2023-01-07 09:18:08,196 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,196 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,196 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,197 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 27.879711151123047
2023-01-07 09:18:08,197 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.298063278198242
2023-01-07 09:18:08,198 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 255.98422241210938
2023-01-07 09:18:08,198 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,198 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,198 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 62.452510833740234
2023-01-07 09:18:08,198 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,198 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,198 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 87.71307373046875
2023-01-07 09:18:08,198 > [DEBUG] 0 :: before allreduce fusion buffer :: 23.053977966308594
2023-01-07 09:18:08,199 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 437.079345703125
2023-01-07 09:18:08,199 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,199 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,199 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 62.452510833740234
2023-01-07 09:18:08,199 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,199 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,199 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 107.88679504394531
2023-01-07 09:18:08,200 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.434606552124023
2023-01-07 09:18:08,200 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 279.6912536621094
2023-01-07 09:18:08,201 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,201 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,201 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 62.452510833740234
2023-01-07 09:18:08,201 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,201 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,201 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 131.3774871826172
2023-01-07 09:18:08,201 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.461593747138977
2023-01-07 09:18:08,202 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 279.63177490234375
2023-01-07 09:18:08,202 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,202 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,202 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 64.79994201660156
2023-01-07 09:18:08,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -0.46577340364456177
2023-01-07 09:18:08,202 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,202 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,202 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,202 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 149.2295684814453
2023-01-07 09:18:08,203 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.054710388183594
2023-01-07 09:18:08,204 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.0936050415039
2023-01-07 09:18:08,204 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,204 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,204 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 62.452510833740234
2023-01-07 09:18:08,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,204 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 327.35321044921875
2023-01-07 09:18:08,204 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,204 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,205 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.765427589416504
2023-01-07 09:18:08,205 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.048490047454834
2023-01-07 09:18:08,206 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 10.445817947387695
2023-01-07 09:18:08,206 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,206 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,206 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 65.0
2023-01-07 09:18:08,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,206 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -2.1905555725097656
2023-01-07 09:18:08,206 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,206 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,206 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,206 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 58.07090759277344
2023-01-07 09:18:08,206 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.1291618347168
2023-01-07 09:18:08,207 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.95516204833984
2023-01-07 09:18:08,208 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,208 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,208 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 412.8675231933594
2023-01-07 09:18:08,208 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,208 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,208 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -69.64725494384766
2023-01-07 09:18:08,208 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.311941146850586
2023-01-07 09:18:08,209 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 266.0600280761719
2023-01-07 09:18:08,209 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,209 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,209 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 238.3463592529297
2023-01-07 09:18:08,209 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,209 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,209 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 17.065990447998047
2023-01-07 09:18:08,209 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.830672740936279
2023-01-07 09:18:08,210 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 280.0235595703125
2023-01-07 09:18:08,210 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,210 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,210 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 238.3463592529297
2023-01-07 09:18:08,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,211 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 1.0777792930603027
2023-01-07 09:18:08,211 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,211 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,211 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -79.30738067626953
2023-01-07 09:18:08,211 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.979707717895508
2023-01-07 09:18:08,212 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 388.73101806640625
2023-01-07 09:18:08,212 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,212 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,212 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 412.8675231933594
2023-01-07 09:18:08,212 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,212 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,212 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -39.33146667480469
2023-01-07 09:18:08,213 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.544273376464844
2023-01-07 09:18:08,213 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.55729675292969
2023-01-07 09:18:08,213 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,213 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,213 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 412.8675231933594
2023-01-07 09:18:08,214 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,214 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,214 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -102.86198425292969
2023-01-07 09:18:08,214 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0968918800354004
2023-01-07 09:18:08,215 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 711.0712890625
2023-01-07 09:18:08,215 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,215 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,215 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 63.599910736083984
2023-01-07 09:18:08,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,215 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -1.0052800178527832
2023-01-07 09:18:08,215 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,215 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,215 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,215 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 27.6226806640625
2023-01-07 09:18:08,216 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,216 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,216 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,216 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -6.049993515014648
2023-01-07 09:18:08,216 > [DEBUG] 0 :: before allreduce fusion buffer :: 37.85041046142578
2023-01-07 09:18:08,217 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.53167724609375
2023-01-07 09:18:08,217 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,217 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,217 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 583.39892578125
2023-01-07 09:18:08,218 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,218 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,218 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 8.375061988830566
2023-01-07 09:18:08,218 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.775315284729004
2023-01-07 09:18:08,219 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 70.9794921875
2023-01-07 09:18:08,219 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,219 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,219 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 588.2679443359375
2023-01-07 09:18:08,219 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,219 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,219 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 83.52515411376953
2023-01-07 09:18:08,219 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.278003692626953
2023-01-07 09:18:08,220 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 284.26300048828125
2023-01-07 09:18:08,220 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,220 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,221 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 583.39892578125
2023-01-07 09:18:08,221 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,221 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,221 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 2.3238799571990967
2023-01-07 09:18:08,221 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.822494983673096
2023-01-07 09:18:08,222 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 965.7755126953125
2023-01-07 09:18:08,222 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,222 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,222 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 583.39892578125
2023-01-07 09:18:08,222 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,222 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,222 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -14.05541706085205
2023-01-07 09:18:08,222 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.299192428588867
2023-01-07 09:18:08,223 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 140.53555297851562
2023-01-07 09:18:08,223 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,223 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,223 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 583.39892578125
2023-01-07 09:18:08,223 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,223 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,224 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -14.295924186706543
2023-01-07 09:18:08,224 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7222774028778076
2023-01-07 09:18:08,225 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 985.2049560546875
2023-01-07 09:18:08,225 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,225 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,225 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 125.79998779296875
2023-01-07 09:18:08,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.6574759483337402
2023-01-07 09:18:08,225 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,225 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,225 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,225 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -17.71426773071289
2023-01-07 09:18:08,225 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.526683807373047
2023-01-07 09:18:08,226 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 124.5130615234375
2023-01-07 09:18:08,226 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,226 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,227 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 342.8404846191406
2023-01-07 09:18:08,227 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,227 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,227 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -26.00078582763672
2023-01-07 09:18:08,227 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.96198844909668
2023-01-07 09:18:08,228 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 570.7982788085938
2023-01-07 09:18:08,228 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,228 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,228 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 507.9998779296875
2023-01-07 09:18:08,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,228 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 2.272921562194824
2023-01-07 09:18:08,228 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,228 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,228 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,229 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.702016830444336
2023-01-07 09:18:08,229 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.058355808258057
2023-01-07 09:18:08,230 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 503.80084228515625
2023-01-07 09:18:08,230 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,230 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,230 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 233.4884033203125
2023-01-07 09:18:08,230 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,230 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,230 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 19.380672454833984
2023-01-07 09:18:08,230 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.515261173248291
2023-01-07 09:18:08,231 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 360.1312255859375
2023-01-07 09:18:08,231 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,231 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,231 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 413.2260437011719
2023-01-07 09:18:08,231 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,231 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,231 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 15.986953735351562
2023-01-07 09:18:08,232 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.675199031829834
2023-01-07 09:18:08,232 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 557.9110107421875
2023-01-07 09:18:08,233 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,233 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,233 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 413.2260437011719
2023-01-07 09:18:08,233 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,233 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,233 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 25.448978424072266
2023-01-07 09:18:08,233 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1907191276550293
2023-01-07 09:18:08,234 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 666.30126953125
2023-01-07 09:18:08,234 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,234 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,234 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.0
2023-01-07 09:18:08,234 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,234 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,234 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: 0.47795331478118896
2023-01-07 09:18:08,234 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,235 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,235 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,235 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 116.7540054321289
2023-01-07 09:18:08,235 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.846969127655029
2023-01-07 09:18:08,236 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 127.38790893554688
2023-01-07 09:18:08,236 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,236 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,236 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 445.1630859375
2023-01-07 09:18:08,236 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,236 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,236 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 118.8136215209961
2023-01-07 09:18:08,236 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2805451452732086
2023-01-07 09:18:08,237 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 708.8363037109375
2023-01-07 09:18:08,237 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,237 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,237 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 126.60018157958984
2023-01-07 09:18:08,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,238 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 0.24110683798789978
2023-01-07 09:18:08,238 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,238 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,238 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,238 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 70.28740692138672
2023-01-07 09:18:08,238 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.139419555664062
2023-01-07 09:18:08,239 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 125.48588562011719
2023-01-07 09:18:08,239 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,239 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,239 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 420.2672424316406
2023-01-07 09:18:08,239 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,239 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,239 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 57.746246337890625
2023-01-07 09:18:08,240 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.096003532409668
2023-01-07 09:18:08,241 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 630.7352294921875
2023-01-07 09:18:08,241 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,241 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,241 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 366.466796875
2023-01-07 09:18:08,241 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,241 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,241 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 32.5678596496582
2023-01-07 09:18:08,241 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.772568702697754
2023-01-07 09:18:08,242 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 568.209228515625
2023-01-07 09:18:08,242 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,242 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,242 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 366.466796875
2023-01-07 09:18:08,242 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,242 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,242 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 29.887680053710938
2023-01-07 09:18:08,243 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.688452124595642
2023-01-07 09:18:08,244 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 559.971435546875
2023-01-07 09:18:08,244 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,244 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,244 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 127.00009155273438
2023-01-07 09:18:08,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,244 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5344046950340271
2023-01-07 09:18:08,244 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,244 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,244 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,244 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 64.86380767822266
2023-01-07 09:18:08,244 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.8539206981658936
2023-01-07 09:18:08,245 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 127.8348388671875
2023-01-07 09:18:08,245 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,245 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,246 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 129.55148315429688
2023-01-07 09:18:08,246 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,246 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,246 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 71.89279174804688
2023-01-07 09:18:08,246 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.615591049194336
2023-01-07 09:18:08,247 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 185.00270080566406
2023-01-07 09:18:08,247 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,247 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,247 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 128.40000915527344
2023-01-07 09:18:08,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,247 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -0.05482994019985199
2023-01-07 09:18:08,247 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,247 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,247 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,248 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 21.187843322753906
2023-01-07 09:18:08,248 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.809389114379883
2023-01-07 09:18:08,249 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 129.32791137695312
2023-01-07 09:18:08,249 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,249 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,249 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 381.4235534667969
2023-01-07 09:18:08,249 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,249 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,249 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 9.242376327514648
2023-01-07 09:18:08,249 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8031581044197083
2023-01-07 09:18:08,250 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 692.2383422851562
2023-01-07 09:18:08,250 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,250 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,250 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 511.20037841796875
2023-01-07 09:18:08,250 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,250 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,251 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -0.16226306557655334
2023-01-07 09:18:08,251 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,251 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,251 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,251 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -29.740652084350586
2023-01-07 09:18:08,251 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1183444261550903
2023-01-07 09:18:08,252 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 511.3611145019531
2023-01-07 09:18:08,252 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,252 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,252 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 364.59735107421875
2023-01-07 09:18:08,252 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,252 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,252 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -16.855188369750977
2023-01-07 09:18:08,252 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.06114387512207
2023-01-07 09:18:08,253 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 634.5221557617188
2023-01-07 09:18:08,253 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,254 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,254 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 128.19921875
2023-01-07 09:18:08,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,254 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: 0.5294498205184937
2023-01-07 09:18:08,254 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,254 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,254 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,254 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -7.173519134521484
2023-01-07 09:18:08,254 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.725196838378906
2023-01-07 09:18:08,255 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 127.41616821289062
2023-01-07 09:18:08,255 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,255 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,255 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 506.47265625
2023-01-07 09:18:08,255 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,255 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,256 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -12.070785522460938
2023-01-07 09:18:08,256 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.415034770965576
2023-01-07 09:18:08,257 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 832.5647583007812
2023-01-07 09:18:08,257 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,257 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,257 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 129.79989624023438
2023-01-07 09:18:08,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,257 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 0.3638800382614136
2023-01-07 09:18:08,257 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,257 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,257 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,257 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -12.773557662963867
2023-01-07 09:18:08,257 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1144369840621948
2023-01-07 09:18:08,259 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 130.27679443359375
2023-01-07 09:18:08,259 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,259 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,259 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 518.67138671875
2023-01-07 09:18:08,259 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,259 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,259 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -23.932483673095703
2023-01-07 09:18:08,259 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.880845785140991
2023-01-07 09:18:08,260 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 893.8502197265625
2023-01-07 09:18:08,260 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,260 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,260 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 515.3980712890625
2023-01-07 09:18:08,260 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,261 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -0.12271520495414734
2023-01-07 09:18:08,261 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,261 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,261 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,261 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -59.252410888671875
2023-01-07 09:18:08,261 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.024571418762207
2023-01-07 09:18:08,262 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 519.9266357421875
2023-01-07 09:18:08,262 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,262 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,262 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 460.4952087402344
2023-01-07 09:18:08,262 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,262 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,262 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -64.968994140625
2023-01-07 09:18:08,262 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.037407875061035
2023-01-07 09:18:08,264 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 800.4705200195312
2023-01-07 09:18:08,264 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,264 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,264 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 255.19952392578125
2023-01-07 09:18:08,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 0.6084473729133606
2023-01-07 09:18:08,264 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,264 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,264 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,264 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 296.03448486328125
2023-01-07 09:18:08,264 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3910388946533203
2023-01-07 09:18:08,265 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 256.26641845703125
2023-01-07 09:18:08,265 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,265 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,266 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -23131.078125
2023-01-07 09:18:08,266 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,266 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,266 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 289.0070495605469
2023-01-07 09:18:08,266 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.478717803955078
2023-01-07 09:18:08,267 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -38829.77734375
2023-01-07 09:18:08,267 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,267 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,267 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 428.1945495605469
2023-01-07 09:18:08,267 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,267 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,267 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 81.60134887695312
2023-01-07 09:18:08,268 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33488428592681885
2023-01-07 09:18:08,269 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 310.11065673828125
2023-01-07 09:18:08,269 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,269 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,269 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 428.1945495605469
2023-01-07 09:18:08,269 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,269 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,269 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 88.94596862792969
2023-01-07 09:18:08,269 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.1534745693206787
2023-01-07 09:18:08,270 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 236.6617431640625
2023-01-07 09:18:08,270 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,270 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,270 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31405.92578125
2023-01-07 09:18:08,270 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,270 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,270 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -63.71742630004883
2023-01-07 09:18:08,271 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.301215171813965
2023-01-07 09:18:08,272 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1279.87646484375
2023-01-07 09:18:08,272 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,272 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,272 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -31405.92578125
2023-01-07 09:18:08,272 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,272 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,272 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -61.85326385498047
2023-01-07 09:18:08,272 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.1429712474346161
2023-01-07 09:18:08,273 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -52215.81640625
2023-01-07 09:18:08,273 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,273 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,273 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22592.25390625
2023-01-07 09:18:08,273 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,273 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,273 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 8.906322479248047
2023-01-07 09:18:08,274 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.944528341293335
2023-01-07 09:18:08,275 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1284.49462890625
2023-01-07 09:18:08,275 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,275 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,275 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -22592.25390625
2023-01-07 09:18:08,275 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,275 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,275 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 6.836993217468262
2023-01-07 09:18:08,275 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.755551278591156
2023-01-07 09:18:08,276 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -37788.55859375
2023-01-07 09:18:08,276 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,276 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,276 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 257.5999755859375
2023-01-07 09:18:08,276 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,276 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,277 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: 0.15445464849472046
2023-01-07 09:18:08,277 > [DEBUG] 0 :: check shard size is equal to param_size
2023-01-07 09:18:08,277 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,277 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,277 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 6.519670486450195
2023-01-07 09:18:08,277 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.400963306427002
2023-01-07 09:18:08,278 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 249.72132873535156
2023-01-07 09:18:08,278 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,278 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,278 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -55966.29296875
2023-01-07 09:18:08,278 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,278 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,278 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 5.276750564575195
2023-01-07 09:18:08,279 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5575401186943054
2023-01-07 09:18:08,280 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -93470.75
2023-01-07 09:18:08,280 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,280 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,280 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11585.720703125
2023-01-07 09:18:08,280 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,280 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,280 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -3.1808948516845703
2023-01-07 09:18:08,280 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7494946122169495
2023-01-07 09:18:08,281 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 321.8188781738281
2023-01-07 09:18:08,281 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,281 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,281 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -11585.720703125
2023-01-07 09:18:08,281 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,281 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,281 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -14.147879600524902
2023-01-07 09:18:08,282 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.441783905029297
2023-01-07 09:18:08,283 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -19314.248046875
2023-01-07 09:18:08,283 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,283 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,283 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 830.00048828125
2023-01-07 09:18:08,283 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,283 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,283 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 31.755910873413086
2023-01-07 09:18:08,283 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0659618377685547
2023-01-07 09:18:08,284 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1309.686767578125
2023-01-07 09:18:08,284 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,284 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,284 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 830.00048828125
2023-01-07 09:18:08,284 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,284 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,284 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 37.02063751220703
2023-01-07 09:18:08,285 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3954339325428009
2023-01-07 09:18:08,286 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1443.6314697265625
2023-01-07 09:18:08,286 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,286 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,286 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 845.1528930664062
2023-01-07 09:18:08,286 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,286 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,286 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -32.84898376464844
2023-01-07 09:18:08,286 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1857043206691742
2023-01-07 09:18:08,287 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 324.188720703125
2023-01-07 09:18:08,287 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,287 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,287 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 845.1528930664062
2023-01-07 09:18:08,287 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,287 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,288 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -32.45947265625
2023-01-07 09:18:08,288 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18964716792106628
2023-01-07 09:18:08,289 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1630.104736328125
2023-01-07 09:18:08,289 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,289 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,289 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1056.15185546875
2023-01-07 09:18:08,289 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,289 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,289 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 57.99701690673828
2023-01-07 09:18:08,289 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7948459386825562
2023-01-07 09:18:08,290 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 325.2947998046875
2023-01-07 09:18:08,290 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,290 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,290 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1056.15185546875
2023-01-07 09:18:08,290 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,290 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,291 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 55.65705871582031
2023-01-07 09:18:08,291 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2883455753326416
2023-01-07 09:18:08,292 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1678.5760498046875
2023-01-07 09:18:08,292 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,292 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,292 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 964.4322509765625
2023-01-07 09:18:08,292 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,292 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,292 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -23.811073303222656
2023-01-07 09:18:08,292 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2150058150291443
2023-01-07 09:18:08,293 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1325.6829833984375
2023-01-07 09:18:08,293 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,293 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,293 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 964.4322509765625
2023-01-07 09:18:08,293 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,294 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,294 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -24.83150863647461
2023-01-07 09:18:08,294 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.27492427825927734
2023-01-07 09:18:08,295 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1776.72265625
2023-01-07 09:18:08,295 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,295 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,295 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1360.0450439453125
2023-01-07 09:18:08,295 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,295 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,295 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 231.17767333984375
2023-01-07 09:18:08,295 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03373591601848602
2023-01-07 09:18:08,296 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 326.93048095703125
2023-01-07 09:18:08,296 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,296 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,296 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1360.0450439453125
2023-01-07 09:18:08,297 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,297 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,297 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 230.54054260253906
2023-01-07 09:18:08,297 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4049440026283264
2023-01-07 09:18:08,298 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1851.67041015625
2023-01-07 09:18:08,298 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,298 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,298 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1148.284423828125
2023-01-07 09:18:08,298 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,298 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,298 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -25.240346908569336
2023-01-07 09:18:08,298 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.4375814199447632
2023-01-07 09:18:08,299 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 329.8385009765625
2023-01-07 09:18:08,299 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,299 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,300 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1148.284423828125
2023-01-07 09:18:08,300 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,300 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,300 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -24.31342887878418
2023-01-07 09:18:08,300 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9406834840774536
2023-01-07 09:18:08,301 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1918.638916015625
2023-01-07 09:18:08,301 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,301 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,301 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1130.82763671875
2023-01-07 09:18:08,301 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,301 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,301 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -21.603681564331055
2023-01-07 09:18:08,301 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0052484869956970215
2023-01-07 09:18:08,302 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1331.4853515625
2023-01-07 09:18:08,302 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,302 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,302 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1130.82763671875
2023-01-07 09:18:08,303 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,303 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,303 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -24.975370407104492
2023-01-07 09:18:08,303 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.32941436767578125
2023-01-07 09:18:08,304 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1751.41162109375
2023-01-07 09:18:08,304 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,304 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,304 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 607.5145263671875
2023-01-07 09:18:08,304 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,304 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,304 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -194.1544647216797
2023-01-07 09:18:08,304 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.31987524032592773
2023-01-07 09:18:08,305 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 330.3626708984375
2023-01-07 09:18:08,305 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,305 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,305 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 607.5145263671875
2023-01-07 09:18:08,306 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,306 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,306 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -193.61659240722656
2023-01-07 09:18:08,306 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24522659182548523
2023-01-07 09:18:08,307 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1262.6656494140625
2023-01-07 09:18:08,307 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,307 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,307 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1943.7474365234375
2023-01-07 09:18:08,307 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,307 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,307 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 595.5013427734375
2023-01-07 09:18:08,307 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6599018573760986
2023-01-07 09:18:08,308 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 331.77899169921875
2023-01-07 09:18:08,308 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,308 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,309 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -1943.7474365234375
2023-01-07 09:18:08,309 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,309 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,309 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 597.2708129882812
2023-01-07 09:18:08,309 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9548678398132324
2023-01-07 09:18:08,310 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -3609.418701171875
2023-01-07 09:18:08,310 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,310 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,310 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19707.310546875
2023-01-07 09:18:08,310 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,310 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,310 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -17.06578254699707
2023-01-07 09:18:08,310 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.042021632194519
2023-01-07 09:18:08,311 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1335.280517578125
2023-01-07 09:18:08,311 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,311 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,312 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -19707.310546875
2023-01-07 09:18:08,312 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,312 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,312 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -19.35685920715332
2023-01-07 09:18:08,312 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1958531141281128
2023-01-07 09:18:08,313 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -32868.52734375
2023-01-07 09:18:08,313 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,313 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,313 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -7868.537109375
2023-01-07 09:18:08,313 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,313 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,313 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 2.4715957641601562
2023-01-07 09:18:08,313 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13149142265319824
2023-01-07 09:18:08,314 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 332.17822265625
2023-01-07 09:18:08,314 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,314 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,315 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -7868.537109375
2023-01-07 09:18:08,315 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,315 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,315 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 4.733245849609375
2023-01-07 09:18:08,315 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.008737673982977867
2023-01-07 09:18:08,316 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -13320.259765625
2023-01-07 09:18:08,316 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,316 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,316 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22593.80859375
2023-01-07 09:18:08,316 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,316 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,316 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.9674814939498901
2023-01-07 09:18:08,317 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7948442697525024
2023-01-07 09:18:08,318 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 334.7889099121094
2023-01-07 09:18:08,318 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,318 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,318 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -22593.80859375
2023-01-07 09:18:08,318 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,318 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,318 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -1.3593246936798096
2023-01-07 09:18:08,318 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7532454133033752
2023-01-07 09:18:08,319 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -37718.0859375
2023-01-07 09:18:08,319 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,319 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,319 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49228.3359375
2023-01-07 09:18:08,319 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,319 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,320 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 6.348921775817871
2023-01-07 09:18:08,320 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.742117881774902
2023-01-07 09:18:08,321 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1342.8863525390625
2023-01-07 09:18:08,321 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,321 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,321 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -49228.3359375
2023-01-07 09:18:08,321 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,321 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,321 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 14.397011756896973
2023-01-07 09:18:08,321 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3070905208587646
2023-01-07 09:18:08,322 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -82224.578125
2023-01-07 09:18:08,322 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,322 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,323 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41334.5546875
2023-01-07 09:18:08,323 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,323 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,323 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 511.63623046875
2023-01-07 09:18:08,323 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0659840926527977
2023-01-07 09:18:08,324 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 667.70361328125
2023-01-07 09:18:08,324 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,324 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,324 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -41334.5546875
2023-01-07 09:18:08,324 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,324 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,324 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 511.8087158203125
2023-01-07 09:18:08,324 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.188299298286438
2023-01-07 09:18:08,325 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -69011.109375
2023-01-07 09:18:08,325 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,326 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,326 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 964.8635864257812
2023-01-07 09:18:08,326 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,326 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,326 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -402.7691345214844
2023-01-07 09:18:08,326 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.7063815593719482
2023-01-07 09:18:08,327 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 514.9073486328125
2023-01-07 09:18:08,327 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,327 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,327 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 964.8635864257812
2023-01-07 09:18:08,327 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,327 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,327 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -401.6793212890625
2023-01-07 09:18:08,327 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.033574968576431274
2023-01-07 09:18:08,329 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1623.4814453125
2023-01-07 09:18:08,329 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,329 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,329 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121582.1875
2023-01-07 09:18:08,329 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,329 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,329 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1122.5264892578125
2023-01-07 09:18:08,329 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5023316144943237
2023-01-07 09:18:08,330 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2712.6298828125
2023-01-07 09:18:08,330 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,330 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,330 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -121582.1875
2023-01-07 09:18:08,330 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,330 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,330 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 1122.1453857421875
2023-01-07 09:18:08,331 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6455137729644775
2023-01-07 09:18:08,332 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -203423.828125
2023-01-07 09:18:08,332 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,332 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,332 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25675.189453125
2023-01-07 09:18:08,332 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,332 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,332 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 521.190185546875
2023-01-07 09:18:08,332 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2946760058403015
2023-01-07 09:18:08,333 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2720.997314453125
2023-01-07 09:18:08,333 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,333 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,333 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -25675.189453125
2023-01-07 09:18:08,333 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,333 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,333 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 525.00341796875
2023-01-07 09:18:08,334 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7850445508956909
2023-01-07 09:18:08,335 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -42776.0390625
2023-01-07 09:18:08,335 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,335 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,335 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1175.573974609375
2023-01-07 09:18:08,335 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,335 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,335 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -391.49810791015625
2023-01-07 09:18:08,335 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1048413515090942
2023-01-07 09:18:08,336 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 674.869140625
2023-01-07 09:18:08,336 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,336 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,337 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1175.573974609375
2023-01-07 09:18:08,337 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,337 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,337 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -389.42010498046875
2023-01-07 09:18:08,337 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06092420592904091
2023-01-07 09:18:08,338 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1970.398193359375
2023-01-07 09:18:08,338 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,338 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,338 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -32613.185546875
2023-01-07 09:18:08,338 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,338 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,338 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2321.1279296875
2023-01-07 09:18:08,338 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.09656012058258057
2023-01-07 09:18:08,339 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 510.4331970214844
2023-01-07 09:18:08,339 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,340 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,340 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -32613.185546875
2023-01-07 09:18:08,340 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,340 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,340 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 2321.36865234375
2023-01-07 09:18:08,340 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3253023028373718
2023-01-07 09:18:08,341 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -54970.328125
2023-01-07 09:18:08,341 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,341 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,341 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101711.0703125
2023-01-07 09:18:08,341 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,341 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,341 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.5943068265914917
2023-01-07 09:18:08,342 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8671730756759644
2023-01-07 09:18:08,342 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2716.9814453125
2023-01-07 09:18:08,343 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,343 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,343 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -101711.0703125
2023-01-07 09:18:08,343 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,343 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,343 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.2607957720756531
2023-01-07 09:18:08,343 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9274318218231201
2023-01-07 09:18:08,344 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -169947.46875
2023-01-07 09:18:08,344 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,344 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,344 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11509.0078125
2023-01-07 09:18:08,344 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,344 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,344 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2008.013427734375
2023-01-07 09:18:08,345 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19959548115730286
2023-01-07 09:18:08,346 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 675.8388061523438
2023-01-07 09:18:08,346 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,346 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,346 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -11509.0078125
2023-01-07 09:18:08,346 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,346 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,346 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 2007.904296875
2023-01-07 09:18:08,346 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4411589205265045
2023-01-07 09:18:08,347 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -19239.41796875
2023-01-07 09:18:08,347 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,347 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,347 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 868.9623413085938
2023-01-07 09:18:08,347 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,347 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,348 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1818.3040771484375
2023-01-07 09:18:08,348 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5500879287719727
2023-01-07 09:18:08,349 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 678.6475830078125
2023-01-07 09:18:08,349 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,349 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,349 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 868.9623413085938
2023-01-07 09:18:08,349 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,349 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,349 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -1818.839599609375
2023-01-07 09:18:08,349 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5458922386169434
2023-01-07 09:18:08,350 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1658.03857421875
2023-01-07 09:18:08,350 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,350 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,350 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149172.28125
2023-01-07 09:18:08,350 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,351 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,351 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4463.48486328125
2023-01-07 09:18:08,351 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:08,352 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2761.852294921875
2023-01-07 09:18:08,352 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -149172.28125
2023-01-07 09:18:08,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,352 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 4463.48486328125
2023-01-07 09:18:08,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4901161193847656e-08
2023-01-07 09:18:08,354 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -249495.421875
2023-01-07 09:18:08,354 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:08,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:08,354 > [DEBUG] 0 :: 166.34640502929688
2023-01-07 09:18:08,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,357 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.022186279296875
2023-01-07 09:18:08,357 > [DEBUG] 0 :: before allreduce fusion buffer :: 1120.7843017578125
2023-01-07 09:18:08,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,359 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 56.332950592041016
2023-01-07 09:18:08,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,360 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 1187.6431884765625
2023-01-07 09:18:08,360 > [DEBUG] 0 :: before allreduce fusion buffer :: -555.4990844726562
2023-01-07 09:18:08,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,363 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -37.09040069580078
2023-01-07 09:18:08,364 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0014225355116650462
2023-01-07 09:18:08,365 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,365 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,365 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: 0.7580138444900513
2023-01-07 09:18:08,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,366 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1216.8192138671875
2023-01-07 09:18:08,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9502346515655518
2023-01-07 09:18:08,368 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,368 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,368 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -76.79527282714844
2023-01-07 09:18:08,368 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.00012912263628095388
2023-01-07 09:18:08,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,369 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 0.7651137113571167
2023-01-07 09:18:08,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,369 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1177.8983154296875
2023-01-07 09:18:08,369 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7667732834815979
2023-01-07 09:18:08,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,371 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 0.1568811535835266
2023-01-07 09:18:08,371 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005583886057138443
2023-01-07 09:18:08,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,372 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 46.43003845214844
2023-01-07 09:18:08,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,372 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.488298416137695
2023-01-07 09:18:08,372 > [DEBUG] 0 :: before allreduce fusion buffer :: 46.47180938720703
2023-01-07 09:18:08,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,373 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 162.44923400878906
2023-01-07 09:18:08,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004284006077796221
2023-01-07 09:18:08,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 0.47732865810394287
2023-01-07 09:18:08,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,374 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 176.46316528320312
2023-01-07 09:18:08,375 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.952089309692383
2023-01-07 09:18:08,376 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,376 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 516.031982421875
2023-01-07 09:18:08,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.035854458808898926
2023-01-07 09:18:08,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,377 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -0.642184853553772
2023-01-07 09:18:08,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,377 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 518.968505859375
2023-01-07 09:18:08,377 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6499274969100952
2023-01-07 09:18:08,378 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,378 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,378 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -28.43584442138672
2023-01-07 09:18:08,379 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.018594972789287567
2023-01-07 09:18:08,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,379 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 33.21729278564453
2023-01-07 09:18:08,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,380 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -14.214408874511719
2023-01-07 09:18:08,380 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.619834899902344
2023-01-07 09:18:08,381 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,381 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,381 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -46.455631256103516
2023-01-07 09:18:08,381 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.017268836498260498
2023-01-07 09:18:08,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,382 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 40.80244445800781
2023-01-07 09:18:08,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,382 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 0.9469995498657227
2023-01-07 09:18:08,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.681663513183594
2023-01-07 09:18:08,383 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,383 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -512.3177490234375
2023-01-07 09:18:08,384 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.003378838300704956
2023-01-07 09:18:08,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,384 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.4889172911643982
2023-01-07 09:18:08,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,385 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -484.3096923828125
2023-01-07 09:18:08,385 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.277532577514648
2023-01-07 09:18:08,386 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,386 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,386 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -189.14096069335938
2023-01-07 09:18:08,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0005701940972357988
2023-01-07 09:18:08,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,387 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 5.133943557739258
2023-01-07 09:18:08,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,387 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -189.14096069335938
2023-01-07 09:18:08,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.004834175109863
2023-01-07 09:18:08,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,389 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -3.68587589263916
2023-01-07 09:18:08,389 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.11021088063716888
2023-01-07 09:18:08,389 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,389 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,390 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 1.1652122735977173
2023-01-07 09:18:08,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,390 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.949084281921387
2023-01-07 09:18:08,390 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31937962770462036
2023-01-07 09:18:08,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,391 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 0.5800142288208008
2023-01-07 09:18:08,391 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01431933045387268
2023-01-07 09:18:08,392 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,392 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,392 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -12.312017440795898
2023-01-07 09:18:08,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,393 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 18.80267333984375
2023-01-07 09:18:08,393 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.273189544677734
2023-01-07 09:18:08,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,394 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1573.916259765625
2023-01-07 09:18:08,394 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08927980065345764
2023-01-07 09:18:08,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 6.233271598815918
2023-01-07 09:18:08,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,395 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1580.7840576171875
2023-01-07 09:18:08,395 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.015385627746582
2023-01-07 09:18:08,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 30.410974502563477
2023-01-07 09:18:08,397 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.15268394351005554
2023-01-07 09:18:08,397 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,397 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,397 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 0.38604673743247986
2023-01-07 09:18:08,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,398 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 44.046409606933594
2023-01-07 09:18:08,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.3923048973083496
2023-01-07 09:18:08,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,399 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -47.681617736816406
2023-01-07 09:18:08,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.03972010686993599
2023-01-07 09:18:08,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,400 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: 0.7631993293762207
2023-01-07 09:18:08,400 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,400 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,400 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -39.18304443359375
2023-01-07 09:18:08,400 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7991483211517334
2023-01-07 09:18:08,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -124.9842300415039
2023-01-07 09:18:08,402 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05578940361738205
2023-01-07 09:18:08,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,402 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 0.3217955231666565
2023-01-07 09:18:08,403 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,403 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,403 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -116.65949249267578
2023-01-07 09:18:08,403 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2618541717529297
2023-01-07 09:18:08,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,404 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 64.85643005371094
2023-01-07 09:18:08,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.007256189361214638
2023-01-07 09:18:08,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.2534024119377136
2023-01-07 09:18:08,405 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,405 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,405 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 74.32746124267578
2023-01-07 09:18:08,405 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19591641426086426
2023-01-07 09:18:08,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,406 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -425.0136413574219
2023-01-07 09:18:08,407 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.039911720901727676
2023-01-07 09:18:08,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,407 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -0.8451770544052124
2023-01-07 09:18:08,408 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,408 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,408 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -417.0825500488281
2023-01-07 09:18:08,408 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9201410412788391
2023-01-07 09:18:08,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,409 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -13.773380279541016
2023-01-07 09:18:08,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005821223836392164
2023-01-07 09:18:08,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.4608532786369324
2023-01-07 09:18:08,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,410 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 25.513511657714844
2023-01-07 09:18:08,410 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.46915292739868164
2023-01-07 09:18:08,411 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,411 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,411 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -135.11160278320312
2023-01-07 09:18:08,412 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.012301014736294746
2023-01-07 09:18:08,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,412 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -2.254929780960083
2023-01-07 09:18:08,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,413 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -95.21951293945312
2023-01-07 09:18:08,413 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3680167198181152
2023-01-07 09:18:08,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,414 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 652.5391235351562
2023-01-07 09:18:08,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01313309371471405
2023-01-07 09:18:08,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,415 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: 3.1982884407043457
2023-01-07 09:18:08,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,415 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 661.9434204101562
2023-01-07 09:18:08,415 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.201967716217041
2023-01-07 09:18:08,416 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,416 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,416 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -343.84716796875
2023-01-07 09:18:08,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.022034242749214172
2023-01-07 09:18:08,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,417 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: 1.263751745223999
2023-01-07 09:18:08,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,418 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -330.2518005371094
2023-01-07 09:18:08,418 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.192025899887085
2023-01-07 09:18:08,419 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,419 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,419 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -286.5474548339844
2023-01-07 09:18:08,419 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.061109378933906555
2023-01-07 09:18:08,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -5.632532119750977
2023-01-07 09:18:08,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,420 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -272.7387390136719
2023-01-07 09:18:08,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.463750839233398
2023-01-07 09:18:08,421 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,421 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,421 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1208.6812744140625
2023-01-07 09:18:08,421 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012140740640461445
2023-01-07 09:18:08,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,423 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 12.399702072143555
2023-01-07 09:18:08,423 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,423 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,423 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1228.8408203125
2023-01-07 09:18:08,423 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.39967155456543
2023-01-07 09:18:08,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,424 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -7.410604476928711
2023-01-07 09:18:08,424 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.44698911905288696
2023-01-07 09:18:08,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,425 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 72.66960906982422
2023-01-07 09:18:08,425 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6268558502197266
2023-01-07 09:18:08,426 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,426 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,426 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -26.881195068359375
2023-01-07 09:18:08,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1045185327529907
2023-01-07 09:18:08,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,427 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 59.66581726074219
2023-01-07 09:18:08,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,427 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 56.3570556640625
2023-01-07 09:18:08,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.78588104248047
2023-01-07 09:18:08,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,428 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,428 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -481.1834411621094
2023-01-07 09:18:08,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04180169850587845
2023-01-07 09:18:08,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,429 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -4.498003959655762
2023-01-07 09:18:08,429 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -377.6891174316406
2023-01-07 09:18:08,430 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.498793125152588
2023-01-07 09:18:08,431 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,431 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,431 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 601.8635864257812
2023-01-07 09:18:08,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4092949628829956
2023-01-07 09:18:08,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,432 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: -60.346073150634766
2023-01-07 09:18:08,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,432 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 611.7947998046875
2023-01-07 09:18:08,432 > [DEBUG] 0 :: before allreduce fusion buffer :: -55.230445861816406
2023-01-07 09:18:08,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,433 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,433 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 725.7929077148438
2023-01-07 09:18:08,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.015104060992598534
2023-01-07 09:18:08,434 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,434 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 755.1561279296875
2023-01-07 09:18:08,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.131570503115654
2023-01-07 09:18:08,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,435 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 939.3347778320312
2023-01-07 09:18:08,436 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.564891517162323
2023-01-07 09:18:08,436 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,436 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,436 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 990.4383544921875
2023-01-07 09:18:08,437 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6042971611022949
2023-01-07 09:18:08,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,437 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -198.90737915039062
2023-01-07 09:18:08,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.18186068534851074
2023-01-07 09:18:08,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,438 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -185.5086669921875
2023-01-07 09:18:08,438 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1233904361724854
2023-01-07 09:18:08,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,439 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,439 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -483.8071594238281
2023-01-07 09:18:08,439 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2810181975364685
2023-01-07 09:18:08,440 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,440 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -592.952880859375
2023-01-07 09:18:08,440 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.24420025944709778
2023-01-07 09:18:08,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,441 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 28.343372344970703
2023-01-07 09:18:08,441 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.175700381398201
2023-01-07 09:18:08,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,442 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,442 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -158.27197265625
2023-01-07 09:18:08,442 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1958293318748474
2023-01-07 09:18:08,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,443 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -162.46978759765625
2023-01-07 09:18:08,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.368183135986328
2023-01-07 09:18:08,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,444 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -272.3437194824219
2023-01-07 09:18:08,444 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.31961441040039
2023-01-07 09:18:08,445 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,445 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,445 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 222.91461181640625
2023-01-07 09:18:08,445 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7061962485313416
2023-01-07 09:18:08,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,446 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 233.91683959960938
2023-01-07 09:18:08,446 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.900205373764038
2023-01-07 09:18:08,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,447 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 40.998748779296875
2023-01-07 09:18:08,447 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.584484100341797
2023-01-07 09:18:08,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 34.38007736206055
2023-01-07 09:18:08,448 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,448 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,448 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 154.1070556640625
2023-01-07 09:18:08,448 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.214725494384766
2023-01-07 09:18:08,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,450 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 296.1120910644531
2023-01-07 09:18:08,450 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.070398330688477
2023-01-07 09:18:08,451 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,451 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,451 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 383.9198913574219
2023-01-07 09:18:08,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.715751647949219
2023-01-07 09:18:08,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,452 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1124.0755615234375
2023-01-07 09:18:08,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6386574506759644
2023-01-07 09:18:08,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,453 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,453 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -1037.5780029296875
2023-01-07 09:18:08,453 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.6400976181030273
2023-01-07 09:18:08,454 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,454 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 292.55218505859375
2023-01-07 09:18:08,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.328842163085938
2023-01-07 09:18:08,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,455 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 23.57185935974121
2023-01-07 09:18:08,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,455 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 422.2043762207031
2023-01-07 09:18:08,455 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.88734817504883
2023-01-07 09:18:08,456 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,456 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,456 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1909.6527099609375
2023-01-07 09:18:08,456 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.856113433837891
2023-01-07 09:18:08,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,457 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1869.049072265625
2023-01-07 09:18:08,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 26.27078628540039
2023-01-07 09:18:08,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,458 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1909.5184326171875
2023-01-07 09:18:08,458 > [DEBUG] 0 :: before allreduce fusion buffer :: 13.043304443359375
2023-01-07 09:18:08,459 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,459 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,459 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1867.4088134765625
2023-01-07 09:18:08,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.459212303161621
2023-01-07 09:18:08,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,460 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4881.17236328125
2023-01-07 09:18:08,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.40898066759109497
2023-01-07 09:18:08,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,461 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: 5.941686630249023
2023-01-07 09:18:08,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,461 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4817.47802734375
2023-01-07 09:18:08,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.514154434204102
2023-01-07 09:18:08,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,463 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4764.12646484375
2023-01-07 09:18:08,463 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1446805000305176
2023-01-07 09:18:08,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,464 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: 5.085609436035156
2023-01-07 09:18:08,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -366.60992431640625
2023-01-07 09:18:08,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,464 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4685.7607421875
2023-01-07 09:18:08,464 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.820152282714844
2023-01-07 09:18:08,465 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,465 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,465 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4685.76171875
2023-01-07 09:18:08,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.9849557876586914
2023-01-07 09:18:08,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,466 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -332.6243896484375
2023-01-07 09:18:08,466 > [DEBUG] 0 :: before allreduce fusion buffer :: -98.9013671875
2023-01-07 09:18:08,467 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,467 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,467 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 242.5765380859375
2023-01-07 09:18:08,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.677764892578125
2023-01-07 09:18:08,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,468 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -13.040937423706055
2023-01-07 09:18:08,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,469 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 255.09756469726562
2023-01-07 09:18:08,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -14.120705604553223
2023-01-07 09:18:08,470 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,470 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,470 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 229.32106018066406
2023-01-07 09:18:08,470 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.16413116455078
2023-01-07 09:18:08,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,471 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -4.806984901428223
2023-01-07 09:18:08,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,471 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 451.23284912109375
2023-01-07 09:18:08,471 > [DEBUG] 0 :: before allreduce fusion buffer :: 81.01655578613281
2023-01-07 09:18:08,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,472 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 260.2728271484375
2023-01-07 09:18:08,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 35.23118591308594
2023-01-07 09:18:08,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,473 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -225.2041473388672
2023-01-07 09:18:08,473 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,473 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,473 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4636.15625
2023-01-07 09:18:08,474 > [DEBUG] 0 :: before allreduce fusion buffer :: -323.5778503417969
2023-01-07 09:18:08,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,474 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 276.2525634765625
2023-01-07 09:18:08,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.057924270629883
2023-01-07 09:18:08,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,475 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -64.12562561035156
2023-01-07 09:18:08,476 > [DEBUG] 0 :: before allreduce fusion buffer :: -99.3931884765625
2023-01-07 09:18:08,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,477 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 211.1105499267578
2023-01-07 09:18:08,477 > [DEBUG] 0 :: before allreduce fusion buffer :: -49.39605712890625
2023-01-07 09:18:08,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,478 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 18.233360290527344
2023-01-07 09:18:08,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,478 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 491.835693359375
2023-01-07 09:18:08,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,478 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 93.19631958007812
2023-01-07 09:18:08,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,478 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4698.28369140625
2023-01-07 09:18:08,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.001673698425293
2023-01-07 09:18:08,479 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,480 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 9.839012145996094
2023-01-07 09:18:08,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.490677833557129
2023-01-07 09:18:08,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,480 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -64.54888916015625
2023-01-07 09:18:08,481 > [DEBUG] 0 :: before allreduce fusion buffer :: 201.82192993164062
2023-01-07 09:18:08,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,482 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -66.88264465332031
2023-01-07 09:18:08,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.379983901977539
2023-01-07 09:18:08,482 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,482 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,482 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -591.5294799804688
2023-01-07 09:18:08,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,483 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4683.03662109375
2023-01-07 09:18:08,483 > [DEBUG] 0 :: before allreduce fusion buffer :: -310.95233154296875
2023-01-07 09:18:08,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,484 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,484 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 733.3792724609375
2023-01-07 09:18:08,484 > [DEBUG] 0 :: before allreduce fusion buffer :: 114.10192108154297
2023-01-07 09:18:08,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,485 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -13.458866119384766
2023-01-07 09:18:08,485 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,485 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 268.30462646484375
2023-01-07 09:18:08,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -109.96934509277344
2023-01-07 09:18:08,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,486 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -56.58515167236328
2023-01-07 09:18:08,486 > [DEBUG] 0 :: before allreduce fusion buffer :: 124.94758605957031
2023-01-07 09:18:08,487 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,487 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,487 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -284.3095703125
2023-01-07 09:18:08,487 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.452821731567383
2023-01-07 09:18:08,491 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:18:08,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,491 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -2399.49462890625
2023-01-07 09:18:08,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,492 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -504.81561279296875
2023-01-07 09:18:08,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,492 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -4679.59228515625
2023-01-07 09:18:08,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,493 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -1875.6556396484375
2023-01-07 09:18:08,493 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,493 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,493 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -1638.613525390625
2023-01-07 09:18:08,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,494 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 601.1036987304688
2023-01-07 09:18:08,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,494 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 1488.9962158203125
2023-01-07 09:18:08,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,495 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 1029.9453125
2023-01-07 09:18:08,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,495 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 410.25714111328125
2023-01-07 09:18:08,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,496 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -127.92349243164062
2023-01-07 09:18:08,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,496 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -473.5129089355469
2023-01-07 09:18:08,496 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,496 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,497 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -465.4598388671875
2023-01-07 09:18:08,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,497 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -732.321044921875
2023-01-07 09:18:08,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,498 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -145.00608825683594
2023-01-07 09:18:08,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 1039.94140625
2023-01-07 09:18:08,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,498 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 977.7050170898438
2023-01-07 09:18:08,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 613.11474609375
2023-01-07 09:18:08,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -369.13323974609375
2023-01-07 09:18:08,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,499 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 67.13495635986328
2023-01-07 09:18:08,499 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,499 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 151.9829864501953
2023-01-07 09:18:08,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1238.899658203125
2023-01-07 09:18:08,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -271.817626953125
2023-01-07 09:18:08,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -326.7630920410156
2023-01-07 09:18:08,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,500 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 664.196533203125
2023-01-07 09:18:08,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -95.48486328125
2023-01-07 09:18:08,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,501 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 59.105079650878906
2023-01-07 09:18:08,501 > [DEBUG] 0 :: before allreduce fusion buffer :: -8589.84375
2023-01-07 09:18:08,502 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,502 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,502 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -414.3433837890625
2023-01-07 09:18:08,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 74.31282806396484
2023-01-07 09:18:08,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -116.70288848876953
2023-01-07 09:18:08,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -36.668426513671875
2023-01-07 09:18:08,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,503 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 44.254974365234375
2023-01-07 09:18:08,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 1587.04833984375
2023-01-07 09:18:08,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,504 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 24.296857833862305
2023-01-07 09:18:08,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,504 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.454363822937012
2023-01-07 09:18:08,504 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,504 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,504 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -183.61813354492188
2023-01-07 09:18:08,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 780.2825317382812
2023-01-07 09:18:08,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,505 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -497.5855712890625
2023-01-07 09:18:08,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,505 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 0.6354475021362305
2023-01-07 09:18:08,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,506 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -14.567161560058594
2023-01-07 09:18:08,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,506 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 536.6094970703125
2023-01-07 09:18:08,506 > [DEBUG] 0 :: before allreduce fusion buffer :: -191.3737030029297
2023-01-07 09:18:08,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,507 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 176.70462036132812
2023-01-07 09:18:08,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,507 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.512068748474121
2023-01-07 09:18:08,507 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,507 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,507 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 1211.369384765625
2023-01-07 09:18:08,507 > [DEBUG] 0 :: before allreduce fusion buffer :: 212.2797393798828
2023-01-07 09:18:08,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 1215.0426025390625
2023-01-07 09:18:08,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:08,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:08,508 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 503.3132019042969
2023-01-07 09:18:08,508 > [DEBUG] 0 :: before allreduce fusion buffer :: -1120.239013671875
2023-01-07 09:18:09,352 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 326.3998718261719
2023-01-07 09:18:09,352 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,352 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,352 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 61.35772705078125
2023-01-07 09:18:09,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,352 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: -20.05166244506836
2023-01-07 09:18:09,352 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,352 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,352 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -881.11181640625
2023-01-07 09:18:09,352 > [DEBUG] 0 :: before allreduce fusion buffer :: 43.75819396972656
2023-01-07 09:18:09,354 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 60.339168548583984
2023-01-07 09:18:09,354 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,354 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,354 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: -3.0653915405273438
2023-01-07 09:18:09,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -1841.9857177734375
2023-01-07 09:18:09,354 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,354 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,354 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1052.6568603515625
2023-01-07 09:18:09,355 > [DEBUG] 0 :: before allreduce fusion buffer :: -1844.0184326171875
2023-01-07 09:18:09,356 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 12.875980377197266
2023-01-07 09:18:09,356 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,356 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,356 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 424.54644775390625
2023-01-07 09:18:09,356 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,356 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,356 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -1083.51611328125
2023-01-07 09:18:09,356 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9858970642089844
2023-01-07 09:18:09,357 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.06493377685547
2023-01-07 09:18:09,357 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,357 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,358 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 424.54644775390625
2023-01-07 09:18:09,358 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,358 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,358 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -894.403564453125
2023-01-07 09:18:09,358 > [DEBUG] 0 :: before allreduce fusion buffer :: 182.1861572265625
2023-01-07 09:18:09,359 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 511.82196044921875
2023-01-07 09:18:09,359 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,359 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,359 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 64.17098999023438
2023-01-07 09:18:09,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,359 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -1.8018875122070312
2023-01-07 09:18:09,359 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,359 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,359 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: 323.1571044921875
2023-01-07 09:18:09,360 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,360 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,360 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -366.82171630859375
2023-01-07 09:18:09,360 > [DEBUG] 0 :: before allreduce fusion buffer :: 364.84747314453125
2023-01-07 09:18:09,361 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.52970886230469
2023-01-07 09:18:09,361 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,361 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,361 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 711.0712890625
2023-01-07 09:18:09,361 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,361 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,361 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -398.23046875
2023-01-07 09:18:09,362 > [DEBUG] 0 :: before allreduce fusion buffer :: -57.848426818847656
2023-01-07 09:18:09,362 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -94.33091735839844
2023-01-07 09:18:09,362 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,362 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,363 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 255.98422241210938
2023-01-07 09:18:09,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 7.1444268226623535
2023-01-07 09:18:09,363 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,363 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,363 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: 26.232620239257812
2023-01-07 09:18:09,363 > [DEBUG] 0 :: before allreduce fusion buffer :: 590.5235595703125
2023-01-07 09:18:09,364 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 254.76202392578125
2023-01-07 09:18:09,364 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,364 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,364 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.445817947387695
2023-01-07 09:18:09,364 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,364 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,365 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -305.88555908203125
2023-01-07 09:18:09,365 > [DEBUG] 0 :: before allreduce fusion buffer :: 28.44845199584961
2023-01-07 09:18:09,365 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 550.5118408203125
2023-01-07 09:18:09,365 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,366 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,366 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.445817947387695
2023-01-07 09:18:09,366 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,366 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,366 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -528.9169921875
2023-01-07 09:18:09,366 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.568695068359375
2023-01-07 09:18:09,367 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 282.39776611328125
2023-01-07 09:18:09,367 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,367 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,367 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.445817947387695
2023-01-07 09:18:09,367 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,367 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,367 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -158.39175415039062
2023-01-07 09:18:09,367 > [DEBUG] 0 :: before allreduce fusion buffer :: 135.9443359375
2023-01-07 09:18:09,368 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 315.92254638671875
2023-01-07 09:18:09,368 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,368 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,368 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 65.0936050415039
2023-01-07 09:18:09,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,369 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -15.383800506591797
2023-01-07 09:18:09,369 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,369 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,369 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -202.66360473632812
2023-01-07 09:18:09,369 > [DEBUG] 0 :: before allreduce fusion buffer :: -198.4691162109375
2023-01-07 09:18:09,370 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.0391845703125
2023-01-07 09:18:09,370 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,370 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,371 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: 10.445817947387695
2023-01-07 09:18:09,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,371 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 247.96817016601562
2023-01-07 09:18:09,371 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,371 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,371 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 75.44539642333984
2023-01-07 09:18:09,371 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.6727986335754395
2023-01-07 09:18:09,372 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -115.1277847290039
2023-01-07 09:18:09,372 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,372 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,372 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 65.95516204833984
2023-01-07 09:18:09,372 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,372 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,373 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: -5.227245330810547
2023-01-07 09:18:09,373 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,373 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,373 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -202.7288818359375
2023-01-07 09:18:09,373 > [DEBUG] 0 :: before allreduce fusion buffer :: 94.66942596435547
2023-01-07 09:18:09,374 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 66.7445068359375
2023-01-07 09:18:09,374 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,374 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,374 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 711.0712890625
2023-01-07 09:18:09,374 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,374 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,374 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -530.5394287109375
2023-01-07 09:18:09,374 > [DEBUG] 0 :: before allreduce fusion buffer :: 161.1031494140625
2023-01-07 09:18:09,375 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 324.7992858886719
2023-01-07 09:18:09,375 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,375 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,375 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 388.73101806640625
2023-01-07 09:18:09,375 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,376 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,376 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 290.444091796875
2023-01-07 09:18:09,376 > [DEBUG] 0 :: before allreduce fusion buffer :: 354.66656494140625
2023-01-07 09:18:09,377 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 283.5525817871094
2023-01-07 09:18:09,377 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,377 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,377 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 388.73101806640625
2023-01-07 09:18:09,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,377 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: -39.474586486816406
2023-01-07 09:18:09,377 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,377 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,377 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -605.883056640625
2023-01-07 09:18:09,377 > [DEBUG] 0 :: before allreduce fusion buffer :: 31.071462631225586
2023-01-07 09:18:09,378 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 486.4544372558594
2023-01-07 09:18:09,379 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,379 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,379 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 711.0712890625
2023-01-07 09:18:09,379 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,379 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,379 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -438.2203063964844
2023-01-07 09:18:09,379 > [DEBUG] 0 :: before allreduce fusion buffer :: 99.61123657226562
2023-01-07 09:18:09,380 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 66.5931167602539
2023-01-07 09:18:09,380 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,380 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,380 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 711.0712890625
2023-01-07 09:18:09,380 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,380 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,380 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: -573.94189453125
2023-01-07 09:18:09,380 > [DEBUG] 0 :: before allreduce fusion buffer :: -106.54037475585938
2023-01-07 09:18:09,381 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 886.81689453125
2023-01-07 09:18:09,381 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,381 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,381 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 63.53167724609375
2023-01-07 09:18:09,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,382 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: 15.380105018615723
2023-01-07 09:18:09,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,382 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: 141.03189086914062
2023-01-07 09:18:09,382 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,382 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,382 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -333.1082763671875
2023-01-07 09:18:09,382 > [DEBUG] 0 :: before allreduce fusion buffer :: 118.46641540527344
2023-01-07 09:18:09,384 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.31095886230469
2023-01-07 09:18:09,384 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,384 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,384 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 985.2049560546875
2023-01-07 09:18:09,384 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,384 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,384 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 73.25920104980469
2023-01-07 09:18:09,384 > [DEBUG] 0 :: before allreduce fusion buffer :: 27.219327926635742
2023-01-07 09:18:09,385 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.97106170654297
2023-01-07 09:18:09,385 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,385 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,385 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 965.7755126953125
2023-01-07 09:18:09,385 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,385 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,385 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -38.399070739746094
2023-01-07 09:18:09,386 > [DEBUG] 0 :: before allreduce fusion buffer :: -176.5494842529297
2023-01-07 09:18:09,386 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 287.620361328125
2023-01-07 09:18:09,387 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,387 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,387 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 985.2049560546875
2023-01-07 09:18:09,387 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,387 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,387 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -40.68471908569336
2023-01-07 09:18:09,387 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.276094436645508
2023-01-07 09:18:09,388 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1231.914794921875
2023-01-07 09:18:09,388 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,388 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,388 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 985.2049560546875
2023-01-07 09:18:09,388 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,388 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,388 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -51.39997482299805
2023-01-07 09:18:09,388 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.759812355041504
2023-01-07 09:18:09,389 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 142.25669860839844
2023-01-07 09:18:09,389 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,389 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,390 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 985.2049560546875
2023-01-07 09:18:09,390 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,390 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,390 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 235.64065551757812
2023-01-07 09:18:09,390 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.5231053829193115
2023-01-07 09:18:09,391 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1282.4329833984375
2023-01-07 09:18:09,391 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,391 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,391 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 124.5130615234375
2023-01-07 09:18:09,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,391 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 19.75994873046875
2023-01-07 09:18:09,391 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,391 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,392 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 91.223876953125
2023-01-07 09:18:09,392 > [DEBUG] 0 :: before allreduce fusion buffer :: 73.92577362060547
2023-01-07 09:18:09,393 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 124.5502700805664
2023-01-07 09:18:09,393 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,393 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,393 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 570.7982788085938
2023-01-07 09:18:09,393 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,393 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,393 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 175.11572265625
2023-01-07 09:18:09,393 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7960057258605957
2023-01-07 09:18:09,394 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 724.8955078125
2023-01-07 09:18:09,394 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,394 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,394 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 503.80084228515625
2023-01-07 09:18:09,394 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,394 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,395 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 35.648555755615234
2023-01-07 09:18:09,395 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,395 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,395 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 201.53146362304688
2023-01-07 09:18:09,395 > [DEBUG] 0 :: before allreduce fusion buffer :: -130.15347290039062
2023-01-07 09:18:09,396 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 500.788818359375
2023-01-07 09:18:09,396 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,396 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,396 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 360.1312255859375
2023-01-07 09:18:09,396 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,396 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,396 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -40.876617431640625
2023-01-07 09:18:09,396 > [DEBUG] 0 :: before allreduce fusion buffer :: 77.35324096679688
2023-01-07 09:18:09,397 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 432.7272033691406
2023-01-07 09:18:09,397 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,397 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,397 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 666.30126953125
2023-01-07 09:18:09,398 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,398 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,398 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 245.34548950195312
2023-01-07 09:18:09,398 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.686639785766602
2023-01-07 09:18:09,399 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 559.2782592773438
2023-01-07 09:18:09,399 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,399 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,399 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 666.30126953125
2023-01-07 09:18:09,399 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,399 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,399 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 97.48355102539062
2023-01-07 09:18:09,399 > [DEBUG] 0 :: before allreduce fusion buffer :: -66.97367858886719
2023-01-07 09:18:09,400 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 848.0306396484375
2023-01-07 09:18:09,400 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,400 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,400 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 127.38790893554688
2023-01-07 09:18:09,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,401 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -27.73024559020996
2023-01-07 09:18:09,401 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,401 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,401 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2397.472900390625
2023-01-07 09:18:09,401 > [DEBUG] 0 :: before allreduce fusion buffer :: -217.0380096435547
2023-01-07 09:18:09,402 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.06356811523438
2023-01-07 09:18:09,402 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,402 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,402 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 708.8363037109375
2023-01-07 09:18:09,402 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,402 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,402 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 2450.240234375
2023-01-07 09:18:09,403 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.295015335083008
2023-01-07 09:18:09,404 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1203.515869140625
2023-01-07 09:18:09,404 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,404 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,404 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 125.48588562011719
2023-01-07 09:18:09,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,404 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: 14.079111099243164
2023-01-07 09:18:09,404 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,404 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,404 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 688.8175048828125
2023-01-07 09:18:09,404 > [DEBUG] 0 :: before allreduce fusion buffer :: 56.373252868652344
2023-01-07 09:18:09,405 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 125.95027160644531
2023-01-07 09:18:09,405 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,405 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,405 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 630.7352294921875
2023-01-07 09:18:09,406 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,406 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,406 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 731.2434692382812
2023-01-07 09:18:09,406 > [DEBUG] 0 :: before allreduce fusion buffer :: -65.52102661132812
2023-01-07 09:18:09,407 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 808.9779052734375
2023-01-07 09:18:09,407 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,407 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,407 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 559.971435546875
2023-01-07 09:18:09,407 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,407 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,407 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 403.4183044433594
2023-01-07 09:18:09,407 > [DEBUG] 0 :: before allreduce fusion buffer :: -48.4861946105957
2023-01-07 09:18:09,408 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 572.6715087890625
2023-01-07 09:18:09,408 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,408 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,408 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 559.971435546875
2023-01-07 09:18:09,409 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,409 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,409 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 244.90884399414062
2023-01-07 09:18:09,409 > [DEBUG] 0 :: before allreduce fusion buffer :: 69.74662017822266
2023-01-07 09:18:09,410 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 639.4859619140625
2023-01-07 09:18:09,410 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,410 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,410 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 127.8348388671875
2023-01-07 09:18:09,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,410 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -9.518094062805176
2023-01-07 09:18:09,410 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,410 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,410 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -151.81431579589844
2023-01-07 09:18:09,411 > [DEBUG] 0 :: before allreduce fusion buffer :: -32.81832504272461
2023-01-07 09:18:09,411 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.99205017089844
2023-01-07 09:18:09,412 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,412 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,412 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 185.00270080566406
2023-01-07 09:18:09,412 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,412 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,412 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -86.88923645019531
2023-01-07 09:18:09,412 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.229034662246704
2023-01-07 09:18:09,413 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 36.24235534667969
2023-01-07 09:18:09,413 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,413 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,413 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 129.32791137695312
2023-01-07 09:18:09,413 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,413 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,413 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -1.7179335355758667
2023-01-07 09:18:09,414 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,414 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,414 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -482.0635986328125
2023-01-07 09:18:09,414 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.244121551513672
2023-01-07 09:18:09,415 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 130.8379669189453
2023-01-07 09:18:09,415 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,415 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,415 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 692.2383422851562
2023-01-07 09:18:09,415 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,415 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,415 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -619.1235961914062
2023-01-07 09:18:09,415 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.043351173400879
2023-01-07 09:18:09,416 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 893.7721557617188
2023-01-07 09:18:09,416 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,416 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,417 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 511.3611145019531
2023-01-07 09:18:09,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,417 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -1.8674211502075195
2023-01-07 09:18:09,417 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,417 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,417 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -474.43304443359375
2023-01-07 09:18:09,417 > [DEBUG] 0 :: before allreduce fusion buffer :: 59.07013702392578
2023-01-07 09:18:09,418 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 513.94580078125
2023-01-07 09:18:09,418 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,418 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,418 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 634.5221557617188
2023-01-07 09:18:09,418 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,418 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,419 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: -544.6994018554688
2023-01-07 09:18:09,419 > [DEBUG] 0 :: before allreduce fusion buffer :: 64.4784164428711
2023-01-07 09:18:09,420 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 821.74462890625
2023-01-07 09:18:09,420 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,420 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,420 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 127.41616821289062
2023-01-07 09:18:09,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,420 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -4.200568675994873
2023-01-07 09:18:09,420 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,420 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,420 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -749.3790893554688
2023-01-07 09:18:09,420 > [DEBUG] 0 :: before allreduce fusion buffer :: -68.9859390258789
2023-01-07 09:18:09,421 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 126.9459457397461
2023-01-07 09:18:09,421 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,422 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,422 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 832.5647583007812
2023-01-07 09:18:09,422 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,422 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,422 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -712.0923461914062
2023-01-07 09:18:09,422 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.59381103515625
2023-01-07 09:18:09,423 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1265.8963623046875
2023-01-07 09:18:09,423 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,423 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,424 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 130.27679443359375
2023-01-07 09:18:09,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: 12.895505905151367
2023-01-07 09:18:09,424 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,424 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,424 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -163.81321716308594
2023-01-07 09:18:09,424 > [DEBUG] 0 :: before allreduce fusion buffer :: 71.18644714355469
2023-01-07 09:18:09,425 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 131.599365234375
2023-01-07 09:18:09,425 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,425 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,425 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 893.8502197265625
2023-01-07 09:18:09,425 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,425 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,425 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -281.7002868652344
2023-01-07 09:18:09,426 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.449108123779297
2023-01-07 09:18:09,426 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1245.638671875
2023-01-07 09:18:09,427 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,427 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,427 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 519.9266357421875
2023-01-07 09:18:09,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,427 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: 0.3750572204589844
2023-01-07 09:18:09,427 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,427 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,427 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -190.4298553466797
2023-01-07 09:18:09,427 > [DEBUG] 0 :: before allreduce fusion buffer :: 44.185420989990234
2023-01-07 09:18:09,428 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 520.7325439453125
2023-01-07 09:18:09,428 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,428 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,428 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 800.4705200195312
2023-01-07 09:18:09,428 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,429 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,429 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 3.3335418701171875
2023-01-07 09:18:09,429 > [DEBUG] 0 :: before allreduce fusion buffer :: 15.2310209274292
2023-01-07 09:18:09,430 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1142.5445556640625
2023-01-07 09:18:09,430 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,430 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,430 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 256.26641845703125
2023-01-07 09:18:09,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: 13.933187484741211
2023-01-07 09:18:09,430 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,430 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,430 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -666.5111083984375
2023-01-07 09:18:09,431 > [DEBUG] 0 :: before allreduce fusion buffer :: 62.0023307800293
2023-01-07 09:18:09,432 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 255.81512451171875
2023-01-07 09:18:09,432 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,432 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,432 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -38829.77734375
2023-01-07 09:18:09,432 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,432 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,432 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -656.9564208984375
2023-01-07 09:18:09,432 > [DEBUG] 0 :: before allreduce fusion buffer :: 22.371475219726562
2023-01-07 09:18:09,433 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -50676.7109375
2023-01-07 09:18:09,433 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,433 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,433 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 236.6617431640625
2023-01-07 09:18:09,433 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,434 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,434 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 613.11474609375
2023-01-07 09:18:09,434 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.198574066162109
2023-01-07 09:18:09,435 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 319.6250915527344
2023-01-07 09:18:09,435 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,435 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,435 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: 236.6617431640625
2023-01-07 09:18:09,435 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,435 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,435 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 624.7301635742188
2023-01-07 09:18:09,435 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.10090160369873
2023-01-07 09:18:09,436 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -2136.298583984375
2023-01-07 09:18:09,436 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,436 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,436 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -52215.81640625
2023-01-07 09:18:09,437 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,437 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,437 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -369.0757751464844
2023-01-07 09:18:09,437 > [DEBUG] 0 :: before allreduce fusion buffer :: -62.07984924316406
2023-01-07 09:18:09,438 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1294.768310546875
2023-01-07 09:18:09,438 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,438 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,438 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -52215.81640625
2023-01-07 09:18:09,438 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,438 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,438 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -374.134033203125
2023-01-07 09:18:09,438 > [DEBUG] 0 :: before allreduce fusion buffer :: 30.025733947753906
2023-01-07 09:18:09,439 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -67691.09375
2023-01-07 09:18:09,439 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,439 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,439 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -37788.55859375
2023-01-07 09:18:09,439 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,440 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,440 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 64.25310516357422
2023-01-07 09:18:09,440 > [DEBUG] 0 :: before allreduce fusion buffer :: -24.962459564208984
2023-01-07 09:18:09,441 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1297.70947265625
2023-01-07 09:18:09,441 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,441 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,441 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -37788.55859375
2023-01-07 09:18:09,441 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,441 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,441 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -21.039962768554688
2023-01-07 09:18:09,441 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.025188446044922
2023-01-07 09:18:09,442 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -49502.2578125
2023-01-07 09:18:09,442 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,442 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,442 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 249.72132873535156
2023-01-07 09:18:09,442 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,443 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -14.550213813781738
2023-01-07 09:18:09,443 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,443 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,443 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 152.6419677734375
2023-01-07 09:18:09,443 > [DEBUG] 0 :: before allreduce fusion buffer :: -17.265621185302734
2023-01-07 09:18:09,444 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 265.0194091796875
2023-01-07 09:18:09,444 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,444 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,444 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -93470.75
2023-01-07 09:18:09,444 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,444 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,445 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 148.34117126464844
2023-01-07 09:18:09,445 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.216074466705322
2023-01-07 09:18:09,446 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -122516.875
2023-01-07 09:18:09,446 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,446 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,446 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19314.248046875
2023-01-07 09:18:09,446 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,446 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,446 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -9.173892974853516
2023-01-07 09:18:09,446 > [DEBUG] 0 :: before allreduce fusion buffer :: -27.53057861328125
2023-01-07 09:18:09,447 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 327.38031005859375
2023-01-07 09:18:09,447 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,447 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,447 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -19314.248046875
2023-01-07 09:18:09,447 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,447 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,448 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 30.556686401367188
2023-01-07 09:18:09,448 > [DEBUG] 0 :: before allreduce fusion buffer :: -9.144461631774902
2023-01-07 09:18:09,449 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -25302.576171875
2023-01-07 09:18:09,449 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,449 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,449 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1443.6314697265625
2023-01-07 09:18:09,449 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,449 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,449 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 11.836807250976562
2023-01-07 09:18:09,449 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.729361534118652
2023-01-07 09:18:09,450 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1374.4140625
2023-01-07 09:18:09,450 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,450 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,450 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1443.6314697265625
2023-01-07 09:18:09,450 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,450 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,450 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 12.567923545837402
2023-01-07 09:18:09,451 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.034536838531494
2023-01-07 09:18:09,452 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1908.5323486328125
2023-01-07 09:18:09,452 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,452 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,452 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1630.104736328125
2023-01-07 09:18:09,452 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,452 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,452 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -27.93571662902832
2023-01-07 09:18:09,452 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.11143611371517181
2023-01-07 09:18:09,453 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 331.2608337402344
2023-01-07 09:18:09,453 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,453 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,453 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 1630.104736328125
2023-01-07 09:18:09,453 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,454 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,454 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -27.36679458618164
2023-01-07 09:18:09,454 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1018211841583252
2023-01-07 09:18:09,455 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2319.747314453125
2023-01-07 09:18:09,455 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,455 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,455 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1678.5760498046875
2023-01-07 09:18:09,455 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,455 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,455 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -1.6300594806671143
2023-01-07 09:18:09,455 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.487086296081543
2023-01-07 09:18:09,456 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 332.8245544433594
2023-01-07 09:18:09,456 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,457 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,457 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 1678.5760498046875
2023-01-07 09:18:09,457 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,457 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,457 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: -4.734637260437012
2023-01-07 09:18:09,457 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.129185676574707
2023-01-07 09:18:09,458 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2185.876708984375
2023-01-07 09:18:09,458 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,458 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,458 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1776.72265625
2023-01-07 09:18:09,458 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,458 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,458 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 42.024314880371094
2023-01-07 09:18:09,459 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.032927393913269
2023-01-07 09:18:09,459 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1385.138916015625
2023-01-07 09:18:09,460 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,460 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,460 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 1776.72265625
2023-01-07 09:18:09,460 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,460 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,460 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 44.82185363769531
2023-01-07 09:18:09,460 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.7466299533843994
2023-01-07 09:18:09,461 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2325.30810546875
2023-01-07 09:18:09,461 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,461 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,461 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1851.67041015625
2023-01-07 09:18:09,461 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,461 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,461 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1077.136962890625
2023-01-07 09:18:09,462 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.906998872756958
2023-01-07 09:18:09,463 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 335.4510192871094
2023-01-07 09:18:09,463 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,463 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,463 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 1851.67041015625
2023-01-07 09:18:09,463 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,463 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,463 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -1078.8079833984375
2023-01-07 09:18:09,463 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7824897170066833
2023-01-07 09:18:09,464 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2236.621826171875
2023-01-07 09:18:09,464 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,464 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,464 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1918.638916015625
2023-01-07 09:18:09,464 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,464 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,465 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 16.514806747436523
2023-01-07 09:18:09,465 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.885331630706787
2023-01-07 09:18:09,466 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 338.22369384765625
2023-01-07 09:18:09,466 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,466 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,466 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 1918.638916015625
2023-01-07 09:18:09,466 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,466 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,466 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 9.15716552734375
2023-01-07 09:18:09,466 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.492130756378174
2023-01-07 09:18:09,467 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2486.67236328125
2023-01-07 09:18:09,467 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,467 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,468 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1751.41162109375
2023-01-07 09:18:09,468 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,468 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,468 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 80.34130859375
2023-01-07 09:18:09,468 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.4819416999816895
2023-01-07 09:18:09,469 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1402.8563232421875
2023-01-07 09:18:09,469 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,469 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,469 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 1751.41162109375
2023-01-07 09:18:09,469 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,469 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,469 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 77.08726501464844
2023-01-07 09:18:09,469 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1607128530740738
2023-01-07 09:18:09,470 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2746.825927734375
2023-01-07 09:18:09,470 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,470 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,471 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1262.6656494140625
2023-01-07 09:18:09,471 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,471 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,471 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -2205.437255859375
2023-01-07 09:18:09,471 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4177911877632141
2023-01-07 09:18:09,472 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 339.1318359375
2023-01-07 09:18:09,472 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,472 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,472 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 1262.6656494140625
2023-01-07 09:18:09,472 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,472 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,472 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -2206.1044921875
2023-01-07 09:18:09,472 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2983076572418213
2023-01-07 09:18:09,473 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2405.799560546875
2023-01-07 09:18:09,473 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,473 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,474 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3609.418701171875
2023-01-07 09:18:09,474 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,474 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,474 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2259.461669921875
2023-01-07 09:18:09,474 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7967737913131714
2023-01-07 09:18:09,475 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 340.4034423828125
2023-01-07 09:18:09,475 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,475 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,475 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3609.418701171875
2023-01-07 09:18:09,475 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,475 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,475 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -2269.925048828125
2023-01-07 09:18:09,475 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.023794829845428467
2023-01-07 09:18:09,477 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -3937.832763671875
2023-01-07 09:18:09,477 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,477 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,477 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -32868.52734375
2023-01-07 09:18:09,477 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,477 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,477 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -12.88015365600586
2023-01-07 09:18:09,477 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.279093027114868
2023-01-07 09:18:09,478 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1412.43212890625
2023-01-07 09:18:09,478 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,478 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,478 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -32868.52734375
2023-01-07 09:18:09,478 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,478 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,478 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -20.126556396484375
2023-01-07 09:18:09,478 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.991060256958008
2023-01-07 09:18:09,480 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -43130.4453125
2023-01-07 09:18:09,480 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,480 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,480 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -13320.259765625
2023-01-07 09:18:09,480 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,480 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,480 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 617.4453735351562
2023-01-07 09:18:09,480 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.8339590430259705
2023-01-07 09:18:09,481 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 341.3760681152344
2023-01-07 09:18:09,481 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,481 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,481 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -13320.259765625
2023-01-07 09:18:09,481 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,481 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,482 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 603.7334594726562
2023-01-07 09:18:09,482 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13181522488594055
2023-01-07 09:18:09,483 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -19228.357421875
2023-01-07 09:18:09,483 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,483 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,483 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37718.0859375
2023-01-07 09:18:09,483 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,483 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,483 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 24.31707000732422
2023-01-07 09:18:09,483 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6551566123962402
2023-01-07 09:18:09,484 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 348.60931396484375
2023-01-07 09:18:09,484 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,484 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,484 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -37718.0859375
2023-01-07 09:18:09,484 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,485 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,485 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 27.51508903503418
2023-01-07 09:18:09,485 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.07944223284721375
2023-01-07 09:18:09,486 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -49460.80859375
2023-01-07 09:18:09,486 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,486 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,486 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82224.578125
2023-01-07 09:18:09,486 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,486 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,486 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 13.47905445098877
2023-01-07 09:18:09,486 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.24933308362960815
2023-01-07 09:18:09,487 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1417.501220703125
2023-01-07 09:18:09,487 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,487 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,488 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -82224.578125
2023-01-07 09:18:09,488 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,488 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,488 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: 16.46647071838379
2023-01-07 09:18:09,488 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4558067321777344
2023-01-07 09:18:09,489 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -107777.8046875
2023-01-07 09:18:09,489 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,489 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,489 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -69011.109375
2023-01-07 09:18:09,489 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,489 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,489 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -2304.824951171875
2023-01-07 09:18:09,489 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14877162873744965
2023-01-07 09:18:09,490 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 693.3045043945312
2023-01-07 09:18:09,490 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,491 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,491 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -69011.109375
2023-01-07 09:18:09,491 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,491 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,491 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: -2304.958984375
2023-01-07 09:18:09,491 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.254644870758057
2023-01-07 09:18:09,492 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -90209.7578125
2023-01-07 09:18:09,492 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,492 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,492 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1623.4814453125
2023-01-07 09:18:09,492 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,492 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,492 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4056.4111328125
2023-01-07 09:18:09,493 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3209409713745117
2023-01-07 09:18:09,494 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 515.8888549804688
2023-01-07 09:18:09,494 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,494 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,494 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 1623.4814453125
2023-01-07 09:18:09,494 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,494 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,494 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -4057.60009765625
2023-01-07 09:18:09,494 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.05605461075901985
2023-01-07 09:18:09,495 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 3129.7578125
2023-01-07 09:18:09,495 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,495 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,495 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -203423.828125
2023-01-07 09:18:09,495 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,495 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,496 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -3697.322021484375
2023-01-07 09:18:09,496 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5598207712173462
2023-01-07 09:18:09,497 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2863.373046875
2023-01-07 09:18:09,497 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,497 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,497 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -203423.828125
2023-01-07 09:18:09,497 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,497 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,497 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -3696.852783203125
2023-01-07 09:18:09,497 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.122969627380371
2023-01-07 09:18:09,498 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -267356.6875
2023-01-07 09:18:09,498 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,498 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,498 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -42776.0390625
2023-01-07 09:18:09,498 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,498 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,498 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -48.17266845703125
2023-01-07 09:18:09,499 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.4602999687194824
2023-01-07 09:18:09,500 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2929.76513671875
2023-01-07 09:18:09,500 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,500 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,500 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -42776.0390625
2023-01-07 09:18:09,500 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,500 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,500 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -46.238380432128906
2023-01-07 09:18:09,500 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7196435928344727
2023-01-07 09:18:09,501 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -55948.46875
2023-01-07 09:18:09,501 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,501 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,501 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1970.398193359375
2023-01-07 09:18:09,501 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,501 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,502 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -13409.806640625
2023-01-07 09:18:09,502 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09595026075839996
2023-01-07 09:18:09,503 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 710.7182006835938
2023-01-07 09:18:09,503 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,503 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,503 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 1970.398193359375
2023-01-07 09:18:09,503 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,503 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,503 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -13409.73046875
2023-01-07 09:18:09,503 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08763279020786285
2023-01-07 09:18:09,504 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 3973.951171875
2023-01-07 09:18:09,504 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,504 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,505 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -54970.328125
2023-01-07 09:18:09,505 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,505 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,505 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -6741.2548828125
2023-01-07 09:18:09,505 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4584496021270752
2023-01-07 09:18:09,506 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 507.1408386230469
2023-01-07 09:18:09,506 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,506 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,506 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -54970.328125
2023-01-07 09:18:09,506 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,506 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,506 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -6741.8896484375
2023-01-07 09:18:09,506 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5176160931587219
2023-01-07 09:18:09,507 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -73408.78125
2023-01-07 09:18:09,507 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,508 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,508 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -169947.46875
2023-01-07 09:18:09,508 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,508 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,508 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 11.453245162963867
2023-01-07 09:18:09,508 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03693319857120514
2023-01-07 09:18:09,509 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2864.90478515625
2023-01-07 09:18:09,509 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,509 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,509 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -169947.46875
2023-01-07 09:18:09,509 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,509 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,509 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 10.984454154968262
2023-01-07 09:18:09,509 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.024936139583587646
2023-01-07 09:18:09,511 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -222704.265625
2023-01-07 09:18:09,511 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,511 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,511 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -19239.41796875
2023-01-07 09:18:09,511 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,511 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,511 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -8390.365234375
2023-01-07 09:18:09,511 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0064502889290452
2023-01-07 09:18:09,512 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 717.9227294921875
2023-01-07 09:18:09,512 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,512 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,512 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -19239.41796875
2023-01-07 09:18:09,512 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,512 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,512 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -8390.095703125
2023-01-07 09:18:09,513 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.002762051299214363
2023-01-07 09:18:09,514 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -25189.46875
2023-01-07 09:18:09,514 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,514 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,514 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1658.03857421875
2023-01-07 09:18:09,514 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,514 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -16485.8359375
2023-01-07 09:18:09,514 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.04749385267496109
2023-01-07 09:18:09,515 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 630.4434814453125
2023-01-07 09:18:09,515 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 1658.03857421875
2023-01-07 09:18:09,515 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,515 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,516 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -16486.17578125
2023-01-07 09:18:09,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0549580417573452
2023-01-07 09:18:09,517 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 4082.592529296875
2023-01-07 09:18:09,517 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -249495.421875
2023-01-07 09:18:09,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,517 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,517 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -12087.466796875
2023-01-07 09:18:09,517 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:09,518 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2796.059814453125
2023-01-07 09:18:09,518 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,518 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,518 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -249495.421875
2023-01-07 09:18:09,518 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,519 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -12087.466796875
2023-01-07 09:18:09,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:09,520 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -328655.96875
2023-01-07 09:18:09,520 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:09,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:09,521 > [DEBUG] 0 :: 188.29689025878906
2023-01-07 09:18:09,522 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,522 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,523 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -0.0159912109375
2023-01-07 09:18:09,523 > [DEBUG] 0 :: before allreduce fusion buffer :: -468.75140380859375
2023-01-07 09:18:09,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,524 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 65.08322143554688
2023-01-07 09:18:09,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,524 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -301.9808654785156
2023-01-07 09:18:09,525 > [DEBUG] 0 :: before allreduce fusion buffer :: 1140.087158203125
2023-01-07 09:18:09,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,527 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.521512985229492
2023-01-07 09:18:09,527 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0015212412690743804
2023-01-07 09:18:09,528 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,528 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,528 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -0.7397657632827759
2023-01-07 09:18:09,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,529 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -3.525815486907959
2023-01-07 09:18:09,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.10196176171302795
2023-01-07 09:18:09,531 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,531 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,531 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -51.22264862060547
2023-01-07 09:18:09,531 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.00022382465249393135
2023-01-07 09:18:09,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,532 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: 2.567824363708496
2023-01-07 09:18:09,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,532 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -51.2221794128418
2023-01-07 09:18:09,532 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.572923421859741
2023-01-07 09:18:09,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,534 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.06266152858734131
2023-01-07 09:18:09,534 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0031616389751434326
2023-01-07 09:18:09,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,535 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 57.21080017089844
2023-01-07 09:18:09,535 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,535 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,535 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.06266152858734131
2023-01-07 09:18:09,535 > [DEBUG] 0 :: before allreduce fusion buffer :: 57.100547790527344
2023-01-07 09:18:09,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,536 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 44.205352783203125
2023-01-07 09:18:09,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0022051483392715454
2023-01-07 09:18:09,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,537 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: -0.45007312297821045
2023-01-07 09:18:09,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,537 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 44.205352783203125
2023-01-07 09:18:09,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 9.166587829589844
2023-01-07 09:18:09,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,539 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3035.841796875
2023-01-07 09:18:09,539 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.001990152522921562
2023-01-07 09:18:09,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,540 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: -1.2179489135742188
2023-01-07 09:18:09,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,540 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3035.841796875
2023-01-07 09:18:09,540 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2168235778808594
2023-01-07 09:18:09,541 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,541 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,541 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 14.583602905273438
2023-01-07 09:18:09,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.03984496742486954
2023-01-07 09:18:09,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,542 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 49.09947967529297
2023-01-07 09:18:09,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,542 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 14.624130249023438
2023-01-07 09:18:09,542 > [DEBUG] 0 :: before allreduce fusion buffer :: 41.364051818847656
2023-01-07 09:18:09,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,544 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -34.76723861694336
2023-01-07 09:18:09,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006717421114444733
2023-01-07 09:18:09,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,545 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 55.65390396118164
2023-01-07 09:18:09,545 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,545 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,545 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -34.7752571105957
2023-01-07 09:18:09,545 > [DEBUG] 0 :: before allreduce fusion buffer :: 55.18178176879883
2023-01-07 09:18:09,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,546 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -266.43841552734375
2023-01-07 09:18:09,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.002790171653032303
2023-01-07 09:18:09,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,547 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: -0.8746603727340698
2023-01-07 09:18:09,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,547 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -256.6810607910156
2023-01-07 09:18:09,548 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08929368853569031
2023-01-07 09:18:09,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,549 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 58.83033752441406
2023-01-07 09:18:09,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0009533018455840647
2023-01-07 09:18:09,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,550 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 6.531777381896973
2023-01-07 09:18:09,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,550 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 58.83033752441406
2023-01-07 09:18:09,550 > [DEBUG] 0 :: before allreduce fusion buffer :: 11.447456359863281
2023-01-07 09:18:09,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,551 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -2.91251802444458
2023-01-07 09:18:09,551 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.015720099210739136
2023-01-07 09:18:09,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,552 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: -11.437298774719238
2023-01-07 09:18:09,552 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,552 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,552 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -27.5937442779541
2023-01-07 09:18:09,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.992013931274414
2023-01-07 09:18:09,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,554 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,554 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 8.201431274414062
2023-01-07 09:18:09,554 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0006289258599281311
2023-01-07 09:18:09,554 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: -0.1620779037475586
2023-01-07 09:18:09,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,555 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -20.720596313476562
2023-01-07 09:18:09,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1942908763885498
2023-01-07 09:18:09,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,557 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -226.33218383789062
2023-01-07 09:18:09,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0399056002497673
2023-01-07 09:18:09,557 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,557 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: 4.994909286499023
2023-01-07 09:18:09,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,558 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -256.2164306640625
2023-01-07 09:18:09,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.413074016571045
2023-01-07 09:18:09,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,559 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 24.76544189453125
2023-01-07 09:18:09,559 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0024479329586029053
2023-01-07 09:18:09,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,560 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: 1.0745422840118408
2023-01-07 09:18:09,560 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,560 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,560 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -0.0699920654296875
2023-01-07 09:18:09,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9209811687469482
2023-01-07 09:18:09,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 192.13978576660156
2023-01-07 09:18:09,562 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.004583895206451416
2023-01-07 09:18:09,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,562 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.3987182080745697
2023-01-07 09:18:09,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,563 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 169.93081665039062
2023-01-07 09:18:09,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3804222047328949
2023-01-07 09:18:09,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,564 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 64.45864868164062
2023-01-07 09:18:09,564 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.004419281613081694
2023-01-07 09:18:09,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: -0.24795293807983398
2023-01-07 09:18:09,565 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,565 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,565 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 64.45774841308594
2023-01-07 09:18:09,565 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2697652578353882
2023-01-07 09:18:09,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 22.867036819458008
2023-01-07 09:18:09,567 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0096548181027174
2023-01-07 09:18:09,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,567 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 0.9795299172401428
2023-01-07 09:18:09,568 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,568 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,568 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 31.667919158935547
2023-01-07 09:18:09,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9750438928604126
2023-01-07 09:18:09,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,569 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 484.2360534667969
2023-01-07 09:18:09,569 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00026063621044158936
2023-01-07 09:18:09,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -1.1914758682250977
2023-01-07 09:18:09,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,570 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 509.40167236328125
2023-01-07 09:18:09,570 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.1589365005493164
2023-01-07 09:18:09,571 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,571 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,571 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -100.08256530761719
2023-01-07 09:18:09,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012060465291142464
2023-01-07 09:18:09,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,572 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: -3.3420801162719727
2023-01-07 09:18:09,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,573 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,573 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -76.13522338867188
2023-01-07 09:18:09,573 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3416242599487305
2023-01-07 09:18:09,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,574 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -140.0744171142578
2023-01-07 09:18:09,574 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.29462528228759766
2023-01-07 09:18:09,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -1.5372116565704346
2023-01-07 09:18:09,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,575 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -118.97834777832031
2023-01-07 09:18:09,575 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2883446216583252
2023-01-07 09:18:09,576 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,576 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,576 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 318.8306579589844
2023-01-07 09:18:09,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.007221631705760956
2023-01-07 09:18:09,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,577 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -1.8684041500091553
2023-01-07 09:18:09,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,578 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 338.10565185546875
2023-01-07 09:18:09,578 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9142470359802246
2023-01-07 09:18:09,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,579 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 102.28754425048828
2023-01-07 09:18:09,579 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06310862302780151
2023-01-07 09:18:09,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,580 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -0.5623377561569214
2023-01-07 09:18:09,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,580 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 102.1800765991211
2023-01-07 09:18:09,580 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.5495892763137817
2023-01-07 09:18:09,581 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,581 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,581 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -91.47166442871094
2023-01-07 09:18:09,581 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.14951428771018982
2023-01-07 09:18:09,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,582 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: 0.044114381074905396
2023-01-07 09:18:09,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,582 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -92.52799987792969
2023-01-07 09:18:09,583 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.030296921730041504
2023-01-07 09:18:09,584 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,584 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,584 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1102.814453125
2023-01-07 09:18:09,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0010783824836835265
2023-01-07 09:18:09,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,585 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 12.527294158935547
2023-01-07 09:18:09,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,585 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1104.3336181640625
2023-01-07 09:18:09,585 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.52253246307373
2023-01-07 09:18:09,586 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,586 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,587 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 2.3232486248016357
2023-01-07 09:18:09,587 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37131190299987793
2023-01-07 09:18:09,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,588 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 3.3511650562286377
2023-01-07 09:18:09,588 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.7499240040779114
2023-01-07 09:18:09,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 5.012458324432373
2023-01-07 09:18:09,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9303481578826904
2023-01-07 09:18:09,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: 0.31538623571395874
2023-01-07 09:18:09,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 4.304183483123779
2023-01-07 09:18:09,590 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.06635883450508118
2023-01-07 09:18:09,591 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,591 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,591 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 6.6916913986206055
2023-01-07 09:18:09,591 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0029628872871398926
2023-01-07 09:18:09,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -6.69180965423584
2023-01-07 09:18:09,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 3.851771116256714
2023-01-07 09:18:09,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.659345626831055
2023-01-07 09:18:09,593 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,593 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,593 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 47.44464874267578
2023-01-07 09:18:09,594 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0710577741265297
2023-01-07 09:18:09,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,594 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 3.276238441467285
2023-01-07 09:18:09,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 46.028663635253906
2023-01-07 09:18:09,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.056831359863281
2023-01-07 09:18:09,596 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,596 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,596 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -216.44129943847656
2023-01-07 09:18:09,596 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02189064770936966
2023-01-07 09:18:09,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,597 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -269.1851806640625
2023-01-07 09:18:09,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15557557344436646
2023-01-07 09:18:09,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,598 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 652.9752197265625
2023-01-07 09:18:09,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08099006116390228
2023-01-07 09:18:09,599 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,599 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,599 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 651.596923828125
2023-01-07 09:18:09,599 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01584634929895401
2023-01-07 09:18:09,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,600 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -71.2988510131836
2023-01-07 09:18:09,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.10052452981472015
2023-01-07 09:18:09,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,601 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -57.347599029541016
2023-01-07 09:18:09,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.34133145213127136
2023-01-07 09:18:09,602 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,602 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,602 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -18.35960578918457
2023-01-07 09:18:09,602 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.055665500462055206
2023-01-07 09:18:09,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,603 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -12.293946266174316
2023-01-07 09:18:09,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0004961211234331131
2023-01-07 09:18:09,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,604 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 83.75236511230469
2023-01-07 09:18:09,604 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.042231060564517975
2023-01-07 09:18:09,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,605 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 83.85855102539062
2023-01-07 09:18:09,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05931489169597626
2023-01-07 09:18:09,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,606 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -128.37086486816406
2023-01-07 09:18:09,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.39155149459838867
2023-01-07 09:18:09,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,607 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,607 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -128.72116088867188
2023-01-07 09:18:09,607 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.553343772888184
2023-01-07 09:18:09,607 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,608 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 71.34425354003906
2023-01-07 09:18:09,608 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.025960423052310944
2023-01-07 09:18:09,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,609 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 78.10044860839844
2023-01-07 09:18:09,609 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.432743638753891
2023-01-07 09:18:09,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 95.2420883178711
2023-01-07 09:18:09,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.13881385326385498
2023-01-07 09:18:09,610 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,610 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,610 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 2.0123062133789062
2023-01-07 09:18:09,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,611 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 95.16230010986328
2023-01-07 09:18:09,611 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.126709461212158
2023-01-07 09:18:09,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,612 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: -56.989952087402344
2023-01-07 09:18:09,612 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.9291592836380005
2023-01-07 09:18:09,613 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,613 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,613 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 7.863849639892578
2023-01-07 09:18:09,613 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.8495774269104
2023-01-07 09:18:09,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,614 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -0.5976896286010742
2023-01-07 09:18:09,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0718834400177002
2023-01-07 09:18:09,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,615 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -0.09820079803466797
2023-01-07 09:18:09,615 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.3982431888580322
2023-01-07 09:18:09,616 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,616 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,616 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 77.00812530517578
2023-01-07 09:18:09,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7881701588630676
2023-01-07 09:18:09,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,617 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: 7.5070414543151855
2023-01-07 09:18:09,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,617 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 90.71806335449219
2023-01-07 09:18:09,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.711759567260742
2023-01-07 09:18:09,618 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,618 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,619 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 424.7657470703125
2023-01-07 09:18:09,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.027771949768066
2023-01-07 09:18:09,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,619 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 550.220947265625
2023-01-07 09:18:09,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4964810609817505
2023-01-07 09:18:09,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 25.848567962646484
2023-01-07 09:18:09,621 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6389309763908386
2023-01-07 09:18:09,621 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,621 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,621 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: 357.05731201171875
2023-01-07 09:18:09,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.177473247051239
2023-01-07 09:18:09,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,622 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -443.12432861328125
2023-01-07 09:18:09,623 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7099280953407288
2023-01-07 09:18:09,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,623 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -1.1453608274459839
2023-01-07 09:18:09,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,624 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,624 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -99.84151458740234
2023-01-07 09:18:09,624 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.3446414470672607
2023-01-07 09:18:09,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,625 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -99.84151458740234
2023-01-07 09:18:09,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.15861767530441284
2023-01-07 09:18:09,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,626 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -2.4844210147857666
2023-01-07 09:18:09,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,626 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 79.09988403320312
2023-01-07 09:18:09,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,626 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 296.5604553222656
2023-01-07 09:18:09,626 > [DEBUG] 0 :: before allreduce fusion buffer :: -13.609886169433594
2023-01-07 09:18:09,627 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,628 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 296.8517761230469
2023-01-07 09:18:09,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.2357134073972702
2023-01-07 09:18:09,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,629 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 327.8387451171875
2023-01-07 09:18:09,629 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.2408390045166
2023-01-07 09:18:09,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,630 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 34.117523193359375
2023-01-07 09:18:09,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7090392112731934
2023-01-07 09:18:09,630 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,630 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,631 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: -0.08420145511627197
2023-01-07 09:18:09,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,631 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 131.59671020507812
2023-01-07 09:18:09,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.4475202560424805
2023-01-07 09:18:09,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,632 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 231.8653106689453
2023-01-07 09:18:09,632 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.271517276763916
2023-01-07 09:18:09,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,633 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -0.5468279719352722
2023-01-07 09:18:09,633 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,633 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,633 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 29.891010284423828
2023-01-07 09:18:09,633 > [DEBUG] 0 :: before allreduce fusion buffer :: 18.041458129882812
2023-01-07 09:18:09,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,634 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 327.3462829589844
2023-01-07 09:18:09,635 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.603649139404297
2023-01-07 09:18:09,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,635 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -11.929039001464844
2023-01-07 09:18:09,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,635 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,636 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 394.3090515136719
2023-01-07 09:18:09,636 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.918699264526367
2023-01-07 09:18:09,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,637 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 430.40643310546875
2023-01-07 09:18:09,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.21714797616004944
2023-01-07 09:18:09,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,638 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -37.65813064575195
2023-01-07 09:18:09,638 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.538739204406738
2023-01-07 09:18:09,638 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,639 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 547.9764404296875
2023-01-07 09:18:09,639 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6520180702209473
2023-01-07 09:18:09,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,640 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: 6.325000762939453
2023-01-07 09:18:09,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,640 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -106.59967041015625
2023-01-07 09:18:09,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,640 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 665.0203857421875
2023-01-07 09:18:09,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,641 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 420.5693054199219
2023-01-07 09:18:09,641 > [DEBUG] 0 :: before allreduce fusion buffer :: -37.317481994628906
2023-01-07 09:18:09,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,642 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 689.8565673828125
2023-01-07 09:18:09,642 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.6836733818054199
2023-01-07 09:18:09,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,643 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 23.197742462158203
2023-01-07 09:18:09,643 > [DEBUG] 0 :: before allreduce fusion buffer :: 40.21290969848633
2023-01-07 09:18:09,644 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,644 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,644 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 795.5345458984375
2023-01-07 09:18:09,644 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.1980748176574707
2023-01-07 09:18:09,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,645 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: 141.02056884765625
2023-01-07 09:18:09,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,645 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 418.0032653808594
2023-01-07 09:18:09,645 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.651371002197266
2023-01-07 09:18:09,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,646 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 34.732948303222656
2023-01-07 09:18:09,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.538717269897461
2023-01-07 09:18:09,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: -0.937288761138916
2023-01-07 09:18:09,647 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,647 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,647 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 147.27529907226562
2023-01-07 09:18:09,647 > [DEBUG] 0 :: before allreduce fusion buffer :: -10.848156929016113
2023-01-07 09:18:09,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 261.7010803222656
2023-01-07 09:18:09,649 > [DEBUG] 0 :: before allreduce fusion buffer :: 48.623321533203125
2023-01-07 09:18:09,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,649 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 369.72998046875
2023-01-07 09:18:09,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3848423957824707
2023-01-07 09:18:09,652 > [DEBUG] 0 :: communication is scheduled in BWTOFW
2023-01-07 09:18:09,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,652 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,652 > [DEBUG] 0 :: before allreduce param grad sum conv1._dp_wrapped_module.flat_param_0  :: -12.69162368774414
2023-01-07 09:18:09,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,653 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 503.7658996582031
2023-01-07 09:18:09,653 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 415.2073974609375
2023-01-07 09:18:09,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,654 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -51.960182189941406
2023-01-07 09:18:09,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: 213.22312927246094
2023-01-07 09:18:09,655 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,655 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,655 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -143.5972137451172
2023-01-07 09:18:09,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,656 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -265.6524963378906
2023-01-07 09:18:09,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,656 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 92.28971862792969
2023-01-07 09:18:09,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,657 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 138.82070922851562
2023-01-07 09:18:09,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,658 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 81.09613037109375
2023-01-07 09:18:09,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,658 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -139.64788818359375
2023-01-07 09:18:09,658 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,658 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,659 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 32.3525276184082
2023-01-07 09:18:09,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,659 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 2.3277275562286377
2023-01-07 09:18:09,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,659 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -35.88036346435547
2023-01-07 09:18:09,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 654.429931640625
2023-01-07 09:18:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -271.4544677734375
2023-01-07 09:18:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 23.61553192138672
2023-01-07 09:18:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,660 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.029431343078613
2023-01-07 09:18:09,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -10.826318740844727
2023-01-07 09:18:09,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 4.2464518547058105
2023-01-07 09:18:09,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: 1107.167236328125
2023-01-07 09:18:09,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,661 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,661 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -91.76802062988281
2023-01-07 09:18:09,661 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: 102.23741912841797
2023-01-07 09:18:09,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 337.69903564453125
2023-01-07 09:18:09,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: -120.96275329589844
2023-01-07 09:18:09,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,662 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -73.01992797851562
2023-01-07 09:18:09,663 > [DEBUG] 0 :: before allreduce fusion buffer :: 1623.277587890625
2023-01-07 09:18:09,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 512.943359375
2023-01-07 09:18:09,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,664 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 31.89729118347168
2023-01-07 09:18:09,664 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,664 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 64.45138549804688
2023-01-07 09:18:09,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: 148.3063507080078
2023-01-07 09:18:09,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 0.027826309204101562
2023-01-07 09:18:09,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,665 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -278.6258239746094
2023-01-07 09:18:09,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,666 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -41.529563903808594
2023-01-07 09:18:09,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -27.171897888183594
2023-01-07 09:18:09,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,666 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,666 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 71.08430480957031
2023-01-07 09:18:09,666 > [DEBUG] 0 :: before allreduce fusion buffer :: 708.929443359375
2023-01-07 09:18:09,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,667 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -256.0321044921875
2023-01-07 09:18:09,667 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,667 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -34.365047454833984
2023-01-07 09:18:09,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: 8.94622802734375
2023-01-07 09:18:09,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: -3037.016357421875
2023-01-07 09:18:09,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -2705.07421875
2023-01-07 09:18:09,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: 31.552688598632812
2023-01-07 09:18:09,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 12.872198104858398
2023-01-07 09:18:09,669 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,669 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,669 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -51.23004150390625
2023-01-07 09:18:09,670 > [DEBUG] 0 :: before allreduce fusion buffer :: -611.9285888671875
2023-01-07 09:18:09,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -0.7106823921203613
2023-01-07 09:18:09,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:09,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:09,670 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 229.39422607421875
2023-01-07 09:18:09,671 > [DEBUG] 0 :: before allreduce fusion buffer :: -617.6101684570312
2023-01-07 09:18:10,513 > [DEBUG] 0 :: after allreduce param grad sum conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 391.4840393066406
2023-01-07 09:18:10,513 > [DEBUG] 0 :: param_name :: conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,513 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,513 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  bn1._dp_wrapped_module.flat_param_0 value:: 60.339168548583984
2023-01-07 09:18:10,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,513 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,513 > [DEBUG] 0 :: before allreduce param grad sum bn1._dp_wrapped_module.flat_param_0  :: 4.370713233947754
2023-01-07 09:18:10,513 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,514 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,514 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 458.7588195800781
2023-01-07 09:18:10,514 > [DEBUG] 0 :: before allreduce fusion buffer :: -25.946834564208984
2023-01-07 09:18:10,515 > [DEBUG] 0 :: after allreduce param grad sum bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 59.95161819458008
2023-01-07 09:18:10,515 > [DEBUG] 0 :: param_name :: bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,515 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,515 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv1._dp_wrapped_module.flat_param_0 value:: 12.875980377197266
2023-01-07 09:18:10,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,516 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0  :: -52.278587341308594
2023-01-07 09:18:10,516 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,516 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,516 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 454.4022521972656
2023-01-07 09:18:10,516 > [DEBUG] 0 :: before allreduce fusion buffer :: -52.93709945678711
2023-01-07 09:18:10,517 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 23.9301700592041
2023-01-07 09:18:10,517 > [DEBUG] 0 :: param_name :: layer1.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,517 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,517 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 511.82196044921875
2023-01-07 09:18:10,517 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,518 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,518 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 454.962158203125
2023-01-07 09:18:10,518 > [DEBUG] 0 :: before allreduce fusion buffer :: -30.899381637573242
2023-01-07 09:18:10,519 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.76142120361328
2023-01-07 09:18:10,519 > [DEBUG] 0 :: param_name :: layer1.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,519 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,519 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.conv2._dp_wrapped_module.flat_param_0 value:: 511.82196044921875
2023-01-07 09:18:10,519 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,519 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,519 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: 333.5958251953125
2023-01-07 09:18:10,519 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.9394367933273315
2023-01-07 09:18:10,520 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 581.5518798828125
2023-01-07 09:18:10,520 > [DEBUG] 0 :: param_name :: layer1.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,520 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,520 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn2._dp_wrapped_module.flat_param_0 value:: 63.52970886230469
2023-01-07 09:18:10,520 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,520 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,520 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0  :: -3.2686877250671387
2023-01-07 09:18:10,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,521 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0  :: -42.95391082763672
2023-01-07 09:18:10,521 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,521 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,521 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 902.1588134765625
2023-01-07 09:18:10,521 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.01322937011719
2023-01-07 09:18:10,522 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 63.69284439086914
2023-01-07 09:18:10,522 > [DEBUG] 0 :: param_name :: layer1.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,522 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,523 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 886.81689453125
2023-01-07 09:18:10,523 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,523 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,523 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 818.5394287109375
2023-01-07 09:18:10,523 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.432343482971191
2023-01-07 09:18:10,524 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -113.9372329711914
2023-01-07 09:18:10,524 > [DEBUG] 0 :: param_name :: layer1.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,524 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,524 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.0.bn3._dp_wrapped_module.flat_param_0 value:: 254.76202392578125
2023-01-07 09:18:10,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,524 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0  :: 1.897417426109314
2023-01-07 09:18:10,524 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,524 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,524 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -58.920066833496094
2023-01-07 09:18:10,524 > [DEBUG] 0 :: before allreduce fusion buffer :: -76.43246459960938
2023-01-07 09:18:10,526 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 253.34490966796875
2023-01-07 09:18:10,526 > [DEBUG] 0 :: param_name :: layer1.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,526 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,526 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -115.1277847290039
2023-01-07 09:18:10,526 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,526 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,526 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 89.7638931274414
2023-01-07 09:18:10,526 > [DEBUG] 0 :: before allreduce fusion buffer :: 12.402803421020508
2023-01-07 09:18:10,527 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 634.5482788085938
2023-01-07 09:18:10,527 > [DEBUG] 0 :: param_name :: layer1.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,527 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,527 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -115.1277847290039
2023-01-07 09:18:10,527 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,527 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,527 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 31.55270767211914
2023-01-07 09:18:10,527 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.29552459716797
2023-01-07 09:18:10,528 > [DEBUG] 0 :: after allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 283.13519287109375
2023-01-07 09:18:10,528 > [DEBUG] 0 :: param_name :: layer1.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,528 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,528 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -115.1277847290039
2023-01-07 09:18:10,529 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,529 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,529 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -32.401573181152344
2023-01-07 09:18:10,529 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14175772666931152
2023-01-07 09:18:10,530 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 333.8759765625
2023-01-07 09:18:10,530 > [DEBUG] 0 :: param_name :: layer1.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,530 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,530 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn1._dp_wrapped_module.flat_param_0 value:: 65.0391845703125
2023-01-07 09:18:10,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,530 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0  :: -0.2659492492675781
2023-01-07 09:18:10,530 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,530 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,530 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: -49.934303283691406
2023-01-07 09:18:10,530 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.864503860473633
2023-01-07 09:18:10,532 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 65.245361328125
2023-01-07 09:18:10,532 > [DEBUG] 0 :: param_name :: layer1.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,532 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,532 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.conv2._dp_wrapped_module.flat_param_0 value:: -115.1277847290039
2023-01-07 09:18:10,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,532 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 50.521995544433594
2023-01-07 09:18:10,532 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,532 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,532 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 850.3843994140625
2023-01-07 09:18:10,532 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.53012752532959
2023-01-07 09:18:10,533 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -187.42141723632812
2023-01-07 09:18:10,533 > [DEBUG] 0 :: param_name :: layer1.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,534 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,534 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.1.bn2._dp_wrapped_module.flat_param_0 value:: 66.7445068359375
2023-01-07 09:18:10,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,534 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0  :: 1.7575557231903076
2023-01-07 09:18:10,534 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,534 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,534 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: 121.20439910888672
2023-01-07 09:18:10,534 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2493295669555664
2023-01-07 09:18:10,535 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 67.62588500976562
2023-01-07 09:18:10,535 > [DEBUG] 0 :: param_name :: layer1.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,535 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,535 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 886.81689453125
2023-01-07 09:18:10,536 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,536 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,536 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 810.1426391601562
2023-01-07 09:18:10,536 > [DEBUG] 0 :: before allreduce fusion buffer :: 29.111919403076172
2023-01-07 09:18:10,537 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 379.59271240234375
2023-01-07 09:18:10,537 > [DEBUG] 0 :: param_name :: layer1.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,537 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,537 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 486.4544372558594
2023-01-07 09:18:10,537 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,537 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,537 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 74.37225341796875
2023-01-07 09:18:10,537 > [DEBUG] 0 :: before allreduce fusion buffer :: 14.619071960449219
2023-01-07 09:18:10,538 > [DEBUG] 0 :: after allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 284.906494140625
2023-01-07 09:18:10,538 > [DEBUG] 0 :: param_name :: layer1.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,538 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,538 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv1._dp_wrapped_module.flat_param_0 value:: 486.4544372558594
2023-01-07 09:18:10,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,539 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 220.7861328125
2023-01-07 09:18:10,539 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,539 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,539 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 830.4454956054688
2023-01-07 09:18:10,539 > [DEBUG] 0 :: before allreduce fusion buffer :: -20.12301254272461
2023-01-07 09:18:10,540 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 548.908203125
2023-01-07 09:18:10,540 > [DEBUG] 0 :: param_name :: layer1.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,540 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,540 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 886.81689453125
2023-01-07 09:18:10,540 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,540 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,540 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 864.3434448242188
2023-01-07 09:18:10,541 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.287616729736328
2023-01-07 09:18:10,542 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 67.45150756835938
2023-01-07 09:18:10,542 > [DEBUG] 0 :: param_name :: layer1.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,542 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,542 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.conv2._dp_wrapped_module.flat_param_0 value:: 886.81689453125
2023-01-07 09:18:10,542 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,542 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,542 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 814.7498779296875
2023-01-07 09:18:10,542 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.31890869140625
2023-01-07 09:18:10,543 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1030.6712646484375
2023-01-07 09:18:10,543 > [DEBUG] 0 :: param_name :: layer1.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,543 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,543 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer1.2.bn2._dp_wrapped_module.flat_param_0 value:: 63.31095886230469
2023-01-07 09:18:10,543 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,543 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,543 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0  :: -2.3614935874938965
2023-01-07 09:18:10,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,544 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0  :: -38.378684997558594
2023-01-07 09:18:10,544 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,544 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,544 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 425.381103515625
2023-01-07 09:18:10,544 > [DEBUG] 0 :: before allreduce fusion buffer :: -56.75908660888672
2023-01-07 09:18:10,545 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 62.735496520996094
2023-01-07 09:18:10,545 > [DEBUG] 0 :: param_name :: layer1.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,546 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,546 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 1282.4329833984375
2023-01-07 09:18:10,546 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,546 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,546 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 839.2909545898438
2023-01-07 09:18:10,546 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.71390438079834
2023-01-07 09:18:10,547 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 75.184814453125
2023-01-07 09:18:10,547 > [DEBUG] 0 :: param_name :: layer1.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,547 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,547 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv1._dp_wrapped_module.flat_param_0 value:: 1231.914794921875
2023-01-07 09:18:10,547 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,547 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,547 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: 556.6903686523438
2023-01-07 09:18:10,547 > [DEBUG] 0 :: before allreduce fusion buffer :: 87.56725311279297
2023-01-07 09:18:10,548 > [DEBUG] 0 :: after allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 288.78302001953125
2023-01-07 09:18:10,548 > [DEBUG] 0 :: param_name :: layer1.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,548 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,549 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 1282.4329833984375
2023-01-07 09:18:10,549 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,549 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,549 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 792.9674682617188
2023-01-07 09:18:10,549 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8107513785362244
2023-01-07 09:18:10,550 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1431.866943359375
2023-01-07 09:18:10,550 > [DEBUG] 0 :: param_name :: layer2.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,550 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,550 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 1282.4329833984375
2023-01-07 09:18:10,550 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,550 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,550 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 783.150634765625
2023-01-07 09:18:10,550 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.024181842803955
2023-01-07 09:18:10,551 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 143.58828735351562
2023-01-07 09:18:10,551 > [DEBUG] 0 :: param_name :: layer2.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,551 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,551 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv2._dp_wrapped_module.flat_param_0 value:: 1282.4329833984375
2023-01-07 09:18:10,551 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,551 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,552 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: 803.7694091796875
2023-01-07 09:18:10,552 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7915859222412109
2023-01-07 09:18:10,553 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1545.5462646484375
2023-01-07 09:18:10,553 > [DEBUG] 0 :: param_name :: layer2.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,553 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,553 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn2._dp_wrapped_module.flat_param_0 value:: 124.5502700805664
2023-01-07 09:18:10,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0  :: 0.9336373209953308
2023-01-07 09:18:10,553 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,553 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,553 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -89.20599365234375
2023-01-07 09:18:10,553 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.649750709533691
2023-01-07 09:18:10,554 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 124.44683837890625
2023-01-07 09:18:10,554 > [DEBUG] 0 :: param_name :: layer2.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,554 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,555 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.conv3._dp_wrapped_module.flat_param_0 value:: 724.8955078125
2023-01-07 09:18:10,555 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,555 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,555 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -181.3931884765625
2023-01-07 09:18:10,555 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4836196899414062
2023-01-07 09:18:10,556 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 815.5172119140625
2023-01-07 09:18:10,556 > [DEBUG] 0 :: param_name :: layer2.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,556 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,556 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.bn3._dp_wrapped_module.flat_param_0 value:: 500.788818359375
2023-01-07 09:18:10,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,556 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0  :: 3.228137254714966
2023-01-07 09:18:10,556 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,556 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,557 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -209.84909057617188
2023-01-07 09:18:10,557 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8154358863830566
2023-01-07 09:18:10,558 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 496.9466552734375
2023-01-07 09:18:10,558 > [DEBUG] 0 :: param_name :: layer2.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,558 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,558 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.0.downsample.0._dp_wrapped_module.flat_param_0 value:: 432.7272033691406
2023-01-07 09:18:10,558 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,558 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,558 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -190.26651000976562
2023-01-07 09:18:10,558 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.123102188110352
2023-01-07 09:18:10,559 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 471.8292236328125
2023-01-07 09:18:10,559 > [DEBUG] 0 :: param_name :: layer2.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,559 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,559 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 848.0306396484375
2023-01-07 09:18:10,559 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,559 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,559 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -216.3162384033203
2023-01-07 09:18:10,560 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.037668228149414
2023-01-07 09:18:10,561 > [DEBUG] 0 :: after allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 560.8489990234375
2023-01-07 09:18:10,561 > [DEBUG] 0 :: param_name :: layer2.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,561 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,561 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv1._dp_wrapped_module.flat_param_0 value:: 848.0306396484375
2023-01-07 09:18:10,561 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,561 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,561 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: -194.40835571289062
2023-01-07 09:18:10,561 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7150037288665771
2023-01-07 09:18:10,562 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 915.2337646484375
2023-01-07 09:18:10,562 > [DEBUG] 0 :: param_name :: layer2.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,562 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,562 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn1._dp_wrapped_module.flat_param_0 value:: 128.06356811523438
2023-01-07 09:18:10,562 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,562 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,562 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0  :: -2.86521315574646
2023-01-07 09:18:10,563 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,563 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,563 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -270.2205810546875
2023-01-07 09:18:10,563 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.50379753112793
2023-01-07 09:18:10,564 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 128.40396118164062
2023-01-07 09:18:10,564 > [DEBUG] 0 :: param_name :: layer2.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,564 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,564 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv2._dp_wrapped_module.flat_param_0 value:: 1203.515869140625
2023-01-07 09:18:10,564 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,564 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,564 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: -263.7513122558594
2023-01-07 09:18:10,564 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9449255466461182
2023-01-07 09:18:10,565 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1478.7049560546875
2023-01-07 09:18:10,565 > [DEBUG] 0 :: param_name :: layer2.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,565 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,566 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.bn2._dp_wrapped_module.flat_param_0 value:: 125.95027160644531
2023-01-07 09:18:10,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0  :: -1.102420449256897
2023-01-07 09:18:10,566 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,566 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,566 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 185.15658569335938
2023-01-07 09:18:10,566 > [DEBUG] 0 :: before allreduce fusion buffer :: 7.264327049255371
2023-01-07 09:18:10,567 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 126.3302993774414
2023-01-07 09:18:10,567 > [DEBUG] 0 :: param_name :: layer2.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,567 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,567 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.1.conv3._dp_wrapped_module.flat_param_0 value:: 808.9779052734375
2023-01-07 09:18:10,567 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,567 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,567 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 137.8906707763672
2023-01-07 09:18:10,568 > [DEBUG] 0 :: before allreduce fusion buffer :: 19.265216827392578
2023-01-07 09:18:10,569 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 998.247314453125
2023-01-07 09:18:10,569 > [DEBUG] 0 :: param_name :: layer2.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,569 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,569 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 639.4859619140625
2023-01-07 09:18:10,569 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,569 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,569 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 141.26991271972656
2023-01-07 09:18:10,569 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.2168622016906738
2023-01-07 09:18:10,570 > [DEBUG] 0 :: after allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 576.1199951171875
2023-01-07 09:18:10,570 > [DEBUG] 0 :: param_name :: layer2.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,570 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,570 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv1._dp_wrapped_module.flat_param_0 value:: 639.4859619140625
2023-01-07 09:18:10,570 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,570 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,570 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: 134.84567260742188
2023-01-07 09:18:10,571 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.8600897789001465
2023-01-07 09:18:10,572 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 602.6302490234375
2023-01-07 09:18:10,572 > [DEBUG] 0 :: param_name :: layer2.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,572 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,572 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn1._dp_wrapped_module.flat_param_0 value:: 128.99205017089844
2023-01-07 09:18:10,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0  :: -8.497953414916992
2023-01-07 09:18:10,572 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,572 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,572 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 92.44999694824219
2023-01-07 09:18:10,572 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.3238213062286377
2023-01-07 09:18:10,573 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 130.04763793945312
2023-01-07 09:18:10,573 > [DEBUG] 0 :: param_name :: layer2.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,573 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,573 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv2._dp_wrapped_module.flat_param_0 value:: 36.24235534667969
2023-01-07 09:18:10,574 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,574 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,574 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: 71.4052963256836
2023-01-07 09:18:10,574 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.537489652633667
2023-01-07 09:18:10,575 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -194.63458251953125
2023-01-07 09:18:10,575 > [DEBUG] 0 :: param_name :: layer2.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,575 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,575 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn2._dp_wrapped_module.flat_param_0 value:: 130.8379669189453
2023-01-07 09:18:10,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,575 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0  :: -4.128727436065674
2023-01-07 09:18:10,575 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,575 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,575 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -123.86119842529297
2023-01-07 09:18:10,576 > [DEBUG] 0 :: before allreduce fusion buffer :: -22.650409698486328
2023-01-07 09:18:10,577 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 132.11697387695312
2023-01-07 09:18:10,577 > [DEBUG] 0 :: param_name :: layer2.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,577 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,577 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.conv3._dp_wrapped_module.flat_param_0 value:: 893.7721557617188
2023-01-07 09:18:10,577 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,577 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,577 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: -154.6405487060547
2023-01-07 09:18:10,577 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.59576416015625
2023-01-07 09:18:10,578 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1140.144775390625
2023-01-07 09:18:10,578 > [DEBUG] 0 :: param_name :: layer2.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,578 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,578 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.2.bn3._dp_wrapped_module.flat_param_0 value:: 513.94580078125
2023-01-07 09:18:10,578 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,578 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,578 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0  :: -3.079274892807007
2023-01-07 09:18:10,579 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,579 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,579 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 36.65873718261719
2023-01-07 09:18:10,579 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.1080265045166
2023-01-07 09:18:10,580 > [DEBUG] 0 :: after allreduce param grad sum layer2.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 515.11572265625
2023-01-07 09:18:10,580 > [DEBUG] 0 :: param_name :: layer2.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,580 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,580 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv1._dp_wrapped_module.flat_param_0 value:: 821.74462890625
2023-01-07 09:18:10,580 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,580 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,580 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 94.40298461914062
2023-01-07 09:18:10,580 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.3703477382659912
2023-01-07 09:18:10,581 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 876.7828369140625
2023-01-07 09:18:10,581 > [DEBUG] 0 :: param_name :: layer2.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,581 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,582 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn1._dp_wrapped_module.flat_param_0 value:: 126.9459457397461
2023-01-07 09:18:10,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,582 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0  :: -3.4563817977905273
2023-01-07 09:18:10,582 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,582 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,582 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 14.473138809204102
2023-01-07 09:18:10,582 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.44105339050293
2023-01-07 09:18:10,583 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 125.65321350097656
2023-01-07 09:18:10,583 > [DEBUG] 0 :: param_name :: layer2.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,583 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,583 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv2._dp_wrapped_module.flat_param_0 value:: 1265.8963623046875
2023-01-07 09:18:10,583 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,583 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,583 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: 12.281482696533203
2023-01-07 09:18:10,584 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0115503072738647
2023-01-07 09:18:10,585 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1614.14306640625
2023-01-07 09:18:10,585 > [DEBUG] 0 :: param_name :: layer2.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,585 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,585 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn2._dp_wrapped_module.flat_param_0 value:: 131.599365234375
2023-01-07 09:18:10,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0  :: -2.5826480388641357
2023-01-07 09:18:10,585 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,585 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,585 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: -24.101043701171875
2023-01-07 09:18:10,585 > [DEBUG] 0 :: before allreduce fusion buffer :: -16.52178955078125
2023-01-07 09:18:10,587 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 131.9517822265625
2023-01-07 09:18:10,587 > [DEBUG] 0 :: param_name :: layer2.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,587 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,587 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.conv3._dp_wrapped_module.flat_param_0 value:: 1245.638671875
2023-01-07 09:18:10,587 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,587 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,587 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 1.428922176361084
2023-01-07 09:18:10,587 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.26421165466308594
2023-01-07 09:18:10,588 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1547.7413330078125
2023-01-07 09:18:10,588 > [DEBUG] 0 :: param_name :: layer2.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,588 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,588 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer2.3.bn3._dp_wrapped_module.flat_param_0 value:: 520.7325439453125
2023-01-07 09:18:10,588 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,588 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,588 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0  :: -2.3349108695983887
2023-01-07 09:18:10,589 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,589 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,589 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 171.62962341308594
2023-01-07 09:18:10,589 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.359347820281982
2023-01-07 09:18:10,590 > [DEBUG] 0 :: after allreduce param grad sum layer2.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 522.19677734375
2023-01-07 09:18:10,590 > [DEBUG] 0 :: param_name :: layer2.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,590 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,590 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv1._dp_wrapped_module.flat_param_0 value:: 1142.5445556640625
2023-01-07 09:18:10,590 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,590 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,590 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: 263.9336242675781
2023-01-07 09:18:10,590 > [DEBUG] 0 :: before allreduce fusion buffer :: 52.89558029174805
2023-01-07 09:18:10,591 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1255.8607177734375
2023-01-07 09:18:10,592 > [DEBUG] 0 :: param_name :: layer3.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,592 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,592 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.bn1._dp_wrapped_module.flat_param_0 value:: 255.81512451171875
2023-01-07 09:18:10,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0  :: -18.567609786987305
2023-01-07 09:18:10,592 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,592 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,592 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -848.9424438476562
2023-01-07 09:18:10,592 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.42631721496582
2023-01-07 09:18:10,593 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 255.51458740234375
2023-01-07 09:18:10,593 > [DEBUG] 0 :: param_name :: layer3.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,593 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,594 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv2._dp_wrapped_module.flat_param_0 value:: -50676.7109375
2023-01-07 09:18:10,594 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,594 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,594 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: -834.840087890625
2023-01-07 09:18:10,594 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.693925857543945
2023-01-07 09:18:10,595 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -61062.2421875
2023-01-07 09:18:10,595 > [DEBUG] 0 :: param_name :: layer3.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,595 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,595 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -2136.298583984375
2023-01-07 09:18:10,595 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,595 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,595 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 23.61553192138672
2023-01-07 09:18:10,595 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8355281949043274
2023-01-07 09:18:10,596 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 327.1109313964844
2023-01-07 09:18:10,596 > [DEBUG] 0 :: param_name :: layer3.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,597 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,597 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.conv3._dp_wrapped_module.flat_param_0 value:: -2136.298583984375
2023-01-07 09:18:10,597 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,597 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,597 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 15.656228065490723
2023-01-07 09:18:10,597 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.147507667541504
2023-01-07 09:18:10,598 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -4171.26611328125
2023-01-07 09:18:10,598 > [DEBUG] 0 :: param_name :: layer3.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,598 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,598 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -67691.09375
2023-01-07 09:18:10,598 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,598 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,598 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 4.03115177154541
2023-01-07 09:18:10,598 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0680179595947266
2023-01-07 09:18:10,599 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1310.1961669921875
2023-01-07 09:18:10,599 > [DEBUG] 0 :: param_name :: layer3.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,600 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,600 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -67691.09375
2023-01-07 09:18:10,600 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,600 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,600 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: 10.651368141174316
2023-01-07 09:18:10,600 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1683998107910156
2023-01-07 09:18:10,601 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -80336.5
2023-01-07 09:18:10,601 > [DEBUG] 0 :: param_name :: layer3.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,601 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,601 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49502.2578125
2023-01-07 09:18:10,601 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,601 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,601 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: -10.850666046142578
2023-01-07 09:18:10,601 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.2966222763061523
2023-01-07 09:18:10,602 > [DEBUG] 0 :: after allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1308.49853515625
2023-01-07 09:18:10,603 > [DEBUG] 0 :: param_name :: layer3.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,603 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,603 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv1._dp_wrapped_module.flat_param_0 value:: -49502.2578125
2023-01-07 09:18:10,603 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,603 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,603 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 11.687019348144531
2023-01-07 09:18:10,603 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1428860425949097
2023-01-07 09:18:10,604 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -59106.25
2023-01-07 09:18:10,604 > [DEBUG] 0 :: param_name :: layer3.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,604 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,604 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.bn1._dp_wrapped_module.flat_param_0 value:: 265.0194091796875
2023-01-07 09:18:10,604 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,604 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,604 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0  :: -22.36604118347168
2023-01-07 09:18:10,605 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,605 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,605 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 4.0738525390625
2023-01-07 09:18:10,605 > [DEBUG] 0 :: before allreduce fusion buffer :: -18.461715698242188
2023-01-07 09:18:10,606 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 285.7110900878906
2023-01-07 09:18:10,606 > [DEBUG] 0 :: param_name :: layer3.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,606 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,606 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv2._dp_wrapped_module.flat_param_0 value:: -122516.875
2023-01-07 09:18:10,606 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,606 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,606 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: -7.376655101776123
2023-01-07 09:18:10,606 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.1322875022888184
2023-01-07 09:18:10,607 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -146296.359375
2023-01-07 09:18:10,608 > [DEBUG] 0 :: param_name :: layer3.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,608 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,608 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25302.576171875
2023-01-07 09:18:10,608 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,608 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,608 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -11.546815872192383
2023-01-07 09:18:10,608 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5413086414337158
2023-01-07 09:18:10,609 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 329.5936279296875
2023-01-07 09:18:10,609 > [DEBUG] 0 :: param_name :: layer3.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,609 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,609 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.1.conv3._dp_wrapped_module.flat_param_0 value:: -25302.576171875
2023-01-07 09:18:10,609 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,609 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,609 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -11.98900318145752
2023-01-07 09:18:10,610 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.96030855178833
2023-01-07 09:18:10,611 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -30160.5390625
2023-01-07 09:18:10,611 > [DEBUG] 0 :: param_name :: layer3.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,611 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,611 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1908.5323486328125
2023-01-07 09:18:10,611 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,611 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,611 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: -0.5803262591362
2023-01-07 09:18:10,611 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.026221752166748
2023-01-07 09:18:10,612 > [DEBUG] 0 :: after allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1425.334716796875
2023-01-07 09:18:10,612 > [DEBUG] 0 :: param_name :: layer3.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,612 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,612 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv1._dp_wrapped_module.flat_param_0 value:: 1908.5323486328125
2023-01-07 09:18:10,612 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,612 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,612 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 15.932538986206055
2023-01-07 09:18:10,613 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.72928524017334
2023-01-07 09:18:10,614 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2300.650146484375
2023-01-07 09:18:10,614 > [DEBUG] 0 :: param_name :: layer3.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,614 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,614 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 2319.747314453125
2023-01-07 09:18:10,614 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,614 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,614 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -11.169204711914062
2023-01-07 09:18:10,614 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.9396699666976929
2023-01-07 09:18:10,615 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 336.43023681640625
2023-01-07 09:18:10,615 > [DEBUG] 0 :: param_name :: layer3.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,615 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,615 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv2._dp_wrapped_module.flat_param_0 value:: 2319.747314453125
2023-01-07 09:18:10,615 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,615 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,615 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -8.839546203613281
2023-01-07 09:18:10,616 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.82917320728302
2023-01-07 09:18:10,617 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2979.11474609375
2023-01-07 09:18:10,617 > [DEBUG] 0 :: param_name :: layer3.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,617 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,617 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2185.876708984375
2023-01-07 09:18:10,617 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,617 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,617 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 19.0247745513916
2023-01-07 09:18:10,617 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.5718536376953125
2023-01-07 09:18:10,618 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 335.02423095703125
2023-01-07 09:18:10,618 > [DEBUG] 0 :: param_name :: layer3.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,618 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,618 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.2.conv3._dp_wrapped_module.flat_param_0 value:: 2185.876708984375
2023-01-07 09:18:10,619 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,619 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,619 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 50.013797760009766
2023-01-07 09:18:10,619 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.1317375898361206
2023-01-07 09:18:10,620 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2560.0869140625
2023-01-07 09:18:10,620 > [DEBUG] 0 :: param_name :: layer3.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,620 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,620 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2325.30810546875
2023-01-07 09:18:10,620 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,620 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,620 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 7.782195091247559
2023-01-07 09:18:10,620 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37204068899154663
2023-01-07 09:18:10,621 > [DEBUG] 0 :: after allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1426.669677734375
2023-01-07 09:18:10,621 > [DEBUG] 0 :: param_name :: layer3.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,621 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,622 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv1._dp_wrapped_module.flat_param_0 value:: 2325.30810546875
2023-01-07 09:18:10,622 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,622 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,622 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 13.078712463378906
2023-01-07 09:18:10,622 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.806291103363037
2023-01-07 09:18:10,623 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2812.162109375
2023-01-07 09:18:10,623 > [DEBUG] 0 :: param_name :: layer3.3.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,623 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,623 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2236.621826171875
2023-01-07 09:18:10,623 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,623 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,623 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 150.87513732910156
2023-01-07 09:18:10,623 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.8581548929214478
2023-01-07 09:18:10,624 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 338.1213684082031
2023-01-07 09:18:10,624 > [DEBUG] 0 :: param_name :: layer3.3.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,624 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,625 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv2._dp_wrapped_module.flat_param_0 value:: 2236.621826171875
2023-01-07 09:18:10,625 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,625 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,625 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: 152.37474060058594
2023-01-07 09:18:10,625 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.969283103942871
2023-01-07 09:18:10,626 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2641.074951171875
2023-01-07 09:18:10,626 > [DEBUG] 0 :: param_name :: layer3.3.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,626 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,626 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2486.67236328125
2023-01-07 09:18:10,626 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,626 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,626 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -352.71075439453125
2023-01-07 09:18:10,626 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1112200021743774
2023-01-07 09:18:10,628 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 340.36590576171875
2023-01-07 09:18:10,628 > [DEBUG] 0 :: param_name :: layer3.3.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,628 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,628 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.3.conv3._dp_wrapped_module.flat_param_0 value:: 2486.67236328125
2023-01-07 09:18:10,628 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,628 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,628 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: -342.23492431640625
2023-01-07 09:18:10,628 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.19616489112377167
2023-01-07 09:18:10,629 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 3138.29638671875
2023-01-07 09:18:10,629 > [DEBUG] 0 :: param_name :: layer3.3.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,629 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,629 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2746.825927734375
2023-01-07 09:18:10,629 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,629 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,629 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 105.90852355957031
2023-01-07 09:18:10,630 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.7789807319641113
2023-01-07 09:18:10,631 > [DEBUG] 0 :: after allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1454.69189453125
2023-01-07 09:18:10,631 > [DEBUG] 0 :: param_name :: layer3.3.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,631 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,631 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv1._dp_wrapped_module.flat_param_0 value:: 2746.825927734375
2023-01-07 09:18:10,631 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,631 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,631 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: 92.93719482421875
2023-01-07 09:18:10,631 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.0975571870803833
2023-01-07 09:18:10,632 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 3381.953125
2023-01-07 09:18:10,632 > [DEBUG] 0 :: param_name :: layer3.4.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,632 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,632 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2405.799560546875
2023-01-07 09:18:10,632 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,632 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,633 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 373.37969970703125
2023-01-07 09:18:10,633 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.21091940999031067
2023-01-07 09:18:10,634 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 345.8179016113281
2023-01-07 09:18:10,634 > [DEBUG] 0 :: param_name :: layer3.4.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,634 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,634 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv2._dp_wrapped_module.flat_param_0 value:: 2405.799560546875
2023-01-07 09:18:10,634 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,634 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,634 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: 372.53375244140625
2023-01-07 09:18:10,634 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0465286523103714
2023-01-07 09:18:10,635 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2989.018798828125
2023-01-07 09:18:10,635 > [DEBUG] 0 :: param_name :: layer3.4.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,635 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,635 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3937.832763671875
2023-01-07 09:18:10,635 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,636 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,636 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1648.8695068359375
2023-01-07 09:18:10,636 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.5675320625305176
2023-01-07 09:18:10,637 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 346.8033447265625
2023-01-07 09:18:10,637 > [DEBUG] 0 :: param_name :: layer3.4.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,637 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,637 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.4.conv3._dp_wrapped_module.flat_param_0 value:: -3937.832763671875
2023-01-07 09:18:10,637 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,637 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,637 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -1651.826416015625
2023-01-07 09:18:10,637 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.244853973388672
2023-01-07 09:18:10,638 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -4716.0322265625
2023-01-07 09:18:10,638 > [DEBUG] 0 :: param_name :: layer3.4.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,638 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,638 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -43130.4453125
2023-01-07 09:18:10,639 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,639 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,639 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: -2.7985496520996094
2023-01-07 09:18:10,639 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.9450631141662598
2023-01-07 09:18:10,640 > [DEBUG] 0 :: after allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1486.811767578125
2023-01-07 09:18:10,640 > [DEBUG] 0 :: param_name :: layer3.4.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,640 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,640 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv1._dp_wrapped_module.flat_param_0 value:: -43130.4453125
2023-01-07 09:18:10,640 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,640 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,640 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 11.961450576782227
2023-01-07 09:18:10,640 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5693308115005493
2023-01-07 09:18:10,641 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -51613.53125
2023-01-07 09:18:10,641 > [DEBUG] 0 :: param_name :: layer3.5.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,641 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,642 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -19228.357421875
2023-01-07 09:18:10,642 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,642 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,642 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 32.96276092529297
2023-01-07 09:18:10,642 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.2741899490356445
2023-01-07 09:18:10,643 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 348.2938232421875
2023-01-07 09:18:10,643 > [DEBUG] 0 :: param_name :: layer3.5.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,643 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,643 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv2._dp_wrapped_module.flat_param_0 value:: -19228.357421875
2023-01-07 09:18:10,643 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,643 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,643 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: 33.8296012878418
2023-01-07 09:18:10,643 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.6191307306289673
2023-01-07 09:18:10,644 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -24189.478515625
2023-01-07 09:18:10,645 > [DEBUG] 0 :: param_name :: layer3.5.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,645 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,645 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49460.80859375
2023-01-07 09:18:10,645 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,645 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,645 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -41.51546096801758
2023-01-07 09:18:10,645 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.772341728210449
2023-01-07 09:18:10,646 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 361.4194030761719
2023-01-07 09:18:10,646 > [DEBUG] 0 :: param_name :: layer3.5.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,646 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,646 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer3.5.conv3._dp_wrapped_module.flat_param_0 value:: -49460.80859375
2023-01-07 09:18:10,646 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,646 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,646 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: -30.437572479248047
2023-01-07 09:18:10,646 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4747753143310547
2023-01-07 09:18:10,648 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -59144.3515625
2023-01-07 09:18:10,648 > [DEBUG] 0 :: param_name :: layer3.5.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,648 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,648 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107777.8046875
2023-01-07 09:18:10,648 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,648 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,648 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -26.998695373535156
2023-01-07 09:18:10,648 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.4877209663391113
2023-01-07 09:18:10,649 > [DEBUG] 0 :: after allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 1497.350341796875
2023-01-07 09:18:10,649 > [DEBUG] 0 :: param_name :: layer3.5.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,649 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,649 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv1._dp_wrapped_module.flat_param_0 value:: -107777.8046875
2023-01-07 09:18:10,649 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,649 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,649 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -25.365497589111328
2023-01-07 09:18:10,650 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.31884080171585083
2023-01-07 09:18:10,651 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -128709.046875
2023-01-07 09:18:10,651 > [DEBUG] 0 :: param_name :: layer4.0.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,651 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,651 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -90209.7578125
2023-01-07 09:18:10,651 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,651 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,651 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 307.5464782714844
2023-01-07 09:18:10,651 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.19946569204330444
2023-01-07 09:18:10,652 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 702.8079833984375
2023-01-07 09:18:10,652 > [DEBUG] 0 :: param_name :: layer4.0.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,652 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,652 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv2._dp_wrapped_module.flat_param_0 value:: -90209.7578125
2023-01-07 09:18:10,652 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,653 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,653 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 307.7217712402344
2023-01-07 09:18:10,653 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.800535202026367
2023-01-07 09:18:10,654 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -107347.1875
2023-01-07 09:18:10,654 > [DEBUG] 0 :: param_name :: layer4.0.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,654 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,654 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 3129.7578125
2023-01-07 09:18:10,654 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,654 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,654 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -324.34686279296875
2023-01-07 09:18:10,654 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.33231693506240845
2023-01-07 09:18:10,655 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 516.871337890625
2023-01-07 09:18:10,655 > [DEBUG] 0 :: param_name :: layer4.0.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,655 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,656 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.conv3._dp_wrapped_module.flat_param_0 value:: 3129.7578125
2023-01-07 09:18:10,656 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,656 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,656 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -325.0792236328125
2023-01-07 09:18:10,656 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.191489815711975
2023-01-07 09:18:10,657 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 4246.1904296875
2023-01-07 09:18:10,657 > [DEBUG] 0 :: param_name :: layer4.0.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,657 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,657 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -267356.6875
2023-01-07 09:18:10,657 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,657 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,657 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2619.213623046875
2023-01-07 09:18:10,658 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4108561873435974
2023-01-07 09:18:10,658 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2928.129638671875
2023-01-07 09:18:10,659 > [DEBUG] 0 :: param_name :: layer4.0.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,659 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,659 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.0.downsample.0._dp_wrapped_module.flat_param_0 value:: -267356.6875
2023-01-07 09:18:10,659 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,659 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,659 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -2622.69970703125
2023-01-07 09:18:10,659 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5976575613021851
2023-01-07 09:18:10,660 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -320655.40625
2023-01-07 09:18:10,660 > [DEBUG] 0 :: param_name :: layer4.0.downsample.0._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,660 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,660 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -55948.46875
2023-01-07 09:18:10,660 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,660 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,660 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -104.06854248046875
2023-01-07 09:18:10,660 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012393306940793991
2023-01-07 09:18:10,661 > [DEBUG] 0 :: after allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 3025.44677734375
2023-01-07 09:18:10,662 > [DEBUG] 0 :: param_name :: layer4.0.downsample.1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,662 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,662 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv1._dp_wrapped_module.flat_param_0 value:: -55948.46875
2023-01-07 09:18:10,662 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,662 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,662 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -101.33392333984375
2023-01-07 09:18:10,662 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.13490867614746094
2023-01-07 09:18:10,663 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -66595.1953125
2023-01-07 09:18:10,663 > [DEBUG] 0 :: param_name :: layer4.1.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,663 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,663 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3973.951171875
2023-01-07 09:18:10,663 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,663 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,663 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 878.038330078125
2023-01-07 09:18:10,664 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.535383462905884
2023-01-07 09:18:10,665 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 735.61865234375
2023-01-07 09:18:10,665 > [DEBUG] 0 :: param_name :: layer4.1.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,665 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,665 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv2._dp_wrapped_module.flat_param_0 value:: 3973.951171875
2023-01-07 09:18:10,665 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,665 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,665 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 880.8782958984375
2023-01-07 09:18:10,665 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.7360849976539612
2023-01-07 09:18:10,666 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 5308.10791015625
2023-01-07 09:18:10,666 > [DEBUG] 0 :: param_name :: layer4.1.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,666 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,666 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -73408.78125
2023-01-07 09:18:10,666 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,667 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,667 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -6111.58203125
2023-01-07 09:18:10,667 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.05300892889499664
2023-01-07 09:18:10,668 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 508.54595947265625
2023-01-07 09:18:10,668 > [DEBUG] 0 :: param_name :: layer4.1.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,668 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,668 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.1.conv3._dp_wrapped_module.flat_param_0 value:: -73408.78125
2023-01-07 09:18:10,668 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,668 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,668 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -6113.03955078125
2023-01-07 09:18:10,668 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12012198567390442
2023-01-07 09:18:10,669 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -88977.328125
2023-01-07 09:18:10,669 > [DEBUG] 0 :: param_name :: layer4.1.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,669 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,669 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222704.265625
2023-01-07 09:18:10,670 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,670 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,670 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 12.93399429321289
2023-01-07 09:18:10,670 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.01857481338083744
2023-01-07 09:18:10,671 > [DEBUG] 0 :: after allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2943.57470703125
2023-01-07 09:18:10,671 > [DEBUG] 0 :: param_name :: layer4.1.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,671 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,671 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv1._dp_wrapped_module.flat_param_0 value:: -222704.265625
2023-01-07 09:18:10,671 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,671 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,671 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: 12.99448013305664
2023-01-07 09:18:10,671 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08764871954917908
2023-01-07 09:18:10,672 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -265954.875
2023-01-07 09:18:10,672 > [DEBUG] 0 :: param_name :: layer4.2.conv1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,672 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,673 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -25189.46875
2023-01-07 09:18:10,673 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,673 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,673 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2391.083251953125
2023-01-07 09:18:10,673 > [DEBUG] 0 :: before allreduce fusion buffer :: -6.8949174880981445
2023-01-07 09:18:10,674 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 753.174560546875
2023-01-07 09:18:10,674 > [DEBUG] 0 :: param_name :: layer4.2.bn1._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,674 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,674 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv2._dp_wrapped_module.flat_param_0 value:: -25189.46875
2023-01-07 09:18:10,674 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,674 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,674 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: -2394.783447265625
2023-01-07 09:18:10,674 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.01232999935746193
2023-01-07 09:18:10,675 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -29475.984375
2023-01-07 09:18:10,676 > [DEBUG] 0 :: param_name :: layer4.2.conv2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,676 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,676 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4082.592529296875
2023-01-07 09:18:10,676 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,676 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,676 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4274.19970703125
2023-01-07 09:18:10,676 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.12035174667835236
2023-01-07 09:18:10,677 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 591.4253540039062
2023-01-07 09:18:10,677 > [DEBUG] 0 :: param_name :: layer4.2.bn2._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,677 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,677 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  layer4.2.conv3._dp_wrapped_module.flat_param_0 value:: 4082.592529296875
2023-01-07 09:18:10,677 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,677 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,677 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 4274.0537109375
2023-01-07 09:18:10,677 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.588037371635437
2023-01-07 09:18:10,678 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 4749.31787109375
2023-01-07 09:18:10,679 > [DEBUG] 0 :: param_name :: layer4.2.conv3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,679 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,679 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -328655.96875
2023-01-07 09:18:10,679 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,679 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,679 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -11683.9482421875
2023-01-07 09:18:10,679 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:10,680 > [DEBUG] 0 :: after allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: 2784.572998046875
2023-01-07 09:18:10,680 > [DEBUG] 0 :: param_name :: layer4.2.bn3._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,680 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,680 > [DEBUG] 0 :: param variable tracking model? rank :: 0 param name ::  fc._dp_wrapped_module.flat_param_0 value:: -328655.96875
2023-01-07 09:18:10,680 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,680 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,680 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -11683.9482421875
2023-01-07 09:18:10,681 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0
2023-01-07 09:18:10,682 > [DEBUG] 0 :: after allreduce param grad sum fc._dp_wrapped_module.flat_param_0 :: 0.0 param sum :: -393735.8125
2023-01-07 09:18:10,682 > [DEBUG] 0 :: param_name :: fc._dp_wrapped_module.flat_param_0 communicated param num : 0
2023-01-07 09:18:10,682 > [DEBUG] 0 :: ########### task is assigned to module############
2023-01-07 09:18:10,683 > [DEBUG] 0 :: 363.493896484375
2023-01-07 09:18:10,685 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,685 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,685 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: 0.0
2023-01-07 09:18:10,685 > [DEBUG] 0 :: before allreduce fusion buffer :: -477.88922119140625
2023-01-07 09:18:10,687 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,687 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,687 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn3._dp_wrapped_module.flat_param_0  :: 129.93386840820312
2023-01-07 09:18:10,688 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,688 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,688 > [DEBUG] 0 :: before allreduce param grad sum fc._dp_wrapped_module.flat_param_0  :: -360.78857421875
2023-01-07 09:18:10,688 > [DEBUG] 0 :: before allreduce fusion buffer :: -378.23480224609375
2023-01-07 09:18:10,691 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,691 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,691 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: 18.636154174804688
2023-01-07 09:18:10,692 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.006144436076283455
2023-01-07 09:18:10,693 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,694 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn2._dp_wrapped_module.flat_param_0  :: -1.2450103759765625
2023-01-07 09:18:10,694 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,694 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,694 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv3._dp_wrapped_module.flat_param_0  :: -131.7374725341797
2023-01-07 09:18:10,695 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.0031299591064453
2023-01-07 09:18:10,696 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,696 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,696 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 22.48610496520996
2023-01-07 09:18:10,696 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0001655660744290799
2023-01-07 09:18:10,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,697 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.bn1._dp_wrapped_module.flat_param_0  :: -1.6710984706878662
2023-01-07 09:18:10,697 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,697 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,697 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv2._dp_wrapped_module.flat_param_0  :: 20.805757522583008
2023-01-07 09:18:10,697 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.6673927307128906
2023-01-07 09:18:10,698 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,699 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -0.29530516266822815
2023-01-07 09:18:10,699 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0010822061449289322
2023-01-07 09:18:10,699 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,699 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,700 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn3._dp_wrapped_module.flat_param_0  :: 112.08183288574219
2023-01-07 09:18:10,700 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,700 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,700 > [DEBUG] 0 :: before allreduce param grad sum layer4.2.conv1._dp_wrapped_module.flat_param_0  :: -150.728759765625
2023-01-07 09:18:10,700 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.75210571289062
2023-01-07 09:18:10,701 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,701 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,701 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -93.29460906982422
2023-01-07 09:18:10,701 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0017617596313357353
2023-01-07 09:18:10,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,702 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn2._dp_wrapped_module.flat_param_0  :: 3.101215362548828
2023-01-07 09:18:10,702 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,702 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,702 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv3._dp_wrapped_module.flat_param_0  :: -234.64315795898438
2023-01-07 09:18:10,702 > [DEBUG] 0 :: before allreduce fusion buffer :: -5.508200645446777
2023-01-07 09:18:10,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,704 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,704 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 297.29974365234375
2023-01-07 09:18:10,704 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.1877389624714851e-05
2023-01-07 09:18:10,704 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,705 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.bn1._dp_wrapped_module.flat_param_0  :: 2.364827871322632
2023-01-07 09:18:10,705 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,705 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,705 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv2._dp_wrapped_module.flat_param_0  :: 297.55145263671875
2023-01-07 09:18:10,705 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.350908041000366
2023-01-07 09:18:10,706 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,706 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,706 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -586.2074584960938
2023-01-07 09:18:10,706 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.027241330593824387
2023-01-07 09:18:10,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,707 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.1._dp_wrapped_module.flat_param_0  :: 110.34074401855469
2023-01-07 09:18:10,707 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,707 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,707 > [DEBUG] 0 :: before allreduce param grad sum layer4.1.conv1._dp_wrapped_module.flat_param_0  :: -666.6373291015625
2023-01-07 09:18:10,708 > [DEBUG] 0 :: before allreduce fusion buffer :: 111.40652465820312
2023-01-07 09:18:10,709 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,709 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,709 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: -49.233360290527344
2023-01-07 09:18:10,709 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.003880523145198822
2023-01-07 09:18:10,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,710 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn3._dp_wrapped_module.flat_param_0  :: 96.98431396484375
2023-01-07 09:18:10,710 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,710 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,710 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.downsample.0._dp_wrapped_module.flat_param_0  :: 11.646745681762695
2023-01-07 09:18:10,710 > [DEBUG] 0 :: before allreduce fusion buffer :: 96.28828430175781
2023-01-07 09:18:10,711 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,711 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,711 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -952.01904296875
2023-01-07 09:18:10,712 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.005813881754875183
2023-01-07 09:18:10,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,712 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,712 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn2._dp_wrapped_module.flat_param_0  :: 3.0482096672058105
2023-01-07 09:18:10,712 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,713 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,713 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv3._dp_wrapped_module.flat_param_0  :: -926.6534423828125
2023-01-07 09:18:10,713 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.203946113586426
2023-01-07 09:18:10,714 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,714 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,714 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 165.41429138183594
2023-01-07 09:18:10,714 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.0018678379710763693
2023-01-07 09:18:10,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,715 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.bn1._dp_wrapped_module.flat_param_0  :: 3.025425434112549
2023-01-07 09:18:10,715 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,715 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,715 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv2._dp_wrapped_module.flat_param_0  :: 165.41429138183594
2023-01-07 09:18:10,715 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.107652187347412
2023-01-07 09:18:10,716 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,716 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,717 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -3.2169482707977295
2023-01-07 09:18:10,717 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.08540457487106323
2023-01-07 09:18:10,717 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,717 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,717 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn3._dp_wrapped_module.flat_param_0  :: 1.5651659965515137
2023-01-07 09:18:10,718 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,718 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,718 > [DEBUG] 0 :: before allreduce param grad sum layer4.0.conv1._dp_wrapped_module.flat_param_0  :: -0.2808094024658203
2023-01-07 09:18:10,718 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.388911247253418
2023-01-07 09:18:10,719 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,719 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,719 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 1.4823704957962036
2023-01-07 09:18:10,719 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.006521448493003845
2023-01-07 09:18:10,720 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,720 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,720 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn2._dp_wrapped_module.flat_param_0  :: 10.513001441955566
2023-01-07 09:18:10,721 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,721 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,721 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv3._dp_wrapped_module.flat_param_0  :: 7.460436820983887
2023-01-07 09:18:10,721 > [DEBUG] 0 :: before allreduce fusion buffer :: 10.524394989013672
2023-01-07 09:18:10,722 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,722 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,722 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -312.478759765625
2023-01-07 09:18:10,722 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09194180369377136
2023-01-07 09:18:10,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,723 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.bn1._dp_wrapped_module.flat_param_0  :: -12.540338516235352
2023-01-07 09:18:10,723 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,723 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,723 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv2._dp_wrapped_module.flat_param_0  :: -301.050537109375
2023-01-07 09:18:10,723 > [DEBUG] 0 :: before allreduce fusion buffer :: -12.73362922668457
2023-01-07 09:18:10,724 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,724 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 37.82524871826172
2023-01-07 09:18:10,725 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.37765008211135864
2023-01-07 09:18:10,725 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,725 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,725 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn3._dp_wrapped_module.flat_param_0  :: -3.6884615421295166
2023-01-07 09:18:10,726 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,726 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,726 > [DEBUG] 0 :: before allreduce param grad sum layer3.5.conv1._dp_wrapped_module.flat_param_0  :: 36.006874084472656
2023-01-07 09:18:10,726 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.680229663848877
2023-01-07 09:18:10,727 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,727 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,727 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -222.19720458984375
2023-01-07 09:18:10,727 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0512937530875206
2023-01-07 09:18:10,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,728 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn2._dp_wrapped_module.flat_param_0  :: -0.28648054599761963
2023-01-07 09:18:10,728 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,728 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,728 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv3._dp_wrapped_module.flat_param_0  :: -225.77896118164062
2023-01-07 09:18:10,728 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4043920040130615
2023-01-07 09:18:10,729 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,729 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -5.956294059753418
2023-01-07 09:18:10,730 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.005196116864681244
2023-01-07 09:18:10,730 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,730 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,730 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.bn1._dp_wrapped_module.flat_param_0  :: 1.8657076358795166
2023-01-07 09:18:10,731 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,731 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,731 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv2._dp_wrapped_module.flat_param_0  :: -4.241745948791504
2023-01-07 09:18:10,731 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.8535566329956055
2023-01-07 09:18:10,732 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,732 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,732 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -27.400583267211914
2023-01-07 09:18:10,732 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.012117680162191391
2023-01-07 09:18:10,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn3._dp_wrapped_module.flat_param_0  :: 2.2183971405029297
2023-01-07 09:18:10,733 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,733 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,733 > [DEBUG] 0 :: before allreduce param grad sum layer3.4.conv1._dp_wrapped_module.flat_param_0  :: -27.0411319732666
2023-01-07 09:18:10,733 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.5598905086517334
2023-01-07 09:18:10,734 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,734 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,734 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 740.4183349609375
2023-01-07 09:18:10,735 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.006340354681015015
2023-01-07 09:18:10,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,735 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn2._dp_wrapped_module.flat_param_0  :: -2.2234740257263184
2023-01-07 09:18:10,735 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,735 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,736 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv3._dp_wrapped_module.flat_param_0  :: 744.4261474609375
2023-01-07 09:18:10,736 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.353687286376953
2023-01-07 09:18:10,737 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,737 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,737 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -447.16876220703125
2023-01-07 09:18:10,737 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0479339063167572
2023-01-07 09:18:10,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.bn1._dp_wrapped_module.flat_param_0  :: 0.3613405227661133
2023-01-07 09:18:10,738 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,738 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,738 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv2._dp_wrapped_module.flat_param_0  :: -449.2774658203125
2023-01-07 09:18:10,738 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33321189880371094
2023-01-07 09:18:10,739 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,739 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,739 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 570.6974487304688
2023-01-07 09:18:10,740 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.00901786983013153
2023-01-07 09:18:10,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,740 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn3._dp_wrapped_module.flat_param_0  :: -3.3712973594665527
2023-01-07 09:18:10,740 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,740 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,741 > [DEBUG] 0 :: before allreduce param grad sum layer3.3.conv1._dp_wrapped_module.flat_param_0  :: 569.0762939453125
2023-01-07 09:18:10,741 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.2860779762268066
2023-01-07 09:18:10,742 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,742 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,742 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 219.08233642578125
2023-01-07 09:18:10,742 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08976142108440399
2023-01-07 09:18:10,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn2._dp_wrapped_module.flat_param_0  :: -0.25738584995269775
2023-01-07 09:18:10,743 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,743 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,743 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv3._dp_wrapped_module.flat_param_0  :: 222.07432556152344
2023-01-07 09:18:10,743 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16212809085845947
2023-01-07 09:18:10,744 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,744 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,744 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -18.02215576171875
2023-01-07 09:18:10,744 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04940197616815567
2023-01-07 09:18:10,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.bn1._dp_wrapped_module.flat_param_0  :: -1.8990284204483032
2023-01-07 09:18:10,745 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,745 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,745 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv2._dp_wrapped_module.flat_param_0  :: -15.332359313964844
2023-01-07 09:18:10,746 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.9143624305725098
2023-01-07 09:18:10,747 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,747 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,747 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 690.9664306640625
2023-01-07 09:18:10,747 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.02114567905664444
2023-01-07 09:18:10,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn3._dp_wrapped_module.flat_param_0  :: -0.5915414094924927
2023-01-07 09:18:10,748 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,748 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,748 > [DEBUG] 0 :: before allreduce param grad sum layer3.2.conv1._dp_wrapped_module.flat_param_0  :: 691.8797607421875
2023-01-07 09:18:10,748 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.3654593229293823
2023-01-07 09:18:10,749 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,749 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,749 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2862.6240234375
2023-01-07 09:18:10,749 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.04004981368780136
2023-01-07 09:18:10,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.bn2._dp_wrapped_module.flat_param_0  :: 4.304802894592285
2023-01-07 09:18:10,750 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,750 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,750 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv3._dp_wrapped_module.flat_param_0  :: -2864.64599609375
2023-01-07 09:18:10,751 > [DEBUG] 0 :: before allreduce fusion buffer :: 4.295889377593994
2023-01-07 09:18:10,752 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,752 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,752 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 8.949901580810547
2023-01-07 09:18:10,752 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.16098946332931519
2023-01-07 09:18:10,753 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,753 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,753 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv2._dp_wrapped_module.flat_param_0  :: 11.50599193572998
2023-01-07 09:18:10,753 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4530094265937805
2023-01-07 09:18:10,754 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,754 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,754 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 1.5412724018096924
2023-01-07 09:18:10,754 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.5581390857696533
2023-01-07 09:18:10,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.1._dp_wrapped_module.flat_param_0  :: -7.487311363220215
2023-01-07 09:18:10,755 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,755 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,755 > [DEBUG] 0 :: before allreduce param grad sum layer3.1.conv1._dp_wrapped_module.flat_param_0  :: 3.9610042572021484
2023-01-07 09:18:10,755 > [DEBUG] 0 :: before allreduce fusion buffer :: -8.138195991516113
2023-01-07 09:18:10,756 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,756 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,757 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -55.8682861328125
2023-01-07 09:18:10,757 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.0261093620210886
2023-01-07 09:18:10,757 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,757 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,757 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn3._dp_wrapped_module.flat_param_0  :: -7.760425090789795
2023-01-07 09:18:10,758 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,758 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,758 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.downsample.0._dp_wrapped_module.flat_param_0  :: -48.216522216796875
2023-01-07 09:18:10,758 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.765427589416504
2023-01-07 09:18:10,759 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,759 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,759 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 10.297531127929688
2023-01-07 09:18:10,759 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.09174670279026031
2023-01-07 09:18:10,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.bn2._dp_wrapped_module.flat_param_0  :: 0.1601581573486328
2023-01-07 09:18:10,760 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,760 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,760 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv3._dp_wrapped_module.flat_param_0  :: 11.17987060546875
2023-01-07 09:18:10,760 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.915177345275879
2023-01-07 09:18:10,761 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,761 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,762 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 357.28759765625
2023-01-07 09:18:10,762 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.011966511607170105
2023-01-07 09:18:10,762 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,762 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,762 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv2._dp_wrapped_module.flat_param_0  :: 358.76715087890625
2023-01-07 09:18:10,763 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.006086401641368866
2023-01-07 09:18:10,763 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,763 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -919.8414306640625
2023-01-07 09:18:10,764 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.4130799472332001
2023-01-07 09:18:10,764 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,764 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,764 > [DEBUG] 0 :: before allreduce param grad sum layer3.0.conv1._dp_wrapped_module.flat_param_0  :: -916.4296875
2023-01-07 09:18:10,765 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.5091749429702759
2023-01-07 09:18:10,765 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,765 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,765 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 29.470211029052734
2023-01-07 09:18:10,766 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.720507025718689
2023-01-07 09:18:10,766 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,766 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,766 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv3._dp_wrapped_module.flat_param_0  :: 22.311473846435547
2023-01-07 09:18:10,766 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.4636276066303253
2023-01-07 09:18:10,767 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,767 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,767 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -21.621938705444336
2023-01-07 09:18:10,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.2450900375843048
2023-01-07 09:18:10,768 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,768 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,768 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv2._dp_wrapped_module.flat_param_0  :: -20.824851989746094
2023-01-07 09:18:10,768 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.029576191678643227
2023-01-07 09:18:10,769 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,769 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,769 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 29.347627639770508
2023-01-07 09:18:10,770 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.08452361822128296
2023-01-07 09:18:10,770 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,770 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,770 > [DEBUG] 0 :: before allreduce param grad sum layer2.3.conv1._dp_wrapped_module.flat_param_0  :: 31.872493743896484
2023-01-07 09:18:10,770 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.14570429921150208
2023-01-07 09:18:10,771 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,771 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,771 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 57.48412322998047
2023-01-07 09:18:10,771 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.183230400085449
2023-01-07 09:18:10,772 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,772 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,772 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv3._dp_wrapped_module.flat_param_0  :: 58.988243103027344
2023-01-07 09:18:10,772 > [DEBUG] 0 :: before allreduce fusion buffer :: 8.990612030029297
2023-01-07 09:18:10,773 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,773 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,773 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -13.02734375
2023-01-07 09:18:10,773 > [DEBUG] 0 :: before allreduce fusion buffer :: -0.16678157448768616
2023-01-07 09:18:10,774 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,774 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,774 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv2._dp_wrapped_module.flat_param_0  :: -11.553110122680664
2023-01-07 09:18:10,774 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.0149785280227661
2023-01-07 09:18:10,775 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,775 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,775 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -151.47325134277344
2023-01-07 09:18:10,775 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.364729404449463
2023-01-07 09:18:10,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,776 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.bn3._dp_wrapped_module.flat_param_0  :: 1.3267126083374023
2023-01-07 09:18:10,776 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,776 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,776 > [DEBUG] 0 :: before allreduce param grad sum layer2.2.conv1._dp_wrapped_module.flat_param_0  :: -173.6485595703125
2023-01-07 09:18:10,776 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.7941054105758667
2023-01-07 09:18:10,778 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,778 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,778 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 96.40876770019531
2023-01-07 09:18:10,778 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.5138003826141357
2023-01-07 09:18:10,779 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,779 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,779 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv3._dp_wrapped_module.flat_param_0  :: 81.29924011230469
2023-01-07 09:18:10,779 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4490013122558594
2023-01-07 09:18:10,780 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,780 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,780 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 78.25690460205078
2023-01-07 09:18:10,780 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.58558988571167
2023-01-07 09:18:10,781 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,781 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,781 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv2._dp_wrapped_module.flat_param_0  :: 64.5767593383789
2023-01-07 09:18:10,781 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.4899163246154785
2023-01-07 09:18:10,782 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,782 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,782 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 118.88510131835938
2023-01-07 09:18:10,782 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.767820119857788
2023-01-07 09:18:10,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.1._dp_wrapped_module.flat_param_0  :: -5.376598358154297
2023-01-07 09:18:10,783 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,783 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,783 > [DEBUG] 0 :: before allreduce param grad sum layer2.1.conv1._dp_wrapped_module.flat_param_0  :: 89.99757385253906
2023-01-07 09:18:10,783 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.500218391418457
2023-01-07 09:18:10,784 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,784 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,784 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -173.99574279785156
2023-01-07 09:18:10,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 6.88782262802124
2023-01-07 09:18:10,785 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,785 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,785 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.downsample.0._dp_wrapped_module.flat_param_0  :: -169.36849975585938
2023-01-07 09:18:10,785 > [DEBUG] 0 :: before allreduce fusion buffer :: 39.61445999145508
2023-01-07 09:18:10,786 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,786 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,786 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -185.94573974609375
2023-01-07 09:18:10,786 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.9248201847076416
2023-01-07 09:18:10,787 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,787 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,787 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv3._dp_wrapped_module.flat_param_0  :: -162.50205993652344
2023-01-07 09:18:10,787 > [DEBUG] 0 :: before allreduce fusion buffer :: -2.6789674758911133
2023-01-07 09:18:10,788 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,788 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,788 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -672.4746704101562
2023-01-07 09:18:10,788 > [DEBUG] 0 :: before allreduce fusion buffer :: 2.2515063285827637
2023-01-07 09:18:10,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,789 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.bn1._dp_wrapped_module.flat_param_0  :: -2.1270556449890137
2023-01-07 09:18:10,789 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,789 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,789 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -654.1179809570312
2023-01-07 09:18:10,790 > [DEBUG] 0 :: before allreduce fusion buffer :: -4.5536603927612305
2023-01-07 09:18:10,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,791 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,791 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -642.99560546875
2023-01-07 09:18:10,791 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.225154161453247
2023-01-07 09:18:10,791 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,792 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn3._dp_wrapped_module.flat_param_0  :: -0.10844099521636963
2023-01-07 09:18:10,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,792 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -158.1392059326172
2023-01-07 09:18:10,792 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,792 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,792 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -628.4920043945312
2023-01-07 09:18:10,792 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.692280769348145
2023-01-07 09:18:10,793 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,793 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,793 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -625.441650390625
2023-01-07 09:18:10,794 > [DEBUG] 0 :: before allreduce fusion buffer :: 0.33614516258239746
2023-01-07 09:18:10,794 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,794 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,794 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv1._dp_wrapped_module.flat_param_0  :: -176.43853759765625
2023-01-07 09:18:10,794 > [DEBUG] 0 :: before allreduce fusion buffer :: -21.472675323486328
2023-01-07 09:18:10,795 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,795 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,795 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 36.45463562011719
2023-01-07 09:18:10,796 > [DEBUG] 0 :: before allreduce fusion buffer :: -3.017542839050293
2023-01-07 09:18:10,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,796 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.bn1._dp_wrapped_module.flat_param_0  :: 1.796710729598999
2023-01-07 09:18:10,796 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,796 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,797 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 56.344356536865234
2023-01-07 09:18:10,797 > [DEBUG] 0 :: before allreduce fusion buffer :: 16.113567352294922
2023-01-07 09:18:10,798 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,798 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,798 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 56.344356536865234
2023-01-07 09:18:10,798 > [DEBUG] 0 :: before allreduce fusion buffer :: -7.92401123046875
2023-01-07 09:18:10,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,799 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.bn3._dp_wrapped_module.flat_param_0  :: -7.551281929016113
2023-01-07 09:18:10,799 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,799 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,799 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv1._dp_wrapped_module.flat_param_0  :: 26.829547882080078
2023-01-07 09:18:10,799 > [DEBUG] 0 :: before allreduce fusion buffer :: 21.714242935180664
2023-01-07 09:18:10,800 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,800 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,800 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 99.88912200927734
2023-01-07 09:18:10,800 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.6499731540679932
2023-01-07 09:18:10,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,801 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv3._dp_wrapped_module.flat_param_0  :: -3.207634449005127
2023-01-07 09:18:10,801 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,801 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,801 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -610.9990844726562
2023-01-07 09:18:10,802 > [DEBUG] 0 :: before allreduce fusion buffer :: -23.67591667175293
2023-01-07 09:18:10,802 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,802 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,803 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 108.98719787597656
2023-01-07 09:18:10,803 > [DEBUG] 0 :: before allreduce fusion buffer :: 5.872109889984131
2023-01-07 09:18:10,803 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,803 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,803 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv2._dp_wrapped_module.flat_param_0  :: 26.33535385131836
2023-01-07 09:18:10,804 > [DEBUG] 0 :: before allreduce fusion buffer :: 17.198455810546875
2023-01-07 09:18:10,804 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,804 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,805 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 96.26904296875
2023-01-07 09:18:10,805 > [DEBUG] 0 :: before allreduce fusion buffer :: 3.616710662841797
2023-01-07 09:18:10,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,806 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.1._dp_wrapped_module.flat_param_0  :: -3.582733392715454
2023-01-07 09:18:10,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,806 > [DEBUG] 0 :: before allreduce param grad sum layer1.1.conv1._dp_wrapped_module.flat_param_0  :: -44.66039276123047
2023-01-07 09:18:10,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,806 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 93.21478271484375
2023-01-07 09:18:10,806 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,806 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,806 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -655.7730712890625
2023-01-07 09:18:10,807 > [DEBUG] 0 :: before allreduce fusion buffer :: -54.7573356628418
2023-01-07 09:18:10,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,808 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,808 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 69.8895263671875
2023-01-07 09:18:10,808 > [DEBUG] 0 :: before allreduce fusion buffer :: 1.4676491022109985
2023-01-07 09:18:10,808 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,809 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,809 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -81.14778137207031
2023-01-07 09:18:10,809 > [DEBUG] 0 :: before allreduce fusion buffer :: -50.66549301147461
2023-01-07 09:18:10,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,810 > [DEBUG] 0 :: before allreduce param grad sum layer1.2.conv2._dp_wrapped_module.flat_param_0  :: 95.58210754394531
2023-01-07 09:18:10,810 > [DEBUG] 0 :: before allreduce fusion buffer :: -1.7151174545288086
2023-01-07 09:18:10,810 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,810 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,811 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.downsample.0._dp_wrapped_module.flat_param_0  :: -125.7359848022461
2023-01-07 09:18:10,811 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,811 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,811 > [DEBUG] 0 :: before allreduce param grad sum layer2.0.conv2._dp_wrapped_module.flat_param_0  :: -640.2491455078125
2023-01-07 09:18:10,811 > [DEBUG] 0 :: before allreduce fusion buffer :: -31.417055130004883
2023-01-07 09:18:10,812 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,812 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,812 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -239.846923828125
2023-01-07 09:18:10,812 > [DEBUG] 0 :: before allreduce fusion buffer :: -41.44209671020508
2023-01-07 09:18:10,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,813 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.bn1._dp_wrapped_module.flat_param_0  :: 1.3601970672607422
2023-01-07 09:18:10,813 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,813 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,813 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -266.319580078125
2023-01-07 09:18:10,813 > [DEBUG] 0 :: before allreduce fusion buffer :: -81.17777252197266
2023-01-07 09:18:10,814 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,814 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,814 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -266.319580078125
2023-01-07 09:18:10,815 > [DEBUG] 0 :: before allreduce fusion buffer :: -15.745593070983887
2023-01-07 09:18:10,815 > [DEBUG] 0 :: do all reduce async
2023-01-07 09:18:10,815 > [DEBUG] 0 :: same parameter is in the dictionary
2023-01-07 09:18:10,815 > [DEBUG] 0 :: before allreduce param grad sum layer1.0.conv2._dp_wrapped_module.flat_param_0  :: -413.87396240234375
2023-01-07 09:18:10,816 > [DEBUG] 0 :: before allreduce fusion buffer :: -11.049022674560547
